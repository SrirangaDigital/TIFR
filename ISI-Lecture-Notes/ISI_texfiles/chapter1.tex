\chapter{}\label{chap1}

\section{Preliminaries}\label{c1:s1}

Throughout these notes $A$ will denote a commutative ring with
identity. Let

\begin{tabular}{lll}
$A^{\ast}$ & =  &the group of all invertible elements of $A$.\\
$M_n(A)$ & = & the ring of all $n\times n$ matrices with entries in $A$.\\
$GL_n(A)$ &=& the group of all $n\times n$ invertible matrices with\\
& & entries in $A$. \\
$SL_n(A)$ & = &the subgroup of $GL_n(A)$ consisting of all\\ 
& & matrices of determinant $1$.\\
$E_n(A)$& = & the subgroup of $GL_n(A)$ generated by all matrices\\
& & of the form
\end{tabular}
$$
e_{ij}(\lambda)=I+\lambda E_{ij}, \lambda \varepsilon A,\quad i\neq j
$$
where $I$ is the $n\times n$ identity matrix (also denoted by $I_n$)
and $E_{ij}$ is the matrix whose $(i,j)$th  entry is $1$ and all other
entries are zeros. The matrices $e_{ij}(\lambda)$ will be referred to
as \textit{elementary matrices.}

Clearly
$$
E_n(A)\subset SL_n(A)\subset GL_n(A)
$$

We shall regard $GL_n(A)$ as a subgroup of $GL_{n+m}(A)$ by the map
$$
\alpha\mapsto \begin{bmatrix}
\alpha & 0\\
0 & I_m
\end{bmatrix}, \alpha \varepsilon GL_n(A)
$$

Let
\begin{align*}
GL(A)&=\bigcup\limits_{1}^{\infty} GL_n(A)\\
SL(A)&=\bigcup\limits_1^{\infty}SL_n(A)\\
E(A)&=\bigcup\limits_1^{\infty} E_n(A)
\end{align*}


\section{The Whitehead Group}\label{c1:s2}

\begin{dfn}\label{c1:dfn2.1}
The group
$$
K_1(A)=\dfrac{GL(A)}{[GL(A),GL(A)]}
$$
is called the \textit{Whitehead Group} of the ring $A$.
\end{dfn}

\begin{lem}\label{c1:lem2.2}
(whitehead). Let $\alpha$, $\beta \varepsilon GL_n(A)$. Then 
$$
\begin{bmatrix}
\alpha\beta & 0\\
0 & I_n
\end{bmatrix} \equiv \begin{bmatrix}
\alpha & 0\\
0 & \beta
\end{bmatrix} \equiv \begin{bmatrix}
\beta\alpha & 0\\
0 & I_n 
\end{bmatrix} \pmod{E_{2n}(A)}
$$
\end{lem}

\begin{Proof}
Let $u_1$, $u_2\varepsilon A^{\ast}$ and let 
$$
\varepsilon=
\begin{bmatrix}
1 & u_2{-1}\\
0 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0\\
1 & 1
\end{bmatrix} \begin{bmatrix}
1 & u^{-1}_{2}-1\\
0 & 1
\end{bmatrix}\begin{bmatrix}
1 & 0\\
-u_2 & 1
\end{bmatrix} \varepsilon E_2(A).
$$

Then 
$$
\begin{bmatrix}
u_1 & 0\\
0 & u_2
\end{bmatrix} \epsilon = \begin{bmatrix}
u_1u_2 & 0\\
0 & 1
\end{bmatrix}
$$

Putting $u_1=u$, $u_2=u^{-1}$ we get
$$
\begin{bmatrix}
u & 0\\
0 & u^{-1}
\end{bmatrix} \varepsilon E_2(A).
$$

Therefore
$$
\begin{bmatrix}
u_2 & 0\\
0 & u_1
\end{bmatrix}^{-1}\begin{bmatrix}
u_1 & 0\\
0 & u_2
\end{bmatrix} \varepsilon E_2(A).
$$

Hence
$$
\begin{bmatrix}
u_1u_2 & 0\\
0 & 1
\end{bmatrix} \equiv \begin{bmatrix}
u_1 & 0\\
0 & u_2
\end{bmatrix} \equiv \begin{bmatrix}
u_2 & 0\\
0 & u_1
\end{bmatrix} \equiv \begin{bmatrix}
u_2u_1 &0\\
0 & 1
\end{bmatrix} \pmod {E_2(A)}.
$$

Now replacing $A$ by $M_n(A)$ and observing that $E_2(M_n(A))\subset
E_{2n}(A)$ (where $E_2(M_n(A))$ denotes the subgroup of $GL_{2n}(A)$
generated by the matrices of the form $\begin{bmatrix}
I_n & \alpha\\
0 & I_n
\end{bmatrix}$, $\begin{bmatrix}
I_n & 0\\
\beta & I_n
\end{bmatrix}$; $\alpha \beta \varepsilon M_n(A)$), the 
lemma follows.
\enprf
\end{Proof}

\begin{Prop}\label{c1:Prop2.3}
[$GL(A)$, $GL(A)$] $= E(A) =$ [$E(A)$, $E(A)$].
\end{Prop}

\begin{Proof}
We first note that for $n\geq 3$
$$
[E_n(A), E_n(A)]=E_n(A)
$$
which is an immediate consequence of the fact that 
$$
[e_{ij}(\lambda), e_{jk}(\mu)]=e_{ik}(\lambda \mu)
$$
if $1\neq j$, $j\neq k$ and $k\neq i$.

Hence it follows that
$$
[E(A), E(A)]=E(A)
$$

Now let $\alpha, \beta \varepsilon GL_n(A)$. By lemma~\ref{c1:lem2.2}
$$
\begin{bmatrix}
\alpha^{-1}\beta^{-1} & 0\\
0 & I_n
\end{bmatrix} \equiv \begin{bmatrix}
\beta^{-1}\alpha^{-1} & 0\\
0 & I_n
\end{bmatrix}\pmod{E_{2n}(A)}
$$
i.e.
$$
\begin{bmatrix}
\alpha\beta\alpha^{-1}\beta^{-1} & 0\\
0 & I_n
\end{bmatrix} \varepsilon E_{2n}(A).
$$

Therefore every commutator in $GL_n(A)$ is contained in
$E_{2n}(A)$. Thus 
$$
[GL_n(A), GL_n(A)]\subset E_{2n}(A) \subset E(A).
$$

Hence
$$
[GL(A), GL(A)]=E(A)=[E(A),E(A)].
$$
This proves the lemma.

We remark that
$$
K_1(A)=\dfrac{GL(A)}{[GL(A),GL(A)]}=\dfrac{GL(A)}{E(A)}
$$

Further we note that the determinant map $GL_n(A)\rightarrow A^{\ast}$
induces a map 
$$
\det: K_1(A)\rightarrow A^{\ast}
$$
given by \qquad $\alpha E(A)\mapsto \det \alpha$

The kernel of this map, $\dfrac{SL(A)}{E(A)}$ is denoted by $SK_1(A)$.

Moreover the sequence
$$
0\rightarrow SK_1(A)\rightarrow K_1(A)\xrightarrow{\det}
A^{\ast}\rightarrow 0
$$
splits. The splitting is given by 
\begin{align*}
f&:A^{\ast} \rightarrow K_1(A)\\
u&\mapsto \text{class of} \begin{bmatrix}
U_1 & &0\\
& \ddots &\\
0 & &1
\end{bmatrix}
\end{align*}

Therefore
$$
K_1(A)\approx SK_1(A)\oplus A^{\ast}.
$$
\enprf
\end{Proof}

\begin{Prop}\label{c1:Prop2.4}
If $A$ is a euclidean ring (e.g. $A$ is a field or $\mathbb{Z}$), then
$SL_n(A)=E_n(A)$ for every $n\geq 1$. In particular $SK_1(A)=0$.
\end{Prop}

\begin{Proof}
It is sufficient to show that every $\alpha \varepsilon SL_n(A)$ can
be reduced to the identity matrix by applying suitable elementary
transformations.

Let 
$$
\alpha=
\begin{bmatrix}
a_{11} & \cdots & a_{1n}\\
a_{21} & \cdots & a_{2n}\\
\vdots & & \vdots\\
a_{n1} & \cdots & a_{nn}
\end{bmatrix}
$$

By applying elementary transformations if necessary, we may assume
that $a_{11}$ is of least possible norm (among all the entries of
$\alpha$). Clearly then $a_{11}$ divides all the entries in the first
row and the first column. Therefore by applying suitable elementary
transformations $\alpha$ can be reduced to the form
$$
\begin{bmatrix}
\lambda_1 & 0 &\cdots & 0\\
0 & & &\\
\vdots &  &\ast &\\
0 & & &
\end{bmatrix}, \lambda_1\varepsilon A
$$

Continuing in this way it follows that $\alpha$ can be reduced to 
$$
\beta=
\begin{bmatrix}
\lambda_1 & & & & & &\\
& &\lambda_2 & & 0 &\\
& 0 & &\cdot & &\\
& & & & \cdot &\\
& & & & & & \lambda_n
\end{bmatrix}
$$

Clearly $\lambda_1\lambda_2\ldots \lambda_n=1$. By
Lemma~\ref{c1:lem2.2}, we have 
$$
\begin{bmatrix}
\lambda_1 & 0\\
0 & \lambda_2
\end{bmatrix} \epsilon = \begin{bmatrix}
\lambda_1\lambda_2 & 0\\
0 & 1
\end{bmatrix}
$$
for some $\epsilon\varepsilon E_2(A)$. By successively applying this argument
it follows that by elementary transformations, the matrix $\beta$ can
be reduced to the form 
$$
\begin{bmatrix}
\lambda_1\ldots\lambda_n & 0\\
0 & I_{n-1}
\end{bmatrix} = I_n
$$
Hence
$$
SL_n(A)=E_n(A)
$$
and therefore
$$
SK_1(A)=\dfrac{SL(A)}{E(A)} = 0
$$

This proves the proposition.
\enprf
\end{Proof}

We remark that there exists principal ideal domains for which $SK_1$
is not zero.



\section{Cohn's Example}\label{c1:s3}

The purpose of this section is to prove that for $A=k[X,Y]$, $k$ a
field, $SL_2(A)\neq E_2(A)$ (see Corollary\ref{c1:coro3.5}). In fact we
show that 
$$
\alpha=
\begin{bmatrix}
1+XY & X^{2}\\
-Y^{2} & 1-XY
\end{bmatrix} \nvar E_2(A).
$$

\begin{lem}\label{c1:lem3.1}
Let $A$ be any commutative ring. For $a\varepsilon A$, let 
$$
E(a)=
\begin{bmatrix}
a & 1\\
-1 & 0
\end{bmatrix}
$$
Then $E_2(A)$ is generated by $\{E(a): a\varepsilon A\}$.

$$
E(a)=
\begin{bmatrix}
1 & 1-a\\
0 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0\\
-1 & 1
\end{bmatrix} \begin{bmatrix}
1 & 1\\
0 & 1
\end{bmatrix}
$$

So $E(a)\varepsilon E_2(A)$. Further since
$$
e_{12}(a)=
\begin{bmatrix}
1 & a\\
0 &1
\end{bmatrix} \begin{bmatrix}
-a & 1\\
-1 & 0
\end{bmatrix} \begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix}^{-1}=E(-1)(E(0))^{-1} 
$$
and Note that $E(0)^{3}=E(0)^{-1}$
$$
e_{21}(a)=
\begin{bmatrix}
1 & 0\\
a & 1
\end{bmatrix} =\begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix}^{-1} \begin{bmatrix}
a & 1\\
-1 & 0
\end{bmatrix} = (E(0))^{-1}E(a).
$$
\end{lem}

The lemma follows.

Let $D_n(A)$ denote the group of $n\times n$ invertible diagonal
matrices with entries in $A$. We shall use the notation
$$
[\alpha_1,\alpha_2,\ldots,\alpha_n]=\begin{bmatrix}
\alpha_1 & & \\
& \alpha_2 & &0 \\
& 0 & \ddots & \\
& & & \alpha_n 
\end{bmatrix} \quad, \alpha_i\varepsilon A^{\ast}, 1\leq i \leq n
$$

It is clear that 
$$
[\alpha_1, \alpha_2,\ldots,\alpha_n]^{-1}e_{ij}(\lambda)[\alpha_1,\alpha_2,\ldots,\alpha_n]=e_{ij}\left(\alpha_{i}^{-1_\lambda}a_j\right).
$$

Therefore $D_n(A)$ normalizes $E_n(A)$. In particular $D_n(A)E_n(A)$
is a subgroup of $GL_n(A)$.

\begin{lem}\label{c1:lem3.2}
Let $\alpha\varepsilon D_2(A)E_2(A)$. Then $\alpha$ can be written as
$$
\alpha=[a,b]E(a_1)E(a_2)\ldots E(a_r)
$$
such that
$$
a_i\nvar A^{\ast}\cup \{0\}\text{ for }1<i<r.
$$
\end{lem}

\begin{Proof}
The following identities can be easily verified. For $x, y \varepsilon
A$, $a,\\b \varepsilon A^{\ast}$,
\begin{enumerate}[(i)]
\item $E(x)E(0)E(y)=-E(x+y)$
\item $E(x)[a,b]=[b,a]E\left(b^{-1}xa\right)$
\item If $a\varepsilon A^{\ast}$ then
\end{enumerate}
$$
E(x)E(a)E(y)=E\left(x-a^{-1}\right)\left[a,a^{-1}\right]E\left(y-a^{-1}\right)
$$

Let $r$ be the smallest integer such that
$$
\alpha=[a,b]E(a_1)\ldots E(a_r)
$$

If for some $i,1<i<r$, $a_i\varepsilon A^{\ast} \subset \{0\}$ then in
view of the identities (i) - (iii), $\alpha$ can be expressed in the
said form with fewer $E(a_i)$'s. Therefore in the above expression for
$\alpha$ we have 
$$
a_i \nvar A^{\ast}\subset \{0\}\text{ for }1<i<r.
$$
This completes the proof of the lemma.
\enprf
\end{Proof}

We recall that if $f\varepsilon k[X_1,\ldots, X_r]$, and
$f=f_n+f_{n-1}+\cdots+f_0$ where $f_i$ are homogeneous of degree $i$
and $f_n\neq 0$ then $f_n$ is called the \textit{leading form} of $f$.

\begin{Prop}\label{c1:Prop3.3}
Let $k$ be a field and $A=k[X,Y]$. Let 
$$
\alpha=
\begin{bmatrix}
f & g\\
\ast & \ast
\end{bmatrix} \varepsilon GL_2(A)
$$

Suppose that $\deg f=\deg g$ and the leading forms of $f$ and $g$ are
linearly independent over $k$. Then $\alpha \nvar D_2(A)E_2(A)$. 

For a proof of this we need
\end{Prop}

\begin{lem}\label{c1:lem3.4}
Let $A=K[X,Y]$ and $\beta \varepsilon D_2(A)E_2(A)$. Suppose
$$
\beta=\begin{bmatrix}
f & G\\
\ast & \ast
\end{bmatrix} = [u, v]E(a_1)\ldots E(a_r)
$$
where
$$
u,v \varepsilon A^{\ast}=K^{\ast}, a_r \nvar k\text{ and }a_i\nvar
k^{\ast} \cup \{0\}=k, 1<i<r.
$$

The $\deg f> \deg g$.
\end{lem}

\begin{Proof}
Since $u,v$ are non-zero elements of the field $k$, without loss of
generality we may assume that in the expression for $\beta$ the
diagonal matrix $[u,v]$ does not appear. Now suppose $r=1$, then 
$$
\beta=
\begin{bmatrix}
a_1 & 1\\
-1 & 0
\end{bmatrix}
$$
$a_i\nvar k$ so $\deg a_1>0$, If $r=2$, then
$$
\beta=
\begin{bmatrix}
a_1 & 1\\
-1 & 0
\end{bmatrix} \begin{bmatrix}
a_2 & 1\\
-1 & 0
\end{bmatrix} =\begin{bmatrix}
a_1a_{2}-1 & a_1\\
-a_2 & -1
\end{bmatrix}
$$

If $a_1=0$ then $(f,g)=(-1,0)$ and $0=\deg 1> \deg0=-\infty$.

If $a_1\neq 0$ then $\deg (a_1a_2-1)=\deg a_1+\deg a_2>\deg a_1$,
since $a_2\nvar k$. 

So we now assume that $r>2$ and 
$$
\beta= E(a_1)\ldots E(a_{r-1})E(a_r).
$$

Since $a_{r-1}\nvar k$, we have, by induction
$$
E(a_1)\ldots E(a_{r-1})=
\begin{bmatrix}
f_1 & g_1\\
\ast & \ast
\end{bmatrix}, \deg f_1 > \deg  g_1
$$
so
$$
\beta=
\begin{bmatrix}
f_1 & g_1\\
\ast & \ast
\end{bmatrix} \begin{bmatrix}
a_r & 1\\
-1 & 0
\end{bmatrix} = \begin{bmatrix}
f_1a_r-g_1 & f_1\\
\ast & \ast
\end{bmatrix}
$$
since $a_r \nvar k$ and $\deg f_1> \deg g_1$ we have 
\begin{align*}
\deg(f_1a_r-g_1)&=\deg(f_1a_r)\\
&=\deg f_1+\deg a_r\\
&>\deg f_1.
\end{align*}

This completes the proof of the lemma.
\enprf
\end{Proof}

\begin{prf}
Let 
$$
\alpha=
\begin{bmatrix}
f & g\\
\ast & \ast
\end{bmatrix} \varepsilon GL_2(A)
$$
with $\deg f=\deg g$ and the leading forms of $f$ and $g$ linearly
independent over $k$. Suppose $\alpha \varepsilon D_2(A) E_2(A)$. Then
by Lemma~\ref{c1:lem3.2} we can write
$$
\alpha=
\begin{bmatrix}
f & g\\
\ast & \ast
\end{bmatrix}= [u,v] E(a_1)\ldots E(a_r)
$$
where $a_i\nvar k$ for $1<i<r$. Since $u, v \varepsilon k^{\ast}$,
without loss of generality, we may assume that in the expression for
$\alpha$ the diagonal matrix $[u, v]$ does not appear. Since $\deg
f=\deg g$, by Lemma~\ref{c1:lem3.2}, $a_r\varepsilon k$. If $r=1$, then 
$$
\alpha=
\begin{bmatrix}
a_1 & 1\\
-1 & 0
\end{bmatrix}
$$
where $a_1 \varepsilon k$. Clearly this is not possible. If $r=2$,
then 
$$
\alpha=
\begin{bmatrix}
a_1 & 1\\
-1 & 0
\end{bmatrix} \begin{bmatrix}
a_2 & 1\\
-1 & 0
\end{bmatrix} = \begin{bmatrix}
a_1a_2-1 & a_1\\
\ast & \ast
\end{bmatrix}
$$
and since $a_2 \varepsilon k$, the leading forms of $a(a_1a_2-1)$ and
$a_1$ cannot be linearly independent over $k$. Now suppose that
$r>2$. Then 
$$
\alpha E(a_r)^{-1}=
\begin{bmatrix}
f & g\\
\ast & \ast
\end{bmatrix} \begin{bmatrix}
0 & -1\\
1 & a_r
\end{bmatrix} = \begin{bmatrix}
g & -f+ga_r\\
\ast & \ast
\end{bmatrix}
$$

As the leading forms of $f$ and $g$ are linearly independent, $\deg g=
\deg(-f+ga_r)$. But since $a_{r-1} \mbox{$\nvar$} k$, we have by
Lemma~\ref{c1:lem3.4}, $\deg g>\deg(-f+ga_r)$. Contradiction. This
completes the proof of the proposition.
\enprf
\end{prf}

We remark that it is clear from the proof that
Proposition~\ref{c1:Prop3.3} holds for any graded domain.

\begin{coro}\label{c1:coro3.5}
Let $A=k[X,Y]$. Then $SL_2(A)\neq E_2(A)$.
\end{coro}

\begin{Proof}
Let 
$$
\alpha=
\begin{bmatrix}
1+XY & X^{2}\\
-Y^{2} & 1-XY
\end{bmatrix}
$$

Then $\alpha \varepsilon SL_2(A)$ but $\alpha \nvar E_2(A)$ by Proposition~\ref{c1:Prop3.3}
\enprf
\end{Proof}


\begin{coro}\label{c1:coro3.6}
Let $A=K[X,Y]$. Then $E_2(A)$ is not normal in $SL_2(A)$. 
\end{coro}

\begin{Proof}
Let 
$$
\alpha=
\begin{bmatrix}
1+XY & X^{2}\\
-Y^{2} & 1-XY
\end{bmatrix}
$$
Then
\begin{align*}
&\alpha  \begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix} \alpha^{-1}\\
&= \begin{bmatrix}
1+XY & X^{2}\\
-Y^{2}& 1-XY
\end{bmatrix} \begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix} \begin{bmatrix}
1-XY & -X^{2}\\
Y^{2} & 1+XY
\end{bmatrix}\\
&= \begin{bmatrix}
-X^{2}(1-XY)+Y^{2}(1+XY) & X^{4}+(1+XY)^{2}\\
\ast & \ast
\end{bmatrix}
\end{align*}
$$
=
\begin{bmatrix}
f & g\\
\ast & \ast
\end{bmatrix}
$$

It is clear that $\deg f=\deg g$ and their leading forms are linearly
independent over $k$. Hence by Proposition~\ref{c1:Prop3.3}, 
$$
\alpha
\begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix} \alpha^{-1}\nvar D_2(A)E_2(A).
$$
\enprf
\end{Proof}


\section{\texorpdfstring{$E_n(A)$}{EnA} is Normal in \texorpdfstring{$GL_n(A)$, $n\geq 3$}{GLn}}\label{c1:s4}

The purpose of this section is to prove the following:

\begin{thm}[\citet{Suslina}]\label{c1:thm4.1}
Let $A$ be a commutative ring, then $E_n(A)$ is normal in $GL_n(A)$,
for $n\geq 3$.
\end{thm}
 
For a proof of this we need
\begin{lem}[\citeauthor{Vasersteina}]\label{c1:lem4.2}
Let $M_{r,s}(A)$ denote the set of all $r\times s$ matrices over
$A$. Let $\alpha \varepsilon M_{s,r}(A)$. Suppose $I+\alpha\beta
\varepsilon GL_r(A)$, then $I+\beta\alpha \varepsilon GL_s(A)$ and 
$$
\begin{bmatrix}
I+\alpha\beta & 0\\
0 & (I+\beta\alpha)^{-1}
\end{bmatrix} \varepsilon E_{r+s}(A)
$$
\end{lem}

\begin{Proof}
(K.Mukherjea) It is easy to check that 
\begin{align*}
(I+\beta\alpha)^{-1}&=I-\beta(I+\alpha\beta)^{-1}\alpha\\
&=I-(I+\beta\alpha)^{-1}\beta\alpha
\end{align*}

Further since
\begin{align*}
& \begin{bmatrix}
I+\alpha\beta & 0\\
0 & (I+\beta\alpha)^{-1}
\end{bmatrix}\\
&= \begin{bmatrix}
I & 0\\
(I+\beta\alpha)^{-1}\beta & I
\end{bmatrix} \begin{bmatrix}
I &-\alpha\\
0 & I
\end{bmatrix} \begin{bmatrix}
I & 0\\
-\beta & I
\end{bmatrix}\begin{bmatrix}
I & (I+\alpha\beta)^{-1}\alpha\\
0 & 1
\end{bmatrix}
\end{align*} 
and it is easy to see that any triangular matrix with $1$ in the
diagonal is a product of elementary matrices, the lemma follows.
\enprf
\end{Proof}


\begin{coro}\label{c1:coro4.3}
Let 
$$
v=
\begin{bmatrix}
v_1\\
v_2\\
\vdots\\
v_r
\end{bmatrix} \varepsilon M_{r,1}(A),w=[w_1,w_2,\ldots,w_r]\varepsilon M_{1,r}(A)
$$

Suppose $wv=\sum\limits_{i\leq i\leq r} w_iv_i=0$. Then $I+vw$ is
invertible and 
$$
\begin{bmatrix}
i+vw & 0\\
0 & 1
\end{bmatrix} \varepsilon E_{r+1}(A).
$$

We remark that if $A+k[X,Y]$, $k$ a field and $v=\begin{bmatrix}
x\\
-y
\end{bmatrix}$, $w=[Y,X]$, then $wv=0$, and 
$$
i+vw=
\begin{bmatrix}
1+XY & X^{2}\\
-Y^{2} & 1-XY
\end{bmatrix} \nvar E_2(A)
$$
by Corollary~\ref{c1:coro3.5}. But 
$$
\begin{bmatrix}
1+XY & X^{2} & 0\\
-Y^{2} & 1-XY & 0\\
0 & 0 & 1
\end{bmatrix} \varepsilon E_3(A)
$$
\end{coro}

\begin{coro}\label{c1:coro4.4}
Let 
$$
v=\begin{bmatrix}
v_1\\
\vdots\\
v_r
\end{bmatrix} \varepsilon M_{r, 1}(A),w=[w_1,\ldots,w_r]\varepsilon M_{1,r}(A)
$$

Suppose $wv=\sum\limits_{1}^{r}w_iv_i=0$ and $w_i=0$ for some
$i$. Then 
$$
I+VW\varepsilon E_r(A)
$$
\end{coro}

\begin{Proof}
Suppose $w_i=0$, $i<r$. Let 
\begin{align*}
\alpha&=I+E_{ir}-E_{ri}-E_{ii}-E_{rr}\\
&= e_{ir}(1)e_{ri}(-1)e_{ir}(1)
\end{align*}

Then $\alpha \varepsilon E_r(A)$ and we have
$$
(I+vw)=\alpha(I+v^{\ast}w^{\ast})\alpha^{-1},\, w^{\ast}v^{\ast}=0
$$
where
\begin{align*}
v^{\ast}&=\alpha^{-1}v\\
w^{\ast}&=w\alpha=[w_1,\ldots,w_{i-1},-w_r,w_{i+1},\ldots,w_{r-1},0]
\end{align*}

So we may assume that $w_r=0$. Let 
$$
v'=
\begin{bmatrix}
v_1\\
\vdots\\
v_{r-1}
\end{bmatrix}, w'=[w_1,\ldots.w_{r-1}].
$$

Then $w'v'=0$. So by corollary~\ref{c1:coro4.3},
$$
\begin{bmatrix}
I+v'w' & &0\\
& & \vdots\\
0 & \cdots & 1
\end{bmatrix} \varepsilon E_r(A)
$$

Now as
$$
I+vw=
\begin{bmatrix}
I+v'w' & & 0\\
& & \vdots\\
\ast & \cdots & 1
\end{bmatrix}
$$
can be transformed to 
$$
\begin{bmatrix}
I+v'w' && 0\\
& & \vdots\\
0 & \cdots & 1
\end{bmatrix}
$$
by adding suitable multiples of the last column to the other columns,
it follows that $I+vw \varepsilon E_r(A)$. This completes the proof.
\enprf
\end{Proof}


\begin{dfn}\label{c1:dfn4.5}
Let $A$ be a commutative ring. Then $(a_1,a_2,\ldots,a_r)\varepsilon
A^{r}$ is said to be \textit{unimodular} if there exist
$b_i\varepsilon A$, $1\leq i \leq r$ such that 
$$
\sum\limits_{i=1}^{r}b_ia_i=1
$$
\end{dfn}

\begin{lem}\label{c1:lem4.6}
Let $v=(v_1\ldots,v_r)\varepsilon A^{r}$ be unimodular. Let
$f:A^{r}\rightarrow A$ be an A-linear map given by $e_i\mapsto v_i$
($e_i$ for $1\leq i \leq r$ being the canonical basis of
$A^{r}$). Then 
$$
\ker f=\left\{(w_1,\ldots,w_r)\varepsilon A^{r}\mid
\sum\limits_{i=1}^{r} w_iv_i=0\right\}
$$
is generated by the elements
$$
\left\{v_je_i-v_ie_j\mid 1\leq i <j\leq \gamma\right\}
$$
\end{lem}

\begin{Proof}
Since $v$ is unimodular there exist $b_i\varepsilon A$, $1\leq i \leq
r$ such that $\sum b_iv_i=1$. Let $g:A\rightarrow A^{r}$ be an
A-linear map given by $g(1)=\sum b_ie_i$ and let
$\theta:A^{r}\rightarrow \ker f$ be an A-linear map given by
$\theta(x)-(\gof)(x)$. Then clearly $\theta \mid \ker f=1_{\ker
  f}$. Now since
\begin{align*}
\theta(e_i) &= e_i-gf(e_i)\\
&=e_i-g(v_i)\\
&=\sum\limits_{j}b_jv_je_i-\sum\limits_{j}v_ib_je_j\\
&=\sum\limits_{j}b_j(v_je_i-v_ie_j)
\end{align*}
and as $\ker f$ is generated by $\theta(e_i)$, $1\leq i\leq r$, our
assertion follows.
\enprf
\end{Proof}

\begin{Prop}\label{c1:Prop4.7}
Let $v=\begin{bmatrix}
v_1\\
\vdots\\
v_r
\end{bmatrix} \varepsilon M_{r,1}(A)$ with $(v_1,\ldots,v_r)
\varepsilon A^{r}$, be unimodular and let $w=[w_1,\ldots,w_r]
\varepsilon M_{1,r}(A)$ be such that $wv=0$. 

Then for $r\geq 3$
$$
I+vw\varepsilon E_r(A).
$$
\end{Prop}


\begin{Proof}
Let $e_i$, $1\leq i \leq r$ be the canonical basis of $A^{r}$ and
$f:A^{r}\rightarrow A$ be an $A$-linear map given by $e_i\mapsto
v_i$. Then since $wv=0$, $w\varepsilon \ker f$. So by
Lemma~\ref{c1:lem4.6}
$$
w=\sum\limits_{i<j}a_{ij}(v_ie_j-v_je_i)=\sum\limits_{i<j}w_{ij}
$$
where
$$
w_{ij}=a_{ij}(v_ie_j-v_je_i).
$$

Clearly $w_{ij}v=0$. Noting that $r\geq 3$, it follows from
Corollary~\ref{c1:coro4.4} that $I+vw_{ij}\varepsilon E_r(A)$. Now 
$$
I+vw=I+v\sum\limits_{i<j}w_{ij}=I+\sum\limits_{i<j}vw_{ij}= \prod\limits_{i<j}(I+vw_{ij}).
$$

Hence $I+vw \varepsilon E_r(A)$.
\enprf
\end{Proof}

\begin{rem}\label{c1:rem4.8}
By taking transposes it is clear that if $w$ is unimodular and $wv=0$
(v arbitrary) then $I+vw\varepsilon E_r(A)$. 
\end{rem}

\begin{Prf}
Since $E_n(A)$ is generated by the elementary matrices
$$
e_{ij}(\lambda)=I+\lambda E_{ij}, \quad i\neq j,
$$
it is sufficient to prove that $\alpha
e_{ij}(\lambda)\alpha^{-1}\varepsilon E_n(A)$ for $\alpha \varepsilon
GL_n(A)$. Let 
$$
\alpha=[\alpha_1,\alpha_2,\ldots,\alpha_n] \quad
\text{and}\quad\alpha^{-1}=\begin{bmatrix}
\beta_1\\
\vdots\\
\beta_n
\end{bmatrix}
$$
where $\alpha_i$, $1\leq i\leq n$ are the columns of the matrix
$\alpha$ and $\beta_j$, $1\leq j\leq n$ are the rows of the matrix
$\alpha^{-1}$. Then 
$$
\alpha(I+\lambda E_{ij})\alpha^{-1}=I+\lambda \alpha_i\beta_j
$$

Now since $\alpha \varepsilon GL_n(A)$, $\alpha_i$'s and $\beta_j$'s
are unimodular and since $\alpha^{-1}\alpha=1$, we have
$\beta_j\alpha_i=0$ for $i\neq j$. So by Proposition~\ref{c1:Prop4.7}, it
follows that 
$$
I+\lambda\alpha_i\beta_j\varepsilon E_n(A).
$$

This completes the proof.
\enprf
\end{Prf}

\section{Localization Theorem}\label{c1:s5}

In this section we prove a theorem~\ref{c1:thm5.3} of \citeauthor{Suslina} for linear
groups which is inspired by Quillen's localization theorem for
projective modules \cite{Quillen}. We start with some preliminaries
about localization.

Let $A$ be a commutative ring with identity. A subset $S$ of $A$ is
said to be \textit{multiplicative} if $s_1,s_2 \varepsilon
S\Rightarrow s_1s_2\varepsilon S$. 

Consider the set $A\times S$ and define a relation $\sim$ on it as
follows: $(a,s)\sim (a',s')$ if and only if there exists
$s''\varepsilon S$ such that $s''(s'a-sa')=0$.

It is easy to check that this is an equivalence relation. We denote
the equivalence class of $(a,s)$ by $\dfrac{a}{s}$. The quotient set
$A\times \dfrac{s}{\sim}$ will be denoted by $S^{-1}A$ or $A_S$.

We can define a ring structure on $S^{-1}A$ by defining addition and
multiplication by 
\begin{align*}
\frac{a}{s}+\frac{a'}{s'}&=\frac{(s'a+sa')}{ss'}\\
\frac{a}{s}\cdot \frac{a'}{s'}&=\frac{aa'}{ss'}.
\end{align*}

It can be easily seen that these operations are well defined and
$S^{-1}A$ becomes a commutative ring with identity, the identity
element being the equivalence class $\dfrac{s}{s}$ for any
$s\varepsilon S$.

There is a canonical homomorphism denoted by $i_S:A\rightarrow A_S$
given by $a\mapsto \dfrac{as}{s}$, and $\ker i_S=\left\{a\varepsilon A \mid ta=0
\text{ for some }t\varepsilon S\right\}$.

\begin{Rem}\label{c1:Rem5.1}
\begin{enumerate}[(i)]
\item If $0\varepsilon S$ then $S^{-1}A=0$.

\item If $S$ consists of nonzero divisors then the map $i_s$ is
  injective.

\item If $A$ is an integral domain and $S=A-\{0\}$, then $S^{-1}$ is
  the field of fractions of $A$. 

\item If $a\varepsilon A$ and $S=\left\{1,a,a^{2},\ldots\right\}$ ,
  then we denote the ring $S^{-1}A$ by $A_a$.
\item If $\underline{p}$ is a prime ideal in $A$ (i.e. $\underline{p}$ is an ideal in $A,\underline{p}\neq
  A$ and $ab\varepsilon \underline{p} \Rightarrow a \varepsilon \underline{p}$ or
  $b\varepsilon \underline{p}$) then $S=A-\underline{p}$ is a multiplicative subset of $A$. In
  this case we shall write $A\underline{p}$ for $S^{-1}A$. It can be easily
  checked that $A_{\underline{p}}$ is a local ring $\left(\text{ i.e. it has a unique maximal
  ideal namely }\underline{p}^{A}_{\underline{p}}\right)$.
\end{enumerate}
\end{Rem}


\begin{lem}\label{c1:lem5.2}
Let $a\varepsilon A$, and $i_{\underline{m}}(a)=0$ for every maximal
ideal $\underline{m}$ of $A$.  Then $a=0$.
\end{lem}

\begin{Proof}
Since $i_{\underline{m}}(a)=0$, there exists
$s_{\underline{m}}\varepsilon A-\underline{m}$ such that
$s_{\underline{m}}a=0$ for every maximal ideal $\underline{m}$. Let
$I$ be the ideal generated by \\$\left\{s_{\underline{m}}\mid
\underline{m}\text{ maximal if ideal of } A\right\}$. Then clearly
$I=A$ and hence $a=0$.

Let $I$ be an ideal of $A$. We shall denote by $GL_n(A,I)$ the kernel
of the mapping 
$$
GL_n(A)\rightarrow GL_n\left(\dfrac{A}{I}\right)
$$
and by $E_n(A,I)$, the normal subgroup of $E_n(A)$ generated by the
matrices of the type $1+\lambda E_{ij}$, $\lambda \varepsilon I$,
$i\neq j$. Clearly
$$
E_n(A,I)\subset GL_n(A,I)\cap E_n(A)
$$

Moreover for any prime ideal ${\underline{p}}$ of $A$ and $\alpha
\varepsilon M_n(A)$, we shall denote by $\alpha_{\underline{p}}$ the
image of $\alpha$ under the canonical map $M_n(A)\rightarrow
M_n(A_{\underline{p}})$ and for $\alpha \varepsilon M_n(A[X]),
f\varepsilon A[X]$, we
shall denote by $\alpha(f)$ the image of $\alpha$ under the canonical
map $M_n(A[X])\mapsto M_n(A{X})$ induced by $X\rightarrow f$. 
\enprf
\end{Proof}

\begin{thm}[\citeauthor{Suslina}]\label{c1:thm5.3}
Let $\alpha \varepsilon GL_n(\gamma[X])$, suppose that
$a_{\underline{m}}\varepsilon\\
GL_n(A_{\underline{m}})E_n(A_{\underline{m}}[X])$ for every maximal
ideal $\underline{m}$ of $A$. Then $\alpha \varepsilon\\
GL_n(A)E_n(A[X])$.

We first make the following preliminary remark. Replacing $\alpha$ by
$\alpha(0)^{-1}\alpha$, we may assume that $\alpha(0)=1$. Further for
any ring $A,\\ \beta \varepsilon GL_n(A)E_n(A[X])$ and $\beta(0)=1$
implies that $\beta \varepsilon E_n(A[X])$. For if $\beta=\gamma
\epsilon, \gamma \varepsilon GL_n(A)$ and $\epsilon \varepsilon
E_n[A[X])$, then $\gamma=\beta(0)\epsilon (0)^{-1}=\epsilon
(0)^{-1}\varepsilon E_n(A)$.  In view of this Theorem~\ref{c1:thm5.3}
is equivalent to 
\end{thm}

\begin{Thm}
Let $\alpha \varepsilon GL_n(A[X])$ with $\alpha(0)=1$, Suppose
$\alpha_{\underline{m}}\varepsilon \\E_n(A_{\underline{m}}[X])$, for
every maximal ideal $\underline{m}$ of $A$. Then $\alpha\varepsilon E_n(A[X])$.
\end{Thm}

For a proof of this, consider the set 
$$
I=\left\{a \varepsilon A\alpha_a\varepsilon E_n(A_a[X]0\right\}
$$
($\alpha_a$ denotes the image of $\alpha$ in $E_n(A_a[X])$). If we
could show that 
\begin{enumerate}[(i)]
\item $I$ is an ideal of $A$.

\item For every maximal ideal $\underline{m}$ of $A$ there exists
  $a\varepsilon A-\underline{m}$ such that $a\varepsilon I$.
\end{enumerate}

Then this would imply that $I=A$ and hence would complete a proof of
Theorem~\ref{c1:thm5.3}'.

By hypothesis, for every maximal ideal $\underline{m}$,
$\alpha_{\underline{m}} \varepsilon E_n(A_{\underline{m}}[X])$. It is
easy to see that there exists $a\varepsilon A - \underline{m}$ such
that $\alpha_a\varepsilon E_n (A_a[X])$. Therefore (ii) follows.

We devote the rest of this section to prove that $I$ is an ideal of $A$.

\begin{lem}\label{c1:lem5.4}
$GL_n(A[X],(X)) \cap E_n(A[X]\cup)= E_n(A[X],(X))$= the subgroup of
  $E_n(A[X])$ generated by the elements of the type $\gamma
  e_{ij}(Xf)\gamma^{-1}$, where $\gamma \varepsilon E_n(A)$,
  $f\varepsilon A[X]$.
\end{lem}


\begin{Proof}
Let $\alpha \varepsilon GL_n(A[X],(X))\cup E_n(A[X])$. Therefore
$$
\alpha=\prod\limits_{k=1}^{r}e_{i_{k}j_{k}}(f_k)
$$
where $f_k\varepsilon A[X]$. So
$$
\alpha(0)=\prod\limits_{k=1}^{r}e_{i_{k}j_{k}}(f_k(0))=1
$$

Write
$$
\gamma_l=\prod\limits_{k=1}^{l}e_{i_{k}j_{k}}(f_k(0))\text{ for }
1\leq l \leq r
$$

Therefore
$$
\gamma_i=e_{i_{1}j_{1}}(f_1(0)) \text{ and } \gamma_r=\alpha(0)=1.
$$

Let
$$
f_k-f_k(0)=Xg_k, g_k \varepsilon A[X].
$$

Then
$$
e_{i_{k}j_{k}}(f_k)=e_{i_{k}j_{k}}(f_k(0))e_{i_{k}j_{k}}(Xg_k)
$$

Substituting this in $\alpha$ and using that $\gamma_r=1$, it is easy
to check that 
$$
\alpha=\prod\limits_{k=1}^{r}\gamma_ke_{i_{k}j_{k}}(Xg_k)\gamma_{k}^{-1}
$$
\enprf
\end{Proof}

\begin{lem}\label{c1:lem5.5}
Let $a\varepsilon A$ and $\alpha \varepsilon
E_n(A_a[X],(x))=GL_n(A_a[X],(X))\cap \\E_n(A_a[X])$. Then there exists
$\beta \varepsilon E_n(A[X],(X))$ such that 
$$
\beta_a=\alpha\left(a^{N}X\right),
$$
for some large integer $N$.
\end{lem}

\begin{Proof}
By Lemma~\ref{c1:lem5.4}, we can write
$$
\alpha=\prod\limits_{k}\gamma_ke_{i_{k}j_{k}}\left(\frac{Xf_k}{a^{r}}\right)\gamma_k^{-1}
$$
where $\gamma_k\varepsilon E_n(A_a)$, $f_k\varepsilon A[X]$ and $r$ a
suitable integer. Now writing $i_k$-th column of $\gamma_k$ as $v'$ and
$j_k$-th row of $\gamma_k^{-1}$ as $w'$ it follows that $w'v'=0$ and
$\alpha$ is a product of elements of the type
$\left(\dfrac{I+v'w'Xf_k}{a^{r}}\right)$. Let
$v'=\left(\dfrac{1}{a^{r}}\right)v$ where $v=\begin{bmatrix}
v_1\\
\vdots\\
v_n
\end{bmatrix}, v_i\varepsilon A$. Then by Lemma~\ref{c1:lem4.6}, we have 
$$
w'=\sum\limits_{i<j}\left(\frac{a_{ij}}{a^{r}}\right)\left(\left(\frac{v_i}{a^{r}}\right)e_j-\left(\frac{v_j}{a^{r}}\right)e_i\right)
$$

(Take a bigger $r$ if necessary), where $a_{ij}\varepsilon A$. Now it
follows that $\alpha$ is a product of the type
$$
\prod\limits_{K}\left(\prod\limits_{i<j}\left(I+v\left(\frac{a_{ij}}{a^{2r}}\right)\left(\left(\frac{v_i}{a^{r}}\right)e_j-\left(\frac{v_j}{a^{r}}\right)e_i\right)\left(\frac{Xf_k}{a^{r}}\right)\right)\right)
$$
(Note $a_{ij}$ and $v$ also vary with $k$). Taking $N=4r$ and 
$$
\beta=\prod\limits_{K}\left(\prod_{i<j}\left(I+va_{ij}\left(v_ie_j-v_je_i\right)Xf_k\left(a^{N}X\right)\right)\right)
$$
we have $\beta_a=\alpha\left(a^{N}X \right)$. This completes the proof.
\enprf
\end{Proof}

We note that in the above lemma it is sufficient to take\\ $N\geq 4r$
and change $\beta$ appropriately.

\begin{coro}\label{c1:coro5.6}
Let $\alpha \varepsilon GL_n(A[X],(X))$ be such that for some\\
$a\varepsilon A$, $\alpha_a \varepsilon E_n (A_a[X],(X))$. Then 
$$
\alpha\left(a^{N}X\right)\varepsilon E_n(A[X],(X)) \text{ for large } N.
$$
\end{coro}

\begin{Proof}
By Lemma~\ref{c1:lem5.5} there exists $\beta \varepsilon E_n(A[X],(X))$
such that 
$$
\beta_a=\alpha_a\left(a^{m}X\right) \text{ for large integer } m.
$$

Replacing $\alpha(X)$ by $\alpha\left(a^{m}X\right)$, we may assume
that 
$$
\beta_a=(\alpha(X))_a.
$$

We write
$$
\alpha=I+\sum\limits_{i\geq 1}\alpha_iX^{i},\quad \alpha_i\varepsilon M_n(A)
$$
and 
$$
\beta=I+\sum\limits_{i\geq 1} \beta_iX^{i},\quad \beta_i\varepsilon M_n(A)
$$

Clearly $(\alpha_i)_a=(\beta_i)_a$, for all $i$, so there exists $N$
such that 
$$
a^{N}\alpha_i=a^{N}\beta_i
$$
Therefore
$$
I+\sum\limits_{i}\alpha_i\left(a^{N}X\right)^{i}=I+\sum\limits_{i}\beta_i\left(a^{N}X\right)^{i}
$$
i.e.
$$
\alpha\left(a^{N}X\right)=\beta\left(a^{N}X\right)\varepsilon E_n(A[X],(X)).
$$
\enprf
\end{Proof}


\begin{lem}\label{c1:lem5.7}
Let $\alpha \varepsilon GL_n(A[X])$ and $a \varepsilon A$ be such that
$\alpha_a\varepsilon E_n(A_a[X])$. 

Then
$$
\alpha(cX)\alpha(dX)^{-1}\varepsilon E_n(A[X],(X))
$$
where $c\equiv d\pmod{a^{N}}$ for some large integer $N$.
\end{lem}


\begin{Proof}
Let $Y,Z$ be indeterminate. Write
$$
\beta=\beta(X,Y,Z)=\alpha(XY)\alpha((Y+Z)X)^{-1}
$$

Clearly $\beta \varepsilon GL_n(A[X,Y,Z], (Z))$. Since
$\alpha_a\varepsilon E_n(A_a[X])$, it follows that
$\beta_a(X,Y,Z)\varepsilon E_n(A_a[X,Y,Z],(Z))$. By
Corollary~\ref{c1:coro5.6}, we have 
$$
\beta\left(X,Y,a^{N}Z\right)\varepsilon E_n(A[X,Y,Z],(Z)) \text{ for
  large } N
$$
i.e.
$$
\alpha(XY)\alpha\left(\left(Y+a^{N}z\right)X\right)^{-1}\varepsilon E_n(A[X,Y,Z],(Z)).
$$

Let $d=c+a^{N}\lambda$. Then by substituting $Y=c$ and $Z=\lambda$,
our assertion follows.
\enprf
\end{Proof}

\begin{lem}\label{c1:lem5.8}
Let $\alpha\varepsilon GL_n(A[X],(X))$ and $a,b \varepsilon A$ such
that $aA+bA=A$. Suppose $\alpha_a\varepsilon E_n(A_a[X])$ and
$\alpha_b\varepsilon E_n(A_b[X])$. Then $\alpha \varepsilon E_n(A[X])$.
\end{lem}

\begin{Proof}
By Lemma~\ref{c1:lem5.7}, there exists a natural number $N$ such that
$\alpha\\(cX)\alpha(dX)^{-1}\varepsilon E_n(A[X])$ if $c\equiv
d\pmod{a^{n}}$ or $c\equiv d\pmod b^{N}$.

Since $aA+bA=A$ implies that $a^{N}A+b^{N}A=A$, so there exists
$\lambda, \mu \varepsilon A$ such that $a^{N}\lambda+b^{N}\mu=1$. So 
$$
\alpha(X)=\alpha(X)\alpha\left(a^{N}\lambda X\right)^{-1}\alpha\left(a^{N}\lambda
X\right)\alpha(0)^{-1}\varepsilon E_n(A[X]). 
$$

This proves the lemma.
\enprf
\end{Proof}

\begin{lem}\label{c1:lem5.9}
Let $\alpha \varepsilon GL_n(A[X])$ with $\alpha(0)=1$ and\\
$I=\left\{a\varepsilon A\mid \alpha_a\varepsilon E_n(A_a[X])\right\}$
then $I$ is an ideal of $A$.
\end{lem}

\begin{Proof}
It is clear that if $a\varepsilon I$, $\lambda \varepsilon A$ then
$\lambda a\varepsilon I$. Now for $a, b \varepsilon I$, we have to
show that $a+b\varepsilon I$. Replacing $A$ by $A_{(a+b)}$, we are
reduced to Lemma~\ref{c1:lem5.8} Hence $I$ is an ideal.
\enprf
\end{Proof}
