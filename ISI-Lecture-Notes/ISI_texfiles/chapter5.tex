\chapter{}\label{chap5}

\section{\texorpdfstring{$\left(x_0,x_1,x^{2}_2,\ldots,x^{n}_n\right)$}{eq} is completable}.\label{c5:s1}

In this section we prove that if $(x_0,\ldots,x_n)$ is unimodular then\\
$\left(x_0,x_1,x^{2}_2,\ldots,x^{n}_n\right)$ is completable (see
proposition~\ref{c5:Prop1.1}). We need this in the next section to
show that $\left(x^{r_0}_0,\ldots,x^{r_n}_n\right)$ is completable if
$n!\mid r_0\ldots r_n$.

Let $A$ be any commutative ring and $n$ an integer $\geq 1$. We recall
that $U_n(A)$ denotes the set of unimodular elements of $A^{n}$.  

\begin{Prop}(\citet{Suslinc})\label{c5:Prop1.1}
Let $(x_0,x_1,\ldots,x_n)\varepsilon U_{n+1}(A)$. Then\\
$\left(x_0,x_1,x_2^{2},\ldots, x^{n}_n\right)$ is completable.
\end{Prop}

\begin{Proof}
It follows by induction on $n$ and Proposition~\ref{c5:Prop1.2} given below.
\enprf
\end{Proof}

\begin{Prop}\label{c5:Prop1.2}
Let $(x_0,x_1,\ldots,x_n)\varepsilon U_{n+1}(A)$. Suppose\\
$\left(\overline{x}_0,\overline{x}_1,\ldots,\overline{x}_{n-1}\right)$
is completable in $\overline{A}=\dfrac{A}{Ax_n}$. Then
$\left(x_0,x_1,\ldots,x_{n-1},x^{n}_n\right)$ is completable.
\end{Prop}

For a proof of this we need

\begin{lem}\label{c5:lem1.3}
Let $a\varepsilon A$ and $\overline{A}=\dfrac{A}{Aa}$. Let
$\phi\varepsilon M_n(A)$ such that $\overline{\phi}\varepsilon
GL_n(\overline{A})$. Then there exist $\delta_1,\delta_2\varepsilon
M_n(A)$ such that 
$$
\begin{bmatrix}
\phi & \delta_1\\
aI_n& \delta_2
\end{bmatrix} \varepsilon GL_{2n}(A)
$$
\end{lem}

\begin{Proof}
Since $\overline{\phi}\varepsilon GL_n(\overline{A})$, there exists
$\overline{\psi} \varepsilon GL_n(\overline{A})$ such that
$\overline{\phi}\overline{\psi}=\overline{\psi}\overline{\phi}=1_{\overline{A}^{n}}$. Therefore
$$
\phi \psi=I_n+a\alpha
$$
and
$$
\psi\phi=I_n+a\beta
$$
for some $\alpha, \beta \varepsilon M_n(A)$. Let 
$$
\triangle=
\begin{bmatrix}
\phi& \alpha\\
aI_n & \psi
\end{bmatrix} \varepsilon M_{2n}(A).
$$

Then clearly
$$
\begin{bmatrix}
\psi & -\beta\\
-aI_n & \phi
\end{bmatrix} \begin{bmatrix}
\phi & \alpha\\
aI_n & \psi
\end{bmatrix} = \begin{bmatrix}
I_n & \ast\\
0 & I_n
\end{bmatrix} \varepsilon GL_{2n}(A)
$$

So $\triangle \varepsilon GL_{2n}(A)$. This proves the lemma.
\enprf
\end{Proof}

\begin{lem}\label{c5:lem1.4}
Let $a,b \varepsilon A$ be such that $Aa+Ab=A$. Then 
$$
\begin{bmatrix}
a^{n} & 0\\
0 & I_{n-1}
\end{bmatrix} = a\epsilon + b\mu
$$
where $\epsilon \varepsilon E_n(A)$ and $\mu \varepsilon M_n(A)$.
\end{lem}

\begin{Proof}
Let $\overline{A}=\dfrac{A}{Ab}$. Then $\overline{a}\varepsilon
\overline{A}^{\ast}$. By Whitehead Lemma (see Lemma~\ref{c1:lem2.2} of
Chapter~\ref{chap1}),
$$
\begin{bmatrix}
a^{-n} & 0\\
0 & I_{n-1}
\end{bmatrix} =\begin{bmatrix}
\overline{a}& &0\\
&\ddots &\\
0 & &\overline{a}
\end{bmatrix} \overline{\epsilon} =\overline{a}\overline{\epsilon}
$$
where $\overline{\epsilon} \varepsilon E_n(\overline{A})$. Let
$\epsilon \varepsilon E_n(A)$ be a lift of $\overline{\epsilon}$. Then
clearly
$$
\begin{bmatrix}
a^{n} & 0\\
0 & I_{n-1}
\end{bmatrix}=a\epsilon +b\mu
$$
for some $\mu \varepsilon M_n(A)$.
\enprf
\end{Proof}

\begin{PRFF2}
Since $\left(\overline{x}_0,\ldots,\overline{x}_{n-1}\right)$ is
completable in $\overline{A}=\dfrac{A}{Ax_n}$, we have
$\overline{\phi}\varepsilon GL_n(\overline{A})$ such that 
$$
\overline{\phi}=
\begin{bmatrix}
\overline{x}_0 & \ast &\cdots & \ast\\
\vdots & & & \\
\overline{x}_{n-1} & \ast & \cdots & \ast
\end{bmatrix}
$$

Let $\phi$ be a lift of $\overline{\phi}$ in $M_n(A)$, whose first
column is $\begin{bmatrix}
x_0\\
\vdots\\
x_n
\end{bmatrix}$ \\By Lemma~\ref{c5:lem1.3}, there exists
$\delta_1,\delta_2\varepsilon M_n(A)$ such that 
$$
\begin{bmatrix}
\phi& \delta_1\\
x_nI_n & \delta_2
\end{bmatrix} \varepsilon GL_{2n}(A)
$$

Let $\det \phi=b$, so $\det \overline{\phi}=\overline{b}\varepsilon
\overline{A}^{\ast}$. By Lemma~\ref{c5:lem1.4}, there exist $\epsilon
\varepsilon E_n(A)$, $\mu \varepsilon M_n(A)$ such that 
$$
\begin{bmatrix}
x^{n}_n & 0\\
0 & I_{n-1}
\end{bmatrix} = x_n\epsilon +b\mu
$$

Let $\gamma$(adj $\phi$). Then $\gamma\phi=\mu$ (adj
$\phi$)$\phi=b\mu$. Now consider the matrix
$$
\begin{bmatrix}
I_n & 0\\
\gamma & \epsilon
\end{bmatrix} \varepsilon GL_{2n}(A)
$$

Then we have 
$$
\begin{bmatrix}
I_n & 0\\
\gamma & \epsilon
\end{bmatrix} \begin{bmatrix}
\phi &\delta_1\\
x_nI_n & \delta_2
\end{bmatrix} =\begin{bmatrix}
\phi & \delta_1\\
\begin{bmatrix}
x^{n}_n & 0\\
0 & I_{n-1}
\end{bmatrix} & \ast
\end{bmatrix}
$$
\[
=
  \begin{blockarray}{ccccc|cc}
    \begin{block}{[ccccc|cc]}
      x_0 & &\ast & \cdots & \ast &  \\
      \vdots & & & & & \\
      x_{n-1}& &\ast & \cdots  &\ast\ast &  (n+1)\times (n-1) \text{
        matrix }\\
      x^{n}_n& &0 & \cdots &0\ast  & \\
      \cline{1-6}% don't use \hline
      0 &  & & &\ast & \\
      \vdots & & I_{n-1} & &\vdots&  (n-1)\times (n-1) \text{ matrix }\\
      0 &  & & &\ast &\\
      \end{block}
  \end{blockarray}
\]

\[
\xrightarrow{E_{2n}(A)}
  \begin{blockarray}{cccccc|ccccc}
    \begin{block}{[cccccc|ccccc]}
      x_0 & &\ast & \cdots & &\ast & & & & &  \\
      \vdots &&\vdots & & & \vdots & & & & & \\
      x_{n-1}& &\ast & \cdots  &\ast &\ast &  & &\ast &&\\
      x^{n}_n& &0 & \cdots &0&\ast  & & & & &\\
      \cline{1-11}% don't use \hline
      & & & & & & & & & & & &\\
      0 &  & & & &0 & & & & &\\
      \vdots & & I_{n-1} & & &\vdots& & & I_{n-1} &\\
      0 & & & & &0& & & & &\\
      \end{block}
  \end{blockarray}
\]

\[
\xrightarrow{E_{2n}(A)}
  \begin{blockarray}{ccccc|cccc}
    \begin{block}{[ccccc|cccc]}
      x_0 & & && & & &  & \\
    \vdots && \ast & & & & & \ast  \\
    x_{n-1}& && & & &  & &\\
    x^{n}_n& & & && & & &\\
    \cline{1-9}% don't use \hline
    & & & & & & & &  \\
    & & & & & & & &  \\
    & & 0 & & & & & I_{n-1} \\
    & & & & & & & &  \\
      \end{block}
  \end{blockarray}
\]

\[
\xrightarrow{E_{2n}(A)}
  \begin{blockarray}{cccc|ccc}
    \begin{block}{[cccc|ccc]}
      x_0 & & && & & \\
    \vdots && & & & & \\
    x_{n-1}& & \ast & & &   \\
    x^{n}_n& & & & & &  \\
    \cline{1-4}% don't use \hline
    & & & & &&   \\
    & & \ast & & & &   \\
      \end{block}
  \end{blockarray}\varepsilon GL_{2n}(A).
\]
So $\left(x_0,x_1,\ldots,x_{n-1},x^{n}_n\right)$ is completable.
\enprf
\end{PRFF2}

\begin{rem}\label{c5:rem1.5}
For $n=2$, Proposition~\ref{c5:Prop1.1} was discovered by
\citet{Swan}. For this case  \citet{Krusemeyer}, gave an alternative
completion. In fact he gave the following identity.

For $x_i,y_i\varepsilon A, 0\leq i\leq 2$, 
$$
\begin{vmatrix}
x^{2}_0 & x_1 & x_2\\
-x_1+2y_2x_0 & y^{2}_2 & -y_0-y_1y_2\\
-x_2-2y_1x_0 & y_0-y_1y_2 & y^{2}_1
\end{vmatrix} = \left(\sum\limits_{i=0}^{2}x_iy_i\right)^{2}
$$
\end{rem}

\section{\texorpdfstring{$\left(x_0^{r_0},x_1^{r_1},\ldots,x^{r_n}_n\right)$}{ew} is
  completable if \texorpdfstring{$n!\mid r_0r_1\ldots r_n$}{ew}}.\label{c5:s2}

The main result of this section is Theorem~\ref{c5:thm2.1} Let $A$ be
any commutative ring and $n$ be any positive integer.

\begin{thm}(\citet{Suslinc})\label{c5:thm2.1}
Let $(x_0,x_1,\ldots,x_n)\varepsilon U_{n+1}(A)$. Then for any integer
$r_i\geq 1$, $\left(x_0^{r_0},x_1r_1,\ldots,x^{r_n}_n\right)$ is
completable if $n!\mid r_0r_1\ldots r_n$. We need several lemmas: For
any $n\times 2$ matrix.
$$
\alpha=
\begin{bmatrix}
a_1 & b_1\\
\vdots & \vdots\\
a_n & b_n
\end{bmatrix} \varepsilon M_{n,2}(A),
$$
%$$
\begin{align*}
\text{ Let } I(\alpha)&= \text{ ideal of }  A \text{ generated by }
2\times 2 \text{ minors of }
\alpha\\
&=\text{ ideal of A generated by the set } \left\{a_ib_j -a_jb_i\mid
1\leq i,j\leq n\right\}
\end{align*}
\end{thm}

\begin{lem}\label{c5:lem2.2}
For any $d\varepsilon I(\alpha)$, there exists $a2\times n$ matrix
$\beta$ such that $\beta\alpha=dI_2$
\end{lem}

\begin{Proof}
Enough to show for any generator $d=a_ib_j-a_jb_i\varepsilon
I(\alpha)$, for some $i,j$. Take 
$$
\beta=
\kbordermatrix{&& & & i-th & & & & j-th & & &\\
& 0 & \ldots & 0 &b_j & 0 &\cdots & 0 & -b_i &0 &\cdots & 0\\
&0 & \ldots & 0 &-a_j & 0 &\cdots & 0 & a_i &0 &\cdots & 0
}
$$

Then 
$$
\beta\alpha=dI_2
$$

We remark that the above lemma in particular implies that for any
$d=\sum\limits_{i,j}d_{ij}(a_ib_j-a_jb_i)\varepsilon I(\alpha)$, there
exist $\lambda_i\varepsilon A$ such that
$\sum\limits_{i}\lambda_ia_i=d$ and $\sum\limits_{i}\lambda_ib_i=0$. 

We recall from section~\ref{c3:s3} of Chapter~\ref{chap2}, that for
$(a,b)\varepsilon U_2(A)$ such that $ad-bc=1$, the Mennicke symbol
$\begin{pmatrix}
a\\ 
b
\end{pmatrix}$ is defined as 
$$
\begin{pmatrix}
a\\
b
\end{pmatrix} = \text{ class of } \begin{bmatrix}
a & b & 0\\
c & d & 0\\
0 & 0 & 1
\end{bmatrix} \varepsilon \dfrac{SL_3(A)}{E_3(A)}.
$$

It follows from Proposition~\ref{c2:Prop3.5} of Chapter~\ref{chap2}
that 
$$
\begin{pmatrix}
a^{r}\\
b
\end{pmatrix} = \begin{pmatrix}
a\\
b
\end{pmatrix}^{r} = \begin{pmatrix}
a\\
b^{r}
\end{pmatrix} \text{ for } r\geq 1.
$$
\enprf
\end{Proof}

\begin{lem}\label{c5:lem2.3}
Let $(x_0,x_1,\ldots,x_n) \varepsilon U_{n+1}(A)(n\geq 3)$. Suppose
there exist $v_2,\ldots, v_n\varepsilon A$ such that
$Ax_0+Ax_1+I(\alpha)=A$, where
$$
\alpha=
\begin{bmatrix}
x_2 & v_2\\
\vdots & \vdots\\
x_n & v_n
\end{bmatrix} \varepsilon M_{n-1,2}(A)
$$

There for any integer $s\geq 0$, there exists $\gamma \varepsilon
GL_{n+1}(A)$ such that 
$$
\gamma'
\begin{bmatrix}
x^{s}_0\\
x_1\\
\vdots\\
x_n
\end{bmatrix}=\begin{bmatrix}
x_0\\
x^{s}_1\\
\vdots\\
x_n
\end{bmatrix}
$$
\end{lem}

\begin{Proof}
Let $d\varepsilon I(\alpha)$ be such that 
$$
Ax_0+Ax_1+Ad=A
$$

Let $\overline{A}=\dfrac{A}{Ad}$, then
$\left(\overline{x}_0,\overline{x}_1\right)\varepsilon
U_2(\underline{A})$ and since the Mennicke symbol
$$
\begin{pmatrix}
x^{-s}_0\\
\overline{x}_1
\end{pmatrix} = \begin{pmatrix}
\overline{x}_0\\
\overline{x}^{s}_1
\end{pmatrix}
$$
i.e.
$$
\begin{bmatrix}
\overline{x}^{-s}_0 & \overline{x}_1 & 0\\
\ast & \ast & 0\\
0 & 0 & 1 
\end{bmatrix} \equiv \begin{bmatrix}
\overline{x}_0 & \overline{x}^{s}_1 & 0\\
\ast & \ast & 0\\
0 & 0 & 1
\end{bmatrix} \pmod{E_3(\overline{A})},
$$
by applying suitable elementary transformations and taking transpose,
it follows that 
$$
\begin{bmatrix}
0 & 1 & 0\\
\overline{x}^{s}_0 & 0 & \ast\\
\overline{x}_1 & 0 & \ast
\end{bmatrix} \equiv \begin{bmatrix}
0 & 1 & 0\\
\overline{x}_0 & 0 & \ast\\
\overline{x}^{s}_1 & 0 & \ast
\end{bmatrix} \pmod{E_3(\overline{A})}
$$

Therefore there exists $\overline{\epsilon} \varepsilon
E_3(\overline{A})$ such that 
$$
\epsilon
\begin{bmatrix}
0 &1\\
\overline{x}^{s}_0 & 0\\
\overline{x}_1 & 0
\end{bmatrix} = \begin{bmatrix}
0 & 1\\
\overline{x}_0 & 0\\
\overline{x}^{s}_1 & 0
\end{bmatrix}
$$
Let $\epsilon \varepsilon E_3(A)$ be a lift of $\epsilon \varepsilon
E_3(\overline{A})$, then we have 
$$
\epsilon
\begin{bmatrix}
0 & 1\\
x^{s} & 0\\
x_1 & 0
\end{bmatrix} = \begin{bmatrix}
\mu d & 1+\lambda d\\
x_0+\mu_0d & \lambda_0d\\
x^{s}_1+\mu_1d & \lambda_1d
\end{bmatrix}
$$
for some $\mu$, $\lambda$, $\mu_i$, $\lambda_i\varepsilon A$, $i=1,
2$. Now clearly
$$
\begin{bmatrix}
0 & 1\\
x^{s}_0 & 0\\
x_1 & 0\\
x_2 & 0\\
\vdots & \vdots\\
x_n & 0
\end{bmatrix} \xrightarrow{E_{n+2}(A)} \begin{bmatrix}
0 &1\\
x^{s}_0 & 0\\
x_1 & 0\\
x_2 & v_2\\
\vdots & \vdots\\
x_n & v_n
\end{bmatrix} \xrightarrow{\begin{bmatrix} 
\epsilon & 0\\
0 & I_{n-1}
  \end{bmatrix}}
  \begin{bmatrix}
\mu d & 1+\lambda d\\
x_0+\mu_0d & \lambda_0 d\\
x^{s}_1+\mu_1d & \lambda d\\
x_2 & v_2\\
\vdots & \vdots\\
x_n & v_n
\end{bmatrix}
$$
$$
\xrightarrow[\text{see remark
after Lemma\ref{c5:lem2.2}}]{E_{n+2}(A)}
\begin{bmatrix}
0 & 1\\
x_0 & 0\\
x^{s}_1 & 0\\
x_2 & v_2\\
\vdots & \vdots\\
x_n & v_n
\end{bmatrix} \xrightarrow{E_{n+2}(A)}
\begin{bmatrix}
0 &  1\\
x_0 & 0\\
x^{s}_1 &  0\\
x_2 & 0\\
\vdots & \vdots\\
x_n &  0
\end{bmatrix}
$$

So there $\gamma \varepsilon E_{n+2}(A)$ such that 
$$
\gamma
\begin{bmatrix}
0 & 1\\
x^{s}_0 & 0\\
x_1 & 0\\
\vdots &  \vdots\\
x_n & 0
\end{bmatrix} = \begin{bmatrix}
0 & 1\\
x_0 & 0\\
x^{s}_1 & 0\\
\vdots &  \vdots\\
x_n & 0
\end{bmatrix}
$$

This shows that $\gamma$ has to be of the form 
$$
\gamma=
\begin{bmatrix}
1 & \ast & \cdots & \ast\\
0 & & \gamma'&\\
\vdots & &  &\\
0 & & &
\end{bmatrix}, \gamma' \varepsilon GL_{n+1}(A)
$$

Now taking $\gamma'$ as the required $\gamma$, the lemma follows.

Now for
$\underline{x}=(x_0,\ldots,x_n),\underline{y}=(y_0,\ldots,y_n)\varepsilon
U_{n+1}(A)$. We write $\underline{x}\sim \underline{y}$ if and only if
there exists an $\alpha \varepsilon GL_n(A)$ such that 
$$
\alpha
\begin{bmatrix}
x_1\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
y_0\\
\vdots\\
y_n
\end{bmatrix} \Leftrightarrow [x_0,\ldots,x_n]\alpha^{t}=[y_0,\ldots,y_n]
$$
(where $t$ denotes the transpose) i.e. $x$ and $y$ are in the same
orbit under the action of $GL_n(A)$. Sometimes we write
$\underline{x}\displaystyle\mathop{\sim}^{\alpha}\underline{y}$ or
$\underline{x}\displaystyle\mathop{\sim}^{G}\underline{y}$ to specify
$\alpha$ or the group $G$. 
\enprf
\end{Proof}

\begin{coro}\label{c5:coro2.4}
Let $n=2k+1$, $k\geq 0$ and $\underline{x}=(x_0,\ldots,
x_n)\varepsilon U_{n+1}(A)$. Then for any integer $s\geq 0$, 
$$
\left(x^{s}_0,x_1,x_2,\ldots,x_n\right)\sim \left(x_0,x_1^{s},x_2,\ldots,x_n\right).
$$
\end{coro}

\begin{Proof}
For $k=0$, this is obvious. So let $k\geq 1$. Since
$\underline{x}\varepsilon U_{n+1}(A)$, there exist $y_0,\ldots,y_n
\varepsilon A$ such that $\sum\limits_{i=0}^{n} x_iy_i=1$. Now
considering $x_2,\ldots, x_n$ which are even in number and taking 
$$
\alpha=
\begin{bmatrix}
x_2 & y_3\\
x_3 & -y_2\\
x_4 & y_5\\
x_5 & -y_4\\
\vdots & \vdots\\
x_{n-1} &  y_n\\
x_n & -y_{n-1}\\
\end{bmatrix} \varepsilon M_{2k, 2}(A), 
$$
we have 
$$
x_2y_2+\cdots+x_n+y_n\varepsilon I(\alpha)
$$
and
$$
Ax_0+Ax_1+I(\alpha)=A.
$$
So by Lemma~\ref{c5:lem2.3}, we have 
$$
\left(x^{s}_0,x_1,\ldots, x_n\right)\sim\left(x_0,x^{s}_1,\ldots, x_n\right).
$$
\enprf
\end{Proof}


\begin{coro}\label{c5:coro2.5}
Let $n=2k$, $k\geq 1$ and $\underline{x}=(x_0,\ldots,x_n)\varepsilon
U_{n+1}(A)$. Then for any integer $s\geq 0$, 
$$
\left(x^{s}_0,x_1,x^{2}_2,\ldots,x_n\right)\sim\left(x_0,x^{2}_1,x^{2}_2,\ldots,x_n\right).
$$
\end{coro}

\begin{Proof}
Let $k=1$. Then since $\underline{x}=(x_0,x_1,x_2)\varepsilon U_3(A)$,
it follows by Proposition~\ref{c5:Prop1.1}, that 
$$
\left(x^{s}_0,x_1,x^{2}_2\right)\sim (1,0,0)
$$
and
$$
\left(x_0,x^{s}_1,x^{2}_2\right)\sim(1,0,0).
$$
So
$$
\left(x^{s}_0,x_1,x^{2}_2\right)\sim\left(x_0,x^{s}_1,x^{2}_2\right).
$$

Now let $k\geq 2$ and $J=Ax_0+Ax_1+\sum\limits_{i=5}^{n}Ax_i$. Let
$\overline{A}=\dfrac{A}{J}$. Then
$\left(\overline{x}^{2}_2,\overline{x}_3,\overline{x}_4\right)\varepsilon
U_3(\overline{A})$ and by Proposition~\ref{c5:Prop1.1},
$\left(\overline{x}^{2}_2,\overline{x}_3,\overline{x}_4\right)$ is
completable. Therefore there exists a such that 
$$
\begin{bmatrix}
\overline{x}^{2}_2 & \overline{a}_2 & \overline{b}_2\\
\overline{x}_3 & \overline{a}_3 & \overline{b}_3\\
\overline{x}_4 & \overline{a}_4 & \overline{b}_4
\end{bmatrix} \varepsilon GL_3(\underline{A})
$$

Let $M_1,M_2,M_3$ be three minors corresponding to the matrix
$$
\begin{bmatrix}
x^{2}_2 & a_2\\
x_3 &  a_3\\
x_4 & a_4
\end{bmatrix}
$$

Then clearly $\overline{\lambda_1}\overline{{M}_1}+\overline{\lambda_2}\overline{{M}_2}
  +\overline{\lambda_3}\overline{M_3}\varepsilon
  \overline{A}^{\ast}$, for some $\lambda_i\varepsilon A$, $i=1,2,3$
  and
  $a_0x_0+a_1x_1+\lambda_1M_1+\lambda_2M_2+\lambda_3M_3+\sum\limits_{i\geq
  5} a_i x_i=1$. 

Since $x_5,\ldots,x_n$ are even in number, we can suitably couple them
with $a_5,\ldots, a_n$ to form a $(n-4)\times 2$ matrix whose sum of
certain $2\times 2$ minors is $\sum\limits_{i\geq 5}a_ix_i$. Take
$$
\alpha=
\begin{bmatrix}
x^{2}_2 & a_2\\
x_3 & a_3\\
x_4 & a_4\\
x_5 & -a_6\\
x_6 & a_5\\
\vdots & \vdots\\
x_{n-1} & -a_n\\
x_n & a_{n-1}
\end{bmatrix} \varepsilon M_{n-1,2}(A)
$$

Then
$$
\lambda_1 M_1+\lambda_2 M_2+\lambda_3 M_3+\sum\limits_{1\geq 5}
a_ix_i\varepsilon I(\alpha).
$$

So
$$
Ax_0+Ax_1+I(\alpha)=A.
$$

Hence by Lemma~\ref{c5:lem2.3}, we have
$$
\left(x^{s}_0,x_1,x^{2}_2,x_3,\ldots,x_n\right)\sim\left(x_0,x^{s}_1,x^{2}_2,x_3,\ldots,x_n\right).
$$
\enprf
\end{Proof}

\begin{lem}\label{c5:lem2.6}
Let $\underline{x}=(x_1,x_2,x_3)\varepsilon U_3(A)$. Then there exists
$\epsilon \varepsilon E_3(A)$ such that 
$$
\left(x^{2}_1,x_2,x_3\right)\displaystyle\mathop{\sim}^{\epsilon}\left(x_1,x^{2}_2,x_3\right)
$$
\end{lem}

For a proof of this we need

\begin{lem}\label{c5:lem2.7}
Let 
$$
v=
\begin{bmatrix}
v_1\\
v_2\\
v_3
\end{bmatrix}, v'=\begin{bmatrix}
v'_1\\
v'_2\\
v'_3
\end{bmatrix} \varepsilon M_{3,1}(A).
$$

Let $u=(u_1,u_2,u_3)\varepsilon U_3(A)$ be such that $uv=1=uv'$. 

Then there exists $\epsilon \varepsilon E_3(A)$ such that $\epsilon
v=v'$. 
\end{lem}

\begin{Proof}
Since $u(v'-v)=0$ and $u$ is unimodular, we have
$\epsilon=1+(v'-v)u\varepsilon E_3(A)$ by Remark~\ref{c1:rem4.8} of
Chapter~\ref{chap1}. Therefore $\epsilon v=v'$. 
\enprf
\end{Proof}

\begin{lem}\label{c5:lem2.8}
Let 
$$
\alpha_i=
\begin{bmatrix}
\alpha_{1i}\\
\vdots\\
\alpha_{ni}
\end{bmatrix} \varepsilon M_{n,1}(A),\text{ for } n \geq 3,
$$
$1\leq i \leq n$ be such that $[\alpha_1,\ldots,\alpha_n]\varepsilon
GL_n(A)$. Then there exists $\epsilon \varepsilon E_n(A)$ such that
$\epsilon \alpha_1=\alpha_i$. 
\end{lem}

\begin{Proof}
Let 
$$
e_i=
\begin{bmatrix}
0\\
\vdots\\
1\\
\vdots\\
0
\end{bmatrix} i-th
$$

Clearly then there exists $\epsilon \varepsilon E_n(A)$ such that
$\epsilon e_1=e_i$. Since $\alpha e_i=\alpha_i$, we have
$$
\left(\alpha\epsilon \alpha^{-1}\right)(\alpha e_1)=\alpha \epsilon
e_1 =\alpha e_i =\alpha_i
$$
where $\alpha \epsilon \alpha^{-1} \varepsilon E_n(A)$, because
$E_n(A)$ is normal in $GL_n(A)$.
\enprf
\end{Proof}

\begin{PRFF3}
Since $(x_1,x_2,x_3)\varepsilon U_3(A)$, there exist
$y_1,y_2,y_3\varepsilon A$ and such that $x_1y_1+x_2y_2+x_3y_3=1$.
\end{PRFF3}

\setcounter{assr}{0}
\begin{assr}\label{c5:assr1}
$\left(x^{2}_1,x_2,x_3\right)\displaystyle\mathop{\sim}^{\epsilon}\left(x^{2}_1,y_3,-y_2\right),\epsilon
  \varepsilon E_3(A)$. first we note that 
\begin{align*}
1&= x^{2}_1y^{2}_1+\left(1-x^{2}_1y^{2}\right)\\
&=x^{2}_1y^{2}+(1+x_1y_1)(x_2y_2+x_3y_3)\\
&=x^{2}_1y^{2}_1+(1+x_1y_1)((y_2+x_3)x_2+(y_3-x_2)x_3)
\end{align*}
and 
$$
1=x^{2}_1y^{2}_1+(1+x_1y_1)((y_2+x_3)y_3+(y_3-x_2)(-y_2)).
$$

Now taking 
$$
v=\begin{bmatrix}
x^{2}_1\\
x_2\\
x_3
\end{bmatrix} \text{ and  } v'=\begin{bmatrix}
x^{2}_1\\
y_3\\
-y_2
\end{bmatrix}
$$
and
$$
u=\left(y^{2}_1,\left(1+x_1y_1\right)\left(y_2+y_3\right), \left(1+x_1y_1\right)\left(y_3-x_2\right)\right), 
$$
in Lemma~\ref{c5:lem2.7}, assertion~\ref{c5:assr1} follows.
\end{assr}

\begin{assr}\label{c5:assr2}
Since
$\left(x^{2}_1,y_3,-y_2\right)\displaystyle\mathop{\sim}^{\epsilon}\left(x^{2}_1,y_3+x_1x_2,-y_2+x_3x_1\right)$,
$\epsilon \varepsilon E_3(A)$. Since 
$$
x^{2}_1y^{2}_1+(1+x_1y_1)(x_3y_3+(-x_2)(-y_2))=1
$$
and
$$
x^{2}_1y^{2}_1+(1+x_1y_1)((y_3+x_1x_2)x_3+(-y_3+x_3x_1)(-x_2))=1, 
$$
so be taking 
$$
v=
\begin{bmatrix}
x^{2}_1\\
y_3\\
-y_2
\end{bmatrix}, v'=
\begin{bmatrix}
x^{2}_1\\
y_3+x_1x_2\\
-y_2+x_3x_1\\
\end{bmatrix}
$$
and
$$
u=\left(y^{2}_1,(1+x_1y_1\right)x_3,(1+x_1y_1)(-x_2))
$$
in Lemma~\ref{c5:lem2.7}, assertion~\ref{c5:assr2} follows. So 
$$
\left(x^{2}_1,x_2,x_3\right)\displaystyle\mathop{\sim}^{\epsilon}\left(x^{2}_1,y_3+x_1x_2,-y_2+x_3x_1\right)
$$
for some $\epsilon \varepsilon E_3(A)$. Similarly permuting the
indices as\\ $(1,2,3)\rightarrow (2,3,1)$, we have 
$$
\left(x^{2}_2,x_3,x_1\right)\displaystyle\mathop{\sim}^{\epsilon}\left(x^{2}_2,y_1+x_2x_3,-y_3+x_1x_2\right)
$$
for $\epsilon'\varepsilon E_3(A)$. Therefore 
$$
\left(x_1,x^{2}_2,x_3\right)\sim
\left(-y_3+x_1x_2,x^{2}_2,y_1+x_2x_3\right).
$$

Let $M_1, M_2,M_3$ be $2\times 2$ minors of the $3\times 2$ matrix
$$
\begin{bmatrix}
x^{2}_1 & -y_3+x_1x_2\\
y_3+x_1x_2 & x^{2}_2\\
-y_2+x_3x_1 & y_1+x_2x_3
\end{bmatrix}
$$

Then the ideals
$$
(M_1,M_2,M_3)=\left(Y^{2}_3,y_1y_3+x_2,y_2y_3-x_1\right)=\left(y_3,x_1,x_2\right)=A.
$$

Therefore there exist $\lambda_1$, $\lambda_2$, $\lambda_3\varepsilon
A$ such that 
$$
\alpha=
\begin{bmatrix}
x^{2}_1 & -y_3+x_1x_2 & \lambda_1\\
y_3+x_1x_2 & x^{2}_2 & \lambda_2\\
-y_2+x_3x_1 & y_1+x_2x_3 & \lambda_3
\end{bmatrix} \varepsilon GL_3(A)
$$

Now by Lemma~\ref{c5:lem2.8}, there exists $\epsilon' \varepsilon
E_3(A)$ such that 
$$
\epsilon'
\begin{bmatrix}
x^{2}_1\\
y_3+x_1x_2\\-y_2+x_3x_1\\
\end{bmatrix}=\begin{bmatrix}
-y_3+x_1x_2\\
x^{2}_2\\
y_1+x_2x_3\\
\end{bmatrix}
$$

Hence $\left(x^{2}_1,x_2,x_3\right)\displaystyle\mathop{\sim}^{E_n(A)}\left(x_1,x^{2}_2,x_3\right)$.
\end{assr}


\begin{lem}\label{c5:lem2.9}
Let $(x_0,x_1,\ldots,x_n)\varepsilon U_{n+1}(A)$ for $n\geq 2$. Then
there exists $\epsilon \varepsilon E_{n+1}(A)$ such that 
$$
\left(x^{2}_0,x_1,\ldots,x_n\right)\displaystyle\mathop{\sim}^{\epsilon}\left(x_0,x^{2}_1,\ldots,x_n\right).
$$
\end{lem}

\begin{Proof}
Let $j=\sum\limits_{i=3}^{n}Ax_i$ and
$\overline{A}=\dfrac{A}{J}$. Then
$\left(\overline{x}_0,\overline{x}_1,\overline{x}_2\right)\varepsilon
U_3(\overline{A})$ and by Lemma~\ref{c5:lem2.6}, we have 
$$
\left(\overline{x}^{2}_0,\overline{x}_1,\overline{x}_2\right)\displaystyle\mathop{\sim}^{\epsilon} \left(\overline{x}_0\overline{x}^{2}_1\overline{x}_2\right)
$$
for some $\overline{\epsilon}\varepsilon E_3(\overline{A})$. Let
$\epsilon \varepsilon E_3(A)$ be a lift of $\overline{\epsilon}$. Then 
$$
\epsilon
\begin{bmatrix}
x^{2}_0\\
x_1\\
x_2
\end{bmatrix} = \begin{bmatrix}
x_0+j_0\\
x^{2}_1+j_1\\
x_2+j_2
\end{bmatrix}, j_1\varepsilon J \text{ for } 1\leq i\leq 3.
$$

So
$$
\begin{bmatrix}
x^{2}_0\\
x_1\\
x_2\\
x_3\\
\vdots\\
x_n
\end{bmatrix}\xrightarrow{\begin{bmatrix}
\epsilon & 0\\
0 &I_{n-2}
  \end{bmatrix}
} \begin{bmatrix}
x_0+j_0\\
x^{2}_1+j_1\\
x_2+j_2\\
x_3\\
\vdots\\
x_n
\end{bmatrix}\xrightarrow{E_{n+1}(A)} \begin{bmatrix}
x_0\\
x^{2}_1\\
x_2\\
x_3\\
\vdots\\
x_n
\end{bmatrix}
$$

This proves the lemma.
\enprf
\end{Proof}

\begin{PRF1}
We may assume $n\geq 2$. 

\textbf{Case 1}: $r_0=1$, $r_i=1$, $1\leq i\leq n$ follows from
Proposition~\ref{c5:Prop1.1}\\
\quad\textbf{Case 2}: $r_i=1$, $0\leq i\leq n-1$ and $r_n=n!$.

We have
$$
\begin{aligned}
&\left(x_0,x_1,\ldots,\left(x_{n}^{\dfrac{n!}{2}}\right)^{2}\right)\\
\sim&{}\left(x_0,x_1,x^{2}_2,x_3,\ldots,x^{\dfrac{n!}{2}}_n\right)\text{ by Lemma~\ref{c5:lem2.9} }\\
\sim&{}\left(x_0,x_1,x^{2}_2,x^{3}_3,\ldots,x^{n}_n\right) \text{ by
  Corollary~\ref{c5:coro2.4} and \ref{c5:coro2.5} }\\
\sim&{} (1,0,\ldots 0)\text{ by case 1. }
\end{aligned}
$$
\textbf{Case 3:} $n!\mid r_0r_1\ldots r_n$ where $r_i\geq 1$, $0\leq
i<n$. 

Clearly $2\mid r_i$ for some $i$. We may assume $i=2$. Then 
$$
\begin{aligned}
&\left(x^{r_0}_0,x^{r_1}_1,\left(x_2^{\dfrac{r_{2}}{2}}\right)^{2},x^{r_3}_3,\ldots,
  x^{r_n}_n\right)\\
&{}\sim\left(x_0,x_1,\left(x_{2}^{\dfrac{r_2}{2}}\right)^{2},x_3,\ldots,
  x_n^{j\displaystyle\mathop{\neq}^{\prod}2^{r_j}}\right) \text{ by
  Corollary~\ref{c5:coro2.4} and \ref{c5:coro2.5}}\\
&{}\sim\left(x_0,x^{2}_1,x^{\dfrac{r_2}{2}},x_3,\ldots,
  x_n^{j\displaystyle\mathop{\neq}^{\prod}2^{r_j}}\right) \text{ by
  Lemma~\ref{c5:lem2.9} }\\
&{}\sim \left(x_0,x^{2}_1,x_2,\ldots,
  x_n^{j\displaystyle\mathop{\neq}^{\prod}2^{r_j\left(\dfrac{r_2}{2}\right)}}\right) \text{ by
  Corollary~\ref{c5:coro2.4} and \ref{c5:coro2.5}}\\
&{}\sim \left(x_0,x_1,x_2,\ldots,x_n^{\prod r_j}\right) \text{ by
  Lemma~\ref{c5:lem2.9} }\\
&{}\sim(1,0,0,\ldots,0) \text{ by case 2. }
\end{aligned}
$$

Hence $\left(x^{r_0}_0,\ldots,x^{r_n}_n\right)$ is completable.
\end{PRF1}

\begin{rem}\label{c5:rem2.10}
Theorem~\ref{c5:thm2.1} is false if $n!+r_0r_1\ldots r_n$. For a
counter example see \cite{Swan}
\end{rem}
