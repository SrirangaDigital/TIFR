
\chapter{Exact Solutions for Certain Nonlinear Equations}\label{chap8}

FOR\pageoriginale THE WAVE problems of hyperbolic type studied in Chapters \ref{chap1}--\ref{chap3} we noted that the inclusion of dissipation would lead in the simplest case to Burgers' equation
\begin{equation}
\eta_t+\eta\eta_x=\eta_{xx}.\tag{8.1}\label{chap8:eq8.1}
\end{equation}

We also noted the remarkable fact that this equation could be transformed into the heat equation
\begin{align}
& v_t-v_{xx}=0\tag{8.2}\label{chap8:eq8.2}\\
\intertext{by the substitution}
& \eta=-2(\log v)_x=-\frac{2v_x}{v}.\tag{8.3}\label{chap8:eq8.3}
\end{align}

Thomas's equation 
\begin{equation}
u_{xy}+pu_x+qu_y+u_xu_y=0\tag{8.4}\label{chap8:eq8.4}
\end{equation}
could be made linear by a similar transformation:
$$
u=\log v.
$$

In the water wave context, when dispersion rather than dissipation is incorporated, the simplest basic equation is the Korteweg-de Vries equation 
\begin{equation}
\eta_t+\eta\eta_x+\eta_{xxx}=0.\tag{8.5}\label{chap8:eq8.5}
\end{equation}

The derivation of the equation and its background are described in \cite{key1}, Sections 13.11--13.13. In recent years, a remarkable number of developments have led to unusual and quite intricate methods\pageoriginale of finding solutions to \eqref{chap8:eq8.5}. These in turn have led to similar developments for the following equations.
\begin{align}
&\text{Modified KdV}\quad  u_t+3u^2u_x+u_{xxx}=0,\tag{8.6}\label{chap8:eq8.6}\\
&\text{Sine-Gordon}\quad  u_{tt}-u_{xx}+\sin u=0,\tag{8.7}\label{chap8:eq8.7}\\
&\text{Cubic Schrodinger}\quad  iu_t+u_{xx}+|u|^2u=0, \tag{8.8}\label{chap8:eq8.8}\\
&\text{Boussinesq}\quad  u_{tt}-u_{xx}-(u^2)_{xx}-u_{xxxx}=0. \tag{8.9}\label{chap8:eq8.9}
\end{align}

\section{Solitary waves}\label{chap8:sec8.1}

In their original paper \cite{key16} published in 1895, Korteweg-de Vries found special solutions of \eqref{chap8:eq8.5} in the form of steady profile waves moving with constant velocity. These may be obtained by taking 
\begin{equation}
\eta=\eta(X),\;X=x-\alpha^2t,\tag{8.10}\label{chap8:eq8.10}
\end{equation}
where $\alpha^2$ is the constant velocity of translation. We have 
$$
\eta_{XXX}+\eta\eta_X-\alpha^2\eta_X=0,
$$
and after two integrations
\begin{equation}
\frac{1}{2}\eta_X^2 +\frac{1}{6}\eta^3 -\frac{1}{2}\alpha^2\eta^2 +A\eta+B=0, \tag{8.11}\label{chap8:eq8.11}
\end{equation}
where $A,B$ are constants of integration. In general \eqref{chap8:eq8.11} has solutions in periodic elliptic functions (`cnoidal waves'), so that \eqref{chap8:eq8.10} represents a moving wave train. In the special case $A=B=0$ (which corresponds to the limit $cn^2\to\sec h^2$ in the elliptic functions) we have 
\begin{align}
& \eta^2_X=\frac{1}{3}\eta^2(3\alpha^2-\eta),\tag{8.12}\label{chap8:eq8.12}\\
\intertext{with solution}
& \eta=3\alpha^2\sec h^2\frac{1}{2}(\alpha x-\alpha^3 t). \tag{8.13}\label{chap8:eq8.13}
\end{align}

This\pageoriginale is the `solitary wave' consisting of a single hump. It should be noted that the velocity $\alpha^2$ is related to the height $3\alpha^2$; stronger waves move faster. 

Similar solutions can be obtained for \eqref{chap8:eq8.6}- \eqref{chap8:eq8.9}.

However, the big advance came with the discovery of more general solutions for \eqref{chap8:eq8.5}, and in particular solutions for the interaction of solitary waves, by Gardner, Greene, Kruskal, Muira \cite{key17} in 1967. The formula for the interaction of $N$ solitary waves is 
\begin{equation}
\eta=12\frac{\partial^2}{\partial x}\log |D|,\tag{8.14}\label{chap8:eq8.14}
\end{equation}
where $|D|$ is the $N\times N$ determinant with elements
\begin{equation}
D_{mn}=\delta_{mn}+\frac{2\gamma_m}{\alpha_m +\alpha_n} e^{-\alpha_m x+\alpha_m^3 t.}\tag{8.15}\label{chap8:eq8.15}
\end{equation}

Each parameter $\alpha_m$ corresponds to one of the solitary waves \eqref{chap8:eq8.13}; in the case $N=1, \gamma_1=\alpha_1$ \eqref{chap8:eq8.14}-\eqref{chap8:eq8.15} reduces to \eqref{chap8:eq8.13}. The parameters $\gamma_m$ play the monor role of spacing the original positions of the solitary waves. The form of the solution \eqref{chap8:eq8.14} is sufficiently unusual and complicated to indicate that a whole new set of ideas and techniques is involved. Even the thought of verifying \eqref{chap8:eq8.14} by direct substitution is alarming since five derivatives of $|D|$ would be required!

Physically the result is also interesting since it shows that after interaction each solitary wave emerges with its original shape\pageoriginale and velocity.

Similar results were eventually found for the equations in \eqref{chap8:eq8.6}-\eqref{chap8:eq8.9}. As another illustration of the novelty of these solutions it may be noted that \eqref{chap8:eq8.6} has a solution
\begin{align*}
u &=2\sqrt{2}\frac{\partial}{\partial x}\tan^{-1}\left\{\frac{\alpha}{k}\sec h(\alpha x-\beta t)\sin(kx-\omega t)\right.\\
\frac{\omega}{k} &= -k^2+3\alpha^2,\quad \frac{\beta}{\alpha}= -3k^2+\alpha^2.
\end{align*}

This represents a `wave packet' with oscillations moving through an envelope of solitary shape.

The original methods are reviewed in \cite{key1}, Chapter 17. Here an alternative recent method will be described.

\section{Perturbation approaches}\label{chap8:sec8.2}

So much of the progress on nonlinear problems in all fields has been made by perturbation methods that we wondered what the story was here. Could the above solutions be obtained by relatively simple perturbation approaches, even if we have to rely on the existing results to stimulate the correct procedures\@? Secondly, having learnt the correct procedures, is there hope of applying these methods {\bf in approximate form} in cases where it seems most unlikely that exact solutions could be expected\@? The first question does lead to an interesting and rather simple way of finding the solutions, with information of general value for perturbation theory. It also gives a new view on what these special equations have in common. The second question is still more open; as we shall see certain features of\pageoriginale the equation must be just right to make progress.

This programme was carried through by R. Rosales and his account will appear in Ref. \cite{key18}.

We start with Burgers' equation, since a nice feature is that these earlier cases can be included and contrasted with the later ones.

\section{Burgers' and Thomas's equations}\label{chap8:sec8.3}

We shall establish all the ideas and notation on this case so the account may appear long for a `simple' method.

We write the equation as 
\begin{equation}
\eta_t-\eta_{xx}= -\eta\eta_x,\tag{8.16}\label{chap8:eq8.16}
\end{equation}
and we begin with an expansion in terms of a small parameter $\epsilon$ in the form
\begin{equation}
\eta=\sum\limits_1^\infty \epsilon^n\eta^{(n)}(x,t). \tag{8.17}\label{chap8:eq8.17}
\end{equation}

After substitution we read off the usual hierarchy
\begin{gather}
\eta_t^{(1)}-\eta_{xx}^{(1)}=0,\tag{8.18}\label{chap8:eq8.18}\\
\eta_t^{(n)}-\eta_{xx}^{(n)}=-\sum\limits_{j=1}^{n-1}\eta^{(j)}\eta_x^{(n-j)}, \tag{8.19}\label{chap8:eq8.19}
\end{gather}
to be solved successively. At each stage the right side of \eqref{chap8:eq8.19} is known so that only the solution of the inhomogeneous heat equation is ever involved. 

Of course, experience suggests that such a simple approach will\pageoriginale prove inadequate in some way, and the art of perturbation approaches is in learning how to correct the definciencies.

The difficulty in this case is easily seen by considering the shock wave solution of \eqref{chap8:eq8.16}. This solution is the counterpart of \eqref{chap8:eq8.13} and is found in similar fashion. It may be written
\begin{equation}
\eta =\frac{e^{-\alpha x+\alpha^2 t}}{1+\frac{\epsilon}{2\alpha}e^{\alpha x+\alpha^2 t}} \tag{8.20}\label{chap8:eq8.20}
\end{equation}
where a parameter $\epsilon$ has been include to compare with \eqref{chap8:eq8.17}. If \eqref{chap8:eq8.20} is formally expanded in a power series in $\epsilon$, we have 
\begin{equation}
\eta=\epsilon e^{-\alpha x+\alpha^2 t}-\frac{\epsilon^2}{2\alpha}e^{-2\alpha x+2\alpha^2 t}+\cdots \tag{8.21}\label{chap8:eq8.21}
\end{equation}

It may be verified that this is the solution of \eqref{chap8:eq8.19} starting from the special case
$$
\eta^{(1)}= e^{-\alpha x+\alpha^2 t}.
$$

We immediately see the limitation on \eqref{chap8:eq8.21}; it converges only for 
$$
\left|\epsilon e^{-\alpha x+\alpha^2 t}\right|<1,
$$
\ie for sufficiently large $x$. But we also see that the $\epsilon$ is spurious. It could be absorbed by replacing $x$ by 
$$
x-\frac{1}{\alpha}\log \epsilon.
$$

Since this is a trivial change in origin, the $\epsilon$ plays no real role. It serves only to suggest the ordering of terms in \eqref{chap8:eq8.19}. We may set $\epsilon =1$. The real issue is to sum the series \eqref{chap8:eq8.21} in the form \eqref{chap8:eq8.20}, so that the perturbation series valid only for sufficiently large $x$ is extended to be valid for all $x$. This is like finding an\pageoriginale analytic continuation. In this particular case we need only sum
\begin{gather*}
z-z^2+z^3-\ldots\\
\intertext{as}
\frac{z}{1+z},
\end{gather*}
and we have the exact solution.

We now examine the general case starting with $\eta^{(1)}$ as the general solution of the heat equation \eqref{chap8:eq8.18}. Although we are not usually interested in real exponential solutions of linear equations (since they are unbounded), we see from \eqref{chap8:eq8.20} and \eqref{chap8:eq8.21} that they are crucially important in the present context; the final form in \eqref{chap8:eq8.20} is bounded. Indeed one important case would be 
\begin{equation}
\eta^{(1)}(x,t)=\sum\limits_{j=1}^N a_j e^{-\kappa_j x+\kappa_j^2 t}; \tag{8.22}\label{chap8:eq8.22}
\end{equation}
this would lead to the interaction of shocks. But we are also interested in the usual Fourier integral solution
\begin{equation}
\eta^{(1)}(x,t)=\int\limits_{-\infty}^\infty e^{ikx-k^2 t}F(k)\,dk. \tag{8.23}\label{chap8:eq8.23}
\end{equation}

To include both and to avoid the display of too many summations as in \eqref{chap8:eq8.22}, we use the notation
\begin{equation}
\eta^{(1)}(x,t)=\int e^{-\alpha x+\alpha^2 t}\,d\lambda(\alpha). \tag{8.24}\label{chap8:eq8.24}
\end{equation}

This should not conjure up worries of any deep measure theory. It merely means sum over any distribution of $\alpha$ with appropriate weights. In \eqref{chap8:eq8.22} the sum is over 
\begin{align}
& \alpha =\kappa_1,\kappa_2,\ldots,\kappa_N,\tag{8.25}\label{chap8:eq8.25}\\
\intertext{with weights}
& a_1,a_2,\ldots,a_N;\tag{8.26}\label{chap8:eq8.26}
\end{align}\pageoriginale
in \eqref{chap8:eq8.23} the sum is over $-i\infty <\alpha < i\infty$, with $\alpha =ik$ and weighting function $F(k)\,dk$. The general form would include both. We shall further write \eqref{chap8:eq8.24} as 
\begin{align}
& \eta^{(1)}=\int e^\Omega\,d\lambda(\alpha),\tag{8.27}\label{chap8:eq8.27}\\
\intertext{where}
& \Omega = -\alpha x+\alpha^2 t,\tag{8.28}\label{chap8:eq8.28}
\end{align}
and after a while drop the $d\lambda$ altogether.

For the successive equations for the $\eta^{(n)}(x,t)$, it is clear that one may take solutions in the form of $n$-fold integrals 
\begin{equation}
\eta^{(n)}(x,t)=\int\ldots\int\phi^{(n)}\left(\alpha_1,\ldots,\alpha_n\right) e^{\Omega_1+\cdots\Omega_n}\,d\lambda(\alpha_1)\ldots d\lambda(\alpha_n), \tag{8.29}\label{chap8:eq8.29}
\end{equation}
where each $\Omega_j$ has the corresponding $\alpha_j$. For example, 
\begin{align*}
\eta_t^{(2)}-\eta_{xx}^{(2)} &= -\eta^{(1)}\eta_x^{(1)}\\
& = +\int\int\alpha_2 e^{\Omega_1+\Omega_2}\,d\lambda(\alpha_1)\,d\lambda (\alpha_2),
\end{align*}
using the dummy variable $\alpha_1$ for the first factor $\eta^{(1)}$ and dummy variable $\alpha_2$ in the second factor $\eta_x^{(1)}$ Then, using \eqref{chap8:eq8.29} with $n=2$, we have 
\begin{equation}
\left\{\alpha_1^2+\alpha_2^2-\left(\alpha_1+\alpha_2\right)^2\right\}\phi^{(2)} =\alpha_2.\tag{8.31}\label{chap8:eq8.31}
\end{equation}

Therefore,\pageoriginale
\begin{equation}
\phi^{(2)}=-\frac{1}{2\alpha_1}.\tag{8.32}\label{chap8:eq8.32}
\end{equation}

For $\eta^{(3)}$, we have 
\begin{align*}
\eta_t^{(3)}-\eta_{xx}^{(3)} &= -\eta^{(1)}\eta_x^{(2)}-\eta^{(2)}\eta_x^{(1)}\\
&= -\int\int\int\left\{\frac{\alpha_2+\alpha_3}{2\alpha_2}+\frac{\alpha_3}{2\alpha_1}\right\}e^{\Omega_1+\Omega_2+\Omega_3}\,d\lambda_1\,d\lambda_2\,d\lambda_3. \tag{8.33}\label{chap8:eq8.33}
\end{align*}

In the first pair, $\alpha_1$ is used for $\eta^{(1)}$, and $\alpha_2,\alpha_3$ are used in $\eta_x^{(2)}$. In the second pair, $(\alpha_1,\alpha_2)$ are used in $\eta^{(2)}$, and $\alpha_3$ in $\eta_x^{(1)}$. This preservation of symmetry in the right hand side is important. Now, using \eqref{chap8:eq8.29} with $n=3$, we have 
\begin{equation}
\left\{\alpha_1^2+\alpha_2^2+\alpha_3^2-\left(\alpha_1+\alpha_2+\alpha_3\right)^2 \right\}\phi^{(3)}=- \frac{\alpha_1\alpha_2+\alpha_1\alpha_3+\alpha_2\alpha_3} {2\alpha_1\alpha_2}.\tag{8.34}\label{chap8:eq8.34}
\end{equation}

Therefore,
\begin{equation}
\phi^{(3)}=\frac{1}{2^2\alpha_1\alpha_2}.\tag{8.35}\label{chap8:eq8.35}
\end{equation}

At this stage, or after one more iteration, one can compare \eqref{chap8:eq8.27}, \eqref{chap8:eq8.32}, \eqref{chap8:eq8.35} and suggest the general form 
\begin{equation}
\phi^{(n)}\left(\alpha_1,\ldots,\alpha_n\right)= \frac{(-1)^{n-1}} {2^{n-1}\alpha_1 \cdots\alpha_{n-1}}.\tag{8.36}\label{chap8:eq8.36}
\end{equation}

Accepting this for the present, the series solution is 
\begin{equation}
\eta(x,t)=\sum\limits_1^\infty\frac{(-1)^{n-1}}{2^{n-1}}\int\ldots\int \frac{e^{\Omega_1+\cdots+\Omega_n}}{\alpha_1\alpha_2\cdots\alpha_{n-1}}\,d\lambda (\alpha_1)\ldots d\lambda(\alpha_n).\tag{8.37}\label{chap8:eq8.37}
\end{equation}

Now\pageoriginale this series solution is the general counterpart to \eqref{chap8:eq8.21} and is limited in the same way: it will only be valid for sufficiently large $x$. The crux of the matter is whether it can be summed to give a uniformly valid solution for all $x$. But we see that it can. The multiple integral is in fact a product of integrals. If we define 
\begin{gather}
B(x,t)=\int\frac{1}{2}e^\Omega\,d\lambda(\alpha),\quad\text{satisfying}\quad B_t-B_{xx}=0,\tag{8.38}\label{chap8:eq8.38}\\
\intertext{then}
\eta(x,t)=-2\sum\limits_1^\infty(-1)^{n-1}B^{n-1}B_x. \tag{8.39}\label{chap8:eq8.39}
\end{gather}

We have the same simple series as before and it is immediately summed to give
\begin{equation}
\eta=-2\frac{B_x}{1+B}=-2\left\{\log(1+B)\right\}_x. \tag{8.40}\label{chap8:eq8.40}
\end{equation}

Equation \eqref{chap8:eq8.38} and \eqref{chap8:eq8.40} provide exactly the Cole-Hopf transformation \eqref{chap8:eq8.3} with $v=1+B!$ We believe this is the first derivation of the Cole-Hopf transformation.

In the case of Thomas's equation \eqref{chap8:eq8.4}, the perturbation series is found to be 
\begin{equation}
u=\sum\limits_1^\infty\frac{(-1)^{n-1}}{n}\int\ldots\int e^{\Omega_1+\cdots+\Omega_n}\,d\lambda(\alpha_1)\ldots d\lambda(\alpha_n), \tag{8.41}\label{chap8:eq8.41}
\end{equation}
this time with
\begin{equation}
\left.
\begin{aligned}
& \Omega =\alpha x+\beta y\\
& \alpha\beta+p\alpha+q\beta =0,
\end{aligned}
\right\}\tag{8.42}\label{chap8:eq8.42}
\end{equation}
to fit the different linear part. Thus, with 
\begin{equation}
B(x,t)=\int e^\Omega\,d\lambda(\alpha),\tag{8.43}\label{chap8:eq8.43}
\end{equation}\pageoriginale
we have
\begin{align*}
u &= \sum\limits_1^\infty\frac{(-1)^{n-1}}{n}\\
&= \log(1+B).
\end{align*}

And, from \eqref{chap8:eq8.43} and \eqref{chap8:eq8.42}, $B$ satisfies
\begin{equation}
B_{xy}+pB_x+qB_y=0.\tag{8.44}\label{chap8:eq8.44}
\end{equation}

This is Thomas's transformation.

We observe the two key features in both cases: (1) $\eta^{(n)}$ factors into a product, and (2) the resulting series is easily summed.

Detailed verification of \eqref{chap8:eq8.36} will not be given, since it has clearly led to the correct results. We prefer to leave the detailed study to the corresponding steps in the more interesting case of the KdV equation. It should also be stressed that the success of the method is in spotting the form of the $\phi^{(n)}$ after the first few values have been found. Once the general form is strongly indicated, its proof depends on proving an algebraic identity among the $\alpha_1,\ldots,\alpha_n$, and is standard (if not always obvious). Thus, it is more important to add a few further remarks on deducing \eqref{chap8:eq8.35} than on proving the general form.

The common factor
$$
\alpha_1\alpha_2+\alpha_1\alpha_3+\alpha_2\alpha_3
$$
is cancelled through in \eqref{chap8:eq8.34} because the symmetry on the right of \eqref{chap8:eq8.33}\pageoriginale was carefully preserved. To illustrate what is involved, suppose the parameter $\alpha_1$ is used for $\eta^{(1)}$ and parameters $(\alpha_2,\alpha_3)$ in $\eta^{(2)}$ for both terms. Then we should have 
$$
\frac{\alpha_2+\alpha_3+\alpha_1}{2\alpha_2}
$$
as the factor on the right. One then has to spot that 
$$
\frac{\alpha_1\alpha_2+\alpha_1\alpha_3+\alpha_2\alpha_3}{2\alpha_1\alpha_2}= \frac{1}{2}+\frac{\alpha_3}{2\alpha_2}+\frac{\alpha_3}{2\alpha_1}
$$
is an equivalent form under the triple integrals, since the $\alpha's$ are just dummy variables and $\alpha_3/2\alpha_1$ can be relabelled $\alpha_1/2\alpha_2$. 

\section{Korteweg-de Vries equation}\label{chap8:sec8.4}

It simplifies things, but is not essential, to introduce $\eta=-12\psi_x$, integrate once with respect to $x$, and work with 
\begin{equation}
\psi_t+\psi_{xxx}=6\psi_x^2,\tag{8.45}\label{chap8:eq8.45}
\end{equation}
a form that has often been used for other purposes. (The factor 6 is pure convenience to avoid powers of 6 appearing in later expressions).

The perturbation series is
\begin{align*}
& \psi=\sum\limits_1^\infty\psi^{(n)}.\\
& \psi_t^{(n)}+\psi_{xxx}^{(n)}=6\sum\limits_{j=1}^{n-1}\psi_x^{(j)}\psi_x^{(n-j)}. \tag{8.46}\label{chap8:eq8.46}
\end{align*}

To see the nature of the problem one might again check the situation for\pageoriginale the simplest solution, nemely the single solitary wave \eqref{chap8:eq8.13}. If we start with
$$
\psi^{(1)}= e^{-\alpha x+\alpha^3 t}=\alpha P,\quad\text{say},
$$
it is easily found that the series \eqref{chap8:eq8.46} is 
$$
\psi =\alpha\sum\limits_1^\infty(-1)^{n-1}P^n.
$$

The validity is originally limited by convergence, but is immediately summed to 
\begin{equation}
\psi=\frac{\alpha P}{1+P}=-\frac{P_x}{1+P}=-\frac{\partial}{\partial x}\log (1+P).\tag{8.47}\label{chap8:eq8.47}
\end{equation}

This is \eqref{chap8:eq8.13} written in terms of $\psi$. The issue is again to sum the sereis to obtain a solution valid for all $x$. Again the series is no more than $(1+P)^{-1}$. 

For the general solution we start with
\begin{equation}
\psi^{(1)}=\int e^\Omega\,d\lambda(\alpha),\;\Omega=-\alpha x+\alpha^3 t, \tag{8.48}\label{chap8:eq8.48}
\end{equation}
which in our notation is the general solution of the linear equation 
\begin{equation}
\psi_t^{(1)}+\psi_{xxx}^{(1)}=0.\tag{8.49}\label{chap8:eq8.49}
\end{equation}

The change from \eqref{chap8:eq8.27}-\eqref{chap8:eq8.28} is only in the $\Omega$. The expression for $\psi^{(n)}$ will be an $n$-integral \eqref{chap8:eq8.29} as before. The detailed derivation of the coefficient $\phi^{(n)}(\alpha_1,\ldots,\alpha_n)$ is given later so that we can immediately focus on the method and the contrast with the Burgers' and Thomas's cases. The perturbation series is found to be 
\begin{equation}
\psi=\sum\limits_{n=1}^\infty(-1)^{n-1}2^{n-1}\int\ldots\int \frac{e^{-\alpha_1 x+\alpha_1^3 t} e^{-\alpha_2 x+\alpha_2^3 t}\ldots e^{-\alpha_n x+\alpha_n^3 t}} {\left(\alpha_1+\alpha_2\right)\,\left(\alpha_2+\alpha_3\right)\ldots\left( \alpha_{n-1}+\alpha_n\right)},\tag{8.50}\label{chap8:eq8.50}
\end{equation}
and\pageoriginale the $d\lambda(\alpha_1)\,d\lambda(\alpha_2)\ldots d\lambda (\alpha_n)$ is not displayed in this already long expression. In this case the $n$-fold integral does not split into a product, but the successive integrals are only linked pairwise through the factors $(\alpha_j+\alpha_{j+1})$ in the denominator. It is the second class in the order of complexity. Burgers' and Thomas's cases have complete factorization; the coefficient $\phi^{(n)}$ takes the form $f(\alpha_1)f(\alpha_2)\ldots f(\alpha_n)$, although with very simple cases $f\propto 1/\alpha, f\propto 1$. In \eqref{chap8:eq8.50} we have the next class where the factorization is pairwise.
\begin{equation}
f\left(\alpha_1,\alpha_2\right)f\left(\alpha_2,\alpha_3\right)\ldots f\left(\alpha_{n-1},\alpha_n\right),\tag{8.51}\label{chap8:eq8.51}
\end{equation}
again with very simple $f$.

To bring out the pairwise linkage more strongly, we may incorporate the exponentials into the scheme by splitting 
\begin{align*}
& e^{-\alpha_jx+\alpha_jt}\\
\intertext{into}
&e^{-\frac{1}{2}\alpha_jx+\frac{1}{2}\alpha_j^3 t}.e^{-\frac{1}{2}\alpha_jx+\frac{1}{2}\alpha_j^3 t}
\end{align*}
and combining the first with $j-1$ and the second with $j+1$. Part will be left over at beginning and end, but we may write 
\begin{equation}
\psi=\sum\limits_{n=1}^\infty(-1)^{n-1} p (\alpha_1)P\left(\alpha_1,\alpha_2\right) P\left(\alpha_2,\alpha_3\right)\ldots P\left(\alpha_{n-1},\alpha_n\right)P (\alpha_n),\tag{8.52}\label{chap8:eq8.52}
\end{equation}
where
\begin{equation}
\begin{aligned}
p(\alpha) &= e^{-\frac{1}{2}\alpha x+\frac{1}{2}\alpha^3 t,}\\
P(\alpha,\beta) &= 2 \frac{e^{-\frac{1}{2}\alpha x+\frac{1}{2}\alpha^3 t}.e^{-\frac{1}{2} \beta x+\frac{1}{2}\beta^3 t}}{\alpha +\beta}
\end{aligned}\tag{8.53}\label{chap8:eq8.53}
\end{equation}

\begin{align}
\ie &P(\alpha,\beta)=\frac{2p(\alpha)p(\beta)}{\alpha +\beta} \tag{8.54}\label{chap8:eq8.54}\\
& \frac{\partial p}{\partial x}(\alpha,\beta)=-p(\alpha)p(\beta), \tag{8.55}\label{chap8:eq8.55}
\end{align}\pageoriginale
and we use a
\medskip

{\bf\Large Summation Convention:}

Repeated $\alpha's$ are to be integrated $\int\,d\lambda(\alpha)$.

\section{Discrete set of $\alpha's$ interacting solitary waves}\label{chap8:sec8.5}

We now consider the special case \eqref{chap8:eq8.25}-\eqref{chap8:eq8.26} where the $\alpha's$ range over a discrete set 
$$
\alpha=\kappa_1,\kappa_2,\ldots,\kappa_N,
$$
and the integrals are in fact sums. Then each $\alpha_m$ ranges over 
\begin{equation}
\alpha_m = \kappa_{i_m},i_m=1,\ldots,N.\tag{8.56}\label{chap8:eq8.56}
\end{equation}

If we let 
\begin{align*}
p_i &= a_i e^{-\frac{1}{2}\kappa_ix+\frac{1}{2}\kappa_it}\\
P_{ij} &= 2a_ia_j\frac{e^{-\frac{1}{2}\kappa_ix+\frac{1}{2}\kappa_i^3t}.e^{-\frac{1}{2} \kappa_jx+\frac{1}{2}\kappa_j^3t}}{\kappa_i+\kappa_j} \tag{8.57}\label{chap8:eq8.57}\\
&= 2\frac{p_ip_j}{\kappa_i+\kappa_j}, \tag{8.58}\label{chap8:eq8.58}\\
\end{align*}
then \eqref{chap8:eq8.52} may be written 
\begin{equation}
\psi=\sum\limits_1^\infty(-1)^{n-1}p_{i_1 i_2}P_{i_1 i_2}P_{i_1 i_2}\ldots P_{i_{n-1}i_n} P_{i_n},\tag{8.59}\label{chap8:eq8.59}
\end{equation}
where each $i_m$ is summed over $1,\ldots,N$, according to \eqref{chap8:eq8.56}. But this summation convention is now the usual one and \eqref{chap8:eq8.59} involves just ordinary matrix products. In matrix form, \eqref{chap8:eq8.59} is written 
\begin{equation}
\psi=\sum\limits_1^\infty(-1)^{n-1} p^TP^{n-1}p,\tag{8.60}\label{chap8:eq8.60}
\end{equation}
where\pageoriginale $p^T$ is the transpose (row vector) of the column vector $p$. Now, the crucial step is to sum \eqref{chap8:eq8.60} in the matrix form 
\begin{equation}
\psi=p^T(1+P)^{-1}p,\tag{8.61}\label{chap8:eq8.61}
\end{equation}
in order to extend the validity to all $x$. This is an acceptable form, but it can also be manipulated into a more convenient one. First 
\begin{align*}
p^T Ap &= p_iA_{ij}p_j =A_{ij}p_jp_i\\
&= \text{Trace}\quad App^T;
\end{align*}

Therefore 
$$
\psi =\text{Trace}\quad\left\{(1+P)^{-1}pp^T\right\}.
$$

Then, from \eqref{chap8:eq8.55} or \eqref{chap8:eq8.57}-\eqref{chap8:eq8.58}, 
$$
pp^T=-\frac{\partial P}{\partial x};
$$
therefore
\begin{align*}
\psi &= Tr\left\{-(1+P)^{-1}P_x\right\}\\
& = -\frac{\partial}{\partial x}Tr\left\{\log(1+P)\right\}.
\end{align*}

Finally for any matrix $A$
$$
Tr\log A=\log\det|A|;
$$
this is trivially true for a diagonal matrix and any symmetric matrix can be made diagonal by a similarity transformation. Therefore
\begin{equation}
\psi =-\frac{\partial}{\partial x}\log\det |1+P|.\tag{8.61}\label{chap8:eq8.61'}
\end{equation}

With $\eta =-12\psi_x$, this is \eqref{chap8:eq8.14}.

\section{Continuous range; Marcenko integral equation}\label{chap8:sec8.6}\pageoriginale

In the case when $\alpha$ has a continuous range over $-i\infty$ to $i\infty$, \ie we start with a Fourier integral in \eqref{chap8:eq8.48}, then formally at least \eqref{chap8:eq8.52} could still be written
$$
\psi=\sum\limits_{n=1}^\infty (-1)^{n-1}p(\alpha)P^{n-1}(\alpha,\beta)p(\beta),
$$
with the understanding that powers and products have to be interpreted with the summation $\int d\lambda$. If we define
\begin{equation}
(1+P)^{-1}=\sum\limits_{n=1}^\infty(-1)^{n-1}P^{n-1}, \tag{8.62}\label{chap8:eq8.62}
\end{equation}
then formally
$$
\psi=p^T(1+P)^{-1}p,
$$
but any practical use would require some interpretation of the operator $(1+P)^{-1}$ other than the series in \eqref{chap8:eq8.62}. There seems to be no immediate analogue of \eqref{chap8:eq8.61}.

In fact explicit solutions corresponding to this case have not been found by any method. However, the problem can be reduced to a linear integral equation, which is useful for various quesions, such as asymptotics for $t\to\infty$. This can be found most easily from \eqref{chap8:eq8.50} by writing the terms as products in a different way.

In Fourier integrals it is natural to associate additional factors in $\alpha$ with operations with respect to $x$. For example if 
\begin{equation}
B(x)=\int e^{-\alpha x+\alpha^3 t}\,d\lambda(\alpha),\tag{8.63}\label{chap8:eq8.63}
\end{equation}
then
$$
\int\limits_x^\infty B(z)\,dz=\int\frac{1}{\alpha}e^{-\alpha x+\alpha^3 t}\,d\lambda (\alpha).
$$\pageoriginale

But in \eqref{chap8:eq8.50}, we have the pairwise linkage to contend with. However, if the exponentials are split into two halves, as before, and the integration performed on the successive pairs
$$
e^{-\frac{1}{2}\left(\alpha_j+\alpha_{j+1}\right)x},
$$
we obtain the required factors. This leads to 
\begin{align}
\psi&=\sum\limits_1^\infty(-1)^{n-1}\int\limits_x^\infty \ldots
\int\limits_x^\infty
B\left(\frac{x+z_1}{2}\right)B\left(\frac{z_1+z_2}{2}\right)\ldots\nonumber\\
&\qquad\qquad\qquad B\left(\frac{z_{n-1}+x}{2}\right)\,dz_1\cdots dz_{n-1},
\tag{8.64}\label{chap8:eq8.64} 
\end{align}
where $B(x)$ is given by \eqref{chap8:eq8.63}. (The dependence of $B$ on $t$ is not displayed).

Now \eqref{chap8:eq8.64} is another type of product. To bring this out, we must define multiplication in some way as ``multiplication by $B$ and integration $\int_x^\infty$''. First in order to keep clear the different arguments of $B$ in \eqref{chap8:eq8.64}, we temporarily write $B(\frac{x+y}{2})$ as $B(x,y)$. Then to introduce the appropriate multiplication we define the operator $\hat{B}$ acting on functions $f(x,y)$ by 
\begin{equation}
\hat{B}f(x,y)=\int\limits_x^\infty f(x,y)B(z,y)\,dz. \tag{8.65}\label{chap8:eq8.65}
\end{equation}

Then \eqref{chap8:eq8.64} may be written 
\begin{equation}
\psi=\left[\sum\limits_{n=1}^\infty(-1)^{n-1}\hat{B}^{n-1}B(x,y)\right]_{y=x}. \tag{8.66}\label{chap8:eq8.66}
\end{equation}

Notice\pageoriginale that two space-like variables come in automatically in this view, and\footnote{The $t$-dependence is suppressed throughout these manipulations, so that $K$, like $B$, is in fact a funcion of $x,y,t$.}
$$
\psi(x)=K(x,x)
$$
where
\begin{equation}
K(x,y)=\sum\limits_1^\infty (-1)^{n-1}\hat{B}^{n-1}B(x,y). \tag{8.67}\label{chap8:eq8.67}
\end{equation}

The appearance of the extra dimension is a crucial step; in the original inverse scattering methods it arises due to the associated scattering problem. Here it arises in writing \eqref{chap8:eq8.64} precisely as a product.

Formally, we would sum \eqref{chap8:eq8.67} as 
\begin{equation}
K(x,y)=(I+ \hat{B})^{-1}B(x,y).\tag{8.68}\label{chap8:eq8.68}
\end{equation}

Then, again, practical use would require interpretations of the operator $(I+\hat{B})^{-1}$ other than by the series. However it would follow from \eqref{chap8:eq8.68} by applying $I+\hat{B}$ to both sides that 
\begin{equation}
(I+\hat{B})K(x,y)=B(x,y).\tag{8.69}\label{chap8:eq8.69}
\end{equation}

This can be justified (avoiding use of \eqref{chap8:eq8.68} and the definition of $(I+\hat{B})^{-1}$ by applying the well-defined operator $I+\hat{B}$ directly to \eqref{chap8:eq8.67}; the only assumption is that \eqref{chap8:eq8.67} converges for sufficiently large $x$. From the definition of $\hat{B}$, \eqref{chap8:eq8.69} is 
\begin{equation}
K(x,y)+\int\limits_x^\infty K(x,z)B(z,y)\,dz=B(x,y). \tag{8.70}\label{chap8:eq8.70}
\end{equation}

This\pageoriginale is the Marcenko integral equation. So the final prescription is to take a general solution \eqref{chap8:eq8.63} of the linear equation 
$$
B_t+B_{xxx}=0,
$$
solve \eqref{chap8:eq8.70} for $K(x,y,t)$, then
$$
\psi(x,t)=K(x,x,t).
$$

\section{The series solution}\label{chap8:sec8.7}

The derivation of the terms in \eqref{chap8:eq8.50} is now given in detail, since by this approach everything depends on the factored (pairwise) form. As explained in connection with Burgers' equation, we have 
\begin{equation}
\psi^{(n)}(x,t)=\int\ldots\int\phi^{(n)}\left(\alpha_1,\ldots,\alpha_n\right) e^{\Omega_1+\cdots+\Omega_n}\,d\lambda_1\cdots d\lambda_n \tag{8.71}\label{chap8:eq8.71}
\end{equation}
where
$$
\Omega_m=-\alpha_mx+\alpha_m^3 t.
$$

The equation for $\psi^{(2)}$ is 
$$
\psi_t^{(2)}+\psi_{xxx}^{(2)}=6\psi_x^{(1)}\psi_x^{(1)};
$$
hence
\begin{gather}
\left\{\alpha_1^3+\alpha_2^3-\left(\alpha_1+\alpha_2\right)^3\right\}\phi^{(2)}= 6\alpha_1\alpha_2,\tag{8.72}\label{chap8:eq8.72}\\
\intertext{and we have}
\phi^{(2)}=-\frac{2}{\alpha_1+\alpha_2}\tag{8.73}\label{chap8:eq8.73}
\end{gather}

The equation for $\psi^{(3)}$ is 
$$
\psi_t^{(3)}+\psi_{xxx}^{(3)}=6\psi_x^{(1)}\psi_x^{(2)}+6\psi_x^{(2)} \psi_x^{(1)};
$$
hence\pageoriginale
\begin{equation}
\left\{\alpha_1^3+\alpha_2^3+\alpha_3^3-\left(\alpha_1+\alpha_2+\alpha_3\right)^3 \right\}\phi^{(3)}=6\left\{2\alpha_1+2\alpha_3\right\}. \tag{8.74}\label{chap8:eq8.74}
\end{equation}

The factor on the left simplifies to 
$$
-3\left(\alpha_2+\alpha_3\right)\,\left(\alpha_3+\alpha_1\right)\,\left(\alpha_1 +\alpha_2\right),
$$
and we have
\begin{equation}
\phi^{(3)} =\frac{2^2}{\left(\alpha_1+\alpha_2\right)\,\left(\alpha_2+\alpha_3\right)} \tag{8.75}\label{chap8:eq8.75}
\end{equation}

It is, surely, now reasonable to propose
\begin{equation}
\phi^{(n)}=\frac{(-1)^{n-1}2^{n-1}}{\left(\alpha_1+\alpha_2\right)\ldots \left( \alpha_{n-1}+\alpha_n\right)},\;n>1.\tag{8.76}\label{chap8:eq8.76}
\end{equation}

On substitution in
$$
\psi_t^{(n)}+\psi_{xxx}^{(n)}=6\sum\limits_{j=1}^{n-1}\psi_x^{(j)}\psi_x^{n-j)}, \;n>1,
$$
we have 
\begin{align*}
& \left\{\alpha_1^3 +\cdots+\alpha_n^3-\left(\alpha_1+\cdots +\alpha_n\right)^3\right\}\phi^{(n)}\\
&\qquad= 3(-1)^n
  2^{n-1}\sum\limits_{j=1}^{n-1}\frac{\left(\alpha_1+\cdots+
    \alpha_j\right)}{\left(\alpha_1+\alpha_2\right)\ldots
    \left(\alpha_{j-1}+\alpha_j\right)}\times\\
&\qquad\qquad\qquad  \frac{\left(\alpha_{j+1}+\cdots
    +\alpha_n\right)}{\left(\alpha_{j+1}+\alpha_{j+2}
    \right)\ldots\left(\alpha_{n-1}+\alpha_n\right)} 
\end{align*}

Thus, to prove \eqref{chap8:eq8.76}, we need to show that 
\begin{align*}
& \left(\alpha_1+\cdots+\alpha_n\right)^3-\left(\alpha_1^3+\cdots+\alpha_n^3 \right)\\
&\qquad =3\sum_{j=1}^{n-1}\left(\alpha_1+\cdots+\alpha_j\right)\,\left(\alpha_j+ \alpha_{j+1}\right)\,\left(\alpha_{j+1}+\cdots+\alpha_n\right) \tag{8.77}\label{chap8:eq8.77}
\end{align*}

The sum on the right hand side is equal to 
\begin{align*}
&\sum\left(\alpha_1+\cdots+\alpha_{j-1}\right)\alpha_j\left(\alpha_{j+1}+\cdots+ \alpha_n\right)\\
&\qquad +\sum\alpha_j^2\left(\alpha_{j+1}+\cdots+\alpha_n\right)\\
&+\sum\left(\alpha_1+\cdots+\alpha_j\right)\alpha_{j+1}\left(\alpha_{j+2}+\cdots+ \alpha_n\right)\\
&+\sum\left(\alpha_1+\cdots+\alpha_j\right)\alpha_{j+1}^2\\
&=2\sum\limits_{k>\ell >m}\alpha_k\alpha_\ell\alpha_m+\sum\limits_{k\neq\ell} \alpha_k^2\alpha_\ell.
\end{align*}\pageoriginale

The expression on the left of \eqref{chap8:eq8.77} is 3 times this, so the result follows.

\section{Other equations}\label{chap8:sec8.8}

The series approach goes through in a similar way for the other equations noted in \eqref{chap8:eq8.6}-\eqref{chap8:eq8.9}. A common feature is that the nth term always displays the pairwise linking in the integral noted in \eqref{chap8:eq8.51}, and takes the form
\begin{equation}
\int\ldots\int f\left(\alpha_1,\alpha_2\right)\ldots f\left(\alpha_{n-1}, \alpha_n\right)e^{\Omega_1+\cdots+\Omega_n}\,d\lambda(\alpha_1)\ldots d\lambda (\alpha_n).\tag{8.88}\label{chap8:eq8.88}
\end{equation}

In the different cases, the quantity $\Omega$ always corresponds to the linear part of the equation, such that 
$$
e^\Omega=e^{-\alpha x+\beta(\alpha)t}
$$
is a solution. Thus $\beta=\beta(\alpha)$ is essentially the linear dispersion relation (usually written $\omega=\omega(\alpha)$ with $\alpha=-ik, \beta=-i\omega$). It is surprising that in all cases except the Boussinesq equation \eqref{chap8:eq8.9}, $f(\alpha_1,\alpha_2)$ is just
\begin{equation}
\frac{1}{\alpha_1+\alpha_2}\tag{8.89}\label{chap8:eq8.89}
\end{equation}\pageoriginale
and even for \eqref{chap8:eq8.9} takes the form
\begin{equation}
\frac{1}{a_1+b_2}\tag{8.90}\label{chap8:eq8.90}
\end{equation}
where $a_1$ is simply related to $\alpha_1$ and $b_2$ is related to $\alpha_2$. The pairwise linking in \eqref{chap8:eq8.88} classifies a whole group of problems, and for example the matrix form in \eqref{chap8:eq8.59} would go through for any $f$. It would seem surprising if only such special cases as \eqref{chap8:eq8.89} and \eqref{chap8:eq8.90} were the only ones of real interest.

A second point in these examples is that the series to be summed is just the expansion of $(1+P)^{-1}$ or $(1+P^2)^{-1}$ or some slight variant. Again it would be surprising if these very simple series were the only ones of relevance. But the extent of these methods, as well as the possibility mentioned earlier of summing the crucial part of the series to give a satisfactory approximation, is still not known.

In the direction of classification, the next group would involve factors $f$ in \eqref{chap8:eq8.88} depending on trios of $\alpha's$, but it is not clear that such cases occur or what one could do with them.

\addcontentsline{toc}{chapter}{Bibliography}
\begin{thebibliography}{99}\pageoriginale
\bibitem{key1} WHITHAM, G.B. (1974), \emph{Linear and nonlinear waves}. Wiley-Interscience, New York.

\bibitem{key2} TITCHMARSH, E.C. (1962), \emph{Eigenfunction expansions}. Clarendon Press, Oxford.

\bibitem{key3} TAYLOR, G.I. (1921), Tides in the Bristol channel, \emph{Proc. Camb. Phil. Soc. 20, 320--325} (also in \emph{The Scientific papers of G.I. Taylor}, Cambridge, 1960, Vol.~2, 185--189).

\bibitem{key4} CARRIER, G.F., and H.P. GREENSPAN. (1958), Water waves of finite amplitude on a sloping beach, \emph{Jour. Fluid Mech. 4}, 97--109.

\bibitem{key5} GALVIN, C.J. (1972), Waves breaking in shallow water, \emph{Waves on beaches}, Ed. R.E. Meyer, Academic Press, New York.

\bibitem{key6} MUNK, W.H. and WIMBUSH, (1969), \emph{Oceanology 9}, 56--59.

\bibitem{key7} WHITHAM, G.B. (1958), On the propagation of shock waves through regions of non-uniform area or flow, \emph{Jour. Fluid. Mech. 4}, 337--360.

\bibitem{key8} KELLER, H.B., D.A. LEVINE and G.B. WHITHAM, (1960), Motion of a bore over a sloping beach, \emph{Jour. Fluid. Mech. 7}, 302--316.

\bibitem{key9} SACHDEV, P.L. and V.S. SESHADRI, (1976), \emph{Jour. Fluid. Mech. 78}, 481--487.

\bibitem{key10} STOKER, J.J. (1957), \emph{WATER WAVES}, Interscience, New York.

\bibitem{key11} HANSON, E.T. (1926), The theory of Ship waves. \emph{Proc. Royal Soc. A. 111}, 491--529.

\bibitem{key12} FRIDRICHS, K.O. (1948) Water waves on a shallow sloping beach. \emph{Comm. Pure and Appl. Math. 1}, 109--134.

\bibitem{key13} PETERS, A.S. (1952) Water waves over sloping beaches and the solution of a mixed boundary value problem for $\Delta^2\phi-k^2\phi=0$ in a sector. \emph{Comm. Pure and Appl. Math. 5}, 87--108.

\bibitem{key14} URSELL,\pageoriginale F. (1952), Edge Waves on a sloping beach. \emph{Proc. Royal Soc. A. 214}, 79--97.

\bibitem{key15} MINZONI, A.A. (1978), Private communication.

\bibitem{key16} KORTEWEG, D.J. and G. de VRIES. (1895), On the change of form of long waves advancing in a rectangular canal and on a new type of long stationary waves, \emph{Philosophical Magazine (5) 39}, 422--443.

\bibitem{key17} GARDNER, C.S., J.M. GREENE., M.D. KRUKSAL., and R.M. MIURA. (1967) Method for solving the Korteweg - de Vries equation. \emph{Physical Review Letters 19}, 1095--1097.

\bibitem{key18} ROSALES, R.R. (1978), to appear in \emph{Studies in Applied Mathematics}. 
\end{thebibliography}

