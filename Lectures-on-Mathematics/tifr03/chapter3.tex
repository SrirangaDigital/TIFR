 \chapter{Reduction Theory of Positive Definite Quadratic Forms}%cha 3	
 
 By the\pageoriginale reduction of a positive definite quadratic from
 we shall 
 understand the reduction of the corresponding matrix $Y = Y^{(n)}
 > 0$. Investigations in this direction are needed if we wish to
 construct a suitable fundamental domain for the modular group of
 degree $n$ in $\mathscr{Y}$. Let $\mathscr{Y}$ be the
 domain of all real symmetric matrices $Y $ and $\mathfrak{P}$ the
 domain of all $Y > 0$ in $\mathscr{Y}$. Two matrices $Y, Y_1
 \in \mathscr{Y}$ are said to be equivalent if $Y_1 = Y [u] =
 u' Y u$ for some unimodular matrix $u$. We consider only the classes
 of equivalent positive definite matrices, and the theory of reduction
 consists in fixing in each class a typical element, called a
 \textit{reduced matrix}, satisfying certain extremal properties.  
 
 Let $Y >0$ be a given matrix in $\mathscr{Y}$. Choose a primitive
 column $\tilde{u}_1$ such that $Y [\tilde{u}_1] = \tilde{u}_1' Y
 \tilde{u}_1$ is a minimum for all primitive (integral) columns. Such
 a $\tilde{u}_1$ clearly exists. Consider now all integral column
 vectors $\tilde{u}$ such that $(\tilde{u}_1 \tilde{u})$  is primitive
 and choose $\tilde{u}= \tilde{u}_2$ so that $ Y [\tilde{u}_2]$ is a
 minimum. We can further assume that $\tilde{u}_1 Y \tilde{u}_2 \ge 0$
 as otherwise $- \tilde{u}_2$ will serve the role of $\tilde{u}_2$ and
 satisfy this. Continuing in this way, let
 $\tilde{u}_1,\tilde{u}_2,\ldots, \tilde{u}_r$  be already determined
 so that in particular $ (\tilde{u}_1,\tilde{u}_2 \dots \tilde{u}_r)$
 is primitive. Then we choose $\tilde{u}_{r+1}$ such that  
 \begin{enumerate}[i)]
\item $\tilde{u}'_r y \tilde{u}_{r+1} \ge 0$

\item $(\tilde{u}_1 \tilde{u}_2 \dots \tilde{u}_{r+1})$ is primitive
  and  

\item $y [u_{r+1}]$ is a minimum among those $\tilde{u}_{r+1}$
  satisfying (i) and (ii). 
 \end{enumerate} 
 
 In this way we obtain a unimodular matrix $u= (\tilde{u}_1,
 \tilde{u}_2 \dots \tilde{u}_n)$ with some extremal properties and we
 call $R = y[u]$ \textit{a reduced matrix} 
 
 We shall\pageoriginale now obtain explicitly the reduction conditions
 on the  elements $r_{\mu \nu}$ of a reduced matrix $R$  
 
 Let $u_{\mathfrak{K}}$ be a unimodular matrix which has the same first
 $(\mathfrak{K}-1)$ columns as $u$. Such an $u_{\mathfrak{K}}$ can be
 represented by   
 $$
  u_{\mathfrak{K}} = u \begin{pmatrix}  E & A \\ 0 & B\end{pmatrix} 
 $$ 
 where $E =E^{({\mathfrak{K}} - 1)}$, $A$ integral and $B$, unimodular.Let
 $\mathscr{Y}_{\mathfrak{K}}$ be the $\mathfrak{K}^{\rm th}$ column of
 $u^{-1} u _{\mathfrak{K}}$. Then the first column of $B$ is formed
 just by the  $(n - \mathfrak{K} +1)$ last elements $g_\mathfrak{K}
 ,g_{\mathfrak{K}+1} \ldots g_n $ of $\mathscr{Y}_{\mathfrak{K}}$ so
 that these elements are coprime. Conversely if $g_{\mathfrak{K}}
 ,g_{\mathfrak{K}+1} \ldots g_n $ by any $ n- {\mathfrak{K} +1}$
 elements which are coprime, there exists a unimodular matrix $B$ with
 these elements precisely constituting the first column and
 consequently, $a$  $u_{\mathfrak{K}}$ too. Since $u
 \mathscr{Y}_\mathfrak{K}$ is the ${\mathfrak{K}^{th}}$ column of
 $u_{\mathfrak{K}}$ we obtain 
 $$
 Y [u \mathscr{Y}_{\mathfrak{K}}] = R [\mathscr{Y}_{\mathfrak{K}}]
 \ge r_{\mathfrak{K} \mathfrak{K}} \equiv r_{\mathfrak{K}},
 \mathfrak{K} = 1,2\ldots ,n; 
 $$
 $$
 \tilde{u}_{\mathfrak{K}}' Y \tilde{u}_{\mathfrak{K}+1} =
 r_{\mathfrak{K}\mathfrak{K} +1} \ge 0,\; \mathfrak{K} = 1,2 \dots
 \eta . 
 $$
 
 This proves

\setcounter{lem}{2}
 \begin{lem} \label{chap3:lem3} %lem 3
In order that $R = (r_{\mu \nu})$ should be a reduced matrix, it
  is necessary and sufficient that  
\begin{equation*}
R [\mathscr{Y}_\mathfrak{K}] \ge r_\mathfrak{K}, r_{\mathfrak{K}
  \mathfrak{K}+1} \ge 0, \; \mathfrak{K} = 1,2 \dots n  \tag{46}\label{eq46} 
\end{equation*}
where $\mathscr{Y}_{\mathfrak{K}}$ denotes an arbitrary integral
  column whose last $n - \mathfrak{K}+1$ elements are coprime. 
 \end{lem} 
 
 While the necessity part has been shown above, the sufficiency is
 immediate since $R = R[E]$ and this is clearly a reduced matrix by
 virtue of the conditions (\ref{eq46}).  
 
 In case $\mathscr{Y}_{\mathfrak{K}} = \pm n_{\mathfrak{K}}$ the
 $\mathfrak{K}^{\rm th}$ unit vector, the equation $\mathfrak{K}
 [\mathscr{Y}_\mathfrak{K}] = \mathfrak{K}_\mathfrak{K}$ holds
 identically in $\mathfrak{K}$ so that one of the condition (\ref{eq46})
 will now be innocuous. We may therefore assume that the
 $\mathscr{Y}_\mathfrak{K}$ in  
Lemma \ref{chap3:lem3}\pageoriginale is different from $ \pm n_{\mathcal{R}}$.

\begin{lem}\label{chap3:lem4} %leem 4
Every reduced matrix $ R = ( r_{\mu \nu }) $ satisfies the
  following in equalities  
\begin{align*}
 &r_\mathcal{R } \leq r_\ell , \mathfrak{K} \leq \ell \tag{47}\label{eq47}  \\
 &r_\ell \leq  2 r_{\mathfrak{K} \ell}  \leq r_\ell ,   \mathfrak{K} >
  \ell \tag{48}\label{eq48}  \\ 
 &r_1 r_2 \ldots r_n  < c_1 \mid R \mid \text{ where } C_1 = C_1 ( n
  ),  \tag{49}\label{eq49}  
\end{align*}
viz. a positive number  depending only on $n$.
\end{lem}

While (\ref{eq47}) is immediate from Lemma \ref{chap3:lem3} by choosing $
\mathscr{Y}_\mathcal{R} = n_\ell ( \ell \ge \mathfrak{K}$  to prove 
(\ref{eq48})  we have only to set $ \mathfrak{Y}_\mathfrak{K} = n_\mathfrak{K}
\pm  k_\ell ( \ell < \mathfrak{K} )$ The proof of (\ref{eq49}) will be by
induction on $n$. Clearly (\ref{eq49}) is true for the case $n = 1$. Let $
R_\ell $ denote the matrix which arises from $R$ by deleting the last
$n -\ell $ rows and  columns. It is clear that  $ R_\ell > 0 $. Then
by Lemma \ref{chap3:lem3}, it will follow that $ R_\ell $ is reduced too. The
induction hypothesis will now  imply that  
\begin{equation*}
 r_1 r_2 \ldots r_{n-1}   < C_2 \mid R_{n-1} \mid,  C_2 = C_2 (n)
 \tag{50}\label{eq50} 
\end{equation*}

We denote by $D_{ \mathfrak{K} \ell } $ the $ (n-2) $ rowed
sub-determinant of $ R_{n-1} $ obtained by deleting the row and column
containing $ e_{\mathfrak{K} \ell } $. Then $ D_{\mathfrak{K} \ell } $
is the sum of  $ ( n-2 ) $! terms of the type $ r_{\nu_1 1}
r_{\nu_2 2} \ldots r_{\nu_{\ell-1} \ell-1} r_{\nu_{\ell+1} \ell+1}
\ldots r_{\nu_{n-1} n-1} $, Majorising each term with the help of
(\ref{eq48}), we get 
$$
\pm D_{\mathfrak{K} \ell } r_\ell < C_3 r_1 r_2 \ldots r_{n-1}, C_3 =
C_3 (n). 
$$

Consequently, in view of (\ref{eq50}) we have 
$$
\pm  D_{\mathfrak{K} \ell} \mid R_{n-1} \mid^{-1}  < C_2 C_3 r^{-1}_{\ell}
$$

We now define $ \mathscr{H}$  by $ \begin{pmatrix} R_{n-1} &
  \mathscr{H} \\\mathscr{H}'& r_n \end{pmatrix} $ 

and set  $ r = r_n - R^{-1}_{n-1} [ \mathscr{H} ]$

Then $ R = $  $ \begin{pmatrix} R_{n-1}& 0 \\0 & r \end{pmatrix} $
\bigg[ $ \begin{pmatrix}  E & R^{-1}_{n-1}\mathscr{H} \\ 0&
    1 \end{pmatrix} $ $ \bigg]  \mid R \mid = r \mid R_{n-1} \mid $. 

Let now\pageoriginale  $ \xi' = ( \mathfrak{z}' x_n ) $  be a row with  $n$
variable elements.  

We have 
\begin{align*} 
 R [ \xi ]  =& \begin{pmatrix} R_{n-1} & 0  \\ 0 & r \end{pmatrix}
 \bigg[   \begin{pmatrix}  E & R^{-1}_{n-1}\mathscr{H} \\ 0&
     1 \end{pmatrix}    \begin{pmatrix} \mathfrak{z} & \\ x_n &
     . \end{pmatrix}      \bigg]   \\ 
 &=   \begin{pmatrix} R_{n-1} & 0 \\ 0 & r \end{pmatrix}
 \bigg[  \begin{pmatrix}  \mathfrak{z} +  R^{-1}_{n-1} \mathscr{H} x n
     \\ x_n \end{pmatrix}  \bigg] \\ 
 &=  R_{n-1} [  \mathfrak{z}  +  R^{-1}_{n-1} \mathscr{H} x_n ] + r
 x^2_n. 
 \end{align*}
 
 The elements of  $ R^{-1}_{n-1} $ are  $ \pm D_{\mathfrak{K} \ell}
 \mid R_{n-1} \mid^{-1} $ so that by virtue of (\ref{eq47}) and  (\ref{eq48})
 and (\ref{eq50}) we have  
 \begin{align*}
 R^{-1}_{n-1} [ \mathscr{H} ] &<  C_2 C_3  \sum \frac{1}{r_\ell} r_{n
   \mathfrak{K}} r_{n \ell} \\ 
 &<  C_4 r_{n-1},  C_4 = C_4 (n) \tag{51}\label{eq51}  
 \end{align*}

 Also $ r_n = r + R^{-1}_{n-1} $ [$ \mathscr{H} $ ] $ < r + C_4
 r_{n-1} $ so that   $ r_1 r_2 \ldots r_n < C_2 \mid R_{n-1} \mid r_n
 < C_2 \bigg( 1 + C_4 \frac{r_{n-1}}{r} \bigg) \mid R \mid $.   
 We will now show that  $r_{n-1} < C_5 r$, $C_5 = C_5 (n)$  and then
 we would have proved (\ref{eq49}). 
 
 Let
 \begin{equation*}
 C_6 = 4 (n-1)^2, C_7 = ( 2n - 2 )^{n-1} = C_6^{\frac{n-1}{2}}
 \tag{52}\label{eq52}  
 \end{equation*}
 
 Let $ \mathfrak{K} $ be determined such that 
 \begin{equation*}
r_{\ell+1} < C_6 r_\ell \tag{53}\label{eq53} 
 \end{equation*} 
 for $ \ell = n-2, n-3, \ldots \mathfrak{K} + 1, \mathfrak{K} $ but not
 for $ \ell = \mathfrak{K}-1 $ (The statement will have the obvious
 interpretation in the border cases corresponding to $ \mathfrak{K} = 0
 $ and $ \mathfrak{K} = n-1 ) $. Let $x_\nu + a_\nu x_n $ be the $
 \nu^{th} $ element of $ \mathfrak{z} + R^{-1}_{n-1} \mathscr{H} x_n
 $. For each integer $x'_n $  in the interval $ 0 \leq x'_n \leq C^{n-
   \mathfrak{K}}_7 $ we determine a set of $ n- \mathfrak{K} $ integers
 $ x'_\nu, \nu = \mathfrak{K}, \mathfrak{K} +1 \ldots n-1 $ such that
 $ 0 \leq x'_\nu + a_\nu x'_n < 1 $   for each $ \nu $. Thus
 corresponding to  the  $ C^{n-\mathfrak{K}}_7 + 1 $ possible choices
 for $ x'n $ we obtain $ C^{n-\mathfrak{K}}_7 + 1 $  points in the $
 (n-\mathfrak{K} ) $ dimensional Euclidean space, all lying in the half
 open unit\pageoriginale cube $ 0 \leq \mathfrak{Y}_\nu < 1 ,  \nu =
 \mathfrak{K}, 
 \mathfrak{K} + 1,\cdots n-1 $. If we divide this cube into equal cubes
 each of whose sides is of length $ C^{-1}_7 $, there will be $C^{n
   -\mathfrak{K}}_7 $ such cubes and consequently, by a  well known
 principle, one of  these cubes contains at least two of the above $
 C^{ n \ \mathfrak{K}}_7 +1 $ points. Their difference is clearly a
 point with coordinates of the form $ x_\nu  + a_\nu x_n ,  v =
 \mathfrak{K} ,\mathfrak{K} + 1 , \cdots, n-1 $  where $ x_\nu's$ are
 integers,    
 \begin{equation*}
  \mid x_\nu + a_\nu x_n \mid < C^{-1}_7 ,  0 < x_n \leq C^{n -
    \mathfrak{K}}_7  \tag{54}\label{eq54}  
\end{equation*}

In other words we have solved (\ref{eq54}) with integral $ x_\nu, x_n, \nu =
\mathfrak{K} $, $ \mathfrak{K} + 1, \cdots n-1 $. We can assume that  $
x_\mathfrak{K}, \ldots x_n $  are coprime. Now we choose integers $
x_1, x_2 \ldots x_{\mathfrak{K} -1} $  such that  
\begin{equation*}
 1 x_\nu + a_\nu x_n  \mid < 1, \nu = 1,2, \ldots \mathfrak{K} - 1
 \tag{55}\label{eq55}  
\end{equation*}

From Lemma \ref{chap3:lem3}, we have $R$ [$\xi$] $\ge r_\mathfrak{K}
$. On the other 
hand, the relation $ r_\mathfrak{K} > C_6  r_{\mathfrak{K} - 1 } $ and
the relations (\ref{eq52}) - (\ref{eq55}) entitle us to conclude that  
\begin{align*}
R [ \xi ] &=  R_{n-1} [ \mathfrak{z} + R^{-1}_{n-1} \mathscr{H} x_n ]
+ rx^2_n \\ 
&< ( \mathfrak{K} -1 )^2 r_{\mathfrak{K} - 1} + ( \mathfrak{K} -1 ) ( n -
\mathfrak{K} ) r_{\mathfrak{K} - 1 } C^{-1}_7 + \\ 
&+ ( n - \mathfrak{K} )^2 C^{-2}_7 C^{b - \mathfrak{K} -1}_6
r_\mathfrak{K} + r C^{2 (n-\mathfrak{K})}_7 \\ 
&\leq ( \mathfrak{K} - 1 ) ( n -1 ) C^{-1}_6  r_\mathfrak{K} + ( n -
\mathfrak{K} )^2 C^{-\mathfrak{K}}_6 r_\mathfrak{K} + C^{2 n-2}_{7}r 
\end{align*}

Thus $ r_\mathfrak{K} < C_8 r$, $C_8 = C_8 (n) $ and by  (\ref{eq53}), $r_{n-1} <
C_5 r$. This completes the proof. 

Having thus settled the arithmetical properties of reduced matrices,
we proceed with their existential nature. We have already seen in the
beginning of this section that given any matrix $ Y =
Y^{(n)} > 0 $ there exists a unimodular  matrix $u$
such that $Y[u]$ is reduced. In other words, for
any matrix $Y > 0$ there always exists an equivalent  reduced
matrix $R$. Such a matrix $R$ is by no means unique. However, the
number of matrices $R$ equivalent to a given matrix $Y -
Y^{(n)}  > 0 $ is finite\pageoriginale and this number is bounded by
an integer which depends only on $n$. It is our aim now to establish
this result.   

Given any quadratic form $Y[\xi]$, by the method of completion of
squares, it can always be rewritten uniquely as  
\begin{gather*}
Y [ \xi ] = d_1 ( x_1 + b_{12} x_2 + \ldots b_{1 n }
\times_n )^2 + \\ 
+ d_2 ( x_2 + b_{2 3} x_3 + \ldots b_{2 n} x_n)^2 \\
+ \ldots \ldots \ldots \\
+  d_n x^2_n 
\end{gather*}
and, $Y > 0 $  if  and only if the $ d_i's$ are
positive. This shows that any matrix $Y > 0 $ has a unique
representation in the form  
\begin{equation*}
\gamma = D [ B ] \tag{56}\label{eq56} 
\end{equation*}
where  $D$ is a diagonal matrix $ ( \delta_{\mu \nu } d_{\mu \nu} ) $
with the  diagonal elements $ d_{\nu \nu } \equiv d_\nu $, all
positive; and  $B$ is a matrix $ (b_{\mu \nu} ) $ with $ b_{\mu
  \mu} = 1 $ and $ b_{ \mu \nu } = 0 $ for $ \mu > \nu $. A matrix
$ B = ( b_{\mu \nu }) $ whose elements satisfy the above conditions
will be referred to as a \textit{triangular matrix}. Assume now  that
$Y$ is a reduced matrix $ R = ( r_{\mu \nu })$. From (\ref{eq56}) we
will have  
\begin{align*}
r_\ell &= d_\ell + \sum^{\ell - 1}_{\nu = 1} d_\nu b^2_{\nu \ell},
\ell = 1,2, \ldots, n \\ 
\text{ and } \mid R \mid &=  d_1 d_2 \ldots  d_n . 
\end{align*}

In view of (\ref{eq49}), this implies that
\begin{equation*}
1 \leq \frac{r_\ell}{d_\ell} \leq \prod^n_{\nu = 1}
\frac{r_\nu}{d_\nu} < C_1 \tag{57}\label{eq57}  
\end{equation*}
and consequently, with the help of (\ref{eq47}) we have for $ \mathfrak{K}
\leq \ell $ 
\begin{equation*}
0 < \frac{d_\mathfrak{K}}{d_\ell} < C_1 \frac{r_\mathfrak{K}}{r_\ell}
\leq C_1 \tag{58}\label{eq58}  
\end{equation*}

We use these to prove that in the case of reduced matrices $Y$
represented in the form (\ref{eq56}), the $b_{\mu \nu}'s$
have an upper bound  depending  only on  $n$. The proof is by
induction on $u$. Assume then that $ \pm b_{\mathcal{P} \ell} <
C_9$. $C_9 = C_9 (n) $\pageoriginale for $p = 1,2, \ldots \mathfrak{K} -1$
and $\ell > p$. By means of the relation 
$$
r_{\mathfrak{K} \ell} = d_\mathfrak{K} b_{\mathfrak{K} \ell} +
\sum^{\mathfrak{K} - 1}_{p = 1} d_p
b_{p\mathfrak{K}} b_{p\ell}, 
$$

Our assumption will imply in view of (\ref{eq57}), (\ref{eq58}) that for $ \ell >
\mathfrak{K}$, 
$$
\pm b_{\mathfrak{K} \ell} \leq \frac{r_\mathfrak{K}}{2 d_\mathfrak{K}}
+ \sum^{\mathfrak{K} - 1}_{p =1}  \frac{d_p}{d_\mathfrak{K}} C^2_9  <
\frac{1}{2} C_1  + ( n -1 ) C_1C^2_9 = C_{10} (n). 
$$

Thus assuming the result for $p = 1, 2, \ldots \mathfrak{K}
- 1,   \ell > p $, we have established it for $p = \mathfrak{K}$,
$\ell > p$ and by the principle of induction, this completes the
proof. We have now proved   

\begin{lem}\label{chap3:lem5}%lemma 5
Let $ D = ( \delta_{\mu \nu} d_{\mu \nu}) $ be a diagonal matrix
  and $ B = ( b_{\mu \nu}) $ a triangular matrix such that $
  \gamma = D [ B ] > 0 $ is reduced. Then  
\begin{align*}
&d_\nu < C_{11} d_{ \nu + 1}, \nu = 1,2, \ldots n-1, \tag{59}\label{eq59} \\
\pm  & b_{\mu \nu < C_{11}}, \mu < \nu, C_{11} = C_{11} (n), 
\end{align*}
\end{lem}

The possible converse to this is false, viz. if $D^*$, $B^*$ be two
other matrices whose elements satisfy (\ref{eq59}) (with the same constant
$C_{11}$) and $D^*$ is diagonal while $B^*$ is triangular, we
cannot conclude that $ R^* = D^* [ B^* ] $ is a reduced matrix. In this
direction, however, we have 

\begin{lem}\label{chap3:lem6} %lemm 6
$$ If 
 D* = ( \delta_{\mu \nu } d^*_{ \mu \nu } )  ,  d^*_{ \nu \mu} \equiv
 d^*_\mu > 0 ;  B^* = ( b^*_{\mu \nu} ) 
  $$
a triangular matrix and $G$ integral matrix with $ \mid G \mid  \neq 0
$  such that $ D^* [ B^* G ] $ is reduced, then the elements of $G$
all lie between two bounds which depend only on $ \mu $ and  $ n - \mu
$ being a common upper bound for the absolute values of $
\dfrac{d^*\nu}{d^*\nu +1}, \nu = 1,2, \ldots, n-1, b_{\mu \nu} (
\nu > \mu ) $ and $ \mid G \mid $ and $n$ being the order of $ D^*$
or $b^*$. 
\end{lem}

\begin{proof}
We again resort to induction, this time on it. For $n = 1$  the
lemma is clearly true. Since $ D^{*} [B^{*} G ] $ is reduced
have by  (\ref{eq56}),\pageoriginale $D^* [ B^* G ] = D [ B ] $ for
some diagonal matrix $D$ 
and triangular matrix $B$. Let $ G = ( g_{\mu \nu })$, $B^* G B^{-1} =Q
= ( q_{\mu \nu})$, $B^{*^{-1}} = ( \beta_{\mu \nu} )$ Then we have $ D^* [
  Q ] = D $ and $ D[ Q^{-1} ] = D^* $ and  therefore $ d_\ell =
\sum^n_{\mathfrak{K} = 1} d^*_\mathfrak{K} q^z_{\mathfrak{K} \ell}( \ell
= 1,2, \ldots, n)$ Consequently,  
\end{proof}
\begin{equation*}
d^*_\mathfrak{K} q^2_{\mathfrak{K} \ell} \leq d_\ell, \mathfrak{K}, \ell
= 1,2, \ldots n. \tag{60}\label{eq60}  
\end{equation*}

Since $G =B^{*^{-1}} Q B $  and $B$, $B^*$  are triangular matrices, we
have  
$$
g_{\mathfrak{K} \ell} = \sum^n_{\lambda = \mathfrak{K} }
\sum^\ell_{\lambda = 1} \beta_{\mathfrak{K} \lambda} q_{\lambda
  \lambda} \ell_{\lambda \ell}; \mathfrak{K}, \ell = 1,2, \ldots n 
$$

 Hence
 $$
 d^*_\mathfrak{K} g^z_{\mathfrak{K} \ell} = d^*_{\mathfrak{K}} (
 \sum^n_{\lambda = \mathfrak{K}}  \sum^n_{\lambda = 1} \beta_{\mathfrak{K}
   \lambda} q_{\lambda \lambda} \ell_{\lambda \ell} )^2 
 $$

 By assumption the $\ell'^*s$ are bounded, by $\mu$  so
 that the $\beta's$ which are rational functions of the
 $ \ell'^*s$ are also bounded, and by Lemma \ref{chap3:lem5}, the $
 \ell's$ are bounded, in both the cases the bound
 depending only on $\mu$ and $n$. 
 
 We can therefore write
 \begin{align*} 
 d^*_\mathfrak{K} g^2_{\mathfrak{K} \ell}  & <  \mu^* ( \sum^n_{\chi
   = \mathfrak{K}} \sum^\ell_{\lambda = \ell} q_{\chi \lambda} )^2
 d^*_\mathfrak{K}, \mu^* = \mu^* (n)\\ 
 & <  \mu^* ( \sum  q_{\chi \lambda} q_{\chi' \lambda'} 
 d^*_\mathfrak{K} ) , \chi  \chi' \ge \mathfrak{K}  
\end{align*}

Majorising $q_{\chi \lambda} q_{\chi' \lambda'}$ by $( q^2_{\chi
  \lambda} + q^2_{\chi' \lambda'})$ and $ q^2_{\chi \lambda}
d^*_\mathfrak{K} ( \chi \ge \mathfrak{K} )$ by $ q^2_{\chi \lambda}
d^*_\chi$, with the aid of (\ref{eq59}), we get  $d^2_\mathfrak{K}
g^2_{\mathfrak{K} \ell} < \mu^*_1  \sum(q^2_{ \chi \lambda}
d^*_\chi  + q^2_{\chi' \lambda'} d^*_{\chi'})$ and using (\ref{eq60}), we have
finally, 
\begin{equation*} 
d^*_\mathfrak{K} g^2_{\mathfrak{K} \ell} < \mu_1 d_\ell    \mathfrak{K}
\ell = 1,2, \ldots N \tag{61}\label{eq61}   
\end{equation*}
($\mu_1$ and the  $\mu_\nu's$ that occur subsequently in the
course of the proof are positive constants depending only $\mu $ and
$R$). 

Replacing the  equation  xxxxx  $ D [ Q^{-1} ] = D^* $\pageoriginale
and repeating the earlier arguments we have   
\begin{equation*}
d_\mathfrak{K} f^2_{\mathfrak{K} \ell} < \mu_2 d^*_\ell \tag{62}\label{eq62}  
\end{equation*}
where  $ ( f_{\mu \nu} ) = G^{-1} $.

Since $ \mid ( f_{\mu \nu} )\mid = \mid G^{-1} \mid  \neq 0 $ there
exists a permutation of the indices $ 1,2, \ldots $ into $ \ell_1,
\ell_2 \ldots , \ell_n $ with $\prod\limits^n_{\mathfrak{K}=1}
f_{\mathfrak{K} \ell_\mathfrak{K}} \neq 0 $ Since $ \mid G \mid f_{\mathfrak{K}
  \ell_\mathfrak{K}} $ is an integer, its absolute value is at least
$1$. Hence $ 1/ f_{\mathfrak{K} \ell \mathfrak{K}} $ is bounded by the
absolute value of $ \mid G \mid $ and a fortiori by  $ \mu
$. Consequently, from (\ref{eq62}) we have $ d_\mathfrak{K} <  \mu_3
d^*_{\ell_\mathfrak{K}}, \mathfrak{K} = 1 \cdots n $. Among the $ (
n-\mathfrak{K} + 1 )$ indices $ \ell_\mathfrak{K}, \ell_{\mathfrak{K}+1 }
\ldots \ell_n $, there should be at least one not exceeding $
\mathfrak{K} $ so that  
$$
\min ( d_\mathfrak{K}, d_{\mathfrak{K}+1}, \ldots d_n ) < \mu \max (
d^*_1, d^*_2, \ldots d^*_\mathfrak{K} ) 
$$
and hence, by means of (\ref{eq59}) and the analogous assumption on the  $
d^{*'}s $  We have $ d_\mathfrak{K} < \mu_4 d^*_\mathfrak{K},
\mathfrak{K} =1,2 \ldots n $. The relation (\ref{eq61}) now allows us to
conclude that  
\begin{equation*}
d_\mathfrak{K} g^2_{\mathfrak{K} \ell} < \mu_5 d_\ell, \mathfrak{K}, \ell
= 1,2, \ldots n \tag{63}\label{eq63}   
\end{equation*}

Let $p$ denote the largest number among $ 1,2, \ldots n $
such that the relation $ d_\mathfrak{K} \ge \mu_5 d_\ell $ holds for $
\mathfrak{K} = p, p + 1, \ldots n $ and  $ \ell =
1,2, \ldots p-1 $. For each $g$ among  $ p + 1,
p+ 2, \ldots n $ there exists  then a  $ \mathfrak{K} =
\mathfrak{K} ( g) \ge q $  and an  $ \ell = \ell_g < g $ with $
d_\mathfrak{K} < \mu_5 d_\ell $ ( with the appropriate interpretation
for the  border cases $p = 1, n$).  In view of (\ref{eq59}) we have then 
\begin{equation*} 
d_g < \mu_6 d_{g-1}, g = p + 1, p + 1,\ldots n\tag{64}\label{eq64}   
\end{equation*}
(\ref{eq59}) and (\ref{eq64}) together imply that $ d_\ell \mid d_\mathfrak{K} $
is a bounded quotient for  
$$
\ell, \mathfrak{K} = p, p+1, \ldots n. \quad \text{ i.e. } \quad 
d_\ell < \mu_7 d_\lambda 
$$
and consequently, from (\ref{eq63}).
$$
g^2_{\mathfrak{K} \ell} < \mu_5 \mu_4, \ell = p, p+1, \ldots, n
$$

By choice\pageoriginale of  $p,d_\mathfrak{K} \ge  \mu_5 d_\ell , \ell =
1,2, \ldots, p-1 $  and  $ \mathfrak{K} = p,
p+1, \ldots n $. Also $ g_{\mathfrak{K} \ell} $  is an
integer for each $ \mathfrak{K}, \ell $. Hence (\ref{eq63}) can hold only
if  $ g_{\mathfrak{K} \ell} = 0 $ for these indices, viz. $ \mathfrak{K}
=p,p + 1, \ldots n $; $ \ell = 1,2, \ldots p-1 $. 

Thus $G$ is a a matrix of  the type 
$$
G =  
\begin{pmatrix}
 G_1 & G_{1 2}\\
0 & G_2 
\end{pmatrix}
$$
where the elements of $G_2 = G^{ ( n- p + 1)}_2 $  are
bounded by a $ \mu_8 $. In case $ p = 1 $, the proof of
Lemma \ref{chap3:lem6}  is already complete. In the  alternative case,
we write, analogous to $G$, 
\begin{align*}
 D &= \begin{pmatrix} D_1 & 0 \\ 0 & D_2 \end{pmatrix}, &D^*
 = \begin{pmatrix} D^*_1 & 0 \\ 0 & D^*_2 \end{pmatrix} \\ 
 B &= \begin{pmatrix} B_1 & 0 \\ 0 & B_{1 2} \end{pmatrix}, &B^*
 = \begin{pmatrix} B^*_1 & B^*_{1 2} \\ 0 & B^*_2 \end{pmatrix}
\end{align*}
and obtain
\begin{equation*}
D^*_1 [ B^*_1 G_1 ]   = D_1 [ B_1 ] \tag{65}\label{eq65}  
\end{equation*}
by equating
$$ 
 D^* [ B^* G ] = \begin{pmatrix} D^*_1 [ B^*_1 G_1 ] & * \\ * &
   * \end{pmatrix}  \text{ and } D [ B ] = \begin{pmatrix} D_1 [ B_1 ]
   & * \\ * & * \end{pmatrix} 
$$

By assumption $D^* [ B^* G ] $  is reduced so that in particular  $
D^*_1 [ B^*_1 G_1 ] $ is reduced. It is now immediate that our
assumptions on $ D^*$, $B^* $ and $G$  are also true  of $ D^*_1$,
$B^*_1$ and $G_1 $, so that by induction assumption the elements of $G_1 $
are all bound. What remains then to complete the proof is only to show
that the elements of $G_{ 1 2} $ are bounded. 

From the matrix relation
$$
G'_1 D^*_1 [ B^*_1 ] G_{12} + 
G'_1 B^{\ast'}_1 D^\ast_1 B^\ast_{12} G_2
$$
with the help of (\ref{eq65}), we get  $G_{12} = G_1 B^{\ast'}_1 B_{12} -
B^{\ast'^{-1}}_1 B^\ast_{12} G_1,$ 

As the\pageoriginale elements of all the  matrices occurring on the
right side are 
bounded, the same is true of the  elements of $ G_{12} $, and in all
these cases the bound depends only on $ \mu $ and $n$. The proof is
now complete. Lemmas \ref{chap3:lem5} and \ref{chap3:lem6} now yield 

\begin{lem}\label{chap3:lem7}%lemm 7
 If $Y = Y^{(n)} > 0 $ is  a reduced matrix  and
   $\mathcal{U}$, a unimodular matrix such that $Y[\mathcal{U}]
   $ is also reduced, then the elements of $ \mathcal{U} $ all lie
   between  bounds which depend only on $n$. 
\end{lem}
 
As an immediate consequence, we infer that, equivalent to a given
matrix $Y> 0 $ there exists only a finite number of reduced
matrices, and further, this number is bounded by a constant which
depends only upon $n$.  

We now proceed to determine the structure of the space $\mathscr{R}$
of  the  reduced  matrices and its relationship to the space $
p$ of all real positive symmetric matrices. We first observe that $
p$  is an open domain in the  space $ \mathscr{Y} $  of all
real symmetric matrices. For if $ G_\mathfrak{K} \in \mathscr{Y} ,
G_\mathfrak{K} \not\in \mathfrak{P} \mathfrak{K} = 1,2, \ldots $, and if
$G_\mathfrak{K}  \rightarrow G \in \mathscr{Y} $, then for each
$\mathfrak{K} $, there exists a vector $ \mathscr{E}$ which may be
supposed to be of length 1 with  $ G_\mathfrak{K} [
  \mathscr{E}_\mathfrak{K} ] \leq 0 $. Then a subsequence $
\mathscr{E}_{\nu_{\mathfrak{K}}} $ clearly converges to a vector $
\mathscr{E} $, again of length $1$, and  
$$
 G [ \mathscr{E} ] = \lim_\mathfrak{K}  G_{\nu_{\mathfrak{K}}} [ 
   \mathscr{E}{\nu_{\mathfrak{K}}} ] \leq 0  
 $$
 whence $G \not\in \mathfrak{P}$. This certainly implies that
 $\mathfrak{P}$ is an open do main. Let now $G$ be a boundary point of
 $ \mathfrak{P} $ so that $ G \not \in \mathfrak{P} $. Then there
 exists a sequence $ y_\mathfrak{K} \in \mathfrak{P} $ with $
 y_\mathfrak{K} \rightarrow G $. Since $ \gamma_\mathfrak{K} [
   \mathscr{E} ]> 0 $ for every $ \mathscr{E} \neq 0 $, we get $ G [
   \mathscr{E} ] \ge 0 $ for  $ \mathscr{E} \neq 0 $. Also $ G [
   \mathscr{E} ] = 0 $ for at least one $ \mathscr{E} $, as otherwise
 $G$ will belong to $\mathcal{P} $. Every symmetric matrix $G$ with
 this property, viz., $ G [\mathscr{E} ] \ge 0 $ for every $
 \mathscr{E} \neq 0 $, the equality  holding for at least one such
 $\mathscr{E}$, shall be called \textit{semi positive}. We can now
 state that any boundary point of $ \mathfrak{P} $\pageoriginale is
 semi 
 positive. Conversely too, every semi positive matrix $G$ is a
 boundary point of $ \mathfrak{P} $, since $ G + \in E $ lies in  $
 \mathfrak{P} $ for every $ \in > 0 $ but $ G \not\in \mathfrak{P}
 $. In $\mathfrak{P}$, we now consider the domain  $\mathscr{R}$
 of all reduce $\gamma > 0$. Let $ R = (\mathscr{R}_{\mu \nu }) $
 be a point of $\mathscr{R}$. This means by Lemma \ref{chap3:lem3} that 
 \begin{equation*}
R [ \mathscr{Y}_\mathfrak{K} ] \ge r_{\mathfrak{K} \mathfrak{K} } \equiv,
n_{\mathfrak{K} \mathfrak{K} + 2} \ge 0 \tag*{$(46)'$}\label{eq46'}   
 \end{equation*} 
for any integral $\mathscr{Y}_\mathfrak{K}$ whose last $ n-
\mathfrak{K} + 1$ elements are coprime, $ \mathfrak{K} = 1,2, \ldots
n-1 $. 

Interpreting the $ \dfrac{1}{2} n ( n + 1 ) $ independent elements of
$R$ as the cartesian coordinates in the Euclidean space of the same
dimension, the  above inequalities define a cone with the apex at the
origin. We shall show that much more is true.  

Let $R_o $ be a boundary point of $ \mathscr{R} $. Then either $ R_o
\in \mathfrak{P} $ in which case it is positive or $ R_0 \in B d
\mathfrak{P} $ in which case it is  semi positive. Consider first the
case when $R_0 $ is positive. Then it satisfies (\ref{eq46}) and also there
exists a sequence  $ y_\mathfrak{K} > 0 $ in $ \mathscr{Y} $ such that
$ y_\mathfrak{K} \not\in \mathfrak{K} $ and $ y_\mathfrak{K}
\rightarrow R_o $. We represent $R_o$ according  to (\ref{eq56}) in the
form $R_o = D [ B ] $ with a diagonal matrix $ D= ( d_{\mu \nu}\; d_{\mu
  \nu} ) $  and a triangular matrix $ E = ( b_{\mu \nu} ) $. The
transformation $ ( r_{\mu \nu}) \rightarrow ( d_\nu, b_{\mu \nu} ) $
defines  a topological  mapping of a neighbourhood of $ R_o $  on to a
neighbourhood of $ ( d_\nu , b_{\mu\nu} ) $ (where, in the
indices for $b$, we assume $ \mu < \nu $. We represent $
y_\mathfrak{K}$ in the same way, viz. $y_\mathfrak{K} =
D_\mathfrak{K} [ B_\mathfrak{K} ]$, and in view of the topological
character of the above mapping, we conclude that $D_\mathfrak{K} \to
D$ and $E_\mathfrak{K} \to B$. Since $D [P]$ is reduced, the elements
of $D, B$ satisfy 
(\ref{eq59}) and the same is therefore true of the elements of
$D_\mathfrak{K} , B_\mathfrak{K} $ for sufficiently large $ \mathfrak{K}
$. Let $ u_\mathfrak{K} $ be unimodular such that $ y_\mathfrak{K} [
  u_\mathfrak{K} ] $ is reduced. $ y_\mathfrak{K} \not\in
\mathscr{R}$ by assumption so that $ u \neq \mathfrak{K} $. Lemma
\ref{chap3:lem6} 
then shows that for sufficiently large $\mathfrak{K}$, $d_\mathfrak{K} $
belongs to a finite set  of  matrices\pageoriginale and in particular,
there exist an 
infinity of $ \mathfrak{K}'s $ for which the
$\mathcal{U}_\mathfrak{K}'s$ are the same, say, 
$\mathcal{U}_\mathfrak{K} = \mathcal{U} \neq \pm E$. As 
$\mathfrak{K} \rightarrow \infty $ through this sequence of values, we 
have  
$$ 
R_o [ \mathcal{U} ] = \lim_\mathfrak{K} \gamma_\mathfrak{K} [
  \mathcal{U} ] \in \mathscr{R}  
$$
 since each $ y_\mathfrak{K} [ \mathcal{U} ] \in \mathscr{R} $ and
 $\mathscr{R}$ is easily seen to be a closed set. Thus what we have
 shown is that if $ R_o $ is  a positive boundary point of $
 \mathscr{R}$, there exists a unimodular matrix $ u \neq \pm E $
 such that  
 \begin{equation*} 
R_o [ \mathcal{U} ] \in \mathfrak{K}. \tag{66}\label{eq66}  
 \end{equation*}
 
 Indeed $R_o [ \mathcal{U} ] $ is a boundary point of $ \mathscr{R}
 $ and this, we proceed to : establish. More generally we show that
 \textit{if $ R_o , R_1 \in \mathscr{R} $ and $ \mathfrak{K}_1 = \mathfrak{K}_o [
     \mathcal{U} ] $ for some unimodular matrix $ \mathcal{U} \neq \pm
   E $, then both $R_o$ and $  R_1 $ are boundary points of $
   \mathscr{R} $.}  
  
 Assume first that $ \mathcal{U} $ is not a diagonal matrix. Let $
 \mathscr{Y}_1, \mathscr{Y}_2, \ldots \mathscr{Y}_n $  be the columns
 of $ \mathcal{U} $ and let $ \mathscr{Y}_\mathfrak{K} $ be the first
 column which is different form $ \pm n_\mathfrak{K} $ (If no such $
 \mathscr{Y}_\mathfrak{K} $ exists, then $ \mathcal{U} $ would  be
 diagonal, contrary to assumption). Then the $ \mathfrak{K}^{th} $
 column $ \mathscr{Y}_\mathfrak{K} $ of $ \mathcal{U}^{-1} $ has also
 the same property. Let us write $ R_o = ( r_{\mu \nu })$, $R_1 = (
 \mathscr{S}_{\mu \nu }) $. Then we have  
 $$
  \mathscr{S}_\mathfrak{K} = R_o [ \mathscr{Y} ] \ge = R_1 [
    \mathscr{Y}_\mathfrak{K} ] \ge \mathscr{S}_\mathfrak{K} 
 $$
 
Hence the equality holds throughout and 
$$
R_o [ \mathscr{Y}_\mathfrak{K} ] = r_\mathfrak{K} =
\mathscr{S}_\mathfrak{K} = R_1 [ \mathscr{S}_\mathfrak{K} ] 
$$

If $ R_o $ is an interior point of $ \mathscr{R} $, then the strict
inequality must hold in (\ref{eq46}) for all $ \mathscr{Y}_\mathfrak{K} $. As
we have shown the equality to hold for one $ \mathscr{Y}_\mathfrak{K} $
it follows that $R_o \in Bd \mathscr{R}$. The same is of course true of
$R_1 $ too. We stop here to make the following remark.   

From Lemma \ref{chap3:lem7}, it is clear that $
\mathscr{Y}_\mathfrak{K} $  and $ 
\mathcal{S}_\mathfrak{K} $ belong to a finite set of primitive
vectors. So (\ref{eq66}) allows us to conclude that from the  infinite set
of inequalities (\ref{eq46'}) defining $ \mathscr{R} $, it is possible to
determine\pageoriginale a finite subset such that at least one of this
finite set of 
inequalities reduces to an equality in the case of any positive
boundary point of $ \mathscr{R} $. In other words, the positive
boundary points of $ \mathscr{R} $ all lie on finite number of planes
and these planes bound a convex pyramid $ \mathscr{E} $ containing $
\mathscr{R} $. Of course we have still the case when $\mathcal{U}$ is
a diagonal matrix to settle, to complete the proof. In this case all
diagonal elements are $\pm 1$, the sign changing at least once. Let
then the sign change for the first time from the $q^{\rm th}$ to $( q
+ 1 )^{\rm th}$ element. By changing $\mathcal{U}$ to $-\mathcal{U} $
if necessary, we shall have    
$$
 \mathscr{S}_{q q+1} = \mathscr{Y}'_{q + 1} R_0   \mathscr{Y}_q   =
 x'_{ q + 1} R_0    x_q = - r_{q q+1 }. 
 $$ 

But due to one of the reduction in equalities $ r_{q q+1} \ge 0,
\mathscr{S}_{q q+1} \ge 0 $. 

Hence  it follows that 
\begin{equation*}
r_{q q+1} = 0   = \mathscr{S}_{q q+1} \tag{67}\label{eq67}  
\end{equation*}

It is immediate that $R_o $ and $R_1 $ are again boundary points of $
\mathscr{R} $ and one of the inequalities in (\ref{eq46}) reduces to an
equality. Now our earlier remark is unreservedly valid. 

We now show the interior of $\mathscr{R}$ is non void. Consider a
compact set $ \mathscr{L} \subset \mathfrak{P} $ with non null
interior. Then its interior is of the  highest dimension, viz. $
n( n+1 ) / 2$. Let $ y \in \mathscr{L}$. Then by (\ref{eq56})
we can represent  $y$ in the form $y= D [ B ] $ and
then, since $ \mathscr{L} $ is compact the ratios $ d_\nu \mid d_{\nu
  +1 } $  and $ \pm b_{\mu \nu} $ are  bounded by $ \mu $ ( say ),
$ \mu $ depending only on $ \mathscr{L} $. So now, if we determine a
unimodular $ \mathcal{U} $  such that $ y [ \mathcal{U} ] $ is
reduced, then by Lemma \ref{chap3:lem6}, the elements of $\mathcal{U}$ are all
bounded, the bound depending only on $ \mathscr{L} $ and $ n : $  in
other words, even though the $y's$  belonging to $
\mathscr{L} $ may be infinite, there are only a finite number of $
\mathcal{U}'s$ such that $y[\mathcal{U} ] $ is
reduced. If $ \mathscr{R} $ has no interior points, then the $y[
  \mathcal{U} ]'s $ are all boundary points of 
$\mathscr{R}$\pageoriginale and hence belong to a finite set of
planes. This would 
then mean that a finite number of planes is mapped by a finite set of
$ \mathcal{U}'s$ on to a set of dimension  $ n ( n+1) /2 $
which is clearly impossible. Hence we conclude that
$\mathscr{R}$ has interior points. 
	
	Let now $\mathscr{R}$  be an interior point of $ \mathscr{R}
        $. We consider the segment $ (1- \lambda) T + \lambda R,   0
        \leq \lambda \leq 1  $ where  $ T \not\in \mathscr{E} $ but $ T
        \in \mathscr{R} $. Our aim is to  show  that such a $ T $ is
        semi positive. Since $ \mathscr{E} $ is a convex set, all the
        points on the above segment are points of $ \mathscr{E} $ and
        all, except possibly $ T $, are interior points of  $
        \mathscr{E} $. On this line there  exists a boundary point
        $R_o $ of $ \mathscr{R} $. $R_o$ cannot be positive as
        otherwise  $R_o$ would  lie on one of the planes which bound $
        \mathscr{E}$ so that it is a boundary point of $ \mathscr{E} $
        too, and consequently $ R_o  = T \in \mathscr{R} $ - a
        contradiction to the choice of $T$. Hence $R_0$ is semi
        positive. The characteristic roots of a semi positive matrix
        are all $ \ge 0 $ and one at least among them is zero. Hence $
        \mid R_o \mid = o $ so that writing $ R_o = (r^o_{\mu \nu } )
        $  and approaching  $R_o $ through a sequence of matrices $R =
        ( r_{\mu \nu} )  \in \mathscr{R} $  we obtain, in view  of the
        reduction conditions, viz. 
\begin{align*}
&\pm 2 r_{1 \nu } \leq r_1 \leq r_2 \leq \ldots \leq r_n,\\
&r_1 r_2 \ldots r_n  < C_1 \mid R \mid \rightarrow  C_1 \mid R_0 \mid
  = 0 
 \end{align*} 
 that $ r^o_{1 \nu} = 0,  \nu = 1,2, \ldots n $
 
 But $ R_o = ( 1-\lambda_o T ) + \lambda_o R $ for some $ \lambda_0, 0
 \leq \lambda_0 \leq 1 $; we have therefore in particular  
 $$
 0 = ( 1. \lambda_o ) t_{1 \nu} + \lambda_0r_{1 \nu},   \nu = 1, 2,
 \ldots n 
 $$
 
We may consider the above as linear equations  in the variable $
\lambda $ so  that the determinant $ \begin{vmatrix} t_{1 \nu} & r_{1
    \nu} \\ t_{1 1} & r_{1 1} \end{vmatrix}$ $= 0$ or $b_{1\nu}= t_\mu
\dfrac{ r_{1 \nu}}{r_{1 1}}    \nu =2, \ldots n $. 

\setcounter{pageoriginal}{42}
But $ \Gamma $ is a given point and $R $ is an arbitrary point in the
segment. We therefore conclude that $ t_{1 1} = 0 = t_{ 1 \nu } $ and
then from an earlier equation $ \lambda_0 = 0 $. Thus $T =R$ and hence 
is semi\pageoriginale positive. We have therefore shown that $ \varepsilon -
\mathscr{R} $ consists of  semi positive boundary points of $
\mathscr{R} $. Conversely, any such point lies in  $\varepsilon -
\mathscr{R} $  since an arbitrary neighborhood of this point includes
exterior points  of $\mathscr{R}$. Now we obtain the main result of
Minkowski's reduction theory, viz. 

\setcounter{thm}{1}
\begin{thm}\label{chap3:thm2}%theo 2
 The space $ \mathscr{R} $ of the reduced matrices  $y> 0
   $ is a convex pyramid with the vertex at the origin. If
   $\mathcal{U}$ runs over all unimodular  matrices, wherein
   $-\mathcal{U}$ will not be considered as different from
   $\mathcal{U}$, then the images of $\mathscr{R}$ under the group of
   mappings $y \rightarrow y [ \mathcal{U} ] $ cover the space $
   \mathfrak{P} $ of all symmetric matrices  $y
  < 0 $ without over  lapping (except for boundary points). In
  other words, $ \mathscr{R}$ is  a fundamental domain in $
  \mathfrak{P} $ for the group  of mappings  $ Y \rightarrow Y [
    \mathcal{U} ] $ acting on it. 
  
Further, $\mathscr{R}$ has a non void interior and its
  boundary lies on a finite set of planes. 
 \end{thm} 

