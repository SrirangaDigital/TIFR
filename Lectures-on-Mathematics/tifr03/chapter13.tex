\chapter{Definite quadratic forms and Eisenstein series}%chap 13.

A first\pageoriginale access to the theory of modular forms of degree
$n$ was provided by Siegel's work on quadratic forms. The main result
of this theory can be expressed in the shape of an analytic identity
between Theta series and Eisenstein series. A brief account of their 
relationship is given in this section confining ourselves to positive
quadratic forms. We may remark that we shall prefer to speak of
symmetric matrices rather than of quadratic forms. 

We first formulate without proof the main result of Siegel's
theory. This requires a few preliminaries. Let $S = S^{(m)}$ and $T =
T^{(n)}$ denote positive integral matrices, and let $m \ge n$. Denote
by $\alpha (S, T)$ the number of integral $X^{m, n}$ such that
$S[X]= T$ and let $\alpha (\varepsilon, S)$ be denoted by
$\varepsilon (s)$. By $\alpha_q (S, T)$ we shall mean the number of
integral matrices $X^{m ,n }$ distinct modulo $q$ such that
$S[X] \equiv T (\mod q)$. We shall say that $S$ and $T$ belong to
the \textit{same class} if $S [u] = y$ for some unimodular matrix
$\mathcal{U}$ and we shall denote by $(S)$ a special matrix in the
class of $S$. If $m=n$ and if to every integer $q > 0$, there exists a
pair of integral matrices $\chi$, $y$ such that $S (\chi) \equiv T$
and $T(y) \equiv S (\mod q)$, then we say that $S$ is
\textit{related} to $T$, in symbols: $S n T$. The set of all matrices
related to $S$ shall be called the \textit{genus} of $S$. It is well
known that the genus of every positive integral matrix decomposes into
a finite number of distinct classes (If a given genus contains $S$, it
contains the class of $S$ also). We now introduce the measure $\mu
(S)$ of the genus containing $S$ as 
\begin{equation*}
\mu (S) = \sum_{(S_\mathfrak{K})n(S)}1/\varepsilon (S_\mathfrak{K})
\tag{224}\label{eq224}
\end{equation*}

Finally\pageoriginale let 
\begin{equation*}
\alpha_\infty = \frac{\pi^{(2 m - n + 1)}}{y(\frac{m-n+1}{2})
  y (\frac{m-n+2}{2}) \cdots y(\frac{m}{2})}
|S|^{\frac{-1}{2}}|T|^{\frac{m-n-1}{2}} \tag{225}\label{eq225} 
\end{equation*}
and 
\begin{equation*}
\alpha_o (S, T) = \frac{1}{\mu (S)} \sum_{(S_\mathfrak{K}) r (S)} \frac{\alpha
  (S_\mathfrak{K}, T)}{\varepsilon(S_\mathfrak{K})}\tag{226}\label{eq226} 
\end{equation*}

The last expression is called the \textit{mean representation number
  of $T$ by the genus of } $S$. 

We now quote Siegel's main result in  


\setcounter{thm}{16}
\begin{thm}\label{chap13:thm17}%theo 17
 Let $S= S^{(n)}, T=T^{(n)}$ with $m > n + 1$ be positive integral
  matrices. Then 
\begin{equation*}
\alpha_o (S, T) = \alpha_\infty (S, T) \lim_{\mathfrak{K} \to \infty}
\frac{\alpha_q (S, T)}{\frac{mn  n(n + 1)}{2}}, q=\mathfrak{K}!
\tag{227}\label{eq227}  
\end{equation*}

We proceed to express thee $q$-adic representation number $\alpha_q
(S, T)$ by means of the Gauss sums. 
\end{thm}

Let $W^{(n)}$ be a rational symmetric matrix and $d$ the smallest
positive integer such that the quadratic form $d, W[\varepsilon]$ has
all integral coefficients. We shall call such $a'' d''$, the
\textit{denominator } of $W[\varepsilon]$. We now introduce the
\textit{generalized Gauss sum} $g (S, W)$ for any integral symmetric
matrix $S^{(m)}$ and rational symmetric matrix $W^{(n)}$ as follows. 
\begin{equation*}
g(S, W) = \sum_A e^{2 \pi i \sigma (S[A]W)} \tag{228}\label{eq228}
\end{equation*}
where\pageoriginale  in the summation, $A= A^{(m,n)}$ runs through a
complete set of 
integral matrices which are all distinct mod $d$, $d$ denoting the
denominator of $W[\varepsilon]$. We have get to show that the sum in
(\ref{eq228}) does not depend on the choice of the representatives of the
cosets mod $d$. This will be fulfilled if we prove the invariance of
this sum for a replacement of $A$ by $A + d B$ where $B$ is an
arbitrary integral matrix. In this case we have 
\begin{align*}
\sigma (S[A+ dB]W) - \sigma (S[A]W) & = d^2 \sigma (S[B]W) + d \sigma
(B' S A W) \\
& \hspace{2.5cm} + d \sigma (A' S B W)\\ 
& =d^2 \sigma (S[B]W) + 2 d \sigma (B'S A W).
\end{align*}

Since $S[B]$ is integral and symmetric and $d W$ is semi integral, it
follows that $d, \sigma (S[B]W) \equiv 0 (\mod 1)$ and similarly,
since $B' S A$ and $2d W$ are integral, $2 d \sigma (B' S A W) \equiv
0(1)$. Hence  
$$
e^{2 \pi i \sigma (S[A+ d B]W)} \qquad = e^{2 \pi i \sigma (S[A]W)}
$$
and the desired result is immediate.

A useful estimate for $g(S, W)$ when $|S| \neq 0$ is given by 

\setcounter{lem}{18}
\begin{lem}\label{chap13:lem19}%lemm 19 
 Let $S = S^{(m)}$ be a nonsingular integral symmetric matrix, $W
=W^{(n)}$ a rational symmetric matrix and $d$ the denominator of the
quadratic form $W[\varepsilon]$. Then 
\end{lem}
\begin{equation*}
|g(S, W| \le 2^{m/2} || S ||^{n/2} d^{mn - m/2} \tag{229}\label{eq229}
\end{equation*}
\begin{proof}
$$
|g(S,W)|^2 = g (S, W) \overline{g(S, W)} 
$$
$$
= \sum_{A_1, A_2 \mod d} e^{2 \pi i \sigma (S[A_1]W) - 2 \pi i \sigma
  (S[A_2]W)}  
$$\pageoriginale 
\end{proof}

Keeping $A_2$ fixed we first sum over $A_1$ and this we can do after
replacing $A_1$ by $A_1 + A_2$ We then get  
\begin{align*}
|g(S, W)^2 & = \sum_{A_1, A_2 \mod d} e^{2 \pi i \sigma (S[A_1 +
    A_2]W) - 2 \pi i \sigma (S[A_2]W)}\\ 
&  = \sum_{A_1, A_2 \mod d} e^{2 \pi i \sigma \{(A'_1 S_2 + A_2 SA_1 +
  S[A_1]W) \}}\\ 
&  = \sum_{A_1 \mod d} e^{2 \pi i \sigma (S[A_1]W)} \sum_{A_2 \mod d}
e^{2 \pi i \sigma (2A^1_2 SA_1 W)}\\ 
&  = \sum_{A_1 \mod d} e^{2 \pi i \sigma (S[A_1]W)} \sum_{A_2 \mod d}
e^{2 \pi i \sigma (A^1_2 W_1)} \tag{230}\label{eq230} 
\end{align*}
where $W_1 = 2SA_1 W$

If $W_1 = (\omega_{\mu \nu})$ and $A_2 = (a_{\mu \nu})$ then
$$
\sigma (A'_2 W_1) = \sum_{\mu \nu} a_{\mu \nu} \omega_{\mu \nu}
$$
and
\begin{align*}
\sum_{A_2 \mod d} e^{2 \pi i \sigma (A^1_2 W_1)} = & \sum_{a_{\mu \nu
  } \mod d} e^{2 \pi i \Sigma_{\mu, \nu} a_{\mu \nu} \omega_{\mu
    \nu}}\\ 
& = \prod_{\mu, \nu} (\sum_{a_{\mu \nu} \mod d} e^{2 \pi i
  \Sigma_{\mu, \nu}a_{\mu \nu} \omega_{\mu \nu}}) 
\end{align*}

By a standard result, the sum within the parenthesis of $d$ or $0$
according as $\omega_{\mu \nu}$ is or is not an integer. Hence, the
inner sum in the right side of (\ref{eq230}) is $0$ if at least one
$\omega_{\mu \nu}$ is not integral and $d^{mn}$ in the alternative
case. Thus 
\begin{equation*}
|g (S, W)|^2 = d^{mn} \sum_{\substack{A \mod d\\ 2 SA_1 W- integral}}
e^{2 \pi i \sigma (S[A_1]W)} \le d^{mn} L \tag{231}\label{eq231}  
\end{equation*}\pageoriginale 
where $L$ denotes the number of cosets $\mod A_1$ such that $2 S A_1
W$ is integral. 

We now wish to estimate $L$. For this we can assume that $S$ and $W$
are both diagonal matrices. For, in the alternative case, we can find
unimodular matrices $\mathcal{U}_l,V_l, i  = 12$, with
$\mathcal{U}_1 S \mathcal{U}_2 = D = (d_{\mu \nu}d_\nu)$, $V_1 W V_2 = H
= (\delta_{\mu \nu} R_\nu)$. Then, if $A_3 = \mathcal{U}^{-1}_{2} A_1
V^{-1}_1, A_3$ runs over a complete representative system of matrices
$\mod d$ as $A_1$ does and $2 DA_3 H$ is integral when and only when
$2 SA_1 W$ is integral. Consequently the replacement of $S$, $W$ by $D$,
$H$, viz. diagonal matrices, will not interfere with our estimation for
$L$. We assume then that $S$ and $W$ are diagonal matrices, $S=
(\delta_{\mu \nu s_\nu})$, $W = (\delta_{\mu \nu} \omega_\nu)$ and that
$2S A_1 W$ is integral. If $A_1 = (a_{\mu \nu})$ this means that  
$$
a_{\mu \nu} 2 d \omega_\nu s_\nu = 2 s_\nu a_{\mu \nu} d_\nu \equiv 0
(\mod 1), 
$$
$$
1 \le \mu \le m, 1 \le \nu \le n
$$

Since $2 d \omega_\nu$ is an integer, it is obvious that the number of
distinct cosets $\mod d$ is just $(d, 2 d s_\mu \omega_\nu)$ viz. the
greatest common divisor of $d$ and $2 d s_\mu \omega_\nu$. Hence 
$$
L = \prod_{\mu \nu} (d, 2 d s_\mu \omega_\nu) \le \prod_{\mu, \nu}
\{(d, 2 d \omega_\nu)|s_\mu|\} 
$$
\begin{align*}
\le || S ||^n \prod^n_{\nu  = 1} (d, 2 d \omega_\nu)^m & \le ||S||^n
\{(d, 2 d \omega_1, 2 d \omega_2, 2 d \omega_2 )d^{n-1} \}^n\\ 
& \le ||S||^n (2 d^{n-1})^m \tag{232}\label{eq232} 
\end{align*}

In stating\pageoriginale the above inequalities we have used the fact that\\
$(g, g_1, 
\ldots, g_q)g^{q-1} \ge \prod^q_{\nu =1} (g, g_n)$ for any $q+1$
integers $g, g_i (l - 1, q)$ and this is easily proved by induction on
$q$. Combining (\ref{eq231}) and (\ref{eq232}) we get 

$|G (S, W)|^2 \le ||S||^n d^{m n} (2 d^{n-1})^m$ and (\ref{eq229}) is
immediate. We obtain now a representation of the right side of (\ref{eq227})
by an infinite series. 

\begin{lem}\label{chap13:lem20}%lemm 20
 Let $W = W^{(n)}$ run through a complete set of rational matrices
  for which the quadratic forms $W[\in]$ are distinct modulo
  $1$ and let $d$ denote the denominator of $W[\in]$. Then 
\end{lem}
\begin{equation*}
\lim_{\mathfrak{K} \to \infty q} \frac{\alpha_q (S, T)}{mn - \frac{n(n+1)}{2}} =
\sum_W d^{-mn} g(S, W) e^{-2 \pi i \sigma (W T)}, q = \mathfrak{K}!
\tag{233}\label{eq233}   
\end{equation*}
\textit{where } $S = S^{(m)} > 0, T = T^{(n)} > o, m > n^2 + n + 2$
\textit{and the series on} the right side of (\ref{eq233}) converges
absolutely. 

\begin{proof}
We first show that 
\begin{equation*}
q^{\frac{n (n + 1)}{2}} \alpha_q (S, T) = \sum_W \sum_A e^{2 \pi i
  \sigma \{(S[A]-T)W \} } \tag{234}\label{eq234}  
\end{equation*}
where $A$ runs through all integral matrices distinct $\mod q$ and $W$
runs over all rational matrices such that the quadratic forms
$W[\in]$ are distinct $\mod 1$ and $q W[\in]$ has all
its coefficients integral. Let $S[A] - T = R = (r_{\mu \nu})$ and $W =
(\dfrac{1}{2}(1 + \delta_{\mu \nu}) \omega_{\mu \nu})$ 
\end{proof}

Then
\begin{align*} 
\sum_W e^{2 \pi i \sigma (R W)} & = \sum_{\substack{\omega_{\mu \nu}
    \mod 1 \\ q \omega_{\mu \nu} - \text{ integral, } \mu \le \nu}} 
 e^{2 \pi^{\sum} \mu \le \nu^{r} \mu \nu^{\omega} \mu \nu}\\ 
 & = \prod_{\mu \nu}( \sum_{\substack{\omega_{\mu \nu}\mod 1\\ q
     \omega_{\mu \nu} \equiv 0 (1)}} e^{2 \pi i r_{\mu \nu}
   \omega_{\mu \nu}}) 
\end{align*}\pageoriginale 

The sum inside the parenthesis is well known and is equal to $q$ or
0 according as $q$ is or is not a divisor of $q_{\mu \nu}$. It
is now immediate that  
$$
\sum_W e^{2 \pi i \sigma (R W)} = \begin{cases} q \frac{n m + 1}{2},
  if S[A] \equiv T (\mod q)\\ 0 \qquad , \text{otherwise} \end{cases} 
$$

The right side of (\ref{eq234}), by a change of the order of summation
clearly reduces then to $q \dfrac{n (n + 1)}{2} \alpha_q (S, T)$ and
(\ref{eq234}) is established. 
\begin{align*}
\text{Since} \sum_{A \mod q} e^{2 \pi i \sigma (S[A]W)} & = (
\frac{q}{d})^n \sum_{A \mod d} e^{2 \pi i \sigma (S[A]W)}\\ 
& = (\frac{q}{d})^n g (S, W),
\end{align*}
the right side of (\ref{eq234}) can be rewritten as $q^{mn} \sum\limits_{W}
d^{-mn} g(S, W) e^{-2 \pi i \sigma (T W)}$. Hence (\ref{eq234}) now reads as 
\begin{equation*}
q^{\frac{n(n + 1)}{2}} \alpha_q (S, T) = q^{-mn}
\sum_{\substack{W[\varepsilon] \mod 1 \\ q E [\varepsilon] \equiv 0
    (1) }} d^{-mn} g(S, W) e^{-2 \pi i \sigma (T W)}
\tag{235}\label{eq235}  
 \end{equation*} 
 
 This is true for every integral $q = \mathfrak{K}!$ and as
 $\mathfrak{K} \to \infty$ the 
 right side of (\ref{eq235}) is just $\sum\limits_{W[\in] \mod 1}
 d^{-mn} g (S, W) e^{- \pi i \sigma (T W)}$ provided this infinite
 series\pageoriginale  converges. We shall show that this converges
 absolutely and 
 then we would have proved Lemma \ref{chap13:lem20}. We proceed thus
 the number of 
 quadratic forms $W[\in]$ which are distinct $\mod 1$ and have
 a given denominator $d$ can be roughly estimated from above by
 $d^{n(n+1)/2}$. Hence the series under question can be majorised by  
 $$
 \sum_{d = 1}^\infty d^{\frac{n(n+1)}{2}} d^{-mn} g(S, W)
 $$
 which again can be further majorised by means of (\ref{eq229}) \break $K
 \sum^\infty_{d = 1} d^{\dfrac{n(n+1)}{2} - \dfrac{m}{2}}$ with a
 suitable constant $K$. The last series is clea\-rly convergent under
 our assumption on $^{m, n}$ viz. $m > n^2 + n + 2$ or $\dfrac{m}{2} -
 \dfrac{n(n+1)}{2}>1$, and then the absolute convergence of the right
 side of (\ref{eq233}) which we wanted to establish is immediate. 
 
 We wish to have a partial sum representation for the  infinite sum  
 $\sum\limits_{\substack{ \tau > 0 \\ integral }} |T|^{\rho -
   \dfrac{n+1}{2}}e^{2 \pi i \sigma (TZ)} (\rho > n +1 )$ in Lemma
 \ref{chap13:lem21} and as a preliminary, we quote \textit{ Poisson's Summation
   Formula}, viz. that if $f$ is any function defined on the space
 $\mathscr{L}$ of all symmetric matrices $Y$, and $T$ runs through all
 symmetric integral matrices while $F$ runs through all symmetric semi
 integral matrices, then under suitable conditions, 
 \begin{equation*}
\sum_T \mathfrak{f} (T) = \sum_F \int\limits_{\mathscr{L}} \mathfrak{f}
(y) e^{-2 \pi i \sigma (F y)} [d y] \tag{236}\label{eq236}  
 \end{equation*} 
 
 A formal proof of (\ref{eq236}) may be furnished as follows.
 
 If $f(y) = \sum_T \mathfrak{f} (T + y)$ then $g(y)$ is a
 periodic function of $y$ and can be expanded\pageoriginale  in a
 Fourier series  
 
 \noindent
 $g(y) = \sum_F a (F) e^{2 \pi i \sigma (F y)}$ If
 $\mathcal{H}$ denotes the unit cube in $\mathscr{L}$, then  

$\alpha (F) = \int\limits_H g (y) e^{-\pi i \sigma (G y)} [d
   y]$. Replacing $g(y)$ by its infinite sum $\sum_T
 \mathfrak{f}\break (T + Y)$ and changing the order of summation and
 integration (which can be justified under suitable conditions) we
 have 
\begin{align*}
a (F) & = \sum_T \int\limits_T \mathfrak{f} (T + y) e^{-2 \pi i \sigma
  (F y)} [d y]\\ 
& = \sum_t \int\limits_H \mathfrak{f} (T + y) e^{- 2 \pi i \sigma (F(T
  +y))} [d y]\\ 
& = \sum_T \int\limits_{T + \mathcal{H}} \mathfrak{f} (y) e^{-2 \pi i
  \sigma (F y)}[d y]\\ 
& = \int\limits_{\mathscr{L}} \mathfrak{f} (y) e^{-2 \pi i \sigma (F
  y)}[d y] 
\end{align*}

Then
$$
\sum_T \mathfrak{f} (T) = g (0) = \sum_F a(F) = \sum_F
\int\limits_{\mathscr{L}} \mathfrak{f} (y) e^{-2 \pi i \sigma (F y)} [d
  y] 
$$
and this establishes (\ref{eq246}). 

We apply this result in

\begin{lem}\label{chap13:lem21}%lemm 21
 Let $Z \in \mathfrak{f}g_n, \rho > n + 1$ and let $T^{(n)}$ run
  through all positive integral matrices while $F^{(n)}$ runs through
  all semi integral matrices.  
\end{lem}

Then 
{\fontsize{10pt}{12pt}\selectfont
\begin{equation*}
\sum_T |T|^{\rho - \frac{n + 1}{2}} e^{2 \pi i \sigma (TZ)} = \pi
\frac{n (n-1)}{4} y (\rho) y (\rho - \frac{1}{2}). y
(\rho - \frac{n-1}{2}) \sum_F | 1 \pi i (F- Z)|^{- \rho}
\tag{237}\label{eq237}   
\end{equation*}}\relax
\textit{where we put} $| 2 \pi L (F-Z)|^{- \rho} = e^{- \rho \log |2
  \pi i (F-Z)|}$ \textit{and understand by log } $|2 \pi L (F- Z)|$
\textit{that branch of the logarithm which is real for} 

\begin{proof}
We first\pageoriginale note that since $F$ is real and $Z \in
\mathscr{Y}_n, | 2\pi 
l (F - Z)| \neq 0$ so that the right side of (\ref{eq237}) makes sense. Let
us introduce the function $\mathfrak{f} (y) (y-$ symmetric)
as  
$$
\mathfrak{f}(y)=
\begin{cases}
| y| p - \frac{n + 1}{2} e^{2 \pi i \sigma (y z),}\\
\hspace{5cm} , if  y > 0\\
-0, \text{ otherwise }
\end{cases}
$$
and state that Poisson's summation formula is valid for this
function. 
\end{proof}

Then 
$$
\sum_T \mathfrak{f}(T) = \sum_{T > 0} |T|^{p - \frac{ n + 1 }{2}} e^{2
  \pi i \sigma (TZ)} 
$$
which is precisely the left side of (\ref{eq237}) and by (\ref{eq236})
this is equal to  
$$
\sum_F \int\limits_{\mathscr{L}} \mathfrak{f} (y) e^{- 2 \pi i \sigma (F
  y)}[d y ]= \sum_F \int\limits_{y > 0} |y|^{p - \frac{n
    + 1}{2}} e^{2 \pi i \sigma (y (Z - F))}[ d y] 
$$

By Lemma \ref{chap8:lem14} we have
$$
\int\limits_{y > 0} |y|^{{p - \frac{n + 1}{2}} e^{2 \pi i \sigma (y
    (Z - F))}} = \pi^{\frac{n (n - 1)}{4}} y(\rho) y (\rho -
\frac{1}{2}) y (\rho - \frac{n - 1}{2})| 2 \pi i (F -Z)|^{- \rho} 
$$
and then (\ref{eq237}) is immediate. We may note that the convergence of
the right side of (\ref{eq237}) for $\rho > n + 1$ is a consequence of the
convergence of the Eisenstein series in this case. The proof of Lemma
\ref{chap13:lem21} is complete. In the following Lemma we are concerned with a
parametric representation for integral matrices $G$ with a given rank
$r$. 

\begin{lem}\label{chap13:lem22}%lemm 22
Let $1 \le r \le n \le m$ where $r$, $n$, $m$  are all integral. Let $B
  = B^{n , m}$ run over all integral matrices of rank $r$ and $a =
  a^{n ,m}$ run over a complete set of right non associated primitive
  matrices. 

Then\pageoriginale every integral matrix $G$ rank $r$ is obtained as
the matrix product $QB$ once and only once.  
\end{lem}

\begin{proof}
Let $G$ be an integral matrix of rank $n$ with suitable unimodular
matrices $\mathcal{U}_1$, and $\mathcal{U}_2$ and a non-singular
matrix $D = D^{|r|}$ we shall have 
$G = \mathcal{U}_1 \begin{pmatrix} 0 0 \\ 0
  0  \end{pmatrix}\mathcal{U}_2$. Writing $\mathcal{U}_1 = (Q *)$
where $Q = Q^{(n, r)}$  and $\mathcal{U}_2 = ( ^R_*)$ where $R =R^{(n,
  m)}$ we have 
$$
G = (a  *) \begin{pmatrix} D  0 \\ 0  0\end{pmatrix} ( ^R_*) = Q D R =
  Q B \qquad DR = B 
$$
\end{proof}

Since $R$ is primitive and $D$ nonsingular we conclude that $B$ is
integral, and by choice $Q$ is primitive. We may observe that we can
subsequently replace $Q$ by any given element of the equivalence class
of $Q$ by absorbing the right factor (unimodular) introduced thereby,
into $B$. To prove the uniqueness of the representation $G = QB$ we
proceed thus. Let $Q_1 Q_B = Q_2 B_2$ where $Q_\nu$, $B_\nu$ are
matrices in the same sense as $Q, B$ are. We determine $R_1$ such that
$\mathcal{U} = (Q_1 R_1)$ is unimodular and put $\mathcal{U}^{-1} Q_2
= (^A_H)$ with $A = A^{(r)}$. Then since $\mathcal{U}^{-1}Q_1 B_1 =
\mathcal{U}^{-1} Q_2 B_2$ we have on substitution, 

$\begin{pmatrix} E^{(r)}\\ 0 \end{pmatrix} B_1 = \begin{pmatrix} 
  A^{(r)}\\ H \end{pmatrix} B_2$ or $B_1 = AB_2$  and $0 = HB_2$ Since
rank $B_2 = r$ the last condition implies that $H = 0$ and
consequently $A$ is unimodular. Since 
$Q_2 = \mathcal{U}(^A_0) = (Q_1 R_1)(^A_0)$ it now follows that $Q_2 =
Q_1$, $A = E$, and it is immediate that $B_1 = B_2$ by one of the
earlier conditions. 

\newpage

\pageoriginale

\vfill

\begin{center}
\Huge{Missing page 190}
\end{center}

\vfill

\newpage



We now\pageoriginale introduce that $\vartheta$ series $\vartheta (S
Z)$ for any $S = S^{(m)} > 0$ integral, and $Z \in \mathscr{Y}_n$ with
$m \ge n$. By definition  
\begin{equation*}
\vartheta (S,Z) = \sum_G e^{\pi i \sigma (S [G]Z)} \tag{238}\label{eq238} 
\end{equation*}
where the summation for $G$ is extended over all integral $G
=G^{(m,n)}$. It is obvious that the series in (\ref{eq238}) is convergent,
and it is also obvious that $\vartheta (S, Z)$ is a \textit{class
  invariant} of $S$, in other words $\vartheta (S, Z) = \vartheta (S,
Z)$ if $S^*$ is any element of the class of $S$. Besides the class
invariant $\vartheta (S, Z)$, we consider also the \textit{genus
  invariant} 
\begin{equation*}
\mathfrak{f} (S, Z) = \frac{1}{\mu (S)} \sum_{(S_\mathfrak{K}) r (S)}
\frac{\vartheta (S_\mathfrak{K}, Z)}{\varepsilon (S_\mathfrak{K})}
\tag{239}\label{eq239}   
\end{equation*}

This definition of $\mathfrak{f}$ is analogous to the definition
(\ref{eq226}) of the mean representation number of $T$ by the genus of $S$. 

Now $\vartheta (S, Z)$ can be rewritten as 
$$
\vartheta(S, Z) = 1 + \sum^n_{r = 1} \sum_{\substack {G - integral
    \\ rank G = r}}  e^{\pi i \sigma (S[G]Z)} 
$$

By Lemma \ref{chap13:lem22}, $G'$ has a representation of the form $G'
= Q B'$ where 
$Q= Q^{(n, r)}$ is a primitive matrix and $B = B^{(m ,n )}$ is an
integral matrix of rank $r$. Then 
$$
\sigma (S[G]Z) = \sigma (S[BQ']Z) = \sigma (S[B]Z|Q) 
$$
and consequently
$$
\vartheta (S, Z) = 1 + \sum^{n}_{r = 1} \sum_{Q, B} e^{\pi i \sigma (S[B]Z)[Q]}
$$

Introducing\pageoriginale
\begin{equation*}
\chi (S, Z) = \sum_G e^{\pi i \sigma (S[G]Z)} \tag{240}\label{eq240} 
\end{equation*}
where the sum extends over all integral $G$ with rank $G = n$
(i.e. maximal rank) the above series for $\vartheta (S, Z)$ can be
written as  
$$
\vartheta (S,Z) = 1 + \sum^n_{r =1} \sum_Q \chi (S, Z[Q]) 
$$

Since
$$
\chi (S, Z) = \sum_G e^{\pi i \sigma (S[G]Z)} = \sum_{\substack{T^{(n)
      > 0}\\ integral }} \alpha(S, T) e^{\pi i \sigma (TZ)} 
$$
it follows that 
\begin{equation*}
\vartheta(S, Z) = 1 + \sum^n_{r =1} \sum_{Q, T} \alpha (S, T) e^{\pi i
  \sigma (TZ[Q])} \tag{241}\label{eq241}  
\end{equation*}
\hspace{4cm} $T = T^{(r)} > 0$, integral, $Q = Q^{(n , n)}$ primitive,
and consequently 
$$
\mathfrak{f}(S, Z) =  1 + \sum^n_{r =1} \sum_{Q, T} \alpha (S, T)
e^{\pi i \sigma (TZ[Q])}  
$$

We now apply Siegel's main result (\ref{eq227}) and then (\ref{eq233}) in an
obvious way to obtain that, for $m > n^2 + n +2$, 
$$
\mathfrak{f}(S, Z) = 1 + \sum^n_{r =1} \sum_{Q, T} \alpha_\infty (S,
T) \bigg(\lim_{\mathfrak{K} \to \nu} \frac{\alpha_q (S, T)}{q^{m n - r (r + \mu /
    xxxx)}} \bigg) \times e^{\pi i \sigma (TZ[Q])}  
$$ 
\begin{align*}
& = 1 + \sum^n_{r = a} \sum_{T, Q} \frac{\pi \frac{r (2 m -r + l
      )}{4}}{y (\frac{m - r + 1}{2}) y (\frac{m - r + 1}{2)
      \cdots y (\frac{m}{2})}} |S|^{\frac{- r}{2}} |\tau|\frac{m
    - r}{2} \\
& \hspace{3cm} \times \sum_w d ^{mn} g (s . w) e \pi \sigma (T Z [Q] - 2w)
  _{\substack{ w = w^{r} rations \\ W [\varepsilon] \mod 1}}\\ 
& = 1 + \sum^{n}_{ r = 1} \sum_Q \frac{\pi ^{ \pi (2m (- r + 1/4
))}}{ y (\frac{m - r + 1}{2})y (\frac{m - r + 1}{2}) \ldots
    y (\frac{-r}{2})}|S| \frac{- r}{2} \\
& \hspace{3cm} \times \sum_w \frac{g (S,
    w)}{\partial m r} \sum_\tau \frac{m - r + 1}{2} e^{\pi \sigma
    (\tau i \sigma (Tz [Q] - 2w))} 
\end{align*}
the last\pageoriginale result being obtained by a change of the order
of summation the justification for which will appear subsequently. By
means of Lemma \ref{chap13:lem21} we finally get   
$$
f (S, z) = 1 + \sum^n_{r + 1} |S| ^{\frac{-r}{2}} \sum_Q\sum_W \frac{g
  (S, W)}{d ^ {mr}} \sum_F | \Pi (2f + 2w - Z [(Q)])^{y\frac 
{m}{2}}
 F = F ^{(r)} 
$$
semi integral 

It is obvious that every quadratic form of $r$ variables with rational
coefficients has a representation of the form $(F +
W)[\in]$above and conversely, so that, in view of
$W[\in]$ and $( F + W) [\in]$ having the same
denominator $d$ and $g (S, W)$ being invariant for a replacement of
$W$ by $(F + W)$ we can finally write  
\begin{equation*}
f (s, z) = 1 + \sum_{ r + 1}^ n |S|\frac{- r}{2} \sum_Q \sum_W
\frac{g(S , W)}{d^{mr}}|\pi i (d W - Z [Q]))|^{\frac{-m}{2}}
\tag{242}\label{eq242}   
\end{equation*}
where $ W = W^{(r)}$ now runs overall rational matrices. More
explicitly we obtain\pageoriginale  

\begin{lem}\label{chap13:lem23} % lemma 23
 Let $W^{(r)}$ run over all rational symmetric matrices and $Q ^{
    n, r}$ run over a complete set of non right associated primitive
  matrices. Then  
\begin{equation*}
f (s, z) = 1 + \sum^{n}_{r = a} c^{\frac{-r}{w}} \sum_{W, Q}
d^{\frac{-r}{2}} g (s, w)|z|Q|- 2w|^{\frac{-m}{2}}\tag{243}\label{eq243}  
\end{equation*}
 provided $ m > n^2 + n + 2 $ where $d$ denotes the denominator of
  $W[\in]$. 
\end{lem}

The reduction from (\ref{eq242}) to (\ref{eq243}) is straightforward and
immediate. It remains only to show that the double series. $
\sum\limits_{W, Q} \alpha ^{ -m r} g (S,W) |Z[Q] - 2
W|^{-\frac{m}{2}}$ converges absolutely, to justify our 
earlier formal manipulations with this series. In the case $r = n$, we
have $Q = E$ and this series reduces to $\sum\limits_{W{(n)}} d^{-mn}
g (S, W) | Z - 2 W|^{-\frac{m}{2}}$. Since $Z$ is fixed and $||Z -
2W|| \geq |Y|$ the absolute convergence of this series
can be clearly made to depend on that of $\sum\limits_{W^{n}}d^{-mn} g
(S, W)$ and the latter series does converge absolutely under our
assumption $m > n^2 + n + 2$, by means of Lemma \ref{chap13:lem20}. We
have then 
only to consider the case $1 \leq r < n$. Given the point, $Z$, we
can always assume $Q$ so chosen that $y [Q]$ is reduced. We can
determine a real non-singular matrix $F^{(r)}$ and a diagonal matrix, $H
=(\delta _(\mu \nu)) $ so that $Z [Q] - 2W =(H + i E) [F]$ and then 
\begin{equation*}
x[Q] - 2W =h [F]+ y[Q] = F'F \text{  and  } |Y [Q]| = |F|^2
\tag{244}\label{eq244}  
\end{equation*}

If $Q =(q\mu \nu) =(\mathscr{G}_1 \mathscr{G}_2 \ldots \mathscr{G})$,
$q = \max_{ \mu \nu } \pm q_{\mu \nu}$ and $h = \max_ \nu \pm h_\nu$
we have\pageoriginale from (\ref{eq47}-\ref{eq49}),  
\begin{align*}
|Z|[Q] - 2W & = |Y [Q]|\prod^r_{ \nu = q}( i + h_\nu) \text{ and }\\ 
C_i |Y [Q] |& \leq \prod_[\nu = 1] ^{r} Y [ \mathscr{G'}_\nu
  ,\mathscr{G}_\nu] \tag{245}\label{eq245}  
\end{align*}
where $C_1$, denotes a positive constant depending only upon $n$ and
$\lambda$ denotes the smallest characteristic root of $Y$ . In stating
the last of these inequalities we have only to observe that
$\mathscr{G'}_\nu \mathscr{G}_\nu \geq 1$. If $Q$ is a given primitive
matrix $t$, a given integer, we ask for the number of
quadratic forms $W[\in]$ with rational coefficients and with
a given denominator $d$, consistent with the inequality $t - 1 \leq h
\leq t$. From the relations (\ref{eq244}) it is easy to see that this number
has the upper estimate $(d t q ^2 )^ {r (r +1 )/2}$.  

By lemma (\ref{chap13:lem19})
$$
g (s , W) < \ell d ^{mn - m/2} \text{ and } 
$$
$$
|| Z[Q] - 2w || = | (y [Q])| \Pi ^r _{\nu = 1} ( i + h \nu) |
\leq b_1 t |y[Q]| \text{ with } 
$$
appropriate constants $b$, $b_1$, so that the absolute convergence
of the series under consideration can be reduced to the convergence of
the series .  
\begin{align*}
& \sum^{\infty}_{d=1} \sum^{\infty}_{t=1} \sum_Q (dt q^2)^{r(r+1)/2}
  d^{-mr} d^{mr - \frac{m}{2}} t^{-\frac{m}{2}} |y[Q]|\\
& = \sum^\infty_{d=1} \sum^{\infty}_{t=1} (dt)^{r(r+1)/2-\frac{m}{2}}
  \sum_Q q^{r(r+1)} |y[Q]|^{-\frac{m}{2}}\\
& = \left(\sum^{\infty}_{d=1} d^{\frac{r(r+1)}{2} - \frac{m}{2}}
  \right)^2 \sum_Q q^{r(r+1)} |y[Q]|^{-\frac{m}{2}}
\end{align*}

The series\pageoriginale within the parenthesis is clearly convergent
under our 
assumption $m > n ( n + 1) + 2 > r (r +1 ) + 2$ so that we need only
confine ourselves to the series $\Sigma_{Q} q^{ r(r + s)} | y
[Q]|'^{\dfrac{-m}{2}}$. From (\ref{eq245}) it is clear that this last
series, but for a constant multiplier, is majorised by
$\Sigma_{Q}|y|{[Q]}|^{ \dfrac{r (r +1)}{2}- \dfrac{m}{2}}$, and this
in turn is further majorised by a constant multiple of  
$$
\Sigma_{\mathscr{G}_\nu \neq 0} (\prod ^r _{\nu = 1 }))^ {\frac{ r( r
    + 1 )}{2} } = (\sum_{\mathscr{G} \neq 1}
(\mathscr{G'}\mathscr{G}))^{ \frac{r(r+1)}{2}\frac{-m}{2})^r} 
$$

The number of integral columns $\mathscr{G} \neq 0$ with$t^2 \leq
\mathscr{G'}\mathscr{G} < (t + 1)^2$ where $t$ is a given integer, is
of the order of $t^{n - 1}$ so that the last series can be compared
with $\sum_{t = 1}^{\infty} t^{ n - 1 + r( r + 1) - m}$  

The series converges since $ m >n^2 = ( n - 1) n + n \geq (r + 1) + n
$ and the proof of Lemma \ref{chap13:lem23} is complete.  

We now transform series (\ref{eq243}) for the genus invariant in another
form by means of the calculus matrices. Towards this effect, we put $
- 2 W = C^{-1}_1 D_1$ where $C^{(r)}_1, D^{(r)}_1$ denote symmetric
coprime matrices with $|C_1| \neq 0$. The relation between $W$ and the
class $\{C_1, D_1\}$ is bi-unique as we have seen in pp. 44. According
to Lemma \ref{chap1:lem1}, there is also is a $1 - 1$ 
correspondence between the classes $\big\{ C^{r}_1 D^{r}_1\big \} ,
\big\{ Q^{(r , r)}\big\}$ with $|C_1| \neq 0$, and the classes
$C^{r}_1 D^{r}_1$ with rank $C = r$. Since from (\ref{eq69}),  
\begin{equation*} 
|Z[Q] - 2 W | = |Z[Q] + C^{-1}_1 = |C_1^{-1} CZ | Cz +
D|\tag{246}\label{eq246}   
\end{equation*}
we conclude\pageoriginale from (\ref{eq243}) that
$$ 
f(s , z) =\sum\limits_{\big\{ C^{(n)}, D^{(n)} \big\}} h (c. d) | Czz
+D |^{\dfrac{n}{2}}
$$  
with appropriate coefficients $h(C , D)$and it only remains to
determine useful expressions for $h (C, D)$ .  

We stop her to establish 

\begin{lem}\label{chap13:lem24} %lemma 24
 Let $ C^{(r)}_l$ be an integral non-singular matrix and $q$ a
  positive multiple of $|C_1|$. If $G^{m , r}$ runs through a complete
  set of integral matrices distinct mod. $q$ the $GC_1$ runs through
  exactly $q ^{ m n } || C_1 || ^{-m}$ of them , each of these
  appearing the same number of times. i.e., $|| C_1 ||$ times.  
\end{lem}

\begin{proof}
We can assume that $C_1i$ is a diagonal matrix , as otherwise there
exists unimodular matrices $u_1$, $u_2$ such that the product $u_1$, $C
u_2$ is a diagonal matrix $C_1^*$ and then the products $G_1 C_1 = G_1
u_1 ^{-1} C^*_1 u^{-1}_2$ and $G_2 C_1 = G_2 u_1 ^{-1} C^*_1 u^{-1}_1$
are distinct $\mod. q$ for two matrices $G^*_1$, $G^*_2$ which are
themselves distinct $\mod . q$. This in particular implies that for
the purpose of determining the number of matrices $GC$ distinct $\mod
. q$ it is immaterial whether we argue with $C_1$ or $C^{*}_1$. We
therefore assume that $C_1$ is a diagonal matrix, $C_1 = (\delta _{\mu
  \nu} C_\{\nu\})$, $C_\{\nu\} > 0$. Let $G = ( g _{\mu \nu})$ and $G^*
= (g^*_{ \mu \nu})$ and assume $GC_1\equiv G*C_1 (mod q)$. Then $
g_{\mu \nu } C_\nu \equiv g_{\mu \nu }C_\nu (\mod . q), \mu = 1 , 2 ,
\ldots , m \nu = 1, \ldots r$.  
\end{proof}

By assumption\pageoriginale $|C_1|$ is a divisor of $q$ and a fortiori ,
each $C_\nu$ is a divisor of $q$. It therefore follows from the above
congruence that  
\begin{equation*}
g _{\mu\nu} \equiv g^\ast_{\mu\nu}(q | c_\nu ) \tag{247}\label{eq247} 
\end{equation*}

The number of products $G^* C_1$ which are congruent to $G
C_1\pmod{\cdot q}$ is then just the number of solutions of the
congruence (\ref{eq247}) 
and this number is clearly $ \Pi_{ \mu \ nu } c_\nu = || C_1||^m
$. Consequently , the number of products $GC_1$ distinct $\mod \cdot q$ is
exactly $ q ^{m r } || C_i || ^{-m}, a^{mr}$ being the total number of
$ G = G ^{( m , r)}$ distinct $\mod\cdot q$ and the Lemma is proved.  

Before resuming the main thread we note the following : - 

If $A$, $G$ are integral matrices , then 
\begin{align*}
&  \sigma (S [ A + GC_1] C^{ -1} D_1) p \sigma ( S[A] C^{-1}_1 D_1)\\
& = \sigma (C'_i G' S G D_i) + \sigma (C'_1 G' S A C_1^{-1} D_i) +
  \sigma (A' S G D_i)\\ 
& = \sigma (C'_i G' S G D_i) + D_i) + 2 \sigma (A' S G D_i)\\
& = \sigma (S[G] G_i C'_i) + 2 \sigma (A' S G D_i) \tag{248}\label{eq248} 
\end{align*}

In stating the above relations we have only made use of the fact that
$C^{-1}_1 D_1$ is a symmetric matrix.  

Consider now the sum 
\begin{equation*}
\sum_{ A \mod (2 C_i)|} e^{ - \pi \sigma ( S [A] C^{-1}_1 D_1)}
\tag{249}\label{eq249}  
\end{equation*}
 where\pageoriginale the sum is extended over a complete set of
 integral matrices $A$ 
 such that no two of them differ by a matrix of the form $G 2 C_1$
 having $2 C_1$ as a right divisor. It is immediate from (\ref{eq248}) that
 the sum (\ref{eq249}) is independent of the choice of representatives A
 mod.$(2 C_1|)$. Two cases arise.  

\medskip
\noindent{\textbf{Case. i:}}
Suppose that the congruence $\sigma (s [G]D_i C'_i) \equiv 1 (\mod
. 2)$ is solvable for an integral $G$. Then for that $G$, a
replacement of $A$ by $A + GC_1$ changes the sign of each term of the
sum (\ref{eq249}) as is seen from (\ref{eq248}) while the sum itself is left
unaltered due to such a replacement. It therefore follows that in this
case  
\begin{equation*}
\sum{A \mod ( 2 C_1)|} e^{\pi i \sigma (S[A]C_i^{-1}D_1)} = 0
\tag{250}\label{eq250}  
\end{equation*}

\medskip
\noindent{\textbf{Case. ii:}}
Suppose on the other hand $\sigma ( S[G]D_1 C_1' )\equiv 0 (\mod . 2)$ 
for every integral $G$. Then the general term of (\ref{eq249}) depends only
on $A \mod ( C_\nu $ and we can write the sum as  
\begin{equation*}
\sum_{A\mod (2 C_i| )}e^{\pi i \sigma (S[A]C_1^{-1} D_1) } = 2 ^{m r
}\sum_{A\mod (2 C_i| )}e^{\pi i \sigma (S[A]C_1^{-1} D_1) }
\tag{251}\label{eq251}    
\end{equation*}

Let us now consider the Gaussian sums 
\begin{align*}
g (S, w)& = g (s, \frac{1}{2} C^{-1}D_i) \\
& = \sum_{\mod . d }e^{ - \pi i \sigma S[A] C_1^{-1} D_i} 
\end{align*}


The denominator $d$ of the quadratic form $W [\varepsilon] =
\dfrac{1}{2}(C_i ^{-1} D_1) [\varepsilon]$ is obviously a divisor of
$2 |C_1|$ and let us assume that $|2 C_i| $ divides $q$. Then $A
\equiv A^* (\mod . q)$ implies that $A - A^* \equiv (\mod . 12 C_i |
)$ in\pageoriginale other words $(A - A^*) |2C_i| \equiv 0 (\mod . 1)$
and $A \equiv A^* \mod . (2C_i)|$.   

Thus we obtain that 
\begin{align*}
& g (s, - \frac{1}{2}C_1^{-1} D_i) = \sum_{ A \mod . d} e^{ \sigma
    (S[A]C_i ^{-1}D_1)} e^{ -\pi i \sigma (S [A]C_i^{-1}D_i)}\\ 
& = (\frac{d}{q})^{m r } \sum_{ A \mod q} e^{ - \pi i \sigma (S [A]C_i
    ^{-1} D_1)}\\ 
& = (\frac{d}{q})^{ mr } \sum_{ A \mod (2 C_i)} \sum{\substack{ B \mod
      q \\ B \equiv A \mod (2c_i)|}} e^{ \pi i \sigma
    (S[B]C_1^{-1}D_i)}\\ 
& = (\frac{d}{q})^{ mr } e^{ \pi i \sigma (S[B]C_1^{-1}D_i)}\sum_{ A
    \mod (2 C_i)} \sum{\substack{ B \mod q \\ B \equiv A \mod
      (2c_i)|}} 
\end{align*}
since the term $e^{ \pi i \sigma (S[B]C_1^{-1}D_i)}$is
invariant for a replacement of $B$ by $B^* \equiv B \mod (2 , c_i)|$
and in particular for a replacement of $B$ by $A$. In order to compute
the inner sum we put $B = A + 2 G C_i$ with an integral $G$. If $G$
runs over all cosets $\mod . q$ we obtain according to Lemma
\ref{chap13:lem24}, $q^{
  m r }|| 2 C_i ^{-m}$ distinct cosets. $B \mod . q$, each one of them
$|| 2 C_1||^m $ times. The inner sum is thus equal to $q^{ m r } || 2
C_i||^{-m}$ and we obtain that  
$$ 
d^{ m r } g (S -\frac{1}{2} C_1^{-1} D_1) = || 2C ||^{-m} \sum_{ A
  \mod (2 C_1)} e^{ \pi i \sigma (S[A]C_1^{-1}D_i)} 
$$

In view\pageoriginale of (\ref{eq250}) - (\ref{eq251}) it then turns
out that  
\begin{equation*}
d^{ m r } g (S -\frac{1}{2} C_1^{-1} D_1) =
\begin{cases}
0, or \\
 || C_i ||^{-m} \sum_{ A \mod (2 C_1)} e^{ \pi i \sigma
   (S[A]C_1^{-1}D_i)} \tag{252}\label{eq252}   
\end{cases}
\end{equation*}
according as the congruence $\sigma (S [G] D_i C;_i) \equiv 1 (12)$ is
solvable for an integral $G$ or not.  


From lemma (\ref{chap13:lem23}) and (\ref{eq246}) we now obtain 


\begin{thm}\label{chap13:thm18} %theorem 18
 Let $C^{(m)}$, $D ^{(r)}$ runs over a complete set of non
  associated coprime symmetric pairs of matrices and let $m > n^2 + n
  + 2$. Then 
 \begin{equation*}
f (S, z) = \sum_{ C , D} h ( S, C , D) (C Z +D1)^{ m / 2}
\tag{253}\label{eq253}    
 \end{equation*} 
 where $h (s, o E) = 1 $ and for $C \neq 0$, 
 \begin{equation*}
h ( S, C, D) = 
\begin{cases}
 i 6{mr /2} \frac{- r}{2} || C_i || \frac{-m}{2}\sum_{ A \mod (c_i )},
 or e^{ \pi i \sigma (S[A]C_1^{-1}D_i) }\\ 
 0 \tag{254}\label{eq254}
 \end{cases}
 \end{equation*} 
according as the congruence $\sigma (S [G] D_i C'_1) \equiv 1(2)$ is
not or is solvable for an integral matrix $G$ where $\big\{ C^{(r)}_1
D^{(r)_1}\big \}$ denotes the unique class which corresponds to
$\big\{ C, D\big\}$ by lemma \ref{chap1:lem1}.  
\end{thm}

We have now expressed Siegel's main theorem on the theory of modular
forms in the shape of an analytic identity. It can be shown that $ h
(S, C ,D )$ in (\ref{eq253}) depends only on the cosets of $C$ and $D \mod 4
|5|$ and consequently  
$$
f (s , z ) = \sum_{\{ C, D\} \mod 4 |5|} h (S. C. D) \sum_{\substack{
    \{ C, D\}\\ C \equiv C |\mod 4 |S|) \\ D \equiv D_c }} | C Z +
D|^{ -m / 2} 
$$

In other\pageoriginale words $f (S, Z)$ is expressed as a finite
linear combination of the sums   
$$
\sum_{ \substack{ \{C. D \} \\ C \equiv C_o D \equiv D_o(\mod 4|5|)}} 
$$
of the Einstein series which converges for $m > 2 n +2$. The fourth
power of these sums represent forms of degree $n $ and weight $2m$
with respect to the \textit{ congruence group } $M ( 4 |S|)$
consisting of all modular matrices,  
$$
M \equiv 
\left (
\begin{smallmatrix}
 E & 0\\ 0 & E 
\end{smallmatrix} 
\right )
(\mod . 4 |S|)
$$
 
The same is also true of the fourth power of the Theta- series
$\vartheta (S, z)$ 

A more detailed account of these results, in particular for $ n = 1$,
one finds in a paper of Siegel. ``\"Uber die analytische theorie der
quadratischen formen.'' Ann. Math. 36(1935), 527 - 606.  

