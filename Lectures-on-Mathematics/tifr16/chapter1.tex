\chapter{Weierstrass's Theorem}\label{chap1}

\section{Approximation by Polynomials}\label{chap1:sec1}

A\pageoriginale basic property of a polynomial  $ P (x) = \sum \limits_{0}^{n} a_r
x^r $ is that its value for a given $x$  can be calculated  (e.g. by
a machine) in a finite number of steps. A central problem  of
mathematical  analysis is the approximation to more general functions
by polynomials an the estimation of how small the discrepancy can be
made. A discussion of this problem should be included in any
University course of analysis. Not only are the results important, but
their proofs admirably illustrate a number of powerful methods. 

This account will be confined to the leading theorems, stated in their
fundamental rather than their most general forms. There are many
excellent systematic presentations in the literature, to which this
may serve as an introduction. 

Variables and functions will be real. We say that  $ f (x) $ is $ C (
a,b) $ meaning that $ f (x) $ is continuous for  $ a \leq x \leq b.
p (x) $ or $ q (x) $ always denotes a polynomial; $ p_n (x) $ is a
polynomial of degree at most $n$. In this course, the goodness (or
badness!) of the fit of  a particular polynomial $ p (x) $ to the
function $ f (x) $ will always be measured by  
$$
\sup | f (x) - p (x) |,
$$ 
where the sup. is taken over $ a \leq x \leq b $.

There are other useful ways of defining a `distance' between $ f (x) $
and $ p (x) $,  e.g. 
$$
\int^{b}_{a} \big \{f (x) - p (x) \big \}^2 dx, 
$$
but we shall not deal with them here.

The\pageoriginale interval $(a, b)$ will commonly be taken to be $(0,1)$ or $(
-1, 1)$ as may be convenient in particular context; there will be no
loss of generality. Our enquiry is restricted to finite intervals. The
numbers $\epsilon$ will always be supposed greater than $0$. The Halmos symbol
$///$ denotes the end of a proof.  

\begin{theorem}[Weierstrass 1885]\label{chap1:sec1:thm1}% theorem 1
   If $f (x)$ is  $ C (a, b) $, then, given
    $ \varepsilon $, we can find $ p (x) $ such that 
  $$
  \sup | f (x) - p (x) | < \varepsilon.
  $$
  
  This is the fundamental theorem of the subject. An alternative
  statement of it is that a continuous  function is the sum of a
  uniformly convergent series of polynomials. For let $ p_{n_{1}} (x), 
  p_{n_{2}} (x), \cdots (n_1 \leq n_2 \leq \cdots) $ be polynomials
  corresponding to $ \varepsilon, \dfrac{1}{2} \varepsilon,  \ldots, 
  \varepsilon / 2^n \ldots$. Then the series  
  $$
  p_{n_{1}} (x) + \big \{p_{n_{2}} (x) - p_{n_{1}} (x) \big \} + \cdots  
  $$
  converges uniformly to $ f (x) $.
\end{theorem}

We shall give three proofs of Weierstrass's theorem. The first and
simplest is that of Lebesgue  $ (1898) $. It is based on a
polynomial  approximation to the particular function $ | x | $ in $ (
-1, 1) $. We shall study this function closely in Chapter $ III $,
and shall learn a lot from it.  

\begin{lemma*}
  There is a  sequence of polynomials converging uniformly to $ | x |
  $ for $ -1 \leq x \leq 1 $. 
\end{lemma*}

\begin{proof}
  If $ u = 1 - x^2 $. then $ | x | = \surd (1 - u) $, and $ 0 \leq u
  \leq 1 $ corresponds to $ 1 \ge | x | \ge 0 $.  
\end{proof}

$\sqrt{(1 -u)}$\pageoriginale has a binomial expansion in which the term in $
u^n $ is $ - c_n u^n $ where  
$$
c_n = \frac{1.3.5 \ldots (2n -3)}{2.4.6 \ldots ~ 2 n} \qquad (n \ge 2)
$$

We can prove that this series, which certainly converges for $ | u | <
1 $, also converges for $ u = 1 $. This follows either from Gauss's
test applied to   
$$ 
\frac{c_n}{c_{n+1}} = 1 + \frac{3}{2 n} + 0 \left(\frac{1}{n^2}\right) 
$$
or by proving (on the lines of the Lemma following
Theorem \ref{chap1:sec2:thm2}) that $ c_n \sim \frac{A}{n \surd n} $. 

By Abel's limit theorem, the series for $ \surd (1 -u) $ converges
uniformly for  $ 0 \leq u \leq 1 $,  i.e., $|x|$ is uniform limit of a
sequence of polynomials for $-1 \leq x \leq 1$. 

\begin{coro*}
  Let 
  \begin{align*}
    g (x) &= 0 ~\text{for}~ x < 0 \\
    g (x) &= 0 ~\text{for}~ 0 \leq x \leq k.
  \end{align*} 
  
  Then $ g (x) $ is the limit of a uniformly convergent sequence of
  polynomials in $ -k \leq x \leq k $. 
\end{coro*}

\noindent \textit{Proof.}
  Changing the variable by a factor $k$, we may suppose that $k$ is 1. Then
  \begin{equation*}
    g (x) = \frac{1}{2} (x+|x|).\tag*{$\Box$}
  \end{equation*}

\begin{proofoftheorem}\label{chap1:sec1:pofthm1}
  Given $ \varepsilon $, we can find  a function $l(x)$ whose graph
  is a polygon with vertices at $ (a, y_0), (x_1, y_1),  \ldots, 
  (x_i, y_i),  \ldots,  (b,  y_n) $ such that  
  $$
  | f (x) - l (x) | < \frac{1}{2} \varepsilon.
  $$
\end{proofoftheorem}

Now\pageoriginale $l(x)$ is the sum of constant multiples of functions of the
type $ g (x -x_i) $ defined in the Corollary, namely,  
$$
l (x) = y_0 + \sum^{n-1}_{0} c_i g (x - x_i). 
$$

For the right hand side is linear in each  $(x_i, x_{i+1}) $, and
the $c_i$ give the right  value of $l (x)$ at the vertices if  
\begin{align*}
  y_1 = & y_0 + c_0 (x_1 - x_0)\\
  & \cdots \cdots \\
  y_i =  & y_0 + \sum^{i-1}_{k=0} c (x_i - x_k).
\end{align*}

By the lemma and corollary, we can find a polynomial $ p (x) $ such that 
$
  \begin{aligned}
  & | l (x) - p (x) | < \frac{1}{2} \varepsilon, ~ a \leq x \leq b \\  
  ~\text{and this gives} \qquad  
  &| f(x) - p(x) | <  \varepsilon, ~ a \leq x \leq b.
  \end{aligned}
$

\section{Singular Integrals and Landau's Proof}\label{chap1:sec2}

 Weierstrass's  own proof of Theorem \ref{chap1:sec1:pofthm1} rested on the limit as  $ n
 \to \infty $  of the `singular integral'   
 $$
 \frac{n}{\surd \pi} \int^{\infty}_{-\infty} \exp \left\{-n^2 (t -x
)^2 \right\} f (t) dt.  
 $$
 
The essence of the argument is that, if $n$  is large, the exponential
`kernel' is small except in a small interval round $ t = x $, and so
the integral is nearly equal to  $ f (x) $.  This integral is not,
however, a  polynomial in $x$ and, to complete the proof, Weierstrass
had to approximate  to the exponential by the sum of a finite number
of terms of its series. A natural  step, taken, by Landau and by de la
Vallee Poussin, was to start with a singular integral which  is a
polynomial in $x$. An appropriate kernel to replace Weierstrass's
exponential factor is  
$$
\big \{1 - (t-x)^2 \big \}^n
$$ 
which\pageoriginale (for large $n$)  falls away rapidly from the value $1$  as $t$
moves away form $x$.We need a theorem about the convergence of
singular integrals, and this is best stated for a general kernel $ K_n
(t -x) $.  

\begin{theorem}\label{chap1:sec2:thm2} %\theorem 2
  Let 
  \begin{align*}
    J_n &= \int^{1}_{-1} K_n (u) du \\
    L_n (\delta) & = \int^\delta_{-\delta} K_n (u) du  \qquad (0 <
    \delta < 1)  
  \end{align*}
  \textit{Suppose that}
  \begin{enumerate}[(i)]
  \item $ K_n (u) \ge 0 $
  \item for each fixed $ \delta,   L_n (\delta) / J_n \to 1 $, as $
    n \to \infty $. 
\end{enumerate}

Suppose that $ f (x) $ is  $ C(0, 1) $ and $ 0 < a < b < 1
  $. Then, as $ n \to \infty $, 
$$
I_n (x) = \frac{1}{J_n} \int \limits^{1}_{0} K_n  (t-x) f (t) dt \to  f(x)
$$ 
\textit{uniformly for} $ a \leq x \leq b $.
\end{theorem}

\begin{proof}
  In  $ I_n (x) $, we shall split up the integral over $ (0,1) $
  \begin{equation*}
  I_n (x) = \frac{1}{J_n} \left\{\int^{x-\delta}_{0} +
  \int^{x + \delta}_{x-\delta}  + \int^{1}_{x+\delta} \right\}, \tag{1}
  \end{equation*}
  where $ 0 < x - \delta < x + \delta < 1 $. Consider first the
  integral over  $ (x - \delta, x + \delta) $. Given $ \varepsilon$,
  we can, by the continuity of  $ f (x) $, find $ \delta = \delta (
  \varepsilon) $ such that  
  $$
  | f (t) - f (x) | < \varepsilon  ~ \text{if}~ a \leq x \leq b, | t
  - x | \leq \delta. 
  $$
\end{proof}

Suppose further that $\delta < \min (a, 1-b)$. Then the middle
term on the R. H. S. of (1) 
\begin{align*}
  &=  \frac{1}{J_n} \int^{\delta}_{- \delta} K_n (u) f (x + u) du \\
  &= \frac{L_n (\delta)}{J_n} f (x) + \frac{1}{J_n} \int^{\delta}_{-
    \delta} K_n (u)  \big \{f (x + u) - f (x) \big \} du.
\end{align*}

The\pageoriginale first term in the last line tends to $f (x)$, from $ (ii) $ of the
hypothesis. The second term is, by $(i)$, numerically less than $
\varepsilon L_n (\delta) / J_n $, that is, less than  $ \varepsilon
$. 

Now return to equation $ (1) $ and consider the first term on the
R. H. S. Let $ M = \sup | f (x) | in ~ (0,1) $. 
\begin{align*}
  |\frac{1}{J_n} \int^{x - \delta}_{0} K_n (t -x) f (t) dt | &\leq
  \frac{M}{J_n} \int^{-\delta}_{-x} K_n (u) du \\ 
  &\leq M \left\{1 - \frac{L_n (\delta)}{J_n} \right\}
\end{align*}
$ \to 0 $ as $ n \to \infty $.

A similar estimate holds for the third term of (1). 

All the inequalities in the above argument are independent  of $x$,
and, collecting the results, we have proved that $ I_n (x) \to f (x) $
uniformly for $ a \leq x  \leq b $. 

If, in Theorem 2, we take, following Landau 
$$
K_n (u) =  (1 - u^2)^n, 
$$
then $ I_n (x) $ is a polynomial in $x$ of degree $2n$. We have,
therefore, a second proof of Theorem \ref{chap1:sec1:pofthm1} as soon as we have proved, as
we do in the following Lemma, that this $ K_n (u) $ satisfies the
conditions of Theorem \ref{chap1:sec2:thm2}.  

\begin{lemma*}
  In Theorem $2,  K_n (u) $ may be taken to be $ (1 -u^2)^n $. 
\end{lemma*}

\begin{proof}
  \begin{align*}
    J_n &= \int^{1}_{-1} (1 -u^2)^n du = 2 \int^{\frac{1}{2}_0 \pi}{0}
    \sin^{2 n +1} \theta d \theta \\ 
    &= 2 S_{2n + 1},  ~\text{say}. \\
    S_{2 n+1} &=  \frac{2.4 \ldots 2n}{3.5 \ldots (2 n+1)}
  \end{align*}

  From\pageoriginale the inequalities
  $$
  S_{2n} > S_{2 n+1}> S_{2 n+2},
  $$
  it is  easily proved that 
  $$
  J_n \sim \sqrt{\frac{\pi}{n}} ~\text{and}~  J_n > \sqrt{\frac{\pi}{n+1}}.
  $$
\end{proof} 
 
Then  
$$ 
\displaylines{\hfill 
  1 - \frac{L_n (\delta)}{J_n} ~ = ~ \frac{2
    \int^{1}_{\delta} (1 -u^2)^n du}{J_n} \hfill \cr 
  \hfill < 2 (1 - \delta^2)^n \sqrt{\frac{n+1}{\pi}} \to 0 ~\text{as}~ n \to
  \infty.\hfill}   
$$

\section{Bernstein Polynomials}\label{chap1:sec3} %\section3.

We shall give a third proof of Theorem \ref{chap1:sec1:pofthm1}. It has the advantage of
embodying a definite construction  for the approximating polynomials.  

\begin{defi*}
  Write $l_{n,m} (x) = (^{n}_{m}) x^m (1 -x)^{n-m},  0 \leq m \leq n
  $. The $ n^{th} $ Bernstein polynomials of $f(x)$ in $(0, 1)$
  is  defined to be  
  $$
  B_n (x) = B_n (f ; x) = \sum^{n}_{m=0} f (m /n) l_{n,m} (x).
  $$ 
\end{defi*}
$B_n (x)$ has degree $n$ (at most).

\begin{theorem}\label{chap1:sec3:thm3} % \theorem 3
  Let $f (x)$ be $C (0,  1) $.  Then, as  $ n \to  \infty,  B_n
    (x) \to f (x) $ uniformly.  
\end{theorem}

\begin{note*}
  We can see what underlies this. $ l_{n,  m} (x) $ has a maximum at
  $ x = m/n $. So the terms of $ B_n (x) $  for which $ m / n $ is
  near to $x$ are  those which contribute most. It is, in fact, the
  analogue for a finite sum of the 'singular integral' notion. Then
  two schemes, for sum and integral, could be combined into one by
  using a Steltjes integral. 
\end{note*}

\noindent
\textbf{Lemmas on $l_{n,m} (x) $.}

The\pageoriginale sums on the R.H.S. being taken for values of $m$ such that $ 0
\leq m \leq n $, 
\begin{align*}
  1 &=  \sum l_{n,m} (x) \\
  nx &= \sum m l_{n,m} (x) \\
  nx (1-x) & = \sum (nx-m)^2 l_{n,m} (x).
\end{align*}

\begin{proof}
  With a view to differentiating with regard to $y$, we write 
  $$
  \big \{e^y + (1-x) \big \}^n  = \sum  (^n_m) e^{my} (1 -x)^{n-m}
  $$
  Put $ e^y = x $ and we have  the first result. Differentiate with
  regard to  $y$ and put $ e^y = x $ and we have
  second. Differentiating again gives  
  $$
  nx + n (n - 1) x^2  = \sum m^2 l_{n,m} (x)
  $$
  Multiply the three equations in turn by $n^2 x^2, - 2x,1$ and
  add. This gives the third result in the lemma. 
\end{proof}

\begin{proofoftheorem}\label{chap1:sec3:pofthm3}
  Given $ \varepsilon $, there is $ \delta $ such that $ | f (x_1) - f
  (x_2) | < \varepsilon $ if $ | x_1 - x_2 | < \delta $. Now, 
  $$
  f (x) - B_n (x) = \sum^{n}_{m=0} \big \{f (x) - f (m/n) \big \}  l_{n,m} (x).
  $$
\end{proofoftheorem}

Divide the sum on the  R.H.S. into parts:  $\sum_l$ taken over those
values of $m$  for which $ | x - \dfrac{m}{n} | < \delta$, and $
\sum_2 $ the rest. Then $ | \sum_1 | \leq \varepsilon \sum_1  l_{n,m}
(x) \leq \varepsilon \sum \limits^{n}_{0}~ l_{n.m} (x) = \varepsilon
$. If $ M $ is  $ \sup | f (x) | $ in $ 0 \leq x \leq 1$, 
\begin{align*}
  | \sum_2 | &\leq 2 M \sum_2 l_{n,m} (x) \\
  &\leq 2 M \sum_2 \frac{(nx - m)^2}{n^2 \delta^2} l_{n,m} (x) \\
  &\leq 2 M nx (1-x) / n^2 \delta^2,  ~\text{from the Lemma} \\
  &\leq M/ 2n \delta^2
\end{align*}

So\pageoriginale  $ | f (x) - B_n (x) | \leq | \sum_1 | + | \sum_2 | \leq
\varepsilon + M/2n \delta^2 $. Choose $n > M/2 \varepsilon \delta^2 $  and
the R.H.S. $ \leq 2 \varepsilon $.

\section*{Remarks on Bernstein polynomials.}

\begin{enumerate}[(1)]
\item They have applications to the theory of probability, moment
  problems and the  summation of series. See Lorentz, Bernstein
  polynomials, (Toronto $ 1953) $. 
\item In questions of polynomial approximation, it is a disadvantage
  that the Bernstein polynomial of a polynomial $ p_n (x) $ is not, in
  general, $ p_n (x)$, e.g. 
  \begin{align*}
    &\text{for}~ x^2, B_2 (x) ~\text{is}~ \frac{1}{2} x (1 + x) \\
    &\text{for}~ x (1 -x), B_2 (x) ~\text{is}~ \frac{1}{2} x (1 -x).
  \end{align*}
  For most of the useful systems of polynomials, the approximation
  within the system to a given $ p_n (x) $ is $ p_n (x)$, e.g. with
  Legendre polynomials,  
  $$
  x^2 = \frac{1}{3} P_0 (x) + \frac{2}{3} P_2 (x). 
  $$
\end{enumerate}

\begin{center}
\textbf{Notes on Chapter I}
\end{center}

Notes\pageoriginale at the end of a chapter may include exercises (with hints for
solutions), extensions of the theorem and suggestions for further
reading. 

\begin{enumerate}[1.]
\item In the Lemma of $ \S 1 $, prove that the polynomial consisting
  of the terms up to $x^{2n} $ in the expansion of $ \sqrt{\big \{1 -
  (1-x^2) \big \}} $ approximates to $ |x | $ in $ (-1, 1) $ with a
  greatest error which  $ \sim A/ \sqrt{n} $. 
\item Let $ f (x) = \dfrac{1}{2} - | x - \dfrac{1}{2} |$  in $ (0, 1
 ) $. (This is an adaptation of $ | x | $ to the interval $ (0,1)
 )$. As in $1$, investigate the order of magnitude of the error at $
  x = \dfrac{1}{2} $ given by $ (a) $ the Landau singular  integral, $
  (b) $ the Bernstein, approximations to $ f (x) $. 
\item Theorem \ref{chap1:sec1:thm1} can be extended to a function of
  two (or more) 
  variables, say $ f (x,y) $ for $ 0 \leq x \leq 1,   0 \leq y \leq
  1 $. Suggest a method of proof. 
\item If $ f' (x) $ is continuous, then
  $$
  \frac{d}{dx} B_n (f; x) \to f' (x) ~\text{uniformly.}
  $$ 
  
  A similar result for the Landau integral. 
\item Readers  who like to place theorems on analysis in an abstract
  setting will be interested in Stone's extension of Theorem
  \ref{chap1:sec1:thm1}. See
  Math. Magazine 21  (1948) 167 and 237, or Lorentz, 9, or
  Rudin, Principles of Mathematical Analysis (New York  1953), 
  134. 
\end{enumerate}
\newpage

\begin{center}
\textbf{Hints for {$\mathbf{1-4}$}}
\end{center}

\begin{enumerate}
\item  All\pageoriginale the $ c_n $ are positive $ n \ge 2) $. Error is greatest
  when $ u = 1,  $ i.e., $ x = 0 $, and is $ \sum
  \limits^{\infty}_{n+1}  c_r $. 

  This $ \sim \sum \limits^{\infty}_{n+1} \dfrac{A}{r \sqrt{r}} \sim A
  \int^\infty_n  \dfrac{dx}{x \sqrt{x}} \sim  A / \sqrt{n} $. 
\item $ A / \sqrt{n}$. For (b), approximate to factorials by
  Stirling's formula.  
\item Could use
  $$
  \frac{\int^1_0 \int^1_0 \left\{1 - (t-x)^2 \right\}^n ~ \left\{1- (
    u-y)^2 \right\}^n  ~ f (t,u) dt du}{\left\{\int^{1}_{-1} (1-t^2
   )^n dt \right\}^2} 
  $$
  or (with some labour extend Bernstein's, as in P. L. Butzer,
  Canadian Journal of Mathematics, 5(1953),  107, or Lorentz, 
  51.
\item Lorentz, 26.
  
  For Landau, with $ K_n (u) = (1 -u^2)^n$ in Theorem \ref{chap1:sec2:thm2},
  $$
  \frac{d}{dx} \int^1_0 K_n (t-x) f (t) dt = \int^1_0 \frac{\partial
    K_n}{\partial x} f (t) dt = - \int^1_0 \frac{\partial
    K_n}{\partial_t} f (t) dt  
  $$
  and integrate by parts.
\end{enumerate}
