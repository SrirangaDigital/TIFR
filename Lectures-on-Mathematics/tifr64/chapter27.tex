\chapter{On Lipschitz Square Roots}\label{chap27}

\begin{lemma*}
Let\pageoriginale $f:\mathbb{R}\to \mathbb{R}$ be such that $f(x)\geq
0$, $f(x)\in C^{2}$ and $|f''(x)|\leq A$ on $(-\infty,\infty)$; then
$$
|f'(x)|\leq \sqrt{f(x)}\sqrt{2A}.
$$
\end{lemma*}

\begin{proof}
\begin{align*}
0 &\leq f(y)=f(x)+(y-x)f'(x)+\frac{(y-x)^{2}}{2}f''(\xi)\\
&\leq f(x)+Zf'(x)+\frac{Z^{2}}{2}f''(\xi)
\end{align*}
where $Z=y-x$, or $f(y)\leq f(x)+Zf'(x)+\dfrac{AZ^{2}}{2}$. Therefore
\begin{gather*}
\frac{AZ^{2}}{2}+Zf'(x)+f(x)\geq 0,\q \forall Z\in \mathbb{R}\\
|f'(x)|^{2}\leq 2A\ f(x).
\end{gather*}

So 
$$
|f'(x)|\leq \sqrt{2A}f(x).
$$
\end{proof}

\noindent
{\bf Note.}~ If we take $f(x)=x^{2}$, we note that the constant is the
best possible.

\begin{coro*}
If $f\geq 0$, $|f''|\leq A$, then
$$
|\surd (f(x_{1}))-\surd (f(x_{2}))\leq \surd (A/2)|x_{1}-x_{2}|.
$$
\end{coro*}

\begin{proof}
Let $\epsilon>0$, then $\surd(f(x)+\epsilon)$ is a smooth function.
$$
(\surd(f(x)+\epsilon))'=\dfrac{f'(x)}{2\surd
  (f(x)+\epsilon)}=\frac{(f(x)+\epsilon)'}{2\surd (f(x)+\epsilon)}.
$$

Therefore
$$
|(\surd (f(x)+\epsilon))'|\leq \surd (2A/2)\leq \surd (A/2),
$$
or
$$
|\surd (f(x_{1})+\epsilon)-\surd (f(x_{2})+\epsilon)|\leq \surd
(A/2)|x_{1}-x_{2}|. 
$$\pageoriginale

Let $\epsilon\to 0$ to get the result.

We now consider the general case and give conditions on the matrix $a$
so that $\sigma$ defined by $\sigma\sigma^{*}=a$ is Lipschitz.
\end{proof}

\begin{theorem*}
Let $a:\mathbb{R}^{n}\to S^{+}_{d}$ be continuous and bounded
$C^{2}$-function such that the second derivative is uniformly bounded,
i.e.\@ $||D_{s}D_{r}a_{ij}||\leq M$, where $M$ is independent of $i$,
$j$, $r$, $s$; $(D_{r}\equiv \dfrac{d}{dx_{r}})$. If
$\sigma:\mathbb{R}^{n}\to S^{+}_{d}$ is the unique positive square
root of $a$, then
$$
||\sigma(x_{1})-\sigma(x_{2})||\leq A|x_{1}-x_{2}|,\ \forall
x_{1},x_{2},A=A(M,d). 
$$
\end{theorem*}


\begin{proof}
\setcounter{step}{0}
\begin{step}%1
Let $A\in S^{+}_{d}$ be strictly positive such that $||I-A||<1$. Then
\begin{align*}
\surd A &= \surd (I-(I-A))\\
&= \sum\limits^{\infty}_{r=0}\frac{C_{r}}{r!}(I-A)^{r}, 
\end{align*}
so that on the set $\{A:||I-A||<1\}$ the map $A\to \surd A$ is
$C^{\infty}$ (in fact analytic).
\end{step}

Now assume that $A$ is any positive definite matrix. Let
$\lambda_{1},\ldots,\lambda_{n}$ be the eigen values so that
$\lambda_{j}>0$, $j=1,2\ldots n$. Therefore $I-\epsilon A$ is
aymmetric with eigen values $1-\epsilon \lambda_{j}$. By choosing
$\epsilon$ sufficiently small we can make
$$
||I-\epsilon A||=\max \{1-\epsilon
\lambda_{1},\ldots,1-\epsilon\lambda_{n}\}<1. 
$$

Fixing such an $\epsilon$ we observe that
$$
\surd A=\dfrac{1}{\surd \epsilon}\surd (\epsilon A)=\frac{1}{\surd
  \epsilon}\surd (I-(I-\epsilon A)).
$$\pageoriginale

So the map $A\to\surd A$ is smooth on the set of symmetric positive
definite matrices.

\begin{step}%2
Let $n=1$, $\sigma(t_{0})=\surd (a(t_{0})$ where $a(t_{0})$ is
positive definite. Assume $a(t_{0})$ to be diagonal so that
$\sigma(t_{0})$ is also diagonal.
$$
\sum_{j}\sigma_{ij}(t)\sigma_{jk}(t)=a_{ik}(t).
$$

Differentiating with respect to $t$ at $t=t_{0}$ we get
$$
\sum_{j}\sigma_{ij}(t_{0})\sigma_{jk}'(t_{0})+\sum_{j}\sigma_{ij}'(t_{0})\sigma_{jk}(t_{0})=a'_{ik}(t_{0}) 
$$
or
$$
\surd a_{ii}(t_{0})\sigma'_{ik}(t_{0})+\surd
a_{kk}(t_{0})\sigma'_{ik}(t_{0})=a'_{ik}(t_{0}) 
$$
or
$$
\sigma'_{ik}(t_{0})=\frac{a'_{ik}(t_{0})}{\surd (a_{ii}(t_{0}))+\surd
  (a_{kk}(t_{0}))}.
$$
\end{step}

Since the second derivatives are bounded by $4M$ and
$a_{ii}-2a_{ij}+a_{jj}\geq 0$, we get
\begin{align*}
|a'_{ii}(t)+2a'_{ij}(t)+a'_{jj}(t)| &\leq \surd (8M)\surd
(a_{ii}(t)+2a_{ij}(t)+a_{jj}(t))\\ 
&\leq \surd (8M)\surd 2\surd (a_{ii}+a_{jj})(t)
\end{align*}
or
\begin{equation*}
|a'_{ii}(t)+2a'_{ij}(t)+a'_{jj}(t)|\leq 4\surd M(\surd a_{ii}+\surd
a_{jj}).\tag{1}\label{chap27-eq1} 
\end{equation*}

Since $a$ is non-negative definite,
$$
|a'_{ii}(t)|\leq \surd (2M)\surd (a_{ii}(t)),\ \forall i.
$$\pageoriginale
substituting this in \eqref{chap27-eq1}, we get
$$
|a'_{ij}(t)|\leq 4\surd M(\surd a_{ii}+\surd a_{jj}),
$$
and hence
$$
|\sigma'_{ij}(t_{0})|\leq 4\surd M.
$$

\begin{step}%3
Let $a(t_{0})$ be positive definite and $\sigma$ its positive definite
square root. There exists a constant unitary matrix $\alpha$ such that
$\alpha a(t_0)\alpha^{-1}=b(t_{0})$ is a diagonal positive definite
matrix. Let $\lambda(t_{0})$ be the positive square root of $b(t_{0})$
so that
$$
\lambda(t_{0})=\alpha \sigma(t_{0})\alpha^{-1}.
$$

Therefore $\sigma'(t_{0})=(\alpha^{-1}\lambda'\alpha)(t_{0})$ where
$(\sigma'(t_{0}))_{ij}=\sigma'_{ij}(t_{0})$ and
$$
a''(t_{0})=(\alpha^{-1}b''\alpha)(t_{0}).
$$ 
Since $\alpha$ is unitary.
$$
||\lambda ||=||\sigma||,||a''||=||b''||,||\lambda'||=||\sigma'||.
$$

By hypothesis, $||b''||=||a''||\leq C(d)\cdot M$. Therefore
$$
||\lambda'||\leq 4\surd (MC(d)),
$$ 
i.e.\@ 
$$
||\sigma'||\leq 4\surd (MC(d)).
$$ 

Thus $||\sigma(t_{1})-\sigma(t_{2})||\leq |t_{1}-t_{2}|C(M,d)$.
\end{step}

\begin{step}%4
Let $a:\mathbb{R}\to S^{+}_{d}$ and $\sigma$ be the unique
non-negative definite square root of $a$. For each $\epsilon>0$ let
$a_{\epsilon}=a+\epsilon I$, $\sigma_{\epsilon}=$ unique positive
square root of $a_{\epsilon}$. Then by step 3,
$$
||\sigma_{\epsilon}(t_{1})-\sigma_{\epsilon}(t_{2})||\leq
C(M,d)|t_{1}-t_{2}|.
$$

If $a$ is diagonal then it is obvious that $\sigma_{\epsilon}\to
\sigma$ as $\epsilon\to 0$. In the general case reduce $a$ to the
diagonal form and conclude that $\sigma_{\epsilon}\to \sigma$.

Thus\pageoriginale
$$
||\sigma(t_{1})-\sigma(t_{2})||\leq C(M,d)|t_{1}-t_{2}|.
$$
\end{step}

\begin{step}%5
Let $a:\mathbb{R}^{n}\to S^{+}_{d}$ and $\sigma^{2}=a$, with
$||D_{r}D_{s}a_{ij}||\leq M$, $\forall x$, $i$, $j$; $r$, $s\times
\epsilon \mathbb{R}^{n}$. Choose $x_{1}$, $x_{2}\in
\mathbb{R}^{n}$. Let $x_{1}=y_{1}$, $y_{2},\ldots,y_{n+1}=x_{2}$ be
$(n+1)$ points such that $y_{i}$ and $y_{i+1}$ differ almost in one
coordinate. By Step 4, we have
\begin{equation*}
||\sigma(y_{i})-\sigma(y_{i+1})||\leq C|y_{i}-y_{i+1}|.\tag{*}
\end{equation*}

The result follows easily from the fact that
$$
||x||_{1}=\sum\limits^{n}_{i=1}|x_{i}|\q\text{and}\q
||x||_{2}=(x_{1}+\cdots+x_{n})^{1/2} 
$$
are equivalent norms.

This completes the proof of the theorem.
\end{step}




\end{proof}




