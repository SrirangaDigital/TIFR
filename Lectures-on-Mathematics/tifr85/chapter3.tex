
\chapter{Titchmarsh's Phenomenon}\label{c3}

\section{Introduction}\label{c3:sec3.1}

We have\pageoriginale used the term ``TITCHMARSH'S PHENOMENON'' for the swayings of $|\zeta(\sigma + it)|$ as $\sigma$ is fixed and $t$ varies.  More generally we consider the swayings of $|(\zeta(\sigma + it))^z|(=f_{\sigma} (t) \text{ say})$ where $z = e^{i\theta}$, $\theta$ and $\sigma$ being constants (with $\frac{1}{2} \leq \sigma \leq 1$ and $0 \leq \theta < 2\pi$; we will always use $z$ for $e^{i\theta}$). First of all a remark about extending $\sigma$ to be a constant with $\sigma < \frac{1}{2}$. By the functional equation (see equation (4.12.3) on page 78 of E.C. Titchmarsh \cite{Titchmarsh1}), we obtain for $t \geq 20$ and $\sigma < \frac{1}{2}$, 
$$
f_\sigma (t) = |(\zeta(\sigma + it))^z| \asymp \left( \left( \frac{t}{2\pi}\right)^{(\frac{1}{2} - \sigma) \Cos \theta + t \Sin \theta}  e^{-t \Sin \theta} \right) |(\zeta (1-\sigma - it))^z|,
$$
and so RH implies $f_{\sigma} (t) \to \infty$ as $ t \to \infty$ provided that $0 \leq \theta < \pi$, (because $\log |(\zeta (1-\sigma - it))^z| = \re \log (\zeta(1-\sigma - it))^z = o (\log t)$ if $\sigma <\frac{1}{2}$; see equation (14.2.1) on page 336 of E.C. Titchmarsh \cite{Titchmarsh1}). Also when $\pi \leq \theta < 2\pi$ we have $z = -e^{i\phi}$ where $0 \leq \phi <\pi$. Hence (on RH) when $\pi \leq \theta < 2\pi$ and $\sigma < \frac{1}{2}$, we have $f_{\sigma} (t) \to 0$ as $t \to \infty$. The main question which we ask in this chapter is this: Let us fix $\sigma$ in $\frac{1}{2} \leq \sigma \leq 1$, and put
$$
f(H) = \min\limits_{|I| = H} \max\limits_{t \in I} f_{\sigma} (t).
$$
Then does $f(H) \to \infty$ as $H \to \infty$? If so can we obtain an asymptotic formula for $f(H)$? We obtain a completely satisfactory answer if $\sigma =1$. Put $\lambda(\theta) =\prod\limits_{p} \lambda_p(\theta)$ where 
{\fontsize{10}{12}\selectfont
$$
\lambda_p(\theta) = \left(1-\frac{1}{p} \right) \left(\sqrt{1-\frac{\Sin^2 \theta}{p^2}} -  \frac{\Cos \theta}{p} \right)^{-\Cos \theta} \Exp \left( \Sin \theta \Sin^{-1} \left( \frac{\Sin\theta}{p} \right)\right).
$$}
Then we prove that 
$$
|f(H) e^{-\gamma} (\lambda(\theta))^{-1} - \log \log H |\leq \log \log \log H + O(1).
$$
(Putting\pageoriginale $\theta =0$ and $\pi$ we obtain stronger forms of two results due to earlier authors. For the earlier history see Theorems 8.9(A) and 8.9(B) on page 197 and also pages 208 and 209 of E.C. Titchmarsh \cite{Titchmarsh1}). However we are completely in the dark if $\frac{1}{2} \leq \sigma < 1$. We can only prove that if $T \geq H \geq C \log \log T$ where $C$ is a certain positive constant, then
$$
\max\limits_{T \leq t \leq T + H} f_\sigma (t) > 
\begin{cases}
\Exp \left(\frac{3}{4} \sqrt{\frac{\log H}{\log \log H}} \right) & \text{ if } \sigma = \frac{1}{2};\\[6pt]
\Exp\left(\frac{\alpha_\sigma (\log H)^{1-\sigma}}{\log \log H} \right) & \text{ if } \frac{1}{2} < \sigma < 1,
\end{cases}
$$
where $\alpha_\sigma$ is a certain positive constant. The result just mentioned does not need RH if $\theta = 0$. But we need RH if $0 < \theta < 2\pi$. When $\sigma =1$ our results are completely satisfying since the results are all free from RH. It may also be mentioned (see the two equations proceeding (\ref{c1:eq1.1.1})) that we do not need RH in proving  (\text{that if } $\sigma < \frac{1}{2}$)
$$
\max\limits_{|t-t_0|\leq C(\epsilon)} |\zeta(\sigma + it)| \gg t^{\frac{1}{2} -\sigma-\epsilon}_0
$$
where $C(\epsilon)$ depends only on $\epsilon (0<\epsilon <1)$, and is positive. But RH gives $|\zeta (\sigma + it)| \gg t^{\frac{1}{2} - \sigma - \epsilon}$. The results (with the condition $T \geq H \geq C \log \log T$ mentioned above will be proved by an application of the corollary to the third main theorem (in \S\ \ref{c2:sec2.5}) and will form the substance of \S\ \ref{c3:sec3.2} and \S\ \ref{c3:sec3.3}. However we include in \S\ \ref{c3:sec3.3} some other ``Upper bound results'' regarding the maximum of $f_\sigma (t)$ over certain intervals still with $\frac{1}{2} < \sigma <1$). These results will also be used in the proof of the result on $\sigma =1$, which forms the subject matter of \S\ \ref{c3:sec3.4}.

A new approach to the simpler question (than whether $f(H) \to \infty$ and so on) is due to H.L. Montgomery. He developed a method of proving an $\Omega$ result for $f_\sigma (t)$ namely
$$
f_{\frac{1}{2}} (t) = \Omega \left( \Exp \left( \frac{1}{20} \sqrt{\frac{\log t}{\log \log t}}\right)\right) \text{ on } RH,
$$
and\pageoriginale 
$$
f_\sigma (t) = \Omega \left( \Exp \left( \frac{1}{20} \left(\sigma -\frac{1}{2} \right)^{\frac{1}{2}} \frac{(\log t)^{1-\sigma}}{(\log \log t)^{\sigma}}\right)\right) \text{ if } \frac{1}{2} < \sigma < 1
$$
the latter being independent of R.H. In the $\Omega$ result in $\frac{1}{2} <\sigma < 1$ it is possible (using Montgomery's method) to replace $c(\sigma -\frac{1}{2})^{\frac{1}{2}}$ by $c (\sigma -\frac{1}{2})^{\frac{1}{2}} (1-\sigma)^{-1}$. The method also succeeds in getting $\Omega$ results for $\re (e^{i\theta} \zeta(1+ it))$. For references to these results see the notes at the end of this chapter.

\section{On The line $\sigma =\frac{1}{2}$}\label{c3:sec3.2}
 As a first theorem we prove

\begin{theorem}[on RH]\label{c3:thm3.2.1}
Let $z = e^{i\theta}$ where $\theta$ is a constant satisfying $0\leq \theta < 2\pi$. If $H \leq T$ and $H(\log \log T)^{-1}$ exceeds a certain positive constant, then
$$
\max\limits_{T \leq t \leq T + H} |(\zeta(\frac{1}{2} + it))^z| \geq \Exp \left(\frac{3}{4} \sqrt{\frac{\log H}{\log \log H}} \right).
$$
\end{theorem}

\setcounter{remark}{0}
\begin{remark}%%% 1
We do not read $RH$ if $z =1$.
\end{remark}

\begin{remark}%%% 2
We prove the theorem with a certain positive constant $< \frac{3}{4}$ in place of $\frac{3}{4}$. Replacing Lemma \ref{c3:lem3.2.2} of the present section by a more powerful lemma due to R. Balasubramanian we get $\frac{3}{4}$. See the notes at the end of this chapter.
\end{remark}

\begin{sublemma}\label{c3:lem3.2.1}
Given any $t \geq 10$ there exists a real number $\tau$ with $|t-\tau| \leq 1$, such that 
\begin{equation*}
\frac{\zeta'(\sigma + i\tau)}{\zeta(\sigma + i\tau)} = ((\log t)^2) \tag{3.2.1}\label{c3:eq3.2.1}
\end{equation*}
uniformly in $-1 \leq \sigma \leq 2$. Hence
\begin{equation*}
\log \zeta (\sigma + i\tau) = O ((\log t)^2) \tag{3.2.2}\label{c3:eq3.2.2}
\end{equation*}
uniformly in $-1 \leq \sigma \leq 2$.
\end{sublemma}

\begin{proof}
See Theorem 9.6(A), page 217 pf E.C. Titchmarsh \cite{Titchmarsh1}.
\end{proof}

\begin{sublemma}\label{c3:lem3.2.2}
Let 
\begin{equation*}
F(s) = \left(\zeta\left( \frac{1}{2} + s\right) \right)^{kz} = \sum\limits^\infty_{n=1} \frac{a_n}{n^s} (\sigma \geq 2), \tag{3.2.3}\label{c3:eq3.2.3}
\end{equation*}
where\pageoriginale $k \geq 10000$ is any integer. Let $x \geq 1000$,
\begin{equation*}
\prod\limits_{k^3 \leq p \leq k^4} \left( 1+ \frac{k^2}{p^s}\right) = \sum\limits^\infty_{n=1} \frac{b_n}{n^s}. \tag{3.2.4}\label{c3:eq3.2.4}
\end{equation*}
Then
\begin{gather*}
\sum\limits_{n \leq x} \frac{b_n}{n} \leq \sum\limits_{n \leq x} |a_n|^2, \tag{3.2.5}\label{c3:eq3.2.5}\\
\prod\limits_{k^3 \leq p \leq k^4} \left(1+\frac{k^2}{p} \right) > \Exp (k^2 \log \frac{5}{4}), \tag{3.2.6}\label{c3:eq3.2.6}
\end{gather*}
and
\begin{equation*}
\prod\limits_{k^3 \leq p \leq k^4} \left(1 + \frac{k^2}{p^{1+\delta}} \right) < \Exp (k^2 e^{-C_1}), 
\tag{3.2.7}\label{c3:eq3.2.7}
\end{equation*}
where $\delta =\frac{C_1}{\log k}$ and $C_1$ is any positive constant. Also
\begin{gather*}
\sum\limits_{n\leq x} \frac{b_n}{n}  > \Exp \left(k^2 \log \frac{5}{4} \right) - x^\delta \sum\limits_{n >x} \frac{b_n}{n^{1+\delta}} \tag{3.2.8}\label{c3:eq3.2.8}\\
> \frac{1}{2} \Exp \left(k^2 \log \frac{5}{4} \right) \tag{3.2.9}\label{c3:eq3.2.9}
\end{gather*}
provided
\begin{equation*}
k \geq C_2 \left( \frac{\log x}{\log \log x}\right)^{\frac{1}{2}} \tag{3.2.10}\label{c3:eq3.2.10}
\end{equation*}
where $C_2$ is a positive constant.
\end{sublemma}

\begin{proof}
The equation (\ref{c3:eq3.2.5}) is trivial. The equations (\ref{c3:eq3.2.6}) and (\ref{c3:eq3.2.7}) follow from $\log (1+y) <y$ and $\log (1+ y) > y -\frac{1}{2} y^2$ for $0 < y <1$. The equation (\ref{c3:eq3.2.8}) is trivial whereas (\ref{c3:eq3.2.9}) follows if $x^\delta \leq \Exp (k^2 e^{-C_1})$ and $C_1$ is large. This leads to the condition (\ref{c3:eq3.2.10}) for the validity of (\ref{c3:eq3.2.9}).
\end{proof}

\begin{sublemma}\label{c3:lem3.2.3}
We have, with $k = [C_3 (\log H)^{\frac{1}{2}} (\log \log H)^{-\frac{1}{2}}]$,
\begin{equation*}
\left(\frac{1}{2} \sum\limits_{n\leq \alpha H} |a_n|^2 \right)^{\frac{1}{2k}} >\Exp (k \log (100/99)) \tag{3.2.11}\label{c3:eq3.2.11}
\end{equation*}
where\pageoriginale $\alpha$ is an in corollary to the third main theorem (see \S\ \ref{c2:sec2.5}) and $C_3$ is a certain positive constant.
\end{sublemma}

\begin{proof}
Follows from (\ref{c3:eq3.2.5}), (\ref{c3:eq3.2.9}) and (\ref{c3:eq3.2.10}).
\end{proof}

\begin{sublemma}\label{c3:lem3.2.4}
The condition $|a_n| <(nH)^A$ is satisfied for some integer constant $A >0$.
\end{sublemma}

\begin{proof}
Trivial since, for large $H$, we have
$$
\frac{|a_n|}{n^2} \leq \sum\limits^\infty_{n=1}  \frac{|a_n|}{n^2} \leq \sum\limits^{\infty}_{n=1} \frac{d_k (n)}{n^2} = (\zeta(2))^k \leq H^2. 
$$
Lemmas \ref{c3:lem3.2.1}, \ref{c3:lem3.2.2}, \ref{c3:lem3.2.3} and \ref{c3:lem3.2.4} complete the proof of Theorem \ref{c3:thm3.2.1}, since by Lemma \ref{c3:lem3.2.1} we have
\begin{equation*}
(\zeta( \sigma + it))^z = \Exp (O((\log t)^2)) \quad \left(\sigma \geq \frac{1}{2} \right) 
\tag{3.2.12}\label{c3:eq3.2.12}
\end{equation*}
on two suitable lines (necessary for the application of the third main theorem) $t =t_1$ and $t = t_2$. (It is here that we need the condition that $H(\log \log T)^{-1}$ should exceed a large positive constant).
\end{proof}

\begin{theorem}[On RH]\label{c3:thm3.2.2}
For all $H$ exceeding a suitable positive constant, we have,
$$
\max\limits_{\sigma \geq \frac{1}{2}} \max\limits_{T \leq t \leq T + H} |(\zeta(\sigma + it))^z| > \Exp \left(\frac{3}{4} \sqrt{\frac{\log H}{\log \log H}} \right),
$$
$z$ being as in Theorem \ref{c3:thm3.2.1}. 
\end{theorem}

\begin{remark*}
As in Theorem \ref{c3:thm3.2.1} we do not need RH if $z =1$. Also the previous remark regarding the constant $\frac{3}{4}$ stands. 
\end{remark*}

\begin{proof}
Assume that Theorem \ref{c3:thm3.2.2} is false. Then
$$
\max\limits_{\sigma \geq \frac{1}{2}} \max\limits_{T \leq t \leq T + H} |(\zeta(\sigma + it))^z| < H. 
$$
This is\pageoriginale enough to prove (by the method of proof of Theorem \ref{c3:thm3.2.1}) that on $\sigma = \frac{1}{2} + (\log H)^{-1}$ we have
$$
\max\limits_{T \leq t \leq T + H} |(\zeta(\sigma + it))^z| > \Exp \left(\frac{3}{4} \sqrt{\frac{\log H}{\log \log H}} \right).
$$
This completes the proof of Theorem \ref{c3:thm3.2.2}.
\end{proof}

If we are particular about the line $\sigma =\frac{1}{2}$ (and smaller $H$ than in Theorem \ref{c3:thm3.2.1}) we have

\begin{theorem}\label{c3:thm3.2.3}
From the interval $[T,2T]$ we can exclude $T (\log T)^{-20}$ intervals of length $(\log T)^2$ with the property that in the remaining intervals $I$, we have
\begin{equation*}
\max\limits_{s \in (\frac{1}{2}, \infty) \times I} |\zeta(s)|^2 \leq (\log T)^{30}.  \tag{3.2.13}\label{c3:eq3.2.13}
\end{equation*}
Let $H \leq (\log T)^2$ and $I_0$ be any interval of length $H$ contained in $I$. Then for $z =e^{i\theta}$, we have
$$
\max\limits_{t \in I_0} |(\zeta(\frac{1}{2} + it))^z| > \Exp \left( \frac{3}{4} \sqrt{\frac{\log H}{\log \log H}}\right)
$$
provided only that $H(\log \log \log T)^{-1}$ exceeds a suitable positive constant and $z=1$. If $z \neq 1$, then we have to assume $RH$ and to replace $\frac{1}{2}$ by $\frac{1}{2} + (\log \log T)^{-1}$.
\end{theorem}

\setcounter{remark}{0}
\begin{remark}%%% 1
The previous remark about the constant $\frac{3}{4}$ stands. The question of proving Theorem \ref{c3:thm3.2.3} (when $z \neq 1$) without replacing $\frac{1}{2}$ by $\frac{1}{2} + (\log \log T)^{-1}$ is an open question.
\end{remark}

\begin{remark}%%% 2
We can for $z \neq 1$ replace $\frac{1}{2}$ by $\frac{1}{2} + (\log \log T)^{-1000}$.
\end{remark}

\begin{remark}%%% 3
Theorems such as \ref{c3:thm3.2.1}, \ref{c3:thm3.2.2} and \ref{c3:thm3.2.3} have applications to the variation of $\arg \zeta(s)$ over short $t$-intervals.
\end{remark}

\section{On the Line $\sigma$ with $\frac{1}{2} <\sigma < 1$}\label{c3:sec3.3}
As a simple application of the corollary to the third main theorem (see \S\ \ref{c2:sec2.5}) we first prove Theorems \ref{c3:thm3.3.1}, \ref{c3:thm3.3.2}, and\pageoriginale \ref{c3:thm3.3.3} to follows.

\begin{theorem}\label{c3:thm3.3.1}
Let $f_\sigma (t) = |(\zeta (\sigma + it))^z|$ where $z = e^{i\theta}$ and $T^{\frac{1}{3}} \leq H \leq T$. Then for $\frac{1}{2} < \alpha <1$, there holds
$$
\max\limits_{T \leq t \leq T+ H} f_\alpha (t) >\Exp \left(C_1 \frac{(\log H)^{1-\alpha}}{\log \log H} \right)
$$
where $C_1$ is a certain positive constant.
\end{theorem}

\begin{remark*}
If $z=1$ then the restriction on $H$ can be relaxed to $H \leq T$ and that $H(\log \log T)^{-1}$ shall be bounded below by a certain positive constant. However when $z \neq 1$, we need to assume RH to uphold the corresponding result.
\end{remark*}

\begin{theorem}[on RH]\label{c3:thm3.3.2}
For all $H$ exceeding a suitable positive constant, we have, for $\frac{1}{2} < \alpha < 1$, 
$$
\max\limits_{\sigma \geq \alpha} \max\limits_{T \leq t \leq T + H} |(\zeta(\sigma + it))^z| >\Exp \left(C_1 \frac{(\log H)^{1-\alpha}}{\log \log H} \right)
$$
$z$ and $C_1$ being as before.
\end{theorem}

\begin{remark*}
If $z =1$ RH is not necessary.
\end{remark*}

\begin{theorem}\label{c3:thm3.3.3}
Divide $[T,2T]$ into abutting intervals $I$ of fixed length $H$
(ignoring a bit at one end) where $H$ exceeds a sufficiently large
constant and $H(\log \log T)^{-1}$ is bounded above by any fixed
constant. Then there exists a positive constant $\beta'$ such that
with the exception of $T(\Exp\break\Exp (\beta'H))^{-1}$ intervals $I$, we
have, 
$$
\max\limits_{t\in I} |(\zeta(\alpha + it))^z| > \Exp \left(C_1 \frac{(\log H)^{1-\alpha}}{\log \log H} \right)
$$
where as usual $z =e^{i\theta}$.
\end{theorem}

\setcounter{remark}{0}
\begin{remark}%%% 1
Note that this theorem does not depend on RH.
\end{remark}

\begin{remark}%%% 2
We can replace $[T,2T]$ by $[T, T+T^{\frac{1}{3}}]$. The number of exceptions will then be $T^{\frac{1}{3}} (\Exp \Exp (\beta' H))^{-1}$.
\end{remark}

First\pageoriginale we give details of proof for Theorem \ref{c3:thm3.3.1} and then briefly sketch the proof of Theorem \ref{c3:thm3.3.3}. The proof of Theorem \ref{c3:thm3.3.2} is similar to that of Theorem \ref{c3:thm3.2.2}. We begin with

\begin{sublemma}\label{c3:lem3.3.1}
Let $\frac{1}{2} \geq \beta \leq 1$ and $H = T^{\frac{1}{3}}$. Then the number of zeros of $\zeta(s)$ in $(\sigma \geq \beta, \; T \leq t \leq T + H)$ is
\begin{equation*}
\ll H^{4(1-\beta)/(3-2\beta)} (\log T)^{100} \tag{3.3.1}\label{c3:eq3.3.1}
\end{equation*}
where the constant implied by the Vinogradov symbol $\ll$ is absolute.
\end{sublemma}

\begin{proof}
This is a consequence of a deep result of R. Balasubramanian \cite{Balasubramanian2} on the mean square of $|\zeta(\frac{1}{2} + it)|$. (See Theorem 6 on page 576 of his paper; see also K. Ramachandra \cite{Ramachandra2}).
\end{proof}

\begin{sublemma}\label{c3:lem3.3.2}
Let $\alpha$ and $\beta$ be constants satisfying $\frac{1}{2} < \beta <\alpha <1$. Then there exists a $t$-interval $I$ contained in $T \leq t \leq T + H$ of length $T^{\delta}$ (where $\delta > 0$ depends only on $\alpha$ and $\beta$) such that the region $(\sigma \geq \beta, t \in I)$ is free from zeros of $\zeta(s)$.
\end{sublemma}

\begin{proof}
Follows from lemma \ref{c3:lem3.3.1}.
\end{proof}

\begin{sublemma}\label{c3:lem3.3.3}
Let $I_0$ denote the $t$-interval obtained from I by removing, on both sides, intervals of length $\frac{1}{100} T^\delta$. Then in $(\sigma \geq \alpha, t \in I_0)$, we have,
$$
\log \zeta(s) = O(\log T)
$$
\end{sublemma}

\begin{proof}
Follows by Borel-Caratheodory Theorem (see Theorem \ref{c1:thm1.6.1}). 
\end{proof}

\begin{sublemma}\label{c3:lem3.3.4}
We apply the corollary to the third main theorem (see \S\ \ref{c2:sec2.5}) to the interval $I_0$ in place of $T \leq t \leq T + H$. Then (with $z = e^{i\theta}$), we have,
\begin{equation*}
\max\limits_{(\sigma = 0 , t \in I_0)} |(\zeta(\alpha+s))^z| \geq \left(\frac{1}{2} |a_n|^2 \right)^{\frac{1}{2k}} \tag{3.3.2}\label{c3:eq3.3.2}
\end{equation*}
where $n$ is any integer not exceeding $\alpha' H$ where $\alpha'$ is the $\alpha$ of the corollary to the third main theorem and $k \geq 1000$ is $O(\log H)$ and the numbers $a_n$ are defined\pageoriginale by
\begin{equation*}
F(s) = (\zeta(\alpha + s))^{kz} = \sum\limits^{\infty}_{n=1} \frac{a_n}{n^s}. \tag{3.3.3}\label{c3:eq3.3.3}
\end{equation*}
\end{sublemma}

\begin{proof}
It is easily seen (as before) that the conditions for the application of the corollary to the third main theorem are satisfied.
\end{proof}

\begin{sublemma}\label{c3:lem3.3.5}
Let
\begin{equation*}
n = \prod p \tag{3.3.4}\label{c3:eq3.3.4}
\end{equation*}
where $p$ runs over all  the primes in the interval $[(\frac{k}{4})^{\frac{1}{\alpha}}, (\frac{k}{2})^{\frac{1}{\alpha}}]$. Then
$$
|a_n|^2 \geq \Exp \left( \frac{C_1 k^{\frac{1}{\alpha}}}{\log k}\right)
$$
where $C_1$ is a positive constant.
\end{sublemma}

\begin{proof}
Note that $k^2 p^{-2\alpha}$ is bounded below by a constant $>1$ and that the number of primes in (\ref{c3:eq3.3.4}) is $\gg k^{\frac{1}{\alpha}} (\log k)^{-1}$. This proves Lemma \ref{c3:lem3.3.4}.
\end{proof}

\begin{sublemma}\label{c3:lem3.3.6}
Let $k = [C_2 (\log H)^{\alpha}]$ where $C_2 >0$ is a small constant. Then, we have,
\begin{equation*}
n \leq \alpha' H, \tag{3.3.5}\label{c3:eq3.3.5}
\end{equation*}
and so RHS of 3.3.2 exceeds 
\begin{equation*}
\Exp \left( \frac{C_3 (\log H)^{1-\alpha}}{\log \log H}\right) \tag{3.3.6}\label{c3:eq3.3.6}
\end{equation*}
where $C_3 > 0$ is a constant.
\end{sublemma}

Theorem \ref{c3:thm3.3.1} follows from (\ref{c3:eq3.3.2}) and (\ref{c3:eq3.3.6}). We now briefly sketch the proof of Theorem \ref{c3:thm3.3.3}. Let $\beta$ be a constant satisfying $\frac{1}{2} < \beta < \alpha <1$. The number of zeros of $\zeta(s)$ in $(\sigma \geq \beta, T \leq t \leq 2T)$ is $< T^{1-\delta}$ for some positive constant $\delta$. Omit those intervals $I$ for which $(\sigma \geq \beta, t \in I)$ contains a zero of $\zeta(s)$. (The number of intervals omitted is $\leq T^{1-\delta}$). Denote the remaining intervals\pageoriginale by $I'$. For these, we have, by standard methods (see Theorem \ref{c1:thm1.7.1}) the inequality
$$
\sum\limits_{I'} \max\limits_{\sigma \geq \beta, t \in I'} |\zeta(\sigma + it)|^2 \ll T
$$
and so the number of intervals $I'$ for which the maximum exceeds $\Exp\break\Exp (2\beta' H)$ does not exceed $T(\Exp\Exp (2\beta' H))^{-1}$. Omit these also. Call the remaining intervals $I''$. Applying Borel-Caratheodory theorem (see Theorem \ref{c1:thm1.6.1}), we find that for suitable $t$-intervals at both ends of $I''$ and $\sigma \geq \alpha$ we have $\log \zeta(s) = O (\Exp (2\beta' H))$ and so $(\zeta(s))^z= O(\Exp \Exp (2\beta' H))$. We can now apply the corollary to the third main theorem to obtain Theorem \ref{c3:thm3.3.3} since plainly the number of omitted intervals is
$$
\leq T^{1-\delta} + T(\Exp \Exp (2\beta' H))^{-1} \leq T (\Exp \Exp (\beta'H))^{-1}
$$
if $\beta'$ is small enough.

In the remainder of this section namely \S\ \ref{c3:sec3.3}, we concentrate on proving the following theorem. (For the history of this theorem and other interesting results see R. Balasubramanian and K. Ramachandra \cite{Balasubramanian and Ramachandra7}). This theorem will be used in \S\ \ref{c3:sec3.4}. For the sake of convenience we adopt the notation of the paper of R. Balasubramanian and myself cited just now and whenever we apply this theorem we take care to see that there is no confusion of notation.

\begin{theorem}\label{c3:thm3.3.4}
Let $\alpha$ be a fixed constant satisfying $\frac{1}{2} < \alpha < 1$ and $E >1$ an arbitrary constant. Let $C \leq H \leq T/100$ and $K = \Exp \left(\frac{D\log H}{\log \log H} \right)$ where $C$ is a large positive constant and $D$ an arbitrary positive constant. Then there are $\geq T K^{-E}$ disjoint intervals $I$ of length $K$ each, contained in $[T, 2 T]$ such that 
$$
\frac{(\log K)^{1-\alpha}}{(\log \log K)^\alpha} \ll \max\limits_{t \in I}|\log \zeta(\alpha + it)| \ll \frac{(\log K)^{1-\alpha}}{(\log \log K)^{\alpha}}.
$$
Furthermore
$$
\max\limits_{\sigma \geq \alpha, t \in I} |\log \zeta(\sigma + it)| \ll \frac{(\log K)^{1-\alpha}}{(\log\log K)^{\alpha}}.
$$
 \end{theorem}

\begin{remark*}
Here as\pageoriginale elsewhere $\log \zeta(s)$ is the analytic continuation along lines parallel to the $\sigma$-axis (we chose those and only those lines which do not contain a zero or a pole of $\zeta(s)$) of $\log \zeta(s)$ in $\sigma \geq 2$.
\end{remark*}

We first outline the proof of this theorem and reduce it to the proof of Theorem \ref{c3:thm3.3.5} and \ref{c3:thm3.3.6} below. Let $\beta_0, \beta_1$ and $\beta$ be constants satisfying $\frac{1}{2} < \beta_0 <\beta_1 < \beta < \alpha <1$. It is well-known that
$$
\frac{1}{T} \int^{2T}_T |\zeta \left(\frac{1}{2} + it \right)|^2 dt = O (\log T).
$$ 
From this it follows that there are $\gg T H^{-1}$ disjoint intervals $I_0$ for $t$ (ignoring a bit at one end) each of length $H + 20 (\log H)^2$ contained in $[T, 2T]$ for which
\begin{equation*}
\int_{2 \geq \sigma \geq \beta_0} \int_{t\in I_0} |\zeta(s)|^2 dt d\sigma \ll H  \tag{3.3.7}\label{c3:eq3.3.7}
\end{equation*}
and 
\begin{equation*}
\int_{t \in I_0}|\zeta(\beta_1 + it)|^2 dt \ll H.  \tag{3.3.8}\label{c3:eq3.3.8}
\end{equation*}
From (\ref{c3:eq3.3.7}) and (\ref{c3:eq3.3.8}) it follows by standard methods (explained in my booklet on Riemann zeta-function published by Ramanujan Institute, now using Jensen's theorem see E.C. Titchmarsh \cite{Titchmarsh2} on page 125; See also Chapter \ref{c7} of the present book) that $N(\beta, I_O)$ is the number of zeros $\rho$ of $\zeta(s)$ with $\re \rho \geq \beta$ and $\Iim \rho$ lying in $I_0$. Hence if we divide $I_0$ into abutting intervals (ignoring a bit at one end) $I_1$ each of length $H^{\theta} + 20 (\log H)^2$ whee $\theta =\frac{\delta}{2}$, the number of intervals $I_1$ is $\sim H^{1-\theta}$. Out of these we omit those $I_1$ for which $(\sigma \geq \beta, t \text{ in } I_1)$ contains a zero of $\zeta(s)$. (They are not more than a constant times $H^{1-\delta}$ in number). We now consider a typical interval $I_1$ which is such that $(\sigma \geq \beta, t \text{ in } I_1)$ is zero-free. Let us designate this $t$-interval by $[T_0 -10(\log H)^2, T_0 + H^\theta + 10 (\log H)^2]$. Put
\begin{equation*}
H_1 = H^\theta \text{ and } k = \left[ \frac{C_1 \log H}{\log \log H}\right]
\tag{3.3.9}\label{c3:eq3.3.9}
\end{equation*}
where\pageoriginale $C_1$ is a small positive constant. Then we prove the following Theorem. 

\begin{theorem}\label{c3:thm3.3.5}
We have,
\begin{equation*}
\int^{T_0 + H_1}_{T_0} |\log \zeta(\alpha + it)|^{2k} dt > C^k_2 A^{2k}_k H_1  \tag{3.3.10}\label{c3:eq3.3.10}
\end{equation*}
and
\begin{equation*}
\int^{T_0 + H_1}_{T_0} |\log \zeta(\alpha + it)|^{4k} dt < C^{2k}_3 A^{4k}_{2k} H_1 < 2^{4k} C^{2k}_3A^{4k}_k H_1  \tag{3.3.11}\label{c3:eq3.3.11}
\end{equation*}
where $A_k =k^{1-\alpha} (\log k)^{-\alpha}$, and $C_2,C_3$ are positived constants independent of $C_1$.
\end{theorem}

\begin{coro*}
Divide $[T_0, T_0 + H_1]$ into equal (abutting) intervals $I$ each of length $K$ (neglecting a bit at one end). Then the number $N$ of intervals $I$ for which 
\begin{equation*}
\int_I |\log \zeta(\alpha + it)|^{2k} dt > \frac{1}{4} C^k_2 A^{2k}_k K \tag{3.3.12}\label{c3:eq3.3.12}
\end{equation*}
satisfies $N \geq -1 + \frac{1}{16} (\frac{C_2}{4C_3})^{2k} H_1 K^{-1}$ and so, in these intervals
$$
\max\limits_{t \in I} |\log \zeta(\alpha + it)| \gg A_k.
$$
\end{coro*}

\medskip
\noindent{\textbf{Proof of the Corollary.}} Put $J = \int_I |\log \zeta(\alpha + it)|^{2k} dt$. Then
$$
\sum\limits_1 J > \frac{1}{2} C^k_2 A^{2k}_k H_1
$$
since the contribution from the neglected bit is not more than
$K^{\frac{1}{2}} (2^{4k}\break C^{2k}_3 A^{4k}_k H_1)^{\frac{1}{2}}$ on
using (\ref{c3:eq3.3.11}). Let 
$$
\sum\limits_1 J = \sum\limits_{I, J \leq \frac{1}{4} C^k_2 A^{2k}_k K} J 
$$
and $\sum_2$\pageoriginale the sum over the remaining intervals $I$. Then $\sum_2 J > \frac{1}{4} C^k_2 A^{2k}_k H_1$. Put  $\sum_2 1 =N$. Then by H\"older's inequality we have
\begin{align*}
\frac{1}{4} C^k_2 A^{2k}_2 H_1 & < N^{\frac{1}{2}} \left(\sum_2 J^2 \right)^{\frac{1}{2}}\\
& \leq N^{\frac{1}{2}} \left(\sum_2 \int_I |\log \zeta(\alpha + it)|^{4k} dt K \right)^{\frac{1}{2}}\\
& \leq N^{\frac{1}{2}} K^{\frac{1}{2}} (2^{4k} C^{2k}_3 A^{4k}_k H_1)^{\frac{1}{2}}. 
\end{align*}
Hence $N \geq \frac{1}{16} \left( \frac{C_2}{4C_3}\right)^{2k} H_1 K^{-1}$. This proves the corollary.

\begin{theorem}\label{c3:thm3.3.6}
Let $J_1$ be the maximum over $(\re s \geq \alpha,\Iim s \text{ in } I)$ of $|\log \zeta(s)|^{2k}$. Then with the notation introduced above and $I = [a,b]$, we have,
\begin{align*}
E_2 J_1 & \leq (\log H)^2 \sum_2 \int_{\alpha -1 \leq t \leq b+1} \int_{\sigma \geq \alpha - (\log H)^{-1}} |\log \zeta(s)|^{2k} d \sigma \; dt\\
& \leq 2 (\log H)^2 \int\int_{\substack{T_0 - 1 \leq t \leq T_0 + H_1 +1\\ \sigma \geq \alpha - (\log H)^{-1}}} |\log \zeta(s)|^{2k} d \sigma \; dt\\
& \leq 2 (\log H)^2 C^k_4 A^{2k}_k H_1, \tag{3.3.13}\label{c3:eq3.3.13}
\end{align*}
where $C_4 > 0$ is independent of $C_1$. 
\end{theorem}

\begin{coro*}
Of any $\frac{N}{2}$ of the summands $J_1$ appearing in $\sum_2$ the minimum $J_1$ does not exceed
\begin{equation*}
\frac{2(\log H)^2 C^k_4 A^{2k}_k H_1}{-1 + \frac{1}{16} (\frac{C_2}{4C_3})^{2k} H_1 K^{-1}}. 
\tag{3.3.14}\label{c3:eq3.3.14}
\end{equation*}
Hence the $\max\limits_{t \in I} |\log \zeta(\alpha+ it)|$ over those intervals $I$ is $\ll A_k$.
 \end{coro*}

Combining corollaries to Theorems \ref{c3:thm3.3.5} and \ref{c3:thm3.3.6}, we have $\geq -1 + \frac{1}{32} (\frac{C_2}{4C_3})^{2k} H_1 K^{-1} (= M$ say ) intervals $I$ contained in $I_1$ for which there holds 
\begin{equation*}
A_k \ll \max\limits_{t \in I} |\log \zeta(\alpha + it)| \ll A_k. \tag{3.3.15}\label{c3:eq3.3.15}
\end{equation*}
Now by choosing $C_1$ small we have $M \gg H_1 K^{-E} $ where $H_1 = H^{\theta}$ and the number of intervals $I_1$ is $\sim H^{1-\theta}$. Since $I_1$ is contained in $I_0$ and the number of intervals $I_0$ is $\gg TH^{-1}$ we have in all 
\begin{equation*}
H^{\theta} K^{-E} H^{1-\theta} TH^{-1} = T K^{-E} \tag{3.3.16}\label{c3:eq3.3.16}
\end{equation*}\pageoriginale 
disjoint intervals $I$ of length $K$ each, where (\ref{c3:eq3.3.15}) holds. This completes the proof of Theorem \ref{c3:thm3.3.4} provided we prove Theorems \ref{c3:thm3.3.5} and \ref{c3:thm3.3.6}

We now develop some preliminaries to the proofs of Theorems \ref{c3:thm3.3.5} and \ref{c3:thm3.3.6}. From (\ref{c3:eq3.3.7}) using the fact that the absolute value of an analytic function at a point does not exceed its mean-value over a disc (say of radius $(\log H)^{-1}$) round that point as centre, we obtain
{\fontsize{10}{12}\selectfont
$$
|\zeta(s)| \leq H^2 \text{ in }(\sigma \geq \beta + (\log H)^{-1}, T_0
-9 (\log H)^2 \leq  t \leq T_0 + H_1 + 9 (\log H)^2).  
$$}
Hence in this region $\re \log \zeta(s) \leq 2 \log H$. Now $\log \zeta(2+it) = O(1)$ and hence by Borel-Caratheodory theorem, we have,
{\fontsize{10}{12}\selectfont
$$
\log \zeta(s) = O(\log H) \text{ in } (\sigma \geq \frac{1}{2} (\alpha
+ \beta), T_0 - 8 (\log H)^2 \leq t \leq T_0 + H_1 + 8 (\log H)^2).  
$$}
We now put
\begin{equation*}
X = (\log H)^B \tag{3.3.17}\label{c3:eq3.3.17}
\end{equation*}
where $B$ is a large positive constant. We have
$$
\sum\limits_p p^{-s} \Exp \left(-\frac{p}{X} \right) = \frac{1}{2 \pi i} \int^{2+ i \infty}_{2 - i \infty} \log \zeta(s+w) X^{w} \Gamma (w) dw + O(1)
$$
where $\sigma = \alpha$ and $T_0 - 7 (\log H)^2 \leq t \leq T_0 + H_1 + 7(\log H)^2$. Here first break off the portion $|\Iim w | \geq (\log H)^2$ and move the rest of the line of integration to $\re w$ given by $\re (s+w) =\frac{1}{2} (\alpha + \beta)$. Also observe that
$$
\sum\limits_{p\geq X^2} p^{-s} \Exp \left( -\frac{p}{X}\right) = O(1) .
$$
Collecting our results we have (since $|\Gamma (w+1)| \ll \Exp (-|\Iim w|)$),
\begin{equation*}
\log\zeta(s) = \sum\limits_{p\leq X^2} p^{-s} \Exp \left(-\frac{p}{X} \right) + O(1)  \tag{3.3.18}\label{c3:eq3.3.18}
\end{equation*}
where\pageoriginale $\sigma =\alpha$ and $T_0 - 7 (\log H)^2 \leq t \leq T_0 + H_1 + 7 (\log H)^2$. Let 
\begin{equation*}
X^{2k} \leq H^{\frac{1}{2}}_1\text{ and } \left( \sum\limits_{p \leq X^2} p^{-s} \Exp \left( - \frac{p}{X} \right)\right)^k = \sum\limits_{p\leq X^{2k}} a_k(n) n^{-s} = F(s), \tag{3.3.19}\label{c3:eq3.3.19}
\end{equation*}
say.

Then we have
\begin{equation*}
|F(s)|^2 \leq (|\log \zeta(s)| + C_5)^{2k} \leq 2^{2k} |\log \zeta(s)|^{2k} + (2C_5)^{2k}, \tag{3.3.20}\label{c3:eq3.3.20}
\end{equation*}
and also
\begin{equation*}
|\log \zeta(s)|^{2k} \leq 2^{2k} |F(s)|^2 + (2C_5)^{2k} . \tag{3.3.21}\label{c3:eq3.3.21}
\end{equation*}
We now integrate these equations from $t = T_0$ to $t = T_0 + H_1$. Also we note that these inequalities are valid even when $\frac{11}{10} \geq \re s \geq \alpha - (\log H)^{-1}$, $T_0 - 6 (\log H)^2 \leq t \leq T_0 + H_1 + 6(\log H)^2$. Now in $\sigma \geq \frac{11}{10}$, we have $|\log \zeta(s)| \ll 2^{-\sigma}$ and so 
{\fontsize{10}{12}\selectfont
\begin{equation*}
\int \int_{\sigma \geq \frac{11}{10}, T_0 -1 \leq t \leq T_0 + H_1 + 1} |\log \zeta(s)|^{2k} d \sigma dt \leq C^{k}_6 \int \int 2^{-2k\sigma} d\sigma dt \leq H_1 C^k_6. 
\tag{3.3.22}\label{c3:eq3.3.22}
\end{equation*}}
Therefore in order to prove Theorem \ref{c3:thm3.3.6} it suffices to consider
\begin{equation*}
2^{2k} \int \int |F(s)|^2 d\sigma \; dt + H_1 C^k_7
\tag{3.3.23}\label{c3:eq3.3.23}
\end{equation*}
where the area integral extends over
$$
\left(\frac{11}{10} \geq \re s \geq \alpha - (\log H)^{-1}, T_0 -1 \leq t \leq T_0 + H_1 + 1 \right).
$$
By a simple computation, we have since $X^{2k} \leq H^{\frac{1}{2}}_1$,
\begin{equation*}
G(\sigma) \ll \frac{1}{H_1} \int^{T_0 + H_1 +1}_{T_0 -1} |F(s)|^2 dt \ll G(\sigma) \tag{3.3.24}\label{c3:eq3.3.24}
\end{equation*}
where 
\begin{equation*}
G(\sigma) = \sum\limits_{n\leq X^{2k}} (a_k(n))^2 n^{-2\sigma}. 
\tag{3.3.25}\label{c3:eq3.3.25}
\end{equation*}

Thus in\pageoriginale order to prove Theorems \ref{c3:thm3.3.5} and \ref{c3:thm3.3.6}, we see (by (\ref{c3:eq3.3.20}), (\ref{c3:eq3.3.21}), (\ref{c3:eq3.3.24}) and (\ref{c3:eq3.3.25})) that we have to obtain upper and lower bounds  for $(G(\sigma))^{\frac{1}{(2k)}}$. (Things similar to $G(\sigma)$ were first studied by H.L. Montgomery. See the notes at the end of this chapter). Let $p_1 = 2$, $p_2 =3, \ldots, p_k$ be the first $k$ primes. By prime number theorem
\begin{equation*}
p_1 p_2 \ldots p_k = \Exp (p_k + O(k)) = \Exp (k \log k + k \log \log k + O(k)). 
\tag{3.3.26}\label{c3:eq3.3.26}
\end{equation*}
Taking only the contribution to $G(\sigma)$ from $n = p_1 \ldots p_k$, we have, since $\Exp (-\frac{pi}{X}) \geq  \frac{1}{2} (i = 1,2,\ldots, k)$,
\begin{equation*}
(G(\sigma))^{\frac{1}{(2k)}} \geq \left(\frac{(k!)^2 2^{-2k}}{(p_1 \ldots p_k)^{2\sigma}} \right)^{\frac{1}{(2k)}} \gg \frac{k^{1-\sigma}}{(\log k)^{\sigma}} = A_k (\sigma) \text{ say}. 
\tag{3.3.27}\label{c3:eq3.3.27}
\end{equation*}
This proves the lower bound
\begin{equation*}
G(\sigma) \geq (A_k (\sigma))^{2k} C^{2k}_8. \tag{3.3.28}\label{c3:eq3.3.28}
\end{equation*}
As regards the upper bound we write
\begin{equation*}
\sum\limits_{p \leq X^2} p^{-s} \Exp \left(-\frac{p}{X} \right) = \sum\limits_1  + \sum\limits_2 \tag{3.3.29}\label{c3:eq3.3.29}
\end{equation*}
where $\sum_1$ extends over $p\leq k \log k$ and $\sum_2$ the rest. 

Note that
\begin{equation*}
|F(s)|^2 \leq 2^{2k} |\sum\limits_1|^{2k} + 2^{2k} |\sum\limits_2|^{2k}.  
\tag{3.3.30}\label{c3:eq3.3.30}
\end{equation*}
Put 
\begin{equation*}
\left(\sum\limits_1 \right)^k = \sum\limits^\infty_{n=1} \frac{b_k(n)}{n^s} = F_1(s) \text{ say}, \tag{3.3.31}\label{c3:eq3.3.31}
\end{equation*}
and 
\begin{equation*}
\left(\sum\limits_2 \right)^k = \sum\limits^\infty_{n=1} \frac{c_k(n)}{n^s} = F_2 (s) \text{ say}. \tag{3.3.32}\label{c3:eq3.3.32}
\end{equation*}
By a simple computation we have 
{\fontsize{10}{12}\selectfont
\begin{equation*}
\frac{1}{H_1} \int^{T_0 + H_1 + 1}_{T_0 -1} |F_1 (s)|^2 dt \ll G_1 (\sigma) \text{ and } \frac{1}{H_1}\int^{T_0 + H_1 -1}_{T_0-1} |F_2(s)|^2 dt \ll G_2(\sigma) \tag{3.3.33}\label{c3:eq3.3.33}
\end{equation*}}
where\pageoriginale
\begin{equation*}
G_1(\sigma)  = \sum  \frac{(b_k (n))^2}{n^{2\sigma}} \leq \left( \sum \frac{b_k(n)}{n^\sigma}\right)^2 = \left(\sum\limits_{p \leq k \log k} p^{-\sigma \Exp} \left( -\frac{p}{X} \right) \right)^{2k} \tag{3.3.34}\label{c3:eq3.3.34}
\end{equation*}
and 
\begin{equation*}
G_2 (\sigma) = \sum \frac{(c_k(n))^2}{n^{2\sigma}} \leq k! \sum \frac{c_k(n)}{n^{2\sigma}} \leq k! \left( \sum\limits_{p \leq k \log k} p^{-2\sigma} \Exp \left( -\frac{p}{X}\right) \right)^{2k}. 
\tag{3.3.35}\label{c3:eq3.3.35}
\end{equation*}
If $\sigma <1$, we have easily,
\begin{equation*}
(G_1(\sigma))^{\frac{1}{(2k)}} \ll \frac{(k \log k)^{1-\sigma}}{\log k} = \frac{k^{1-\sigma}}{(\log k)^\sigma}  \tag{3.3.36}\label{c3:eq3.3.36}
\end{equation*}
and by Stirling's approximation to $k$! we have also
\begin{equation*}
(G_2 (\sigma))^{\frac{1}{(2k)}}  \ll k^{\frac{1}{2}} \left( \frac{k(\log k)^{1-2\sigma}}{\log k}   \right)^{\frac{1}{2}} = \frac{k^{1-\sigma}}{(\log k)^{\sigma}}. 
\tag{3.3.37}\label{c3:eq3.3.37}
\end{equation*}
This proves the upper bound
\begin{equation*}
(G(\sigma))^{\frac{1}{(2k)}} \ll \left(\frac{1}{H_1} \int^{T_0 + H_1 +1}_{T_0 -1} |F(s)|^2 dt \right)^{\frac{1}{(2k)}} \ll A_k(\sigma)  \tag{3.3.38} \label{c3:eq3.3.38}
\end{equation*}
which in turn gives an upper bound for $(G(\sigma))^{\frac{1}{(2k)}}$ if $\beta_0 \leq \sigma \leq 1-\delta_1$ uniformly in $\sigma$ for every small constant $\delta_1 > 0$. If $\frac{11}{10} \geq \sigma \geq 1 -\delta_1$ the bounds for the area integral are negligible if $\delta_1$ is small since it is
$$
\leq 2 \left(\sum \frac{a_k (n)}{n^{\sigma_1}} \right)^2 \leq 2 \left( \sum\limits_{p\leq X^2} p^{-\sigma_1} \Exp \left(-\frac{p}{X} \right)\right)^{2k}
$$
where $\sigma_1 = 1 - \delta_1$.

This completes the proof of Theorems \ref{c3:thm3.3.5} and \ref{c3:thm3.3.6}. Thus the proof of Theorem \ref{c3:thm3.3.4} is complete. 

\section[Weak Titchmarsh series and Titchmarsh's...]{Weak Titchmarsh
  series and Titchmarsh's Phenomenon on the line $\sigma
  =1$}\label{c3:sec3.4} 
The\pageoriginale main object of this section is to prove the asymptotic formula for $f(H)$ (of course with $\sigma =1$). This is a long story and we will state it as a theorem at the end of this section. We find it convenient to split up this section into part A (weak Titchmarsh series), part B  (application to lower bound), part C (upper bound) and part D (the main theorem).

\begin{center} 
\textbf{PART A}
\end{center}

\medskip
\noindent{\textbf{Weak Titchmarsh Series.}} Let $0 \leq \epsilon < 1$, $D \geq 1 $, $C \geq 1$ and $H \geq 10$. Put $R = H^\epsilon$. Let $a_1 = \lambda_1 = 1$ and $\{\lambda_n\} (n = 1,2,3,\ldots)$ be any sequence of real numbers with $\frac{1}{C} \leq \lambda_{n+1} -\lambda_n \leq C (n = 1,2,3,\ldots)$ and $\{a_n\}(n=1,2,3,\ldots)$ any sequence of complelx numbers satisfying 
$$
\sum\limits_{\lambda_n \leq X} |a_n| \leq D(\log X)^R
$$
for all $X \geq 3C$. Then for complex $s = \sigma + it (\sigma > 0)$ we define the analytic function $F(s) = \sum\limits^\infty_{n=1} a_n \lambda^{-s}_n$ as a weak Titchmarsh series associated with the parameters occuring in the definition.


%%%% parthibha

\begin{alphtheorem}[FOURTH MAIN THEOREM]\label{c3:thm3.4A.1} 
For a weak Titchmarsh series $F(s)$ with $H \geq 36 C^2 H^{\epsilon}$, we have
$$
\lim \inf\limits_{\sigma \to  + 0} \int^H_0 |F(\sigma + it)| dt \geq H - 36 C^2 H^\epsilon - 12 CD.
$$
\end{alphtheorem}

\begin{alphtheorem}[FIFTH MAIN THEOREM] \label{c3:thm3.4A.2} 
For a weak Titchmarsh series $F(s)$ with $\log H \geq 4 320 C^2 (1-\epsilon)^{-5}$, we have,
$$
\lim \inf\limits_{\sigma \to + 0} \int^H_0 |F(\sigma + it)|^2 dt \geq \sum\limits_{n \leq M} \left(H - \frac{H}{\log H} - 100 C^2 n \right)|a_n|^2 - 2 D^2
$$
where\pageoriginale $M = (36C^2)^{-1} H^{1-\epsilon} (\log H)^{-4}$.
\end{alphtheorem}

\begin{remarks*}
The two theorems just mentioned have been referred to in the published papers as the fourth and the fifth main theorems. (See the notes at the end of this chapter). Theorem \ref{c3:thm3.4A.1} will be used later.
\end{remarks*}

\medskip
\noindent{\textbf{Proof of Theorem~\ref{c3:thm3.4A.1}}} 
We can  argue with $\sigma  >0$ and then pass to the limit as $\sigma \to + 0$. But formally the notation is simplified if we treat as though $F(s)$ is convergent absolutely if $\sigma =0$ and there is no loss of generality. Let $r$ be a positive integer and $0< U \leq r^{-1} H$. Then since $|F(s)| \geq 1 + \re (F(s))$, we have (with $\lambda = u_1 + \ldots + u_r$),
\begin{align*}
\int^H_0 |F(it)| dt & \geq  U^{-r} \int^U_0 du_r \ldots \int^U_0 du_1 \int^{H-rU+\lambda}_\lambda |F(it)| dt \\
& \geq U^{-r} \int^U_0 du_r \ldots \int^U_0 du_1 \int^{H-r U+\lambda}_\lambda \{1+ \re (F(it)) \} dt \\
& \geq H - rU - 2^{r+1} U^{-r} J
\end{align*}
where $J = \sum\limits^\infty_{n=2} |a_n| (\log \lambda_n)^{-r-1}$. Now $J = S_0 + \sum\limits^\infty_{j=1} S_j$ where $S_0 = \sum\limits_{\lambda_n \leq 3C}$ $|a_n| (\log \lambda_n)^{-r-1}$ and $S_j = \sum\limits_{3^j C \leq \lambda_n \leq 3^{j+1} C} |a_n| (\log \lambda_n)^{-r-1}$. In $S_0$ we use $\lambda_n \geq \lambda_2 \geq 1 + C^{-1}$ and so $(\log \lambda_n)^{-r-1} \leq (2C)^{r+1}$ and we obtain $S_0 \leq D (2C)^{r+1} (3C)^{r}$. Also, we have,
\begin{align*}
S_j & \leq D (\log (3^{j+1} C))^R (\log (3^j C))^{-r-1}\\
& \leq D 2^R (\log (3^j C))^{R-r-1}, (\text{since } 3^{j+1} C \leq (3^j C)^2). \\
& \leq D 2^{R} j^{-2} \text{ by fixing  } r = [3R].
\end{align*}
Thus for $r = [3R]$ we have
\begin{align*}
J & \leq D (2C)^{r+1} (3C)^{R} + 2D2^R, (\text{since } \sum\limits^\infty_{j=1} j^{-2} < 2),\\
& \leq 3 D(2C)^{r+1} (3C)^{R}.
\end{align*}
Collecting\pageoriginale we have,
\begin{align*}
2^{r+1} U^{-r} J & \leq 12 C D (3C)^R \left(\frac{4C}{U} \right)^r\\
& \leq 12 C D \left(\frac{12C^2}{U} \right)^R \text{ if } U \geq 4C\\
& \leq 12 C D \text{ by fixing } U = 12 C^2.
\end{align*}
The only condition which we have to satisfy is $r U \leq H$ which is secured by $H \geq 36 C^2 H^\epsilon$. This completes the proof of Theorem 1.

\medskip
\noindent{\textbf{Proof of Theorem~\ref{c3:thm3.4A.2}}}

We write $\lambda = u_1 + \ldots + u_r$, where $0 \leq u_i \leq U$ and $0 < U \leq r^{-1} H$. We put $M_1 = [M]$, $A(s) = \sum\limits_{m \leq M_1} a_m \lambda^{-s}_m$ and $B(s) = \sum\limits_{n \leq M_1 + 1} a_n\lambda^{-s}_n$ so that $F(s) = A(s) + B(s)$. For the moment we suppose $M$ to be a free parameter with the restriction $3 \leq M \leq H$. We use
$$
|F(it)|^2 \geq |A(it)|^2 + 2 \re (A (it) \overline{B(it)}).
$$
Now by a well-known theorem of H.L. Montgomery  and R.C. Vaughan (see Theorems \ref{c1:thm1.4.1} and \ref{c1:thm1.4.2}) we have
$$
\int^{H-rU+\lambda}_\lambda |A(it)|^2 dt \geq \sum\limits_{n\leq M} (H-rU - 100 C^2 n) |a_n|^2. 
$$
Next the absolute value of 
\begin{equation*}
2 U^{-r}\int^U_0 du_r \ldots \int^U_0 du_1 \int^{H-r U+\lambda}_\lambda (A(it) \overline{B(it)}) dt \tag{3.4A.1}\label{c3:eq3.4A.1}
\end{equation*}
does not exceed
\begin{align*}
& 2^{r+2} U^{-r} \sum\limits_{m \leq M_1, n \geq M_1 + 1} |a_m
  \bar{a}_n| \left(\log \frac{\lambda_n}{\lambda_m} \right)^{-r-1}\\ 
& \leq 2^{r+2} U^{-r} \left( \sum\limits_{m\leq M_1} |a_n|\right)
  \left( \sum\limits_{n\geq M_1 +1} |a_n| \left(\log
  \frac{\lambda_n}{\lambda M_1} \right)^{r+1}\right). 
 \end{align*}
Here\pageoriginale the $m$-sum is $\leq D (\log\lambda_{M_1})^R \leq D
(\log (3MC))^R$, since $\lambda_{M_1} \leq M_1 C \leq MC$. It is
enough to choose $M \geq 1$ for the bound for the $m$-sum. The $n$-sum
can be broken up into $\lambda_n \leq 3 \lambda_{M_1}$ and $3^j
\lambda_{M_1} < \lambda_n \leq 3^{j+1} \lambda_{M_1} (j=
1,2,3,\ldots)$. Let us denote these sums by $S_0$ and $S_j$. 
Now since 
$$\left(\log \frac{\lambda_n}{\lambda_{M_1}} \right) \geq \left(\log
\frac{\lambda_{M_1+1}}{\lambda_{M_1}} \right) \geq \log
\left(1+\frac{1}{C\lambda_{M_1}} \right) \geq (2C\lambda_{M_1})^{-1}
\geq (2C^2 M)^{-1},$$ 
we obtain
$$
S_0 \leq D(\log (3\lambda_{M_1}))^R (2C^2 M)^{r+1} \leq D (\log (3MC))^R (2C^2 M)^{r+1}.
$$
Also 
\begin{align*}
S_j & \leq D (\log (3^{j+1} \lambda_{M_1}))^R (j \log 3)^{-r-1}\\
& \leq D (j \log 3 + \log (3 MC))^R j^{-r-1}\\
& \leq 2^R D(j \log 3)^R (\log (3 MC))^{R} j^{-r-1}\\
& \leq 4^R D (\log (3 MC))^R j^{-2}, \text{ if } r \geq R + 1,
\end{align*}
and so (since $\sum\limits^\infty_{j=1} j^{-2} < 2$),
$$
\left(\sum\limits_m \ldots  \right) \left( \sum\limits_n \ldots \right) \leq D^2 (\log (3MC))^R(\log (3MC))^R Y
$$
(where $Y = (2C^2 M)^{r+1} + 2 (4^R)$)
$$
\leq D^2 (\log (3MC))^{2R} ((2C^2 M)^{r+1} + 2 (4^R)).
$$
Hence the absolute value of the expression (\ref{c3:eq3.4A.1}) does not exceed
\begin{gather*}
D^2 (\log (3MC))^{2R} \left((8C^2 M) \left(\frac{4C^2 M}{U} \right)^r  + 2 \left( \frac{4^R}{U^r} \right)\right)^r \tag{3.4A.2}\label{c3:eq3.4A.2}\\
\leq D^2 \left\{ 8C^2 M \left(\frac{4C^2 M (\log (3 MC))^2}{U} \right)^{R+ \log (8C^2 M)} + 2 \left(\frac{4}{U} \right)^R\right\}
\end{gather*}
if $U \geq 4 C^2 M$ and $r \geq R + \log (8C^2 M)$. We put $U = 12C^2 M (\log (3MC))^2$ and obtain for (\ref{c3:eq3.4A.2}) the bound $D^2 \{1+1\} \leq 2 D^2$. The conditions to be satisfied are $M \geq 1$ and 
$$
12 C^2 M (\log (3 MC))^2 (R + \log (8C^2 M) +1) \leq H.
$$ 
In fact\pageoriginale we can satisfy $Ur \leq \frac{H}{\log H}$ by requiring 
$$
12 C^2 M (\log (3MC))^2 (R + \log (8C^2 M) +1) \leq \frac{H}{\log H}.
$$
This is satisfied if
$$
36 C^2 M (\log (8C^2 M))^3 R \leq H (\log H)^{-1}.
$$
Let $8C^2 M \leq H$. Then $36C^2 MR \leq H (\log H)^{-4}$ gives what we want. We choose $M = (36 C^2)^{-1} H^{1-\epsilon} (\log H)^{-4}$. Clearly this satisfies $8C^2 M \leq H$. In order to satisfy $M \geq 1$ we have to secure that 
$$
(36 C^2)^{-1} \frac{((1-\epsilon) (\log H))^5}{120} (\log H)^{-4} \geq 1
$$
i.e. $\log H \geq 4320 C^2(1-\epsilon)^{-5}$.

This completes the proof of Theorem \ref{c3:thm3.4A.2}.

\newpage

\begin{center}
\textbf{PART B}\stepcounter{npart}
\end{center}

The main result of part B is 


\setcounter{alphtheorem}{0}
\setcounter{npart}{2}
\begin{alphtheorem}\label{c3:thm3.4B.1}
We have (with $z=e^{i\theta}$)
\begin{gather*}
\min\limits_{T \geq 1} \max\limits_{T \leq t \leq T + H} |(\zeta (1+it))^z|\\
\geq e^\gamma \lambda (\theta) (\log \log H - \log \log \log H) + O(1), \tag{3.4B.1}\label{c3:eq3.4B.1}
\end{gather*}
where $H \geq 10000$ and 
\begin{equation*}
\lambda (\theta) = \prod\limits_{p} \lambda_p(\theta) \tag{3.4B.2}\label{c3:eq3.4B.2}
\end{equation*}
and 
\begin{equation*}
\lambda_p (\theta) = \left(1-\frac{1}{p} \right) \left(\frac{\sqrt{p^2 - \Sin^2 \theta} + \Cos \theta}{p-p^{-1}} \right)^{\Cos \theta} \Exp \left( \Sin \theta \Sin^{-1} \left(\frac{\Sin \theta}{p}\right)\right).  \tag{3.4B.3}\label{c3:eq3.4B.3}
\end{equation*}
\end{alphtheorem}

\begin{remark*}
Note that\pageoriginale
\begin{equation*}
\left(\frac{\sqrt{p^2 - \Sin^2 \theta} + \Cos \theta}{p-p^{-1}} \right)^{\Cos \theta} = \left(\sqrt{1-\frac{\Sin^2 \theta}{p^2}} - \frac{\Cos \theta}{p} \right)^{-\Cos \theta}
\tag{3.4B.4}\label{c3:eq3.4B.4}
\end{equation*}

The outline of the proof of this theorem is as follows. By Theorem \ref{c3:thm3.4A.2} with $\epsilon = \frac{1}{3}$, $k_0 = kz$ and $F(s) = (\zeta(1+s))^{k_0}$ we have
\begin{equation*}
\frac{1}{H} \int^{T+H}_T |(\zeta (1+ it))^{k_0}|^2 dt \geq \frac{1}{2} \sum\limits_{n \leq H^{\frac{1}{4}}} \frac{|d_{k_0} (n)|^2}{n^2} \tag{3.4B.5}\label{c3:eq3.4B.5}
\end{equation*}
uniformly in $T \geq 1$, and $k$ any positive integer satisfying $1 \leq k \leq \log H$, provided $H$ exceeds an absolute positive constant. Denote by $S$ the RHS of (\ref{c3:eq3.4B.5}). We prove (by considering ``the maximum term'' in $S$) the following Theorem.
\end{remark*}


\begin{alphtheorem}\label{c3:thm3.4B.2}
We have
\begin{equation*}
\max\limits_{1\leq k \leq \log H} \left(S^{\frac{1}{(2k)}} \right) \geq e^{\gamma}\lambda (\theta) (\log \log H - \log \log \log H) + O(1) .  \tag{3.4B.6}\label{c3:eq3.4B.6}
\end{equation*}
\end{alphtheorem}

\begin{remark*}
This would complete the proof of Theorem \ref{c3:thm3.4B.1}.

We select a single term of $S$ as follows. To start with we recall that we have to impose $1 \leq k \leq \log H$, $k_0 = kz$. We select $n$ as follows. Let $n\geq 2$ and let $n =\prod\limits_p p^m$ be the prime factor decomposition of $n$. Then (in the notation of Theorem \ref{c3:thm3.4A.2}) we have
\begin{equation*}
a_1 =1, \text{ and } a_n = \prod\limits_p a_{p^m} = \prod\limits_p \frac{k_0 (k_0 +1) \ldots (k_0 + m -1)}{m!p^m}, \tag{3.4B.7}\label{c3:eq3.4B.7}
\end{equation*}
by using the Euler product for $\zeta(s)$. For each $p(\leq k)$ we select an $m = m(p)$ for which $|a_{p^m}|$ is nearly maximum. Then we have to satisfy $n =\prod\limits_{p\leq k}p^m \leq H^{\frac{1}{4}}$. In fact we choose $k$ as large as possible with these properties. We now proceed to the details.
\end{remark*}

\begin{alphlemma}\label{c3:lem3.4B.1}
Let,\pageoriginale for each $p\leq k$,
\begin{equation*}
\ell =k \left( \frac{\Cos \theta + \sqrt{p^2 -\Sin^2 \theta}}{p^2-1}\right) = \frac{k}{q} \text{ say, and }  m = [\ell].  \tag{3.4B.8}\label{c3:eq3.4B.8}
\end{equation*}
Then, putting $n = \prod\limits_{p\leq k } p^m$, we have
\begin{equation*}
\frac{1}{2k} \log |a_n|^2 =\frac{1}{2k} \sum\limits_{p\leq k} \{ - 2m \log m+ 2 m + O(\log m) - 2m \log p + E (k,m) \} \tag{3.4B.9}\label{c3:eq3.4B.9}
\end{equation*}
where
\begin{equation*}
E(k,m) = \sum\limits^{m-1}_{v=0} \log (k^2 + v^2 + 2kv \Cos \theta) . \tag{3.4B.10}\label{c3:eq3.4B.10}
\end{equation*}
\end{alphlemma}

\begin{proof}
Follows from the formula 
$$
\log m! = m \log m - m + O(\log m). 
$$
\end{proof}

\begin{alphlemma}\label{c3:lem3.4B.2}
We have,
\begin{equation*}
E(k,m) = 2m \log k + k \int^{\frac{1}{q}}_0 \log (1+ u^2 + 2u \Cos \theta) du + O\left( \frac{1}{p} \right) . \tag{3.4B.11}\label{c3:eq3.4B.11}
\end{equation*}
\end{alphlemma}

\begin{proof}
We have
\begin{align*}
E(k,m) &= \sum\limits^{m-1}_{v=0} \left\{ \log (k^2 + v^2 + 2kv \Cos
\theta)\right.\\ 
&\left.\qquad - \int^{v+1}_v \log (k^2 + u^2 + 2ku \Cos \theta) du \right\}\\
&\qquad + \int^m_0 \log (k^2 + u^2 + 2ku \Cos \theta) du.
\end{align*}
Here the sum on the right is easily seen to be $O\left(\frac{1}{p} \right)$. The integral on the right is 
$$
2m  \log k + \int^m_0 \log \left( 1+ \frac{u^2}{k^2} + 2 \frac{u}{k} \Cos \theta\right) du. 
$$
Here we can replace the upper limit $m$ of the integral by $\ell$ with an error $O(\frac{m}{k}) = O(\frac{1}{p})$. The lemma now follows by a change of variable.
\end{proof}

\begin{alphlemma}\label{c3:lem3.4B.3}
We have,\pageoriginale
\begin{equation*}
\frac{1}{k} \sum\limits_{p\leq k} \log m = O\left(\frac{1}{\log k} \right)  \tag{3.4B.12}\label{c3:eq3.4B.12}
\end{equation*}
and
$$
\frac{1}{k} \sum\limits_{p\leq k} \frac{1}{p} = O \left(\frac{1}{\log k} \right)
$$
\end{alphlemma}

\begin{proof}
Follows by prime number theorem.
\end{proof}

\begin{alphlemma}\label{c3:lem3.4B.4}
We have,
\begin{align*}
&\frac{1}{2k} \sum\limits_{p\leq k} \{-2 m \log m + 2m - 2m \log p + 2m
\log k\}\\ 
&\qquad = \sum\limits_{p\leq k} \left\{-\frac{1}{q} \log \frac{p}{q}
+ \frac{1}{q} \right\}  + O \left(\frac{1}{\log k} \right)
. \tag{3.4B.13}\label{c3:eq3.4B.13} 
\end{align*}
\end{alphlemma}

\begin{proof}
On the LHS we can replace $m$ by $\ell$ with a total error 
$$\leq \frac{1}{2k} \sum\limits_{p\leq k} O(\log m) = O (\frac{1}{\log
  k}).
$$ 
The rest is  
$$
\sum\limits_{p \leq k} \left\{ -\frac{1}{q} \log \frac{k}{q} + \frac{1}{q} - \frac{1}{q}  \log p + \frac{1}{q} \log k  \right\}
$$
which gives the lemma.
\end{proof}

\begin{alphlemma}\label{c3:lem3.4B.5}
We have,
\begin{align*}
&\frac{1}{2k} \sum\limits_{p\leq k} k \int^{\frac{1}{q}}_0 \log (1+u^2
  + 2u \Cos \theta) du\\ 
&\qquad = \re \sum\limits_{p\leq k} \left( \frac{1+ \frac{1}{q}
    e^{i\theta}}{e^{i\theta}} \log \left( 1+ \frac{1}{q} e^{i\theta}
  \right) -\frac{1}{q}\right) \tag{3.4B.14}\label{c3:eq3.4B.14} 
\end{align*}
\end{alphlemma}

\begin{proof}
Trivial.
\end{proof}

\begin{alphlemma}\label{c3:lem3.4B.6}
We have,
\begin{equation*}
\frac{1}{2k} \log |a_n|^2 = \log \log k + \gamma + \log \lambda(\theta) + O\left( \frac{1}{\log k}\right),  \tag{3.4B.15}\label{c3:eq3.4B.15}
\end{equation*}
where $\lambda (\theta)$ is as in Theorem \ref{c3:lem3.4B.1}.
\end{alphlemma}

\begin{proof}
By Lemmas \ref{c3:lem3.4B.1},\pageoriginale \ref{c3:lem3.4B.2}, \ref{c3:lem3.4B.3} and \ref{c3:lem3.4B.4} we see that LHS of (\ref{c3:eq3.4B.15}) is, (with an error $O(\frac{1}{\log k})$),
$$
\re \sum\limits_{p\leq k} \left\{ -\frac{1}{q} \log \frac{p}{q} + \frac{1}{q} \log \left( 1+ \frac{1}{q} e^{i\theta} \right) + e^{-i\theta} \log \left(1+ \frac{1}{q} e^{i\theta} \right)\right\}.
$$
Now the contribution from the first two terms (in the curly bracket) to the sum is
$$
\re \sum\limits_{p \leq k} \frac{1}{q} \log |\frac{q+e^{i\theta}}{p}| = 0, 
$$
since
\begin{align*}
& |q+ e^{i\theta}|^2 = \left( \frac{p^2-1}{\sqrt{p^2- \Sin^2 \theta + \Cos \theta}} + \Cos \theta\right)^2 + \Sin^2 \theta\\
& = \left( \frac{p^2 -1 + \Cos^2 \theta + \Cos \theta \sqrt{p^2 -\Sin^2 \theta}}{\sqrt{p^2 - \Sin^2} \theta + \Cos \theta}\right)^2 + \Sin^2 \theta\\
& = p^2. 
\end{align*}
The third term contributes
\begin{align*}
& \sum\limits_{p\leq k} \left( \Cos \theta \log \frac{p}{q} + \Sin \theta \tan^{-1} \left( \frac{\Sin \theta}{q + \Cos \theta} \right)\right)\\
& = \sum\limits_{p\leq k} \left\{\log \left(1-\frac{1}{p} \right) + \Cos  \theta \log \frac{p}{q} + \Sin \theta \tan^{-1} \left( \frac{\Sin \theta}{ q + \Cos \theta}\right)\right\}\\
&\qquad {} + \sum\limits_{p\leq k} \log \left(1-\frac{1}{p} \right)^{-1}.
\end{align*} 
This together with the well-known formula $\prod\limits_{p\leq k} \left( 1-\frac{1}{p}\right)^{-1} = e^{\gamma} \log k + O(1)$ proves the lemma.
\end{proof}

\begin{alphlemma}\label{c3:lem3.4B.7}
For the $n$ defined in Lemma \ref{c3:lem3.3.5}, we have,
\begin{equation*}
  \log n =\sum\limits_{p\leq k} m \log p = k \log k + O(k). \tag{3.4B.16}\label{c3:eq3.4B.16}
\end{equation*}
\end{alphlemma}

\begin{proof}
Replacement\pageoriginale of $m$ by $\ell$ involves an error $O(k)$ by the prime number theorem. Now $\ell =\frac{k}{q}$ and
\begin{align*}
q & = p \left(p-\frac{1}{p} \right) \left(p\sqrt{1-\frac{\Sin^2 \theta}{p^2}} + \Cos \theta\right)^{-1}\\
& = p \left(p-\frac{1}{p^2} \right) \left(p\sqrt{1-\frac{\Sin^2 \theta}{p^2}} + \frac{\Cos \theta}{p} \right)^{-1}\\
& = p + O(1).
\end{align*}
This proves the lemma.
\end{proof}

\begin{alphlemma}\label{c3:lem3.4B.8}
Set $k = \left[\frac{\log H}{2 \log \log H} \right]$. Then for all $H$ exceeding a large positive constant, we have,
$$
m \leq \frac{H}{200}. 
$$
\end{alphlemma}

\begin{proof}
Follows from Lemma \ref{c3:lem3.4B.7}.

Lemmas \ref{c3:lem3.4B.6} and \ref{c3:lem3.4B.8} complete the proof of Theorem \ref{c3:thm3.4B.2} and as remarked already this proves Theorem \ref{c3:thm3.4B.1} completely.
\end{proof}

\begin{center}
\textbf{PART C}
\end{center}

The main result of part C is


\setcounter{alphtheorem}{0}
\setcounter{npart}{3}
\begin{alphtheorem}\label{c3:thm3.4C.1}
We have (with $z=e^{i\theta}$),
\begin{gather*}
\min\limits_{T \geq 1} \max\limits_{T \leq t \leq T + H} |(\zeta(1+ it))^z| \\
\leq e^\gamma \lambda (\theta) (\log \log H + \log \log \log H) + O(1), \tag{3.4C.1}\label{c3:eq3.4C.1}
\end{gather*}
where $H \geq 10000$ and $\lambda (\theta)$ is as in Theorem \ref{c3:thm3.4B.1}. 
\end{alphtheorem}

We begin by

\setcounter{alphlemma}{0}
\begin{alphlemma}\label{c3:lem3.4C.1}
Let $T =\Exp ((\log H)^2)$ where $H$ exceeds an absolute constant. Then there exists a sub-interval I of $[T, 2T]$ of length $H + 2(\log H)^{10}$, such that\pageoriginale the rectangle  $(\sigma \geq \frac{3}{4}, t \in I )$ does not contain any zero of $\zeta(s)$ and moreover
\begin{equation*}
\max |\log \zeta(\sigma + it)| = O((\log H)^{\frac{1}{4}} (\log \log H)^{-\frac{3}{4}})  \tag{3.4C.2}\label{c3:eq3.4C.2}
\end{equation*}
the maximum being taken over the rectangle referred to.
\end{alphlemma}

\begin{proof}
Follows from Theorem \ref{c3:thm3.3.4} and the result (due to A.E. Ingham \cite{Ingham1}, see also E.C. Titchmarsh \cite{Titchmarsh1}, page 236, and p. 293-295 of A. Ivi\'c \cite{Ivic1}) that the number of zeros of $\zeta(s)$ in $(\sigma \geq \frac{3}{4}, T \leq t \leq 2T)$ is $O(T^{\frac{3}{4}})$. 
\end{proof}

\begin{alphlemma}\label{c3:lem3.4C.2}
Let $J$ be the interval obtained by removing from I intervals of length $(\log H)^{10}$ from both ends. Then for $t \in J$, we have,
\begin{equation*}
\log \zeta (1+ it) = \sum \sum\limits_{m \geq 1, p} (mp^{ms})^{-1} \Exp \left(-\frac{p^m}{X}  \right) + O ((\log \log H)^{-1})  \tag{3.4C.3}\label{c3:eq3.4C.3}
\end{equation*}
where $X = \log H \log \log H$ and $s = 1+ it$.
\end{alphlemma}

\begin{proof}
The lemma follows from the fact that the double sum on the right is 
\begin{equation*}
\frac{1}{2\pi i} \int^{2+i\infty}_{2-i\infty} \log \zeta(s+w) X^w \Gamma(w) dw  \tag{3.4C.4}\label{c3:eq3.4C.4}
\end{equation*}
where $w = u + iv$ is a complex variable. Here we break off the portion $|v| \geq (\log H)^9$ with an error $O((\log \log H)^{-1})$ and move the line of integration to $u= - \frac{1}{4}$. Using Lemma \ref{c3:lem3.4C.1} it is easily seen that the horizontal portions and the main integral contribute together $O((\log \log H)^{-1})$.
\end{proof}

\begin{alphlemma}\label{c3:lem3.4C.3}
Denote the double sum in (\ref{c3:eq3.4C.3}) by $S$. Then
\begin{equation*}
S = \log \prod\limits_{p \leq X} (1-p^{-s})^{-1} + O((\log \log H)^{-1}) .  \tag{3.4C.5}\label{c3:eq3.4C.5}
\end{equation*}
\end{alphlemma}

\begin{proof}
We use the fact $\Exp (-p^m X^{-1}) =1 + O(p^m X^{-1})$ if $p^m \leq X$ and $= O(X p^{-m})$ if $p^m \geq X$. Using this it is easy to see that
\begin{align*}
S & = \sum \sum\limits_{p^m \leq X} (mp^{ms})^{-1} + O \left( \sum\sum\limits_{p^m \leq X} X^{-1}\right) + O \left( \sum\sum\limits_{p^m \geq X} X(mp^{2m})^{-1}\right)\\
& = \sum\sum\limits_{p^m \leq X} (mp^{ms})^{-1} + O((\log \log H)^{-1}). 
\end{align*}
Denoting\pageoriginale the last double sum by $S_0$, we have,
$$
S_0 - \sum\limits_{p\leq X} \log (1-p^{-s})^{-1} = O\left( \sum \sum\limits_{p^m \geq X, m \geq 2} (mp^m)^{-1}\right) = O((\log \log H)^{-1}). 
$$
\end{proof}

\begin{alphlemma}\label{c3:lem3.4C.4}
We have, for $t \in J$,
\begin{equation*}
\log \zeta(1 + it) = \sum\limits_{p\leq X} \log (1-p^{-s})^{-1} + O((\log \log H)^{-1}),  
\tag{3.4C.6}\label{c3:eq3.4C.6}
\end{equation*}
where $s =1 + it$.
\end{alphlemma}

\begin{proof}
Follows from Lemmas~\ref{c3:lem3.4C.1}, \ref{c3:lem3.4C.2} and \ref{c3:lem3.4C.3}.
\end{proof}

\begin{alphlemma}\label{c3:lem3.4C.5}
Let $0 \leq r < 1$, $0 \leq \phi < 2\pi$. Then, we have,
\begin{gather*}
\log |(1-re^{i\phi})^{-z}| \leq - \Cos \theta \log \left( \sqrt{1-r^2 \Sin^2 \theta} - r \Cos \theta\right) +\\
+ \Sin \theta \Sin^{-1} (r \Sin \theta).  \tag{3.4C.7}\label{c3:eq3.4C.7}
\end{gather*}
\end{alphlemma}

\begin{remark*}
Put 
\begin{align*}
\lambda_p (\theta) &= (1-p^{-1}) \left(\sqrt{1-p^{-2} \Sin^2 \theta} -
p^{-1} \Cos \theta \right)^{- \Cos \theta}\\ 
&\qquad\qquad\Exp \left(\Sin \theta
\Sin^{-1} \left(\frac{\Sin \theta}{p} \right)
\right). \tag{3.4C.8}\label{c3:eq3.4C.8} 
\end{align*}
In the lemma replace $re^{i\phi}$ by $p^{-s}$. Lemmas
\ref{c3:lem3.4C.4} and \ref{c3:lem3.4C.5} complete the proof of
Theorem 
\ref{c3:thm3.4C.1} since $\sum\limits_{p\geq X} \log \lambda_p(\theta) = O(X^{-1})$ and $\prod\limits_{p\leq X} (1-p^{-1})^{-1} = e^\gamma \log X + O(1)$. (See page 81 of K. Prachar \cite{Prachar1}).
\end{remark*}

\medskip
\noindent{\textbf{Proof of Lemma~\ref{c3:lem3.4C.5}.}}
Denote the LHS of (11) by $g(\phi)$. Then
\begin{align*}
g(\phi) & = \sum\limits^\infty_{n=1} n^{-1} r^n \Cos (n\phi + \theta)\\
g'(\phi) & = -\sum\limits^{\infty}_{n=1} r^n \Sin (n\phi + \theta)\\
& = \Iim \left\{ \frac{-re^{i(\phi + \theta)} (1-re^{-i\phi})}{(1-re^{i\phi}) (1-re^{-i\phi})}\right\}. 
\end{align*}
Hence\pageoriginale $g'(\phi) = 0$ if $\Sin (\phi + \theta) = r \Sin \theta$, i.e. if
\begin{equation*}
\phi = -\theta + \Sin^{-1} (r \Sin \theta).  \tag{3.4C.9}\label{c3:eq3.4C.9}
\end{equation*}
At this point $g(\phi)$ attains the maximum as we shall show in the end. Now
{\fontsize{10}{12}\selectfont
\begin{align*}
g(\phi) & = \re \left\{ -e^{i\theta} \left(\log \sqrt{1-2r \Cos \phi + r^2} - i \Sin^{-1} \frac{r \Sin \phi}{\sqrt{1 - 2 r \Cos \phi + r^2}} \right)\right\}\\
& = - \Cos \theta \log \sqrt{1-2r \Cos \phi + r^2} - \Sin \theta \Sin^{-1} \left( \frac{r \Sin \phi}{\sqrt{1-2r \Cos \phi + r^2}}\right). \tag{3.4C.10}\label{c3:eq3.4C.10}
\end{align*}}
From (\ref{c3:eq3.4C.9}) we have
\begin{align*}
\Sin \phi & = r \Sin \theta \Cos \theta - \sqrt{1-r^2 \Sin^2 \theta} \Sin \theta\\
& = - \Sin \theta (\sqrt{1-r^2 \Sin^2 \theta} - r \Cos \theta),\\
\Cos \phi & = \sqrt{1-r^2 \Sin^2 \theta} \Cos \theta + r \Sin^2 \theta,\\
1-2r\Cos \phi + r^2 & = 1 -2r \Cos \theta \sqrt{1-r^2 \Sin^2 \theta} - 2r^2 \Sin^2 \theta + r^2\\
& = (\sqrt{1-r^2 \Sin^2 \theta} - r \Cos \theta)^2,
\end{align*}
since $-r^2 \Sin^2 \theta + r^2 \Cos^2 \theta = -2r^2 \Sin^2 \theta + r^2$. Hence
\begin{equation*}
g(\phi) \leq h (\theta) \tag{3.4C.11}\label{c3:eq3.4C.11}
\end{equation*}
where $h(\theta)$ is the RHS of (\ref{c3:eq3.4C.7}), provided $g(\phi)$ attains its maximum for the value $\phi$ gives by (\ref{c3:eq3.4C.9}). We now show that
\begin{itemize}
\item[{\rm (a)}] If $\Cos \theta \geq 0$ then $g(\pi) < h (\theta)$

and

\item[{\rm (b)}] If $\Cos \theta < 0$ then $g(0) < h (\theta)$.
\end{itemize}
Note that $\Sin \theta \Sin^{-1} (r \Sin \theta) \geq 0$. Hence it suffices to prove (in case (a))
\begin{align*}
g(\pi) & = \re \log \{(1-re^{i\phi})^{-z}\}_{\phi = \pi}\\
& = -\Cos \theta \log (1+r) < -\Cos \theta \log (\sqrt{1-r^2 \Sin^2 \theta} - r \Cos \theta)
\end{align*}

i.e.\pageoriginale $\log (1+r) > \log(\sqrt{1-r^2 \Sin^2 \theta} - r \Cos \theta)$

i.e. $(1+r + r \Cos \theta)^2 > 1 - r^2 \Sin^2 \theta$

i.e. $(1+r)^2 + 2 r (1+r) \Cos \theta > 1 -r^2$

i.e. $1+ r + 2r \Cos \theta > 1 - r$ (true since $\Cos \theta \geq 0$)

In case (b) it suffices to prove
\begin{align*}
g(0) & = \re \log \{(1-re^{i\phi})^{-z}\}_{\phi = 0}\\
& = - \Cos \theta \log (1-r) < - \Cos \theta \log (\sqrt{1-r^2 \Sin^2 \theta } - r \Cos \theta) 
\end{align*}

i.e. $\log (1-r) < \log (\sqrt{1-r^2 \Sin^2 \theta} -r \Cos \theta)$

i.e. $1-r< \sqrt{1-r^2 \Sin^2 \theta} - r \Cos \theta$

i.e. $(1-r + r \cos \theta)^2 < 1- r^2 \Sin^2 \theta$

i.e. $(1-r)^2 + 2 r (1-r) \Cos \theta < 1 - r^2$

i.e. $1-r + 2 r \Cos \theta <1 + r $ (which is true).

Thus Lemma \ref{c3:lem3.4C.5} is completely proved and hence Theorem \ref{c3:thm3.4C.1} is completely proved.

\begin{center}
\textbf{PART D}
\end{center}


Collecting together the main results of parts B and C we conclude \S\ \ref{c3:sec3.4} by stating the following theorem.


\stepcounter{npart}
\setcounter{alphtheorem}{0}
\begin{alphtheorem}\label{c3:thm3.4D.1}
The function $f(H)$ defined by
\begin{equation*}
f(H) = \min\limits_{T \geq 1} \max\limits_{T \leq t \leq T + H} |(\zeta(1+it))^z|  \tag{3.4D.1}\label{c3:eq3.4D.1}
\end{equation*}
where $z = e^{i\theta}$ ($\theta$ being a constant satisfying $0 \leq \theta < 2 \pi$) satisfies the asymptotic estimate
\begin{equation*}
|f(H) e^{-\gamma} (\lambda (\theta))^{-1} - \log \log H| \leq \log \log \log H + O(1). \tag{3.4D.2}\label{c3:eq3.4D.2}
\end{equation*}
We recall\pageoriginale that $\lambda (\theta) = \prod\limits_{p} \lambda_p (\theta)$, where
{\fontsize{10}{12}\selectfont
\begin{equation*}
\lambda_p(\theta) = \left(1-\frac{1}{p} \right)
\left(\sqrt{1-\frac{(\Sin \theta)^2}{p^2}} - \frac{\Cos \theta}{p}
\right)^{-\Cos \theta} \Exp \left( \Sin \theta \Sin^{-1} \left(
\frac{\Sin \theta}{p} \right)\right) \tag{3.4D.3}\label{c3:eq3.4D.3} 
\end{equation*}}
$\Sin^{-1} x$ being as usual the expansion valid in $|x| <1$, vanishing at $x=0$. 
\end{alphtheorem}

\begin{remark*}
It is an open problem to improve the RHS of (\ref{c3:eq3.4D.2}).
\end{remark*}

\begin{center}
\textbf{Notes at the end of Chapter III}
\end{center}

In the year 1928 E.C. Titchmarsh \cite{Titchmarsh4} proved (the earlier discoveries in this direction depended on RH, for references see E.C. Titchmarsh \cite{Titchmarsh1}) that
$$
|\zeta(\sigma + it)| = \Omega (\Exp ((\log t)^{1-\sigma -\epsilon})), (\epsilon, \sigma \text{ fixed } \epsilon > 0, \frac{1}{2} \leq \sigma <1).
$$
Extending this method of Titchmarsh, K. Ramachandra \cite{Ramachandra6} proved the lower bound $\gg \Exp ((\log H)^{1-\sigma -\epsilon})$ for the maximum of $|\zeta (\sigma + it)|$ taken over $T \leq t \leq T + H$ with $T \geq H \geq (\log T)^{\frac{1}{100}}$. (It was not difficult to relax the lower bound for $H$ to $\gg  \log \log T$ with a suitable implied constant). Around the same time (see \S\ 5 of \cite{Ramachandra6} for an explanation of this remark, and further results over short intervals) N. Levinson \cite{Levinson1} independently proved that
$$
\max\limits_{1 \leq t \leq T} \log |\zeta(\sigma + it)| \gg (\log T)^{1-\sigma} (\log \log T)^{-1}, (\sigma \text{ fixed, } \frac{1}{2} \leq \sigma < 1)
$$
and that
$$
\max\limits_{1 \leq t \leq T} |\zeta(1+ it)| \geq e^{\gamma} \log \log T + O(1)
$$
and also
$$
\max\limits_{1\leq t \leq T} |\zeta(1+it)|^{-1} \geq \frac{6}{\pi^2} e^\gamma  (\log \log T - \log \log \log T) + O(1).
$$
A few years\pageoriginale later H.L. Montgomery \cite{Montgomery2} developed a new method of proving things like (note that we write $z = e^{i\theta}$)
$$
|\left(\zeta \left(\frac{1}{2} + it \right) \right)^z| = \Omega \left( \Exp \left( \frac{1}{20} \sqrt{\frac{\log t}{\log \log t}} \right)\right) \quad (\text{on  RH})
$$
and
{\fontsize{10}{12}\selectfont
\begin{equation*}
|(\zeta (\sigma + it))^z| = \Omega \left(\Exp \left( \frac{1}{20}
\left(\sigma - \frac{1}{2} \right)^{\frac{1}{2}} \frac{(\log
  t)^{1-\sigma}}{(\log \log t)^{\sigma}}\right) \right) \quad (\sigma
\text{ fixed, } \frac{1}{2} < \sigma < 1).  
\end{equation*}}
It should be mentioned that Montgomery's method needs RH even for $\theta =0$. Developing the method of K. Ramachandra (\cite{Ramachandra6}), R. Balasubramanian \cite{Balasubramanian1} and R. Balasubramanian and K. Ramachandra (\cite{Balasubramanian and Ramachandra6}) proved Theorem \ref{c3:thm3.2.1} and in particular the result
$$
|\zeta \left(\frac{1}{2} + it \right)| = \Omega \left(\Exp \left(\frac{3}{4} \sqrt{\frac{\log t}{\log \log t}} \right) \right)
$$
without any hypothesis. For the result which asserts the replacement of $(\sigma -\frac{1}{2})^{\frac{1}{2}}$ by $c(\sigma -\frac{1}{2})^{\frac{1}{2}} (1-\sigma)^{-1}$, $(\frac{1}{2} < \sigma <1)$, and some other results see the two papers \cite{Ramachandra and Sankaranarayanan1} \cite{Ramachandra and Sankaranarayanan2} by K. Ramachandra and A. Sankaranarayanan. We have not included the proof of these results in this monograph. A part from a paper \cite{Balasubramanian Ramachandra and Sankaranarayanan1} by R. Balasubramanian, K. Ramachandra and A. Sankaranarayanan all the results of this chapter are completely due to various results of R. Balasubramanian and K. Ramachandra developed in various stages most of the time jointly and very rarely individually. Hence it is  very well justified to refer to all the theorems of this chapter as joint work of R. Balasubramanian and K. Ramachandra (see the following 15 papers by us: (a) Papers I to IX with the title ``On the frequency of Titchmarsh's phenomenon for $\zeta(s)$'', (b) Papers I to III with the title ``Progress towards a conjecture on the mean-value of Titchmarsh series'', (c) one paper with the title ``Proof of some conjectures on the mean-value of Titchmarsh series - I'', (d) one paper with the title ``Proof of some conjectures on the mean-value of Titchmarsh series\pageoriginale with applications to Titchmarsh's phenomenon'' and (e) one paper with the title ``On the zeros of a class of generalished Dirichlet series-III. The paper V of the series (a) uses some ideas of the paper (e)).

Regarding limitation theorems for our method R. Balasubramanian showed in \cite{Balasubramanian1}) that it is not possible to get even 0.76 in pace of $\frac{3}{4}$ in Theorem \ref{c3:thm3.2.1}. Also, R. Balasubramanian and K. Ramachandra showed in (\cite{Balasubramanian and Ramachandra6}) that for $\frac{1}{2} <\sigma < 1$, we cannot get better results than
$$
\max\limits_{T \leq t \leq T + H} \log |\zeta( \sigma + it)| \gg (\log H)^{1-\sigma} (\log \log H)^{-1}
$$
where $H(\leq T)$ and exceeds a certain constant multiple of $\log \log T$. In (H.L. Montgomery \cite{Montgomery3}) H.L. Montgomery showed that (by Balasubramanian Ramachandra method) it is not possible to get better results than even
$$ 
\max\limits_{T \leq t \leq 2 T} |\log \zeta(\sigma + it)| \gg (\log T)^{1-\sigma} (\log \log T)^{-1} \;\; \left(\sigma \text{ fixed, } \frac{1}{2} < \sigma < 1 \right).
$$
This shows the supremacy of some aspects of Montgomery's method although it fails for short intervals. It will be of some interest to examine hte limitation of our method for $\sigma =1$. In view of Levinson's results
$$
\max\limits_{1 \leq t \leq T} |\zeta(1+ it )| > e^\gamma  \log \log T + O(1)
$$
one may conjecture that we may drop the term $\log \log \log H$ in (\ref{c3:eq3.4D.2}). But this may be very very difficult to achieve. 
