
\chapter{Minimisation of Functionals - Theory}\label{chap2}
In\pageoriginale this chapter we shall discuss the local and global minima of functionals on Banach spaces and give some sufficient conditions for their existence, relate them to conditions on their $G$-derivatives (when they exist) and convexity properties. Then we shall show that the problem of minimisation applied to suitable functionals on Sobolev spaces lead to and equivalent to some of the standard examples of linear and non-linear elliptic boundary value problems.

\section{Minimisation Without Convexity}\label{chap2-sec1}
Let $\mathcal{U}$ be a subset of a normed vector space $V$ and $J : \mathcal{U} \subset V \to \mathbb{R}$ be a functional.

\begin{definition}\label{chap2-dfe1.1}
A funvtional $J : \mathcal{U} \subset V \to \mathbb{R}$ is said to have a local minimum at a point $u \epsilon \mathcal{U}$ if there exists a neighbourhood $\mathscr{V}(u)$ of $u$ in $V$ such that
$$
J(u) \leq J(v) \text{ for all } v \epsilon \mathcal{U} \cap \mathscr{V}(u)
$$
\end{definition}

\begin{definition}\label{chap2-def1.2}
A functional $J$ on $\mathcal{U}$ is said to have a global minimum (or an absolute minimum) in $\mathcal{U}$ if there exist a $u \epsilon \mathcal{U}$ such that 
$$
J(u) \leq J(v) \text{ for all } v \epsilon \mathcal{U}.
$$
We have the following existence result.
\end{definition}

\begin{theorem}\label{chap2-thm1.1}
Suppose $V, \mathcal{U}$ and $J : \mathcal{U} \to \mathbb{R}$ satisfy the following hypothesis :
\begin{enumerate}
\item[(H1)] $V$ is a reflexive Banach space,
\item[(H2)] $\mathcal{U}$ is weakly closed.
\item[(H3)] $\mathcal{U}$ is bounded and\pageoriginale
\item[(H4)] $J : \mathcal{U} \subset V \to \mathbb{R}$ is weakly lower semi-continuous.
\end{enumerate}
Then $J$ has a global minimum in $\mathcal{U}$.
\end{theorem}

\begin{proof}
Let $\ell$ denote $\inf\limits_{v \epsilon \mathcal{U}} J(v)$. If $v_{n}$ is a minimising sequence for $J$, i.e. $\ell = \inf\limits_{v \epsilon \mathcal{U}} J(v) = \lim\limits_{n \to \infty} J(v_{n})$, then by the boundedness of $\mathcal{U}$ (i.e. by H3) $v_{n}$ is a bounded sequence in $V$ i.e. there exists a constant $C > 0$ such that $||v_{n}|| \leq C$ for all $n$. By the reflexivity of $V(H1)$ this bounded sequence is weakly relatively compact. So there is a subsequence $v_{n'}$ of $v_{n}$ such that $v_{n'} \rightharpoonup u$ in $V$. $\mathcal{U}$ being weakly closed $(H2)$ $u \epsilon \mathcal{U}$. Finally, since $v_{n'} \rightharpoonup u$ and $J$ is weakly lower semi-continuous
$$
J(u) \leq \mathop{\lim \inf}_{n \to \infty} J(v_{n'})
$$
which implies that
$$
J(u) \leq \lim_{n \to \infty} J(v_{n'}) = \ell \leq J(v) \text{ for all } v \epsilon \mathcal{U}.
$$
\end{proof}

\begin{theorem}\label{chap2-thm1.2}
If $V, \mathcal{U}$ and $J$ satisfy the Hypothesis $(H1)$, $(H2)$, $(H4)$ and $J$ satisfies
\begin{equation*}
\lim_{||v||_{V} \to + \infty} J(v) = + \infty\tag*{$(H3)'$}\label{chap2-eqH3'}
\end{equation*}
then $J$ admits a global minimum in $\mathcal{U}$.
\end{theorem}

\begin{proof}
We shall reduce the problem to the previous case. Let $w \in \mathcal{U}$ be arbitrary fixed. Consider the subset $\mathcal{U}_{0}$ of $\mathcal{U}$ :
$$
\mathcal{U}_{0} = \{v ; v \epsilon \mathcal{U} \text{ such that } J(v) \leq J(w)\}.
$$
\end{proof}

It is immediatly seen that the existence of a minimum in $\mathcal{U}_{0}$ is equivalent to that in $\mathcal{U}$. We claim that $\mathcal{U}_{0}$ is bounded and weakly closed in $V.$ i.e. hypothesis $(H2)$ and $(H3)$ hold for $\mathcal{U}_{0}$. In fact, suppose $\mathcal{U}_{0}$ is not bounded then we can find a sequence\pageoriginale $v_{n} \in \mathcal{U}_{0}$ with $||v_{n}||_{V} \to + \infty$. The, by $(H3)' , J(v_{n}) \to + \infty$ which is impossible since $v_{n} \in \mathcal{U}_{0}$ implies that $J(v_{n}) \leq J(w)$. Hence $\mathcal{U}_{0}$ is bounded. To prove that $\mathcal{U}_{0}$ is weakly closed, let $u_{n} \in \mathcal{U}_{0}$ be a sequence that $u_{n} \rightharpoonup u$ in $V$. Since is weakly closed $u \in \mathcal{U}$. On the other end, since $J$ is weakly lower semi-continuous $u_{n} \rightharpoonup u$ in $V$ implies that
$$
J(u) \leq \lim \inf J(u_{n}) \leq J(w)
$$
proving that $u \in \mathcal{U}_{0}$. Now $\mathcal{U}_{0}$ and $J$ satisfy all the hypothesis of Theorem \ref{chap2-thm1.1} and hence $J$ has a global minimum in $\mathcal{U}_{0}$ and hence in $\mathcal{U}$.

Next we give a necessary condition for the existence of a local minimum in items of the first $G$-derivative (when it exists) of the functional $J$. For this we need the following concept of admissible (or feasible) directions at a points $u$ for a domian $\mathcal{U}$ in $V$. It $u, v \in V$  $u \neq v$ then the nonzero vector $v - u$ can be consider as a direction in $V$.

\begin{definition}\label{chap2-def1.3}
(1) A direction $v-u$ in $V$ is said to be a strongly admissible direction at the points $u$ for the domian $\mathcal{U}$ if there exists a sequence $\epsilon_{n} > 0$ such that
$$
\epsilon_{n} \to 0 \text{ as } n \to \infty \text{ and } u + \epsilon_{n} (v-u) \in \mathcal{U} \text{ for each }n.
$$ 
(2) A direction $v-u$ in $V$ is said to be weakly admissible at the points $u$ for the domian $\mathcal{U}$ if there exist sequence $\epsilon_{n} > 0$ and $w_{n} \in V$ such that
$$
\epsilon_{n} \to 0 \text{ and } w_{n} \to 0 \text{ in } V, u_{n} + \epsilon_{n} (v-u)+ \epsilon_{n} w_{n} \in \mathcal{U} \text{ for each } n.
$$
\end{definition}

We shall mainly use the notion of strongly admissible direction. But some results on minimisation of functionals are known which make use of the notion of weakly admissible directions.

We\pageoriginale have the following necessary condition for the existence of a local minimum.

\begin{theorem}\label{chap2-thm1.3}
Suppose a functional $J : \mathcal{U} \subset V \to \mathbb{R}$ has a local minimum at a point $u \in \mathcal{U}$ and is $G$-differentiable at $u$ in all directions then $J'(u, v - u) \geq 0$ for every $v \in V$ such that $v-u$ is a strongly admissible direction.

Furtheremore, if $\mathcal{U}$ is an open set then
$$
J'(u, \varphi) = 0 \text{ for all } \varphi \in V.
$$ 
\end{theorem}

\begin{proof}
If $u \in \mathcal{U}$ is local minimum for $J$ then there exists a neighbourhood $\mathscr{V}(u)$ of $u$ in $V$ such that
$$
J(u) \leq J(w) \text{ for all } w \in \mathcal{U} \cap \mathscr{V} (u).
$$
\end{proof}

If $v \in V$ and $v-u$ is a strongly admissible direction then, for n large enough,
$$
u + \epsilon_{n} (v - u) \in \mathcal{U} \cap \mathscr{V}(u)
$$
so that
$$
J(u) \leq J(u + \epsilon_{n} (v-u)).
$$

Hence
$$
J'(u, v-u) = \lim_{\epsilon_{n} \to 0} (J(u + \epsilon_{n}(viu)) - J(u))/ \epsilon_{n} \geq 0.
$$

Finally, if $\mathcal{U}$ is an open set in $V$ then $\mathcal{U}$ contains an open ball in $V$ of centre $u$ and hence every direction is strongly admissible at $u$ for $\mathcal{U}$. Taking $v = u \pm \varphi$, $\varphi \in V$ it follows from the first part that
$$
J'(u, \pm \varphi) \geq 0 \text{ or equivalently } J'(u, \varphi) = 0 \text{ for all } \varphi \in V.
$$

In\pageoriginale particualr, if $\mathcal{U}$ is open and $J$ has a gradient $G(u) \in V'$ at $u \in \mathcal{U}$ and if $u$ is a local minimum then
$$
J'(u, \varphi) = <G(u), \varphi>_{V' \times V} = 0 \text{ for all } \varphi \in V ; \text{ i.e. } G(u) = 0 \in V'.
$$

This result is thus in conformity with the classical case of differentiable functions.

\begin{remark}\label{chap2-rem1.1}
The converse of Theorem \ref{chap2-thm1.3} requires convexity assumptions as we shall see in the following section.
\end{remark}

\section{Minimistion with Convexity conditions}\label{chap2-sec2}
We shall show that under convexity assumptions on the domian $\mathcal{U}$ and the functional $J$ the notions of local and global minima coincide. We also give another sufficient condition for the existence of minima.

\begin{lemma}\label{chap2-lem2.1}
If $\mathcal{U}$ is a convex subset of a normed vector space $V$ and $J : \mathcal{U} \subset V \to \mathbb{R}$ is a convex functional then any local minimum is also a global minimum.
\end{lemma}

\begin{proof}
Suppose $u \in \mathcal{U}$ is a local minimum of $J$. Then there is a neighbourhood $\mathscr{V}(u)$ of $u$ in $V$ such that
$$
J(u) \leq J(v) \text{ for all } v \in \mathscr{V} (u) \cap \mathcal{U}.
$$

On the other hand, if $v \in \mathcal{U}$ then $u + \theta (v-u) \in \mathcal{U}$ for all $\theta \in [0, 1]$ by convexity of $\mathcal{U}$.
\end{proof}

Moreover, if $\theta$ is small enough, say $0 \leq \theta \leq \theta_{v}$ then $u + \theta(v-u) \in \mathscr{V}(u)$. Hence
\begin{align*}
J(u) & \leq J(u + \theta(v-u)) \text{ for all } 0 \leq \theta \leq \theta_{v}\\
& \leq J(u) + \theta(J(v) - J(u)) \text{ by convexity of J, for all } 0 \leq \theta \leq \theta_{v},
\end{align*}
which implies that
$$
J(u) \leq J(v) \text{ for all } v \in \mathcal{U}.
$$\pageoriginale

Whenever the assumptions of Lemma \ref{chap2-lem2.1} are satisfied we shall call a minimum without reference to local or global. Next lemma concerns the uniqueness of such a minimum.

\begin{lemma}\label{chap2-lem2.2}
If $\mathcal{U}$ is a convex subset of a normed vector space and $J : \mathcal{U} \subset V \to \mathbb{R}$ is strictly convex then there exixts a unique minimum $u \in \mathcal{U}$ for $J$.
\end{lemma}

\begin{proof}
The existence is proved in Lemma \ref{chap2-lem2.1}. To prove the uniqueness, if $u_{1} \neq u_{2}$ are two minima for $J$ in $\mathcal{U}$ then
$$
J(u_{1}) = J(u_{2}) \leq J(v) \text{ for all } v \in \mathcal{U}
$$
and, in particular, this holds for $v = \frac{1}{2} u_{1} + \frac{1}{2} u_{2}$ which belongs to $\mathcal{U}$ since $\mathcal{U}$ is convex. On the other hand, since $J$ is strictly convex
$$
J(\frac{1}{2} u_{1} + \frac{1}{2} u_{2}) < \frac{1}{2} J(u_{1}) + \frac{1}{2} J(u_{2}) = J(u_{1} \leq J(v)) 
$$
which is impossible if we take $v = \frac{1}{2} (u_{1} + u_{2})$. This proves the uniqueness of the minimum.
\end{proof}

We shall now pass to a sufficient condition for the existence of minima of functionals which is the exact analogue of the case of twice differentiable functions.

\begin{theorem}\label{chap2-thm2.1}
Let $J : V \to \mathbb{R}$ be a functional on $V, \mathcal{U}$ a subset of $V$ satisfying the following hypothesis :
\begin{enumerate}
\item[(H1)] $V$ is a relexive Banach space;
\item[(H2)] $J$ has a gradient $G(u) \in V'$ everywhere in $\mathcal{U}$;
\item[(H3)] $J$ is twice $G$-differentiable in all directions $\varphi, \psi \in V$ and satisfies the condition
$$
J''(u; \varphi, \varphi) \geq ||\varphi||_{V} \chi (||\varphi||_{V}) \text{ for all } \varphi \in V,
$$\pageoriginale
where $t \mapsto \chi (t)$ is afunction on $\{t \in \mathbb{R}; t \geq 0\}$ such that
$$
\chi(t) \geq 0 \text{ and } \lim_{t \to + \infty} \chi(t) = +\infty;
$$
\item[(H4)] $\mathcal{U}$ is a closed convex set.

Then there exists at least one minimum $u \in \mathcal{U}$ of $J$. Furthermore, if in $(H3)$
\item[(H5)] 
$$
\chi(t) > 0 \text{ for } t > 0
$$
is satisfied by $\chi$ then there exists a unique minimu of $J$ in $\mathcal{U}$.
\end{enumerate}
\end{theorem}

\begin{remark}\label{chap2-rem2.1}
We note that a convex set $\mathcal{U}$ is weakly if and only if it is strongly closed and thus in $(H4)$ above $\mathcal{U}$ may be assumed weakly closed.
\end{remark}

\medskip
\noindent{\textbf{Proof of Theorem 2.1.}} First of all by $(H3)$, $J''(u ; \varphi, \varphi) \geq 0$ and hence $J$ is convex by Proposition \ref{chap1}.\ref{chap1-prop3.2}. Similarly $(H5)$ implies that $J$ is strictly convex again by Proposition \ref{chap1}. \ref{chap1-prop3.2}. Then, by Proposition \ref{chap1}. \ref{chap1-prop4.2} $(H2)$ and $(H3)$ together imply that $J$ is weakly lower semi-continuous. We next show that $J$ satisfies condition $(H3)'$ of Theorem \ref{chap2-thm1.2}: namely $J(v) \to + \infty$ as $||v||_{V} \to + \infty$. For this let $w \in \mathcal{U}$ be arbitrarily fixed. Then, because of $(H2)$ and $(H3)$ we can apply Taylor's formula to get, for $v \in V$.
$$
J(v) = J(w) + <G(w), v - w>_{V' \times V} + \frac{1}{2} J''(w+\theta_{0}(v-w), v-w; v-w)
$$
for some $\theta_{0} \in]0,1[$. Using $(H3)$ and estimating the second and third terms on the right side we have
$$
|<G(w), v-w>_{V' \times V} |\leq ||G(w)||_{V'} ||v-w||_{V}'
$$
$$
J''(w+\theta_{0}(v-w), v-w, v-w) \geq ||v-w||_{V} \times (||v-w||_{V}) \text{ and hence }
$$
$$
J(v) \geq J(w) + ||v-w||_{V} [\frac{1}{2} \times (||v-w||_{V}) - ||G(w)||_{V'}].
$$

Here,\pageoriginale since $w \in \mathcal{U}$ is fixed, as $||v||_{V} \to + \infty$
$$
||v-w||_{V} \to + \infty,
$$
$J(w)$ and $||G(w)||_{V'}$ are constants and
$$
\chi (||v-w||_{V}) \to + \infty \text{ by } (H3)
$$
which implies that $J(v) \to +\infty$ as $||v||_{V} \to + \infty$. The theorem then follows on application of Theorem \ref{chap2-thm1.2}.

\begin{theorem}\label{chap2-thm2.2}
Suppose $\mathcal{U}$ is a convex subset of a Banach space and $J : \mathcal{U} \subset V \to \mathbb{R}$ is a $G$-differentiable (in all directions) convex functional. Then

$u \in \mathcal{U}$ is a minimum for $J$ (i.e. $J(u) \leq J(v)$ for all $v \in V$) if and only if
$$
u \in \mathcal{U} \text{ and } J'(u, v - u) \geq 0 \text{ for all } v \in \mathcal{U}.
$$
\end{theorem}

\begin{proof}
Let $u \in \mathcal{U}$ be a minimum for $J$. Then, since $\mathcal{U}$ is convex, $v-u$ is a strongly admissible direction at $u$ for $\mathcal{U}$ for any $v$. Then, by Theorem \ref{chap2-thm1.3}, $J'(u, v-u) \geq 0$ for any $v \epsilon \mathcal{U}$. Conversely, since $J$ is convex and $G$-differentiable, by part (1) of Proposition \ref{chap1}. \ref{chap1-prop3.1}, we find that
$$
J(v) \geq J(u) + J'(u, v-u) \text{ for any } v \epsilon \mathcal{U}.
$$
\end{proof}

Then using the assumption that $J'(u ; v-u) \geq 0$ it follows that $J(u) \leq J(v)$ i.e. $u$ is a minimum for $J$ in $\mathcal{U}$.

Our next result concerns minima of convex functionals in the non-differentaible case.

\begin{theorem}\label{chap2-thm2.3}
Let $\mathcal{U}$ be a convex subset of a Banach space $V$. Suppose $J : \mathcal{U} \subset V \to \mathbb{R}$ is a functional of the form $J = J_{1} + J_{2}$ where $J_{1}, J_{2}$ are convex\pageoriginale functionals and $J_{2}$ is $G$-differentiable in $\mathcal{U}$ in all directions. Then $u \epsilon \mathcal{U}$ is a minimum for $J$
if and only if
$$
u \epsilon \mathcal{U}, J_{1}(v) - J_{1}(u) + J'_{2}(u, v-u) \geq 0 \; \text{ for all }  \; v \epsilon \mathcal{U}
$$
\end{theorem}

\begin{proof}
Suppose $u \epsilon \mathcal{U}$ is a minimum of $J$ then
$$
J(u) = J_{1}(u) + J_{2}(u) \leq J_{1} (u + \theta (v-u)) + J_{2}(u+\theta(v-u))
$$
since $u + \theta(v-u) \epsilon \mathcal{U}$. Here, by convexity of $J_{1}$, we have
$$
J_{1}(u+\theta(v-u)) \leq J_{1}(u) + \theta(J_{1}(v) - J_{1}(u))
$$
so that
$$
J_{2}(u) \leq \theta(J_{1}(v) - J_{1}(u)) + J_{2}(u+\theta(v-u)).
$$

That is 
$$
J_{1}(v) - J_{1}(u) + (J_{2}(u+\theta(v-u)) - J_{2}(u))/\theta \geq 0.
$$
\end{proof}

Taking limits as $\theta \to 0$ we get the required assertion. Conversely, since $J_{2}$ is convex and is $G$-differentiable we have, from part (1) of Proposition \ref{chap1}. \ref{chap1-prop3.1},
$$
J_{2}(v) - J_{2}(u) \geq J'_{2} (u, v-u) \text{ for all } u, v \epsilon \mathcal{U}.
$$

Now we can write, for any $v \epsilon \mathcal{U}$,
\begin{align*}
J(v) - J(u) & = J_{1}(v) - J_{1}(u) + J_{2}(v) - J_{u}\\
& \geq J_{1}(v) - J_{1}(u) + J'_{2}(u, v-u) \geq 0
\end{align*}
by assumption which proves that $u \epsilon \mathcal{U}$ is a minimum for $J$.

\section[Applications to the Model Problem and...]{Applications to the
  Model Problem and Reduction to variational
  Inequality}\label{chap2-sec3} 
We\pageoriginale shall apply the results pf Section \ref{chap2-sec2} to the functional $J$ of Example \ref{chap1}. \ref{chap1-exam1.1} on a Hilbert space. More precisely, let $V$ be a Hilbert space and $J : V \to \mathbb{R}$ be the functional
$$
v \mapsto J(v) = \frac{1}{2}a(v, v) - L(v)
$$ 
where $a(\cdot , \cdot)$ is a symmetric bilinear, bicontinuous, coercive form on $V$ and $L \epsilon V'$. Further, let $K$ be a closed convex subset of $V$. Consider the following

\begin{problem}\label{chap2-prob3.1}
To find
$$
u \epsilon K ; J(u) \leq J(v) \text{ for all } v \epsilon K.
$$
i.e. to find a $u \epsilon K$ which minimizes $J$ on $K$. We have seen in Chapter \ref{chap1} (Section \ref{chap1-sec7}) that $J$ is twice $F$-(and hence also $G$-) differentiable and that 
$$
J'(u, \varphi) = <G(u) , \varphi>_{V' \times V} = a(u, \varphi) - L(\varphi)
$$
$$
J''(u; \varphi, \psi) = <H(u) \varphi, \psi>_{V' \times V} = a(\varphi, \psi)
$$

Moreover, the coercivity of $a(\cdot , \cdot)$ implies that
$$
J''(u ; \varphi, \varphi) = a (\varphi, \varphi) \geq \alpha ||\varphi||_{V}^{2}.
$$
\end{problem}

If we choose $\chi(t) = \alpha t$ then all the assumptions of Theorem \ref{chap2-thm2.1} are satisfied by $V$, $J$ and $K$ so that the Problem \ref{chap2-prob3.1} has a unique solution. Also, by Theorem \ref{chap2-thm2.2}, the problem \ref{chap2-prob3.1} is equivalent to

\begin{problem}\label{chap2-prob3.2}
To find
$$
u \epsilon K ; a(u, v-u) \geq L(v-u) \text{ for all } v \epsilon K.
$$

We can summarise these facts as
\end{problem}

\begin{theorem}\label{chap2-thm3.1}
(1) There\pageoriginale exists a unique solution $u \epsilon K$ of the Problem \ref{chap2-prob3.1} and

(2) Problem \ref{chap2-prob3.1} is equivalent to problem \ref{chap2-prob3.2}.
\end{theorem}

The problem \ref{chap2-prob3.2} is called a variational inequality associted to the closed convex set $K$ and the bilinear form $a(\cdot , \cdot)$. As we shall see in the following section the variational inequality (\ref{chap2-prob3.2}) arises as generalizations of elliptic boundary value problems for suitable elliptic operators. It turns out that in many of the problems solving (numerically) the minimisation problem \ref{chap2-prob3.1} is much easier and faster than solving the equivalent variational inequality (\ref{chap2-prob3.2}).

In the particular case where $K = V$ the Problme \ref{chap2-prob3.1} is nothing but the Problem

(3.3) to find $u \epsilon V ; J(u) \leq J(v)$  for all $v \epsilon V$

which is equivalent to the Problem

(3.4) to find $u \epsilon V ; a(u, \varphi) = L(\varphi)$ for a $\varphi \epsilon V$. As we have seen in Chapter \ref{chap1}, (3.4) is equivalent to (\ref{chap2-prob3.2}) : if $\varphi \epsilon V$ we take $v = u \pm \varphi \epsilon K = V$ in (\ref{chap2-prob3.2}) to get (3.4) and the converse is trivial.

The following result is a generalization of Theorem \ref{chap2-thm3.1} to non-symmetric case and is due to $G$-Stampacchia. This generalizes and includes the classical Lax-Milgram theorem. (See \cite{key43}).

\begin{theorem}\label{chap2-thm3.2}
(Stampacchia). Let $K$ be a closed convex subset of a Hilbert space $V$ and $a(\cdot , \cdot)$ be a bilinear bicontinuous coercive form on $V$. Then for any given $L \epsilon V'$ the variational inequality (\ref{chap2-prob3.2}) has a unique solution $u \epsilon K$. 
\end{theorem}

\begin{proof}
Since, for any $u, v \mapsto a(u, v)$ is continuous linear on $V$ and $L \epsilon V'$ there exist unique elements $Au, f \epsilon V$ by Fr\'{e}chet-Riesz theorem such that
$$
a(u, v) = (Au, v)_{V} \text{ and } L(v) = (f, v)_{V}.
$$\pageoriginale
\end{proof}

Moreover $A \epsilon \mathscr{L} (V, V')$ with $||A||_{\mathscr{L}(V, V')} \leq M$ and $||f||_{V} \leq N$ where $M > 0$, $N > 0$ are constants such that
\begin{align*}
|a(u, v)| & \leq M||u||_{V} ||v||_{V} \text{ for all } u, v \epsilon V,\\
|L(v)| & \leq N||v||_{V} \text{ for all } v \epsilon V.
\end{align*}

Let $\alpha > 0$ be the constant of $V$-coercivity of $a(\cdot , \cdot)$ i.e. 
$$
a(v, v) \geq \alpha||v||_{V}^{2} \text{ for all } v \epsilon V.
$$

Since $K$ is a closed convex set there exists a projection mapping $P : V \to K$ with $||P||_{\mathscr{L}(V, V)} \leq 1$. Let $\gamma > 0$ be a constant which we shall choose suitably later on. Consider the mapping
$$
V \ni v \mapsto v - \gamma (Av-f) = T_{\gamma}(v) \epsilon V.
$$

For $\gamma$ sufficiently small $T_{\gamma}$ is a contraction mapping. In fact, if $v_{1}, v_{2} \epsilon V$ then
$$
T_{\gamma} v_{1} - T_{\gamma} v_{2} - (I - \gamma A) (v_{1} - v_{2}).
$$

Setting $w = v_{1} - v_{2}$ we have
\begin{align*}
||(I -\gamma A)w||_{V}^{2} & = (w - \gamma A w, w - \gamma A w)_{V}\\
& = ||w||_{V}^{2} - \gamma [(w, Aw)_{V} + (A w, w)_{V}] + \gamma^{2} ||Aw||_{V}^{2}\\
& \leq ||w||_{V}^{2} - 2\gamma \alpha ||w||_{V}^{2} + \gamma^{2} M^{2} ||w||_{V}^{2}\\
& = (1 - 2\gamma \alpha + \gamma^{2} M^{2}) ||w||_{V}^{2}
\end{align*}
by $V$-coercivity and continuity of the operator $A$. It is easy to see that if $0 < \gamma < 2\alpha/M^{2}$ then $1 - 2\gamma \alpha + \gamma^{2} M^{2} < 1$ and hence $T_{\gamma}$ becomes a contraction mapping.\pageoriginale Then the mapping $PT_{\gamma}|_{K} : K \to K$ is a contraction mapping and hence has a unique fixed point $u \epsilon K$ by contraction mapping theorem i.e. 
$$
u \epsilon K \text{ and } u = P(u - \gamma(Au - f)).
$$

This is the required solution of the variational inequality (\ref{chap2-prob3.2}) as can easily be checked.

\section{Some Functional Spaces}\label{chap2-sec4}
We shall briefly recall some important Sobolev spaces of distributions on an open set in $\mathbb{R}^{n}$ and some of their properties. These spaces play an important role in the weak (or variational) formulation of elliptic problems which we shall consider in the following. All our functionals in the examples will be defined on these spaces. For details we refer to the book of Lions and Magenes \cite{key32}.

Let $\Omega$ be a bounded open subset in $\mathbb{R}^{n}$ and $\Gamma$ denote its boundary. We shall assume $\Gamma$ to be sufficiently ``regular'' which we shall make precise whenever necessary.

\medskip
\noindent{\textbf{Sobolev spaces.}} We introduce the Sobolev space $H^{1} (\Omega)$:
\begin{equation*}
H^{1} (\Omega) = \{v | v \epsilon L^{2} (\Omega), \partial x_{j} \epsilon L^{2} (\Omega), j=1, \cdots, n\}\tag{4.1}\label{chap2-eq4.1}
\end{equation*}
where $D_{j}v = \partial v/\partial x_{j}$ are taken in the sense of distributions
$$
\text{ i.e. }\qquad <D_{j} v, \varphi> = -<v, D_{j} \varphi> \text{ for all } \varphi \epsilon \mathscr{D} (\Omega)
$$

Here $\mathscr{D} (\Omega)$ denotes the space of all $C^{\infty}$ -functions with compact support in $\Omega$ and $<\cdot , \cdot>$ denotes the duality between $\mathscr{D} (\Omega)$ and the space of distributions $\mathscr{D}' (\Omega)$ on $\Omega$. $H^{1} (\Omega)$ is provided with the inner-product
\begin{align*}
((u, v)) & (u, v)_{L^{2} (\Omega)} + \sum_{j=1}^{n} (D_{j} u, D_{j} v)_{L^{2} (\Omega)}\tag{4.2}\label{chap2-eq4.2}\\
& = \int_{\Omega} \{uv + \sum_{j=1}^{n} (D_{j} u)(D_{j} v)\} dx
\end{align*}
for\pageoriginale which becomes a Hilbert space. The following inclusions are obvious (and are continuous) $\mathscr{D} (\Omega) \subset C^{1} (\overline{\Omega}) \subset H^{1} (\Omega)$.

We also introduce the space
\begin{equation*}
H_{0}^{1} (\Omega) = \text{ the closure of } \mathscr{D} (\Omega) \text{ in } H^{1} (\Omega).\tag{4.3}\label{chap2-eq4.3}
\end{equation*}

We ahve the following well-known results.

\medskip
\noindent{(4.4) \textbf{Theorem of Density:}} If $\Gamma$ is ``regular'' (for instance, $\Gamma$ is a $C^{1}$ (or $C^{\infty}$)-mainfold of dimension $n-1$) then $C^{1}(\overline{\Omega})$ (resp. $C^{\infty}(\overline{\Omega})$) is dense in $H^{1}(\Omega)$. 

\medskip
\noindent{(4.5) \textbf{Theorem of Trace.}} If $\Gamma$ is ``regular'' then the linear mapping $v \mapsto v/\Gamma$ of $C^{1}(\overline{\Omega}) \to C^{1}(\Gamma)$ (resp pf $C^{\infty} (\overline{\Omega}) \to C^{\infty} (\Gamma)$) extends to a continuous linear map of $H^{1}(\Omega)$ into $L^{2}(\Gamma)$ denoted by $\gamma$ and for any $v \epsilon H^{1} (\Omega)$ $\gamma v$ is called the trace of v on $\Gamma$. Moreover, $H_{0}^{1}(\Omega) = \{v \epsilon H^{1} (\omega) \gamma v = 0\}$. We shall more often use this characterization of $H_{0}^{1} (\Omega)$. The trace map is not surjective. For a characterization of the image of $H^{1} (\omega)$ by $\gamma$ (which is proper subspace, denoted by $H^{\frac{1}{2}}(\Gamma)$) we refer to the book of Lions and Magenes \cite{key32}. We can also define spaces $H^{m} (\Omega)$ and $H_{0}^{m} (\Omega)$ in the same way for any $m > 1$.

\begin{remark}\label{chap2-rem4.1}
The Theorem of trace is slightly more precise than our statement above. For this and also for a proof we refer to the book of Lions and Magenes \cite{key32}. 

For some non-linear problems we shall also need spaces of the form
\begin{equation*}
V = H_{0}^{1} (\Omega) \cap L^{p} (\Omega) \text{ where } p \geq 2.\tag{4.6}\label{chap2-eq4.6}
\end{equation*}

The space $V$ is provided with the norm
\begin{equation*}
v \mapsto ||v||_{V} = ||v||_{H^{1}(\Omega)} + ||v||_{L^{p}(\Omega)}
\end{equation*}
for which it becomes a Banach space. If $2 \leq p < + \infty$ then $V$ is a reflexive Banach\pageoriginale space.
\end{remark}

In order to given an interpretation of the solutions of weak formulations of the problems as solutions of certain differential equations with boundary conditions we shall need an extension of the classical Green's formula which we recall here.

\medskip
\noindent{(4.8) \textbf{Green's formula for Sobolev spaces.}} Let $\Omega$ be a bounded open set with sufficiently ``regular'' boundary $\Gamma$. Then there exists a unique outer normal vector $\underline{n}(x)$ at each point x on $\Gamma$. Let $(\underline{n}_{1} (x), \cdots , \underline{n}_{n} (x))$ denote the direction cosines of $\underline{n}(x)$. We define the operator of exterior normal derivation formally as
\begin{equation*}
\partial / \partial \underline{n} = \sum_{j=1}^{n} n_{j} (x) D_{j}.\tag{4.9}\label{chap2-eq4.9}
\end{equation*}

Now if $u, v \epsilon C^{1} (\Omega)$ then by the classical Green's formula we have
$$
\int_{\Omega} (D_{j} u)v dx = -\int_{\Omega} u(D_{j} v) dx + \int_{\Gamma} uv n_{j} d\sigma
$$
where $d\sigma$ is the area element on $\Gamma$. This formual remains valid also if $u, v \epsilon H^{1}(\Omega)$ in view of the trace theorem and density theorem as can be seen using convergence theorems.

Next if $u, v \epsilon C^{2} (\overline{\Omega})$, then applying the above formula to $D_{j} u , D_{j} v$ and summing over $j=1, \cdots . n$ we get
\begin{align*}
\sum_{j=1}^{n} (D_{j} u, D_{j} v)_{L^{2} (\Omega)} & = -\sum_{j=1}^{n} \int_{\Omega} (D_{j}^{2} u) v dx + \int_{\Gamma} \partial u / \partial \underline{n} . vd \sigma\\
\text{ i.e. }\sum_{j=1}^{n} (D_{j} u, D_{j} v)_{L^{2} (\Omega)} & = -\int_{\Omega} (\triangle u) v dx + \int_{\Gamma} \partial u / \partial \underline{n} . v d \sigma.\tag{4.10}\label{chap2-eq4.10}
\end{align*}

Once again this formula remains valid if ; for instance, $u \epsilon H^{2} (\Omega)$, $v \epsilon H^{1} (\Omega)$ using the density and trace theorems. In fact, $u \epsilon H^{2} (\Omega)$ implies that $\triangle u \epsilon L^{2} (\Omega)$ and since $D_{j} u \epsilon H^{1} (\Omega)$, $\gamma(D_{j}u)$ exists and belong to $L^{2}(\Gamma)$ so that $\partial u / \partial \underline{n} = \sum_{j=1}^{n} n_{j} \gamma (D_{j} u) \epsilon L^{2}(\Gamma)$.

\section{Examples}\label{chap2-sec5}
In\pageoriginale this section we shall apply results of the previous sections to some concrete example of functionals on Sobolev spaces and we interprete the corresponding variational inequalities as boundary value problems for differential operators.

Throughout this section $\Omega$ will be a bounded open set with sufficiently ``regular'' boundary $\Gamma$. We shall not make precise the exact regularity conditions on $\Gamma$ except to say that it is such that the trace, density and Green's formula are valid.

We begin with the following abstract linear problem.

\begin{example}\label{chap2-exam5.1}
Let $\Gamma = \overline{\Gamma}_{1} \cup  \overline{\Gamma}_{2}$ where $\Gamma_{j}$ are open subsets of $\Gamma$ such that $\Gamma_{1} \cap \Gamma_{2} = \phi$ Consider the space
\begin{equation*}
V = \{v|v \epsilon H^{1} (\Omega); \gamma v = 0 \text{ on } \Gamma_{1}\}.\tag{5.1}\label{chap2-eq5.1}
\end{equation*}
\end{example}

$V$ is clearly a closed subspace of $H^{1} (\Omega)$ and is provided with the inner product induced from that in $H^{1}(\Omega)$ and hence it is a Hilbert space. Moreover,
\begin{equation*}
H_{0}^{1} (\Omega) \subset V \subset H^{1} (\Omega)\tag{5.2}\label{chap2-eq5.2}
\end{equation*}
and the inclusions are continuous linear. If $f \epsilon L^{2} (\Omega)$ we consider the functional
\begin{equation*}
J(v) = \frac{1}{2} ((u, v)) - (f, v)_{L^{2} (\Omega)}\tag{5.3}\label{chap3-eq5.3}
\end{equation*}
i.e. $a(u, v) = ((u, v))$ and $L(v) = (f, v)_{L^{2}(\Omega)}$. Then $a(\cdot , \cdot)$ is bilinear, bicontinuous and $V$-coercive :
\begin{align*}
|a(u, v)| & \leq ||u||_{V} ||v||_{V} = ||u||_{H^{1} (\Omega)} ||v||_{H^{1}(\Omega)} \text{ for } u, v \epsilon V,\\
a(v, v) & = ||v||_{H^{1}(\Omega)}^{2} \text{ for } v \epsilon V\\
\text{ and } |L(v)| & \leq ||f||_{L^{2}(\Omega)} ||v||_{L^{2}(\Omega)} \leq ||f||_{L^{2}(\Omega)} ||v||_{H^{1}(\Omega)} \text{ for } v \epsilon V.
\end{align*}\pageoriginale

Then the problems (3.3) and (3.4) respectively become
\begin{align*}
&\text{ to find } u \epsilon V, J(u) \leq J(v) \text{ for all } v \epsilon V \text{ and }\tag{5.4}\label{chap2-eq5.4}\\
& \text{ to find } u \epsilon V, ((u, \varphi)) = (f, \varphi)_{L^{2} (\Omega)} \text{ for all } \varphi \epsilon V.\tag{5.5}\label{chap2-eq5.5}
\end{align*}

From what we have seen in Section \ref{chap2-sec3} these two equivalent problems have unique solutions.

{\em The Problem (\ref{chap2-eq5.5}) is the weak (or variational) formulation of the Dirichlet problem (if $\Gamma_{2} = \phi$), Neumann problem if $\Gamma_{1} = \phi$ and the mixed boundery value problem in the general case.}

We now interprete the solutions of Problems (\ref{chap2-eq5.2}) when they are sufficiently regular as solutions of the classical Dirichlet (resp. Neumann of mixed) problems.

Suppose we assume $u \epsilon C^{2} (\overline{\Omega}) \cap V$ and $v \epsilon C^{1} (\overline{\Omega}) \cap V$. We can write using the Green's formula (\ref{chap2-eq4.10})
\begin{align*}
a(u, v) = ((u, v)) & = \int_{\Omega} (-\triangle u + u)v dx + \int_{\Gamma} \partial u / \partial \underline{n} . vd \sigma = \int_{\Omega} fv dx\\
\text{ i.e. } & \int_{\Omega} (-\triangle u + u-f) v dx + \int_{\Gamma} \partial u / \partial \underline{n}. vd\sigma = 0.\tag{5.6}\label{chap2-eq5.6}
\end{align*}

We note that this formula remains valide if $u \epsilon H^{2} (\Omega) \cap V$ for any $v \epsilon V$.

First we choose $v \epsilon \mathscr{D} (\Omega) \subset V$ (enough to take $v \epsilon C_{0}^{1} (\Omega) (\Omega) \subset V$) then the boundary integral vanishes so that we get
$$
\int_{\Omega} (-\triangle u + u-f) v dx = 0 \; \forall v \epsilon \mathscr{D}(\Omega).
$$

Since $\mathscr{D} (\Omega)$ is dense in $L^{2} (\Omega)$ this implies that (if $u \epsilon H^{2} (\Omega)$) $u$ is a solution of the differential equation
\begin{equation*}
-\triangle u + u - f = o \text{ in } \Omega \text{ (in the sense of } L^{2}(\Omega)).\tag{5.7}\label{chap2-eq5.7}
\end{equation*}\pageoriginale

More generally, without the strong regularity assumption as above, $u$ is a solution of the differential equation
\begin{equation*}
-\triangle u + u - f = 0 \text{ in the sense of distributions in }\Omega.\tag{5.8}\label{chap2-eq5.8}
\end{equation*}

Next we choose $v \epsilon V$ arbitrary. Since $u$ satisfies the equation (\ref{chap2-eq5.8}) in $\Omega$ we find from (\ref{chap2-eq5.6}) that
\begin{equation*}
\int_{\Gamma_{2}} \partial u / \partial \underline{n} v d \sigma = 0 \; \forall v \epsilon V,\tag{5.9}\label{chap2-eq5.9}
\end{equation*}
whcih means that $\partial u / \partial \underline{n} = 0$ on $\Gamma$ in some generalized sense. In fact, by trace theorem $\gamma v \epsilon H^{\frac{1}{2}}(\Gamma)$ and hence $\partial u / \partial \underline{n} = 0$ in $H^{-\frac{1}{2}} (\Gamma)$ (see Lions and Magenese \cite{key32}). Thus, if the Problem (\ref{chap2-eq5.2}) has a regular solution then it is the solution of the classical problem
\begin{equation*}
\begin{cases}
-\triangle u + u & = f \text{ in } \Omega\\
u & = 0 \text{ on } \Gamma_{1}\\
\partial u / \partial \underline{n} & = 0 \text{ on } \Gamma_{2}\tag{5.10}\label{chap2-eq5.10}
\end{cases}
\end{equation*}

The Problem (\ref{chap2-eq5.10}) is the classical Dirichlet (resp. Neumann, or mixed) problem for the elliptic differential operator $-\triangle u + u$ if $\Gamma_{2} = \phi$ (resp. $\Gamma_{1} = \phi$ or general $\Gamma_{1}, \Gamma_{2}$).

\begin{remark}\label{chap2-rem5.1}
The variational formualtion (\ref{chap2-eq5.5}) of the problem (\ref{chap2-eq5.5}) is very much used in the Finite elements method.
\end{remark}

Example \ref{chap2-exam5.1} is a special case of the following more general problem.

\begin{dashexam}\label{chap2-exam$(5.1)'$}
Let $\Omega, \Gamma = \Gamma_{1} \cup \Gamma_{2}$ and $V$ be as in Example \ref{chap2-exam5.1}. Suppose given an integro-differentail bilinear form ;
\begin{equation*}
a(u, v) = \int_{\Omega} \sum_{i, j=1}^{n} a_{ij} (x) (D_{i} u) (D_{j} v) dx + \int_{\Omega} a_{0} (x) uv dx,\tag{5.11}\label{chap2-eq5.11}
\end{equation*}
\end{dashexam}
where\pageoriginale the coefficients satisfy the following conditions:
\begin{equation*}
\begin{cases}
& a_{ij} \epsilon L^{\infty} (\Omega), \;  a_{\circ} \epsilon L^{\infty} (\Omega);\\
& \text{ condition of ellipticity there exists a constant } \alpha > 0 \text{ such that }\\
& \sum_{i, j} a_{ij} (x) \overline{\xi}_{i} \overline{\xi}_{j} \geq
  \alpha \sum_{i} \overline{\xi}_{i}^{2} \text{ for } \overline{\xi} =
  (\overline{\xi}_{1}, \cdots , \overline{\xi}_{n}) \epsilon
  \mathbb{R}^{n} a.e. \text{ in } \Omega ;\\
& a_{\circ} (x) \geq \alpha > 0. \tag{5.12}\label{chap2-eq5.12}
\end{cases}
\end{equation*}

It follows by a simple application of Cauchy-Schwarz inequality that the bi-linear form is well defined and bi-continuous on $V$: for all $u, v \epsilon V$,
$$
|a(u, v)| \leq \max (||a_{ij}||_{L^{\infty}(\Omega)}, ||a_{\circ}||_{L^{\infty}(\Omega)}) ||u||_{V} ||v||_{V}
$$

$a(\cdot , \cdot)$ is also coercive ; by the ellipticity and the last condition on $a_{\circ}$
$$
a(v, v) \geq \alpha \int_{\Omega} (\sum_{i} |D_{i}v|^{2} + |v|^{2}) dx = \alpha ||v||_{V}^{2}, v \epsilon V. 
$$

Suppose given $f \epsilon L^{2} (\Omega)$ and $g \epsilon L^{2} (\Gamma_{2})$. Then the linear functional
\begin{equation*}
v \mapsto L(v) = \int_{\Omega} fv dx + \int_{\Gamma} gv f\sigma\tag{5.13}\label{chap2-eq5.13}
\end{equation*}
on $V$ is continuous and we have again by Cauchy-Schwarz inequality
\begin{align*}
|L(v)| & \leq ||f||_{L^{2}(\Omega)} ||v||_{L^{2}(\Omega)} + ||g||_{L^{2}(\Omega)} ||v||_{L^{2}(\Gamma)}\\
& \leq (||f||_{L^{2}(\Omega)} + ||g||_{L^{2}(\Gamma)}) ||v||_{V} \text{ by trace theorem.}
\end{align*}

We introduce the functional
$$
v \mapsto J(v) = a(v, v) - L(v).
$$

For the Problem (\ref{chap2-eq5.4}) of minimising $H$ on $V$ we further assume 
$$
a_{ij} = a_{ji} , 1 \leq i, \leq n.
$$

If $a_{i, j}$ are smooth functions in $\Omega$ and $u$ is a smooth solution of the Problem (\ref{chap2-eq5.5}) we can interprete $u$ as a solution of a classical problem using the Green's\pageoriginale formula as we did in the earlier case. We shall indicate only the essential facts. We introduce the formula differential operator
\begin{equation*}
Au = -\sum_{i, j=1}^{n} D_{j} (a_{ij} D_{i} u) + a_{\circ} u.\tag{5.14}\label{chap2-eq5.14}
\end{equation*}

If $a_{ij}$ are smooth (for instance, $a_{ij} \epsilon C^{1} (\Omega)$) then A is a differential operator in the usual sense. By Green's formula we find that
\begin{equation*}
a(u, v) = -\sum_{i, j} \int_{\Omega} D_{j} (a_{ij} D_{i}u) + \int_{\Gamma} \sum_{i, j} a_{ij} (D_{i u})n_{j} (x) v d\sigma + \int_{\Omega} a_{\circ} uv dx\tag{5.15}\label{chap2-eq5.15}
\end{equation*}
where $(n_{1}(x), \cdots , n_{n}(x))$ are the direction cosines of the exterior normal to $\Gamma$ at x. The operator
\begin{equation*}
\sum_{i, j} a_{ij} (D_{i} u) n_{j}(x) = \partial u / \partial n_{ A}\tag{5.16}\label{chap2-eq5.16}
\end{equation*}
is called the co-normal derivatives of $u$ respect to the form $a(\cdot , \cdot)$. Thus we can write (\ref{chap2-eq5.15}) as
\begin{equation*}
a(u, v) = \int_{\Omega} (Au) v dx + \int_{\Gamma} \partial u/\partial n_{A} vd\sigma\tag*{$(5.15)'$}\label{chap2-eq5.15'}
\end{equation*}
and hence the Problem (\ref{chap2-eq5.2}) becomes
$$
\int_{\Omega} (Au-f)v dx + \int_{\Gamma} (\partial u/\partial n_{A} - g) v d\sigma = 0.
$$

Proceeding exactly as in the previous case we can conclude that the Problem (\ref{chap2-eq5.5}) is equivalent to the classical problem.
\begin{equation*}
\begin{cases}
Au = f & \text{ in } \Omega\\
u = 0  & \text{ on } \Gamma_{1}\\
\partial u / \partial n_{A} = g & \text{ on } \Gamma_{2}\tag{5.17}\label{chap2-eq5.17}
\end{cases}
\end{equation*}

\begin{example}\label{chap2-exam5.2}
Let $V = H_{\circ}^{1} (\Omega) = \{v | v \epsilon H^{1} (\Omega), \gamma v = 0\}$, and $J$ be the functional on $V$:
$$
v \mapsto J(v) = \frac{1}{2} ||v||_{V}^{2} - (f, v)_{L^{2} (\Omega)}
$$\pageoriginale
where $f \epsilon L^{2} (\Omega)$ is a given function. Suppose
\begin{equation*}
K = \{v | v \epsilon V, v(x) \geq 0 \text{ a. e. in }\Omega\}\tag{5.19}\label{chap2-eq5.19}
\end{equation*}
\end{example}

It is clear that $K$ is convex and it is easily checked that $K$ is also closed in $V$.

In fact, if $v_{n} \epsilon K$ and $v_{n} \to v$ in $V$ then, for any $\varphi \epsilon \mathscr{D} (\Omega)$ such that $\varphi > 0$ in $\Omega$ we have
$$
\int_{\Omega} v \varphi dx = \lim_{n \to \infty} \int_{\Omega} v_{n} \varphi dx \geq 0
$$
(the first equality is an immediate consequence of Cauchy-Schwarz inequality since $v, \varphi \epsilon L^{2}(\Omega)$). This immediately implies that $v \geq 0$ a. e. in $\Omega$ and hence $v \epsilon K$. 

We know from Section \ref{chap2-sec3} that the minimising problem.
\begin{equation*}
u \epsilon K ; J(u) \leq J(v), \; \forall v \epsilon K\tag{5.20}\label{chap2-eq5.20}
\end{equation*}
is equivalent to the variational inequality:
\begin{equation*}
u \epsilon K; a(u, v - u) \geq L(v-u) = (f, v-u)_{L^{2}(\Omega)}, \forall v \epsilon K\tag{5.21}\label{chap2-eq5.21}
\end{equation*}
and both have unique solutions. In order to interprete this latter problem we find on applying the Green's formula.
\begin{equation*}
\int_{\Omega} (-\triangle u + u-f)(v-u) dx + \int_{\Gamma} \partial u / \partial n(u-v) d\sigma \geq 0, \forall v \epsilon K.\tag{5.22}\label{chap2-eq5.22}
\end{equation*}

Since $v \epsilon K \subset V = H_{\circ}^{1} (\Omega)$ the boundary integral vanishes and so
\begin{equation*}
\int_{\Omega} (-\triangle u + u - f)(v-u)dx \geq 0, \forall v \epsilon K.\tag{5.23}\label{chap2-eq5.23}
\end{equation*}

If $\varphi \epsilon K$, taking $v = u + \varphi \epsilon K$ we get
$$
\int_{\Omega} (-\triangle u + u - f) \varphi dx \geq 0, \varphi \epsilon K
$$\pageoriginale
from which we conclude that $-\triangle u + u - f \geq 0$ a.e. in $\Omega$. For, if $\omega$ is an open sub-set of $\Omega$ where $-\triangle u + u - f > 0$ we take a $\varphi \epsilon \mathscr{D} (\Omega)$ with $\varphi \geq 0$ and supp $\varphi \subset \omega$. Such a $\varphi$ clearly belongs to $K$ and we would arrive at a contradiction. In particular, this argument also shows that on the subset of $\Omega$ where $u > 0$ is satisfies the equation $- \triangle u + u = f$.

Next if we choose $v = 2u \epsilon K$ in (\ref{chap2-eq5.23}) we find
$$
\int_{\Omega} (-\triangle u + u - f)u dx \geq 0
$$
and if we choose $v = \frac{1}{2} u \epsilon K$ we find
$$
\int_{\Omega} (-\triangle u + u - f)u dx \leq 0.
$$

These two together imply that
\begin{equation}
(-\triangle u + u-f)u = 0\tag{5.24}\label{chap2-eq5.24}
\end{equation}

Thus the solution of the variational inequality can be interpreted (when it is sufficiently smooth) as the (unique) solution of the problem :
\begin{equation*}
\begin{cases}
(-\triangle u + u-f) u & = 0 \text{ in } \Omega\\
- \triangle u + u - f & \geq 0 \text{ a. e. in } \Omega\\
u & \geq 0 \text{ a. e. in } \Omega\\
u & = 0 \text{ on } \Gamma.\tag{5.25}\label{chap2-eq5.25}
\end{cases}
\end{equation*}

\begin{remark}\label{chap2-rem5.2}
The equivalent minimisation problem can be solved numerically (for example, by Gauss-Seidel method). (See Chapter \ref{chap4} \S\ \ref{chap4-subsec4.1}).
\end{remark}

\setcounter{exercise}{1}
\begin{exercise}\label{chap2-exer5.2}
Let $\Omega$ be a bounded open set in $\mathbb{R}^{n}$ with smooth boundary $\Gamma$. Let $V = H^{1} (\Omega)$ and $K$ be the subset
\begin{equation*}
K = \{v | v \epsilon H^{1} (\Omega) ; \gamma v \geq 0 \text{ a. e. on } \Gamma\}\tag{5.26}\label{chap2-eq5.26}
\end{equation*}\pageoriginale
\end{exercise}

Once again $K$ is a closed convex set. To see that it is closed, if $v_{n} \epsilon K$ is a sequence such that $v_{n} \to v$ in $V$ then since $\gamma : H^{1} (\Omega) = V \to L^{2} (\Gamma)$ is continuous linear $\gamma v_{n} \to \gamma v$ in $L^{2} (\Gamma)$. Now, if $\varphi \epsilon L^{2} (\Gamma)$ is such that $\varphi > 0$ a. e. on $\Gamma$ then
$$
\int_{\Gamma} (\gamma v) \varphi d\sigma = \lim_{n \to \infty} \int_{\Gamma} (\gamma v_{n}) \varphi d \geq 0 \text{ since } v_{n} \epsilon K.
$$
from which we deduce as in Example \ref{chap2-exam5.1} that $\gamma v \geq 0$.

Let $f \epsilon L^{2} (\Omega)$ be given

The problem of minimising the functional
\begin{equation*}
v \mapsto J(v) = \frac{1}{2} ((v, v))_{V} - (f, v)_{L^{2} (\Omega)}\tag{5.27}\label{chap2-eq5.27}
\end{equation*}
on the closed convex set $K$ is equivalent to the variational inequality
\begin{equation*}
u \epsilon K : a(u, v-u) \equiv ((u, v-u))_{V} \geq (f, v-u)_{L^{2} (\Omega)}, \forall v \epsilon K.\tag{5.28}\label{chap2-eq5.28}
\end{equation*}

Assumig the solution $u$ (which exists and is unique from section \ref{chap2-sec3}) is sufficiently regular we can interprete $u$ as follows. By Green's formula we have
\begin{equation*}
\int_{\Omega} (-\triangle u - f)(u - v) dx + \int_{\Gamma} \frac{\partial u}{\partial \underline{n}} (v-u) d\sigma \geq 0, \forall v \epsilon K.\tag{5.29}\label{chap2-eq5.29}
\end{equation*}

If $\varphi \epsilon \mathscr{D} (\Omega)$ the boundary intergal vanishes for $v = u \pm \varphi$ which belongs to $K$ and 
$$
\int_{\Omega} (-\triangle u - f)\varphi dx = 0
$$
which implies that $-\triangle u = f$ in $\Omega$.

Next since $v = 2u$ and $v = \frac{1}{2}u$ also belong to $K$ we find that
$$
\int_{\Gamma} \dfrac{\partial u}{\partial \underline{n}} u d\sigma = 0
$$
which\pageoriginale implies that $\dfrac{\partial u}{\partial \underline{n}} u = 0$ a.e. on $\Gamma$.

Thus the variational inequality (\ref{chap2-eq5.28}) is equivalent to the following Problem:
\begin{equation*}
\begin{cases}
-\triangle u & = f \text{ in } \Omega\\
\partial u / \partial \underline{n} & u  = 0 \text{ on } \Gamma\\
\partial u / \partial \underline{n} & \geq 0 \text{ on } \Gamma\\
u \geq 0 & \text{ on } \Gamma\tag{5.30}\label{chap2-eq5.30}
\end{cases}
\end{equation*}

One can also deduce from (\ref{chap2-eq5.30}) that on the subset of $\Gamma$ where $u > 0$, $u$ satisfies the homogeneous Neumann condition
$$
\dfrac{\partial u}{\partial \underline{n}} = 0.
$$

\begin{example}\label{chap2-exam5.3}
Let $\Omega$ be a bounded open set in $\mathbb{R}^{n}$ with smooth boundary $\Gamma$ and $1 \leq p < + \infty$. We introduce the space
\begin{equation*}
V = \{v | v \epsilon L^{2} (\Omega) ; D_{j} v \epsilon L^{2p} (\Omega), j=1, \cdots , n\}\tag{5.31}\label{chap2-eq5.31}
\end{equation*}
provided with its natural norm
\begin{equation*}
v \mapsto ||v||_{V} = ||v||_{L^{2} (\Omega)} + \sum_{j=1}^{n} ||D_{j} v||_{L^{2p} (\Omega)}.\tag{5.32}\label{chap2-eq5.32}
\end{equation*}
\end{example}

Then $V$ becomes a reflexive Banach space. Consider the functional $J : V \to \mathbb{R}$: 
\begin{equation*}
v \mapsto J(v) = \dfrac{1}{2p} \sum_{j=1}^{n} \int_{\Omega} |D_{j} v|^{2p} dx + \frac{1}{2} \int_{\Omega} |v|^{2} dx - \int_{\Omega} fv dx\tag{5.34}\label{chap2-eq5.34}
\end{equation*}
where $f \epsilon L^{2} (\Omega)$ is given. If we set, $g_{j}(t) = \dfrac{1}{2p} |t|^{2p}$ we get a $C^{1}$ -function $g_{j} : \mathbb{R}^{1} \to \mathbb{R}^{1}$ and we have $g'_{j}(t) = |t|^{2p-2} t$ for all $j=1, \cdots , n$. Then from Exerices I. 1.1, the functional
$$
v \mapsto \sum_{j} \int_{\Omega} g_{j} (v) dx = \dfrac{1}{2p} \sum_{j} \int_{\Omega} |D_{j} v|^{2p} dx
$$
is once\pageoriginale $G$-differentiable in all directions and its $G$-derivative in any direction $\varphi$ is given by
$$
\sum_{j} \int_{\varphi} g'_{j} (u) \varphi dx, \; \forall \varphi \epsilon V.
$$

Hence we obtain, in our case,
\begin{equation*}
J'(u, \varphi) = \sum_{j} \int_{\Omega} |D_{j} u|^{2p-2} (D_{j}u) (D_{j} \varphi) dx + \int_{\Omega} u \varphi dx - \int_{\Omega} f \varphi dx.\tag{5.35}\label{chap2-eq5.35}
\end{equation*}

Then the minimisation problem
\begin{equation*}
u \epsilon V ; J(u) \leq J(v), \forall v \epsilon V,\tag{5.36}\label{chap2-eq5.36}
\end{equation*}
is equivalent by Theorem \ref{chap2-thm3.1} to the problem
\begin{equation*}
u \epsilon V; J' (u, \varphi) = 0, \forall \varphi \epsilon V.\tag{5.37}\label{chap2-eq5.37}
\end{equation*}

We can verify that $J$ is strictly convex; for instance, we can compute $J''(u ; \varphi, \varphi)$ for any $\varphi \epsilon V$ and find
\begin{equation*}
J''(u ; \varphi, \varphi) = (2p - 1) \sum_{j} \int_{\Omega} (|D_{j} u|^{2(p-1)} |D_{j} \varphi|^{2} + \frac{1}{2} \varphi^{2})dx > 0\tag{5.38}\label{chap2-eq5.38}
\end{equation*}
for any $\varphi \epsilon V$ with $\varphi \neq 0$. Then Proposition \ref{chap1}. \ref{chap1-prop3.2} implies the strict convexity of $J$.

We claim that
$$
J(v) \to + \infty \text{ as } ||v||_{V} \to + \infty.
$$

In fact, first of all by Cauchy-Schwarz inequality we have
$$
\left|\int_{\Omega} fv dx\right| \leq ||f||_{L^{2} (\Omega)} ||v||_{L^{2} (\Omega)}
$$
and hence
$$
\frac{1}{2} \int_{\Omega} |v|^{2} dx - \int_{\Omega} fv dx \geq \frac{1}{2} ||v||_{L^{2} (\Omega)} (||v||_{L^{2} (\Omega)} - 2||v||_{L^{2} (\Omega)})
$$
so that
$$
J(v) \geq \dfrac{1}{2p} \sum_{j} ||D_{j} V||_{2p}^{2p} + \frac{1}{2} ||v||_{L^{2} (\Omega)} (||v||_{L^{2} (\Omega)} - f ||_{L^{2} (\Omega)} )
$$\pageoriginale
which tends to $+ \infty$ as $||v||_{V} \to +\infty$.

Then by Theorem \ref{chap2-thm1.2} the minimisation problem (\ref{chap2-eq5.36}) has a unique solution.

Finally, if we take $\varphi \epsilon \mathscr{D} (\Omega) \subset V$ in the equation (\ref{chap2-eq5.35}) we get
$$
\int_{\Omega} (\sum_{j} |D_{j} u|^{2p-2} (D_{j}u) (D_{j} \varphi) + u \varphi - f\varphi) dx = 0.
$$

On integration by parts this becomes
$$
\int_{\Omega} (\sum_{j} - D_{j} (|D_{j} u|^{2p-2} D_{j}u) + u-f) \varphi dx = 0,
$$

Thus the solution of the minimising problem (\ref{chap2-eq5.36}) for $J$ in $V$ can interpreted as the solution of the non-linear problem
\begin{equation*}
u \epsilon V, -\sum_{j} D_{j} (|D_{j} u|^{2p-2} D_{j}u) + u = f \text{ in } \Omega.\tag{5.39}\label{chap2-eq5.39}
\end{equation*}

We have used the fact $\mathscr{D} (\Omega)$ is dense in $L^{p'} (\Omega)$ where $\dfrac{1}{p} + \dfrac{1}{p'} = 1$.

The problem (\ref{chap2-eq5.39}) is a generalized Neumann problem for the non-linear (Laplacian) operator
\begin{equation*}
-\sum_{j} D_{j} (|D_{j} u|^{2p-2} D_{j}u) + u.\tag{5.40}\label{chap2-eq5.40}
\end{equation*}

\begin{example}\label{chap2-exam5.4}
Let $\Omega$ and $\Gamma$ be as in the previous example and
\begin{equation*}
V = H_{\circ}^{1} (\Omega) \cap L^{4} (\Omega).\tag{5.41}\label{chap2-eq5.41}
\end{equation*}

We have seen in Section \ref{chap2-sec4} that $V$ is a reflexive Banach space for its natural norm
\begin{equation*}
v \mapsto ||v||_{H^{1} (\Omega)} + ||v||_{L^{4} (\Omega)} = ||v||_{V}.\tag{5.42}\label{chap2-eq5.42}
\end{equation*}
\end{example}

Consider the functional $J$ on $V$ given by
\begin{equation*}
v \mapsto J(v) = \frac{1}{2} ||v||^{2}_{H^{1} (\Omega)} + \dfrac{1}{4} ||v||_{L^{4}(\Omega)}^{4} - (f, v)_{L^{2} (\Omega)},\tag{5.43}\label{chap2-eq5.43}
\end{equation*}\pageoriginale
where $f \epsilon L^{2} (\Omega)$ is given. It is easily verified that
$J$ is twice $G$ - differentiable and
$$
J'(u, \varphi) = ((u, \varphi))_{H^{1} (\Omega)} +  \int_{\Omega} (u^{3} - f) \varphi dx, \; \forall \varphi, \; u \epsilon V.
$$

(Hence $J$ has a gradient)
$$
J''(u ; \varphi, \psi) = ((\psi, \varphi))_{H^{1} (\Omega)}  + 3 \int_{\Omega} u^{2} \psi \varphi dx, \; \forall u, \varphi, \psi \epsilon V.
$$

Thus $J'' (u ; \varphi, \varphi) > 0$ for $u \epsilon V, \varphi \epsilon V$ with $\varphi \neq 0$ which implies that $J$ is strictly convex by Proposition \ref{chap1}. \ref{chap1-prop3.2}. As in the previous example we can show using Cauchy-Schwarz inequatliy (for the term $(f, v)_{L^{2} (\Omega)})$, that
$$
J(v) \to +\infty \text{ as } ||v||_{V} \to +\infty.
$$

Then by Theorem \ref{chap2-thm1.2} the minimisation problem for $J$ on $V$ has a unique solution. An application of Green's formula shows that this unique solution (when it is regular) is the solution of the non-linear problem :
\begin{equation*}
\begin{cases}
-\triangle u + u + u^{3} = f & \text{ in } \Omega\\
u = 0 & \text{ on } \Gamma\tag{5.44}\label{chap2-eq5.44}
\end{cases}
\end{equation*}

\begin{remark}\label{chap2-rem5.3}
It is, ingeneral, difficult to solve the non-linear problem (\ref{chap2-eq5.43}) numerically and it is easier to solve the equivalent minimisation problem for $J$ given by (\ref{chap2-eq5.44}).
\end{remark}

\begin{remark}\label{chap2-rem5.4}
All the functionals considered in the examples discussed in this section are strictly convex and they give rise to strongly monotone operators. We recall the following
\end{remark}

\begin{definition}
An\pageoriginale operator $A : U \subset V \to V'$ on a subset $U$ of a normed vector space into its dual is called monotone if
$$
<Au - Av, u-v>_{V' \times V} \geq 0 \text{ for all } u, v \epsilon U.
$$
A is said to be strictly monotone if $<Au - Av, u-v>_{V' \times V} > 0$ for any pair of distinct elements $u, v \epsilon V$ (i.e. if $u \neq v$). (See, for instance, \cite{key44}).
\end{definition}
