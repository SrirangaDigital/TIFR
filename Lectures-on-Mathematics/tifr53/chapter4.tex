
\chapter{Minimization with Constraints - Algorithms}\label{chap4}
We\pageoriginale have discussded the existence and uniqueness results for solutions of the minimization problems for convex functionals on closed convex subsets of a Hilbert space. This chapter will be devoted to give algorithm for the construction of minimizing sequences for solutions of this problem. We shall describe only a few methods in this direction and we prove that such an algorithm is convergent.

\section{Linearization Method}\label{chap4-sec1}
The problem of minimization of a functional on a convex set is also some-times referred as the problem of (non-linear) programming. If the functional is convex the programming problem is call convex programming.

The main idea of the method we shall describe in this section consists in reducing at each stage of iteration the problem of non-linear convex programming to one of linear programming in one more variable i.e. to a problem of minimizing a linear functional on a convex set defined by linear constraints. However, when we reduce to this case we may not have coercivity. However, if we know that the convex set defined this way by linear constraints is bounded then we have seen in Chapter \ref{chap2} that the linear programming problem has a solution (which is not necessarily unique).

Then the solution of such a linear programming problem is used to obtain convergent choices of $w$ and $\rho$.

Let $V$ be a Hilbert space and $K$ a closed subset of $V$. We shall prescribe some of the constraints of the problem by giving a finite number of convex functionals
$$
J_{i} : V \ni v \mapsto J_{i} (v) \epsilon \mathbb{R}, i = 1, \cdots, k,
$$\pageoriginale
and we define a subset $U$ of $K$ by
$$
U = \{v | v \epsilon K, J_{i} (v) \leq 0, i = 1, \cdots,\}
$$

Then $U$ is again a convex set in $V$. If $v, v' \epsilon U$ then $v, v' \epsilon K$ and $(1-\theta) v + \theta v' \epsilon K$ for any $0 \leq \theta \leq 1$ since $K$ is convex. Now $J_{i} (i = 1, \cdots, k)$ being convex we have
$$
J_{i} ((1-\theta)v + \theta v') \leq (1-\theta) J_{i}(v) + \theta J_{i} (v') \leq 0, i=1, \cdots, k.
$$ 

We note that in practice, the convex set $K$ contains (i.e. is defined by) all the constraints which need not be linearized and the constraints to be linearized asre the $J_{i}(i = 1, \cdots, k)$.

Suppose now
$$
J_{\circ} : v \ni V \to J_{\circ}(v) \epsilon \mathbb{R}
$$
is a convex functional on $V$. We consider the minimization problem:

\begin{problem}\label{chap4-prob1.1}
To find $u \epsilon U, J_{\circ}(u) \leq J_{\circ}(v), \forall v
\epsilon U$. We assume that $J_{\circ}, J_{1},\break \ldots, J_{k}$ satisfy
the following hypothesis: 


{\em Hypothesis on} $J_{\circ} : (HJ)_{\circ}$.  
\begin{enumerate}
\item[(1)] $J_{\circ}(v) \to + \infty \text{ as } ||v|| \to + \infty$

\item[(2)] $J_{\circ}$  is regular:  $J_{\circ}$  is twice differentiable everywhere 

in $V$ and has a gradient $G_{\circ}$ and a hessian $H_{\circ}$ everywhere in $V$ which are bounded on bounded subsets: for every bounded set $U_{1}$ of $V$ there exists a constant $M_{U_{1}} > 0$ such that
$$
||G_{\circ}(v)|| + ||H_{\circ} (v)|| \leq M_{U_{1}} \forall v \epsilon U_{1}.
$$

\item[(3)$_{\circ}$] $H_{\circ}$ is uniformly $V$-coercive on bounded subsets of $V$: for every bounded subset $U_{1}$ of $V$ there exists a constant $\alpha_{U_{1}} > 0$ such that
$$
(H_{\circ}(v) \varphi, \varphi) \geq \alpha_{U_{1}} ||\varphi||^{2} \; \forall \varphi \epsilon V \text{ and } \forall v \epsilon U_{1}.
$$
\end{enumerate}\pageoriginale

\noindent{\textbf{Hypothesis on $J_{i}. (HJ)_{i}$:}} 
\begin{enumerate}
\item[$(1)_{i}$] $J_{i}$ is regular : $J_{i}$ is twice $G$-differentiable everywhere in $V$ and has a gradient $G_{i}$ and a hessian $H_{i}$ bounded on bounded sets of $V$: for every bounded set $U_{1}$ of $V$ there exists a constant $M_{U_{1}} > 0$ such that
$$
||G_{i}(v)|| + ||H_{i}(v)|| \leq M_{U_{1}} \quad \forall v \epsilon U_{1}, i=1, \cdots , k.
$$

\item[$(2)_{i}$] $H_{i}(v)$ is positive semi-definite:
$$
(H_{i}(v) \varphi, \varphi) \geq 0 \quad \forall \varphi \epsilon V (\forall v \epsilon U_{1}).
$$ 
\end{enumerate}

\noindent{\textbf{Hypothesis on $K. (HK)$:}} There exists and element $Z \epsilon K$ such that $J_{i} (Z) < 0$ for all $i = 1, \cdots, k$.

The hypothesis $(HK)$ in particular implies that $U \neq \phi$.
\end{problem}

In order to describe the algorithm let $u_{\circ} \epsilon U$ be the initial point (arbitrarily fixed) of the algorithm. In view of the hypothesis $(HJ)_{\circ} (1)$ we may, without loss of generality, assume that $U$ is bounded since otherwise we can restrict ourselves to the set
$$
\{v \epsilon U ; J_{\circ} (v) \leq J_{\circ} (u)\}
$$
which is bounded by $(HJ)_{i} (1)$. So in the rest of our discussion we assume $U$ to be bounded.

Next, by hypothesis $(HJ)_i (1)$, the bounded convex set $U$ is also closed. In fact, if $v_{n} \epsilon U$ and $v_{n} \to v$ then since $K$ is closed, $v \epsilon K$. Moreover, by the mean value properly applied to $J_{i} (i = 1, \cdots , k)$ we have
$$
|J_{i} (v_{n}) - J_{i} (v)| \leq ||G_{i}|| ||v_{n}-v||
$$
so that $J_{i}(v_{n}) \to J_{i}(v)$ and hence $J_{i}(v) \leq 0$ for $i=1, \cdots , k$ i.e. $v \epsilon U$.

Let\pageoriginale $\mathscr{V}$ be a bounded closed convex subset of $V$ which satisfies the condition: there exist two numbers $r > 0$ and $d > 0$, ehich will be chosen suitably later on, such that
$$
B(0, r) \subset \mathscr{V} \subset B(0, d)
$$
where $B(0, t)$ denotes the ball $\{v \epsilon V | ||v|| < t \}$ in $V (t = r, d)$. Consider the set 
$$
U_{1} = \{v | v \epsilon V ; \exists w \epsilon U \text{ such that } ||v-w||\leq d\}.
$$

Since $U$ is bounded the set $U_{1}$ is also bounded and $U_{1} \supset U$. In the hypothesis $(HJ)_{\circ}$ and $(HJ)_{i}$ we shall use only the bounded set $U_{1}$.

We shall use the following notation : $J_{i}(u_{m}), H_{i}(u_{m})$ will be respectively denoted by $J_{i}^{m}, G_{i}^{m}, H_{i}^{m}$ for $i = 0, 1, \cdots, k $ and all $m \geq 0$.

Now suppose that starting from $u_{\circ} \epsilon U$ we have constructed $u_{m}$. We wish to give an algorithm to obtain $u_{m+1}$. For this purpose we consider a linear programming problem.

\medskip
\noindent{\textit{A linear programming problem }}: Let $U_{m}$ denote subset of $U \times \mathbb{R}$ defined as the set of all $(z, \sigma) \epsilon U \times \mathbb{R}$ satisfying
\begin{equation*}
\begin{cases}
& z-u_{m} \epsilon \mathscr{V},\\
& (G_{\circ}^{m}, z-u_{m}) + \sigma \leq 0, \text{ and }\\
& J_{i}^{m} + (G_{i}^{m}, z-u_{m}) + \sigma \leq 0 \quad\text{ for } i = 1, \cdots, k.
\end{cases}
\end{equation*}

It is easy to see that $U_{m}$ is a nonempty closed convex bounded set: In fact, $(z, \sigma) = (u_{m}, 0) \epsilon U_{m}$ so that $U_{m} \neq \phi$. If $(z, \sigma) \epsilon U_{m}$ then since $z-u_{m} \epsilon V$, which is a bounded set it follows that z is bounded. Then using the other two inequalities in (\ref{chap4-prob1.1}) it follows that $\sigma$ is also bounded. If $(z_{j}, \sigma_{j}) \epsilon U_{m}$ and $(z_{j}, \sigma_{j}) \to (z, \sigma)$ in $U \times \mathbb{R}$ then since $U$ is closed $z \epsilon U$ and hence $(z, \sigma) \epsilon U \times \mathbb{R}$. Again since $\mathscr{V}$ is closed $(z-u_{m}) \epsilon \mathscr{V}$. By the continuity of the (affine) functions
\begin{align*}
& (z, \sigma) \mapsto J_{i}^{m} + (G_{i}^{m}, z-u_{m}) + \sigma\\
& (z, \sigma) \mapsto (G_{\circ}^{m}, z-u_{m}) + \sigma
\end{align*}\pageoriginale
we find that
$$
J_{i}^{m} + (G_{i}^{m}, z-u_{m}) + \sigma \leq 0, (G_{\circ}^{m}, z-u_{m}) + \sigma \leq 0.
$$

Finally to prove the convexity, let $(z, \sigma), (z', \sigma') \epsilon U_{m}$. Then, for any, $0 \leq \theta \leq 1$,
$$
(1-\theta) z + \theta z' - u_{m} = (1-\theta)(z-u_{m}) + \theta (z'-u_{m}) \epsilon \mathscr{V}
$$
since $\mathscr{V}$ is convex. Moreover, we also have
\begin{align*}
& (G_{\circ}^{m}, (1-\theta) z + \theta z' -u_{m}) + (1-\theta) \sigma + \theta \sigma'\\
& = (1-\theta) [(G_{\circ}^{m}, z-u_{m}) + \sigma] + \theta [(G_{\circ}^{m}, z'-u_{m}) + \sigma'] \leq 0 
\end{align*}
and similarly
$$
J_{i}^{m} + (G_{i}^{m}, (-\theta) z + \theta z' - u_{m}) + (1-\theta) \sigma + \theta \sigma' \leq 0.
$$

Next we consider the functional $g : V \times \mathbb{R} \to \mathbb{R}$ given by $g(z, \sigma) = \sigma$ and the linear programming problem :

$(P_{m}):$ to find $(z_{m}, \sigma_{m}) \epsilon U_{m}$ such that $g(z_{m}, \sigma_{m}) \geq g(z, \sigma), \forall (z, \sigma) \epsilon U_{m}$. i.e.

\medskip
\noindent{\textbf{Problem $P_{m}$:}} To find $(z_{m}, \sigma_{m}) \epsilon U_{m}$ such that
\begin{equation*}
\sigma \leq \sigma_{m} \text{ for all } (z, \sigma) \epsilon U_{m}.\tag{1.2}\label{chap4-eq1.2}
\end{equation*}

By the results of Chapter \ref{chap2} we know that the Problem $P_{m}$ has a solution (not necessarily unique).

We are now in a position to formulate our algorithm for the construction of $u_{m+1}$.

\medskip
\noindent{\textbf{Algorithm.}}\pageoriginale
 Suppose we have determined $u_{m}$ starting from $u_{\circ}$. Then we take a solution $(z_{m}, \sigma_{m})$ of the linear programming Problem $(P_{m})$. We set
\begin{equation*}
w_{m} = (z_{m} - u_{m})/ ||z_{m} - u_{m}||\tag{1.3}\label{chap4-eq1.3}
\end{equation*}
and
\begin{equation*}
\rho_{m}^{\ell} = \max \{\rho \epsilon \mathbb{R}, u_{m} + \rho w_{m} \epsilon U\}.\tag{1.4}\label{chap4-eq1.4}
\end{equation*}

We shall prove later on that $w_{m}$ is a direction of descent. We can define the notions of convergent choices of $w_{m}$ and $\rho$ in the same way as in Chapter \ref{chap3}, Section \ref{chap3-sec1} for the functional $J_{\circ}$. We shall therefore not repeat these definitions here.

Let $\rho_{m}^{c}$ be a convergent choice of $\rho$ for the construction of the minimizing sequence for $J_{\circ}$ without constraints. We define
\begin{equation*}
\rho_{m} = \min (\rho_{m}^{c}, \rho_{m}^{\ell})\tag{1.5}\label{chap4-eq1.5}
\end{equation*}
and we set
\begin{equation*}
u_{m+1} = u_{m} + \rho_{m} w_{m}.\tag{1.6}\label{chap4-eq1.6}
\end{equation*}

The following is the main result of this section.

\begin{theorem}\label{chap4-thm1.1}
Suppose that convex set $K$ and the functionals $J_{\circ}, J_{1},\break \ldots, J_{k}$ satisfy the hypothesis $(HK)$ and $(HJ)_{i}, i = 0, 1, \cdots, k$. Suppose (1) the Problem (\ref{chap4-prob1.1}) has a unique solution and (2) $u_{m} \to u$ as $m \to + \infty$.
\end{theorem}

Then the algorithm described above to determine $u_{m+1}$ from $u_{m}$ is convergent.

i.e. If $u \epsilon U$ is the unique solution of the Problem (\ref{chap4-prob1.1}) and if $u_{m}$ is a sequence given by the above algorithm then $J(u_{m}) \to J(u)$ as $m \to + \infty$. 

For this it will be necessary to prove that $w_{m}$ is a direction of descent and $w_{m}, \rho_{m}$ are convergent choices.

The\pageoriginale following two lemmas are crucial for our proof of the Theorem \ref{chap4-thm1.1}. 

Let $u \epsilon U$ be the unique solution of the Problem \ref{chap4-prob1.1}

\begin{lemma}\label{chap4-lem1.1}
Let the hypothesis of Theorem \ref{chap4-thm1.1} be satisfied. If, for some $m \geq 0$ we have $J_{\circ}(u) < J_{\circ} (u_{m})$ then there exists an element $(y_{m}, \epsilon_{m}) \in U_{m}$ such that $\epsilon_{m} > 0$.
\end{lemma}

\begin{proof}
Let $u_{m} \in U$ be such that $J_{\circ}(u) < J_{\circ}(u_{m})$. We first consider the case where $Z \neq u, Z$ being the point of $K$ given in hypothesis $(HK)$. We introduce two real numbers $\ell_{m}, \ell'_{m}$ such that
$$
J_{\circ}(u) < \ell'_{m} < \ell_{m} \leq J_{\circ} (u_{m}) \text{ and } \ell'_{m} < J_{\circ}(Z).
$$
\end{proof}

Let $I \equiv I(u, Z)$ denote the segment in $V$ joining $u$ and $Z$, i.e.
$$
I = \{w | w \epsilon V ; w = (1-\theta)u + \theta Z, 0 \leq \theta \leq 1\}
$$

Since $u$, $Z$ belong to the convex set $U$ we have $I \subset U$.

On the other hand, if $c \epsilon \mathbb{R}$ is any constant then the set
$$
J_{\circ c} = \{v \epsilon U; J_{\circ} (v) \leq c\}
$$
is convex and closed. For, if $v, v' \epsilon J_{\circ c}$ then for any, $0 \leq \lambda \leq 1$,
$$
J_{\circ} ((1-\lambda)v + \lambda v') \leq (1 - \lambda)J_{\circ}(v) + \lambda J_{\circ} (v') \leq c
$$
by the convexity of $J_{\circ}$ and $(1-\lambda)v + \lambda v' \epsilon U$ since $U$ is convex. To see that it is closed, let $v_{j} \in J_{\circ c}$ be a sequence such that $v_{j} \to v$ in $V$. Since $U$ is closed $v \in U$. Moreover, by mean value property for $J_{\circ}$
$$
|J_{\circ}(v_{j}) - J_{\circ}(v)| \leq M_{U} ||v_{j}-v|| \leq M_{U_{1}} ||v_{j}-v||
$$
by Hypothesis $(HJ)_{\circ} (2)$ so that $J_{\circ} (v_{j}) \to J_{\circ}(v)$ as $j \to + \infty$. Hence $J_{\circ}(v) \leq c$ i.e. $v \in J_{\circ c}$.

Now\pageoriginale by the choice of $\ell]_{m}$, $u \epsilon I \cap J_{\circ \ell'_{m}}$ and hence $I_{\circ} \equiv I \cap J_{\circ \ell'_{m}} \neq \phi$. It is clearly closed and bounded. $I_{\circ}$ being a closed bounded subset of a compact set $I$ is itself compact.

Now the function $g : I_{\circ} \to \mathbb{R}$ defined by $g = J_{\circ} / I_{\circ}$ is continuous: In fact, if $w, w' \epsilon I_{\circ}$ then by the mean value property applies to $J_{\circ}$ gives
$$
|g(w) - g(w')| = |J_{\circ} (w) - J_{\circ}(w')| \leq M_{U_{1}} ||w-w'||
$$
by hypothesis $(HJ)_{\circ} (2)$. Moreover, by the very definition of the set $I_{\circ} \subset J_{\circ, \ell'_{m}}$ we have
$$
|g(w)| \leq \ell'_{m}.
$$

Hence g attains its maximum in $I_{\circ}$ i.e. There exists a point $y_{m} \epsilon I_{\circ}$ such that $g(y_{m}) = J_{\circ}(y_{m}) = \ell'_{m}$. i.e. there exists a $\theta_{m}, 0 \leq \theta < 1$ such that
$$
y_{m} = (1-\theta_{m})u + \theta_{m} Z, J_{\circ} (y_{m}) = \ell'_{m}.
$$

Since $J_{\circ}(u) < \ell'_{m}$ we see that $y_{m} \neq u$ and therefore $\theta_{m} \neq 0$. i.e. $0 < \theta_{m} < 1$.

Next we show that $J_{i}(y_{m}) < 0$ for all $i = 1, \cdots, k$. In fact, since $J_{i}$ is convex and has a gradient $G_{i}$ we know from Proposition \ref{chap1-prop3.1} of Chapter \ref{chap1} that
$$
J_{i} (y_{m}) \geq J_{i}^{m} + (G_{i}^{m}, y_{m} - u_{m})
$$
and we also have
$$
J_{i}(y_{m}) \leq (1-\theta_{m}) J_{i}(u) + \theta_{m} J_{i}(Z) < 0
$$
since $0 < \theta_{m} < 1$ and $J_{i}(Z) < 0$.

Similarly, by convexity of $J_{\circ}$ we get
\begin{align*}
& \ell'_{m} = J_{\circ}(y_{m}) \geq J_{\circ}^{m} + (G_{\circ}^{m}, y_{m} - u_{m}) \geq \ell_{m} + (G_{\circ}^{m}, y_{m} - u_{m})\\
\text{ i.e. } & (G_{\circ}^{m}, y_{m} - u_{m}) \leq \ell'_{m} - \ell_{m} < 0 \text{ by the choice of } \ell_{m}, \ell'_{m}
\end{align*}

We\pageoriginale can now take
$$
\in_{m} = \min \{\ell_{m} - \ell'_{m}, -J_{1} (y_{m}), \cdots, -J_{k}(y_{m})\} > 0.
$$

Then it follows immediately that $(y_{m},\epsilon_{m}) \in U_{m}$ and $\epsilon_{m} > 0$.

We now consider the case $u = Z$. Then we can take $y_{m} = Z = u$ and hence $J_{i}(y_{m}) = J_{i}(u) = J_{i} (Z) < 0$. It is enough to take
$$
\in_{m} = \min \{J_{\circ} (u_{m}) - J_{\circ}(u), -J_{t}(Z), \cdots, -J_{k}(Z)\} > 0.
$$

If we now take $r > 0$ sufficiently large then $y_{m} - u_{m} \in \mathscr{V}$. This is possible since both $y_{m}$ and $u_{m}$ are in bounded sets:
$$
||y_{m}|| \leq (1-\theta_{m}) ||u|| + \theta_{m} ||Z|| \leq ||u|| + ||Z||
$$
so that
$$
||y_{m}-u_{m}|| \leq ||y_{m}|| + ||u_{m}|| \leq ||u|| + ||Z|| + ||u_{m}||.
$$
It is enough to take $r > ||u|| + ||Z|| + ||u_{m}|| > 0$. Thus $(y_{m}, c_{m}) \in \mathscr{V}$.

\begin{corollary}\label{chap4-coro1.1}
Under the assumptions of Lemma \ref{chap4-lem1.1} there exists a strongly admissible direction of descent at $U_{m}$ for the domain $U$.
\end{corollary}

\begin{proof}
By Lemma \ref{chap4-lem1.1} there exists an element $(y_{m}, \epsilon_{m}) \in U_{m}$ such that $\epsilon_{m} > 0$. On the other hand, let $(z_{m}, \sigma_{m})$ be a solution in $U_{m}$ of the Linear programming problem $(P_{m})$. Then necessarily $\sigma_{m} \geq \epsilon_{m} > 0$ and we can write
\begin{equation*}
\begin{cases}
& \qquad z_{m} - u_{m} \epsilon \mathscr{V}, z_{m} \epsilon U\\
& J_{i}^{m} + (G_{i}^{m}, z_{m} - u_{m}) + \epsilon_{m} \leq J_{i}^{m} + (G_{i}^{m}, z_{m} - u_{m}) + \sigma_{m} \leq 0\\
& \qquad (G_{\circ}^{m}, z_{m}-u_{m}) + \epsilon_{m} \leq (G_{\circ}^{m}, z_{m}-u_{m}) + \sigma_{m} \leq 0\tag{1.7}\label{chap4-eq1.7}
\end{cases}
\end{equation*}
Thus we have
\begin{equation*}
(G_{\circ}^{m}, z_{m}-u_{m}) \leq - \epsilon_{m} < 0,\tag{1.8}\label{chap4-eq1.8}
\end{equation*}
and\pageoriginale hence
\begin{equation*}
w_{m} = (z_{m}-u_{m}) / ||z_{m}-u_{m}||\tag{1.9}\label{chap4-eq1.9}
\end{equation*}
is a direction of descent. It is  strongly admissible since $U$ is convex and we can take any sequence of numbers $\epsilon_{j} > 0$, $\epsilon_{j} \to 0$.
\end{proof}

\begin{lemma}\label{chap4-lem1.2}
Let the hypothesis of Theorem \ref{chap4-thm1.1} hold and, for some $m \geq 0$, $J_{\circ}(u) < J_{\circ}(u_{m})$. If $(z_{m}, \sigma_{m}) \epsilon U_{m}$ is a solution of the linear programming problem $(P_{m})$ then there exists a number $\mu_{m} > 0$ depending only on $\epsilon_{m}$ of Lemma \ref{chap4-lem1.1} such that
\begin{equation*}
u_{m} + \rho(z_{m}-u_{m}) \epsilon U \text{ for all } 0 \leq \rho \leq \mu_{m}.\tag{1.10}\label{chap4-eq1.10}
\end{equation*}
Furthermore,
$$
(G_{\circ}^{m}, z_{m}-u_{m}) < 0. 
$$
\end{lemma}

\begin{proof}
We have alredy shown the last assertion in the Corollary \ref{chap4-coro1.1} and therefore we have to prove the existence of $\mu_{m}$ such that (\ref{chap4-eq1.10}) holds. For this purpose, if $\rho > 0$, we get on applying Taylor's formula to each $J_{i} (i = 1, \cdots, k)$:
\begin{equation*}
J_{i} (u_{m} + \rho(z_{m}-u_{m})) = J_{i}^{m} + \rho(G_{i}^{m}, z_{m}-u_{m}) + \frac{1}{2} \rho^{2} (\overline{H}_{i}^{m}(z_{m}-u_{m}), z_{m}-u_{m})\tag{1.11}\label{chap4-eq1.11}
\end{equation*}
where
$$
\overline{H}_{i}^{m} = H_{i}^{m} (u_{m} + \rho'(z_{m}-u_{m})) \text{ for some } 0 < \rho' < \rho.
$$
\end{proof}

Here, since $z_{m}-u_{m} \epsilon \mathscr{V}, ||z_{m}-u_{m}|| < d$ and hence $u_{m} + \rho'(z_{m}-u_{m}), (0 < \rho' < \rho)$ belongs to $U_{1}$ if we assume $\rho \leq 1$. $||\overline{H}_{i}^{m}||$ is bounded by $M_{U_{1}}$ and so we get
\begin{equation*}
J_{i} (u_{m} + \rho(z_{m}-u_{m})) \leq J_{i}^{m} + \rho(G_{i}^{m}, z_{m}-u_{m}) + \frac{1}{2} M \rho^{2} d^{2}.\tag{1.12}\label{chap4-eq1.12}
\end{equation*}

Thus if we find a $\mu_{m} > 0$ such that $0 < \rho < \mu_{m}$ implies the right hand side of this last\pageoriginale inequality is $\leq 0$ forall $i=1, \cdots, k$ then $u_{m} + \rho(z_{m}-u_{m}) \epsilon U$.

Using the first inequality (\ref{chap4-eq1.7}) to replace the term $(G_{i}^{m}, z_{m}-u_{m})$ in (\ref{chap4-eq1.12}) we get
\begin{equation*}
J_{i}(u_{m} + \rho(z_{m}-u_{m})) \leq J_{i}^{m} + \rho(-J_{i}^{m} - \epsilon_{m}) + \frac{1}{2} \rho^{2} Md^{2}.\tag{1.13}\label{chap4-eq1.13}
\end{equation*}

The second degree polynomial on the right side of (\ref{chap4-eq1.13}) vanishes for
\begin{equation*}
\rho = \rho_{i}^{m} = [(J_{i}^{m} + \epsilon_{m}) + \{(J_{i}^{m} + \epsilon_{m})^{2} - 2Md^{2} J_{i}^{m}\}^{\frac{1}{2}}]/ Md^{2}.\tag{1.14}\label{chap4-eq1.14}
\end{equation*}

Moreover the right side of (\ref{chap4-eq1.13}) is smaller than
$$
J_{i}^{m} + \rho(-J_{i}^{m}) + \frac{1}{2} \rho^{2} Md^{2}
$$
since $\epsilon_{m} > 0$, $\rho > 0$ and this last expression decreases as $\rho > 0$ decreases as $-J_{i}^{m} = -J_{i}(u_{m}) \leq 0$. Then it follows that, if $0 < \rho \leq \rho_{i}^{m}$, we have
$$
J_{i} (u_{m} + \rho(z_{m}-u_{m})) \leq 0.
$$

We can now take $\mu_{m} = \min (\rho_{1}^{m}, \cdots, \rho_{k}^{m})$ also that we will have
$$
J_{i}(u_{m} + \rho(z_{m}-u_{m})) \leq 0 \text{ for all } 0 < \rho \leq \mu_{m} \text{ and } i = 1, \cdots, k
$$

But each of the $\rho_{i}^{m}$ gives by (\ref{chap4-eq1.14}) depend on $J_{i}^{m}$ and hence on $u_{m}$. In order to get a $\mu > 0$ independent of $u_{m}$ and dependent only on $\epsilon_{m}$ we can proceed as follows. If we set
\begin{equation*}
\varphi(y) = [(y + \epsilon_{m}) + \{(y+\epsilon_{m})^{2} - 2Md^{2} y\}^{\frac{1}{2}}]/ Md^{2}\tag{1.15}\label{chap4-eq1.15}
\end{equation*}
for $y \leq 0$ then, since $y = J_{i}(u_{m}) = J_{i}^{m} \leq 0$, we can write
$$
\rho_{i}^{m} = \varphi (J_{i}^{m}).
$$

It is easily checked that the function $\varphi : ]-\infty, 0] \to \mathbb{R}$ is continuous, $\varphi(y) > 0$ for all $y \leq 0$ and $\lim\limits_{y \to -\infty} \varphi(y) = 1$. Hence $\inf\limits_{y \leq 0} \varphi(y) = \eta(\epsilon_{m})$ exists and $\eta(\epsilon_{m}) > 0$.\pageoriginale

We choose $\mu_{m} = \eta(\epsilon_{m})$. Then, if $0 < \rho \leq \mu_{m} \leq \rho_{i}^{m}$ for each $i=1, \cdots, k$ given by (\ref{chap4-eq1.14}) and consequently, for any such $\rho > 0$, $u_{m} + \rho(x_{m}-u_{m}) \epsilon U$.

We are niw in a position to prove Theorem \ref{chap4-thm1.1}

\medskip
\noindent{\textit{Proof of Theorem 1.1.}} We recall that $(z_{m}, \sigma_{m}) \epsilon U_{m}$ is a solution of the linear programming problem $(P_{m})$ and 
\begin{align*}
w_{m} & = (z_{m}-u_{m})/ ||z_{m}-u_{m}||,\\
\rho_{m} & = \min (\rho_{m}^{\ell}, \rho_{m}^{c}),\\
u_{m+1} & = u_{m} + \rho_{m} w_{m}.
\end{align*}

Then $J_{\circ}(u_{m})$ is a decreasing sequence. In fact, if $\rho_{m} = \rho_{m}^{c}$ then by definition of $\rho_{m}^{c}$ we have $J_{\circ} (u_{m+1}) \leq J_{\circ}(u_{m})$. Suppose $\rho_{m} = \rho_{m}^{\ell} < \rho_{m}^{c}$. If $J_{\circ} (u_{m} + \rho_{m}^{c} w_{m}) \leq J_{\circ}(u_{m} + \rho_{m}^{c} w_{m})$ there is nothing to prove. So we assume $J_{\circ}^{\rho_{m}} > J_{\circ}^{\rho_{m}}$. Consider the convex function $\rho \mapsto J(u_{m} + \rho w_{m})$ in $[0, \rho_{m}^{c}]$. It attains its minimum at $\rho = \rho_{\min} \epsilon ]0, \rho_{m}^{c}[$. Then $0 \leq \rho_{m} \leq \rho_{\min}$. In fact, if $\rho_{\min} < \rho_{m} < \rho_{m}^{c}$ then since $J_{\circ}$, being convex, is increasing in $[\rho_{\min}, \rho_{m}^{c}]$ we have $J_{\circ}^{\rho_{m}^{c}} \leq J_{m}^{\rho^{c}}$ contradicting our assumption. Once again since $J_{\circ}$ is convex $J_{\circ}$ is decreasing in $[0, \rho_{\min}]$. Hence $J_{\circ}^{m} = J_{\circ}(u_{m}) \geq J_{\circ}^{\rho_{m}} = J_{\circ}(u_{m+1})$. Since we know that there exists a (unique) solution $u$ of the minimizing problem \ref{chap4-prob1.1} we have $J_{\circ}(u_{m}) \geq J_{\circ}(u), \forall m \geq 0$. Thus $J_{\circ}(u_{m})$, being a decreasing sequence bounded below, is convergent. Let $\ell = \lim\limits_{m \to + \infty} J_{\circ}(u_{m})$. Clearly $\ell \geq J_{\circ}(u)$. Then there are two possible cases:
\begin{enumerate}
\item[(1)] $\ell = J_{\circ} (u)$ and 
\item[(2)] $\ell > J_{\circ} (u)$.
\end{enumerate}

{\em Case (1)}. Suppose $J_{\circ} (u_{m}) \to \ell = J_{\circ}(u)$. Then, for any $m \geq 0$, we have by Taylor's formula :
$$
J_{\circ} (u_{m}) = J_{\circ}(u) + (G_{\circ}(u), u_{m} - u) + \frac{1}{2} (\overline{H}_{m}(u_{m}-u), u_{m}-u).
$$\pageoriginale
where
$$
\overline{H}_{m} = H_{\circ} (u+\theta(u_{m}-u)) \text{ for some } 0 < \theta < 1
$$

Since $u, u_{m} \in U$ (which is convex), $u + \theta(u_{m}-u) \in U$ ofr any $0 < \theta < 1$ and hence by hypothesis $(HJ)_{\circ} (3)$
$$
(\overline{H}_{m} (u_{m}-u), u_{m} -u) \geq \alpha ||u_{m}-u||^{2}, \alpha = \alpha_{U_{1}} > 0.
$$

Moreover, since $J_{\circ}$ is convex, we have by Theorem \ref{chap2-thm2.2} of Chapter \ref{chap2} 
$$
(G_{\circ}(u), u_{m}-u) \geq 0.
$$

Thus we find that
$$
J_{\circ}(u_{m}) \geq J_{\circ} + \frac{1}{2} \alpha ||u_{m}-u||^{2}
$$
\begin{equation*}
\text{i.e. }\qquad ||u_{m}-u||^{2} \leq 2 / \alpha (J_{\circ}(u_{m}) - J_{\circ}(u)).
\end{equation*}

Since $J_{\circ} (u_{m}) \to J_{\circ}(u)$ as $m \to + \infty$ it then follows that $u_{m} \to u$ as $m \to + \infty$.

{\em Case(2).} We shall prove that this case cannot occur. Suppose, if possible, let $J_{\circ}(u) < \ell \leq J_{\circ}(u_{m}), \forall m \geq 0$. We shall show that the choices of $w_{m}$ and $\rho_{m}$ are convergent for the problem of minimization of $J_{\circ}$ without constraints. i.e. the sequence $u_{m}$ constructed using our algorithm tends to an absolute minimum of $J_{\circ}$ in $V$ which will be a contradiction to our assumption.

$w_{m}$ {\em is a convergent choice}. For this we introduce, as in the proof of Lemma \ref{chap4-lem1.1} another real number $\ell'$ such that
$$
J_{\circ}(u) < \ell' < \ell \leq J_{\circ}(u_{m}), \forall m \geq 0.
$$

Then the proof of Lemma \ref{chap4-lem1.1} gives the existence of $(y, \epsilon) \in U_{m}$ with $\epsilon_{m} = \in > 0$ $\forall m \geq 0$.\pageoriginale On the other hand, $(z_{m}, \sigma_{m}) \in U_{m}$ being a solution of the linear programming problem $(P_{m})$ we have $\sigma_{m} \geq \in > 0$. Hence from (\ref{chap4-eq1.7}) we get
\begin{equation*}
\begin{cases}
& \qquad (G_{\circ}^{m}, z_{m}-u_{m}) + \epsilon \leq 0,\\
& J_{i}^{m} + (G_{i}^{m}, z_{m}-u_{m}) + \epsilon \leq 0.\tag{1.16}\label{chap4-eq1.16}
\end{cases}
\end{equation*}

From the first inequality here together with the Cauchy-Schwarz inequality gives
$$
-||G_{\circ}^{m}|| ||z_{m}-u_{m}|| \leq (G_{\circ}^{m}, z_{m}-u_{m}) \leq - \epsilon
$$
\begin{equation*}
\text{ i.e. }\qquad \epsilon \leq ||G_{\circ}^{m}|| ||z_{m}-u_{m}|| \leq M ||z_{m}-u_{m}||, M = M_{U_{1}},
\end{equation*}
using hypothesis $(HJ)_{\circ} (2)$. So we have
\begin{equation*}
||z_{m} - u_{m}|| \geq \epsilon / M > 0.\tag{1.17}\label{chap4-eq1.17}
\end{equation*}

By Lemma \ref{chap4-lem1.2} there exists a $\mu = \eta(\epsilon) > 0$ such that
\begin{equation*}
u_{m} + \rho(z_{m}-u_{m}) \in U \text{ if } 0 \leq \rho < \eta(\epsilon).\tag{1.10}
\end{equation*}

If we denote by $\overline{\rho}, \overline{\rho} = \rho||(z_{m}-u_{m})||$  then this is equivalent to saying that
$$
u_{m} + \overline{\rho}w_{m} \in U \text{ if } 0 \leq \overline{\rho} < \eta(\epsilon) ||z_{m}-u_{m}||.
$$

Then, in view of (\ref{chap4-eq1.17}), $0 \leq \overline{\rho} < \epsilon \eta (c)/ M$ implies $0 \leq \overline{\rho} < \eta(c)|| z_{m}-u_{m}||$ and hence
$$
u_{m} + \overline{\rho}w_{m} \in U \text{ for all } 0 \leq \overline{\rho} \epsilon \eta(\epsilon)/ M,
$$
which means that
$$
\rho_{m}^{\ell} \geq \epsilon \eta(c)/ M.
$$

Once again from (\ref{chap4-eq1.16}) we have
$$
(G_{\circ}^{m}, w_{m}) \leq - \epsilon / ||z_{m}-u_{m}|| \leq - \epsilon/d
$$
because $z_{m}-u_{m} \in \mathscr{V}$ by (\ref{chap4-prob1.1}) meancs that $||z_{m}-u_{m}|| \leq d$. Since $||G_{\circ}^{m}|| \leq M$ we obtain
$$
(G_{\circ}^{m} / ||G_{\circ}^{m}||, w_{m}) \leq -\epsilon / d ||G_{\circ}^{m}|| (\leq - \epsilon / Md).
$$\pageoriginale

Taking $\epsilon > 0$ small enough we conclude that

$(G_{\circ}^{m} / ||G_{\circ}^{m}||, w_{m}) \leq -C_{1} < 0, 1 \geq C_{1} > 0$ being a constant. This is nothig but saying that the choice of $w_{m}$ is convergent for the minimization problem without constraints by $w$-Algorithm 1 of Section \ref{chap3-subsec1.2} of Chapter \ref{chap3}.

$\rho_{m}$ {\em is a convergent choice}. Since $\rho_{m} = \min (\rho_{m}^{\ell}, \rho_{m}^{c})$ we consider two possible cases
\begin{enumerate}
\item[(a)] If $\rho_{m} = \rho_{m}^{c}$ then there is nothing to prove.

\item[(b)] Suppose $\rho_{m} = \rho_{m}^{\ell}$. We shall that this choice of $\rho_{m}$ is also a convergent choice. For this let $c_{2}$ be a constant such that $0 < c_{2} \leq \rho_{m} = \rho_{m}^{\ell} \leq \rho_{m}^{c}$.
\end{enumerate}

Then $0 < \rho_{m} / \rho_{m}^{c} \leq 1$ and we can write
$$
u_{m+1} = u_{m} + \rho_{m} w_{m} = (1-\rho_{m}/\rho_{m}^{c}) u_{m} + \rho_{m} / \rho_{m}^{c} (u_{m} + \rho_{m}^{c} w_{m}).
$$

The convexity of $J_{\circ}$ then implies that
$$
J_{\circ} (u_{m+1}) \leq (1-\rho_{m} / \rho_{m}^{c}) J(u_{m}) + \rho_{m} / \rho_{m}^{c} J_{\circ} (u_{m} + \rho_{m}^{c} w_{m}).
$$

Hence we obtain
\begin{align*}
\triangle J_{\circ}^{\rho_{m}} & = J_{\circ} (u_{m}) - J_{\circ}(u_{m} + \rho_{m} w_{m}) = J_{\circ}(u_{m}) - J_{\circ} (u_{m+1})\\
& \geq \rho_{m}/\rho_{m}^{c} (J_{\circ} (u_{m}) - J_{\circ}(u_{m} + \rho_{m}^c w_{m}))
\end{align*}
i.e. 
\begin{equation*}
\triangle J_{\circ}^{\rho_{m}} \geq \rho_{m} / \rho_{m}^{c} \triangle J_{\circ}^{\rho_{m}^{c}}\tag{1.18}\label{chap4-eq1.18}
\end{equation*}

We note that $\rho_{m}^{c}$ is necessarily bounded above for any $m \geq 0$. For otherwise since, we find from triangle ineuality that
$$
||u_{m} + \rho_{m}^{c} w_{m}|| \geq \rho_{m}^{c} ||w_{m}|| - ||u_{m}|| = \rho_{m}^{c} - ||u_{m}||.
$$
$u_{m} + \rho_{m}^{c} w_{m}$\pageoriginale would be unbounded. Then by Hypothesis $(HJ_{\circ}) (1) J_{\circ} (u_{m} + \rho_{m}^{c} w_{m})$ would also be unbounded. This is not possible by the definition of convergent choice of $\rho_{m}^{c}$.

Let $C_{3}$ be a constant such that $0 < \rho_{m}^{c} \leq C_{3}$ for all $m \geq 0$. Then (\ref{chap4-eq1.18}) will give
\begin{equation*}
\triangle J_{\circ}^{\rho_{m}} \geq C_{2} / C_{3} \triangle J_{\circ}^{\rho_{m}^{c}}\tag{1.19}\label{chap4-eq1.19} 
\end{equation*}

Hence if $\triangle J_{\circ}^{\rho_{m}} \to 0$ then $\triangle
J_{\circ}^{\rho_{m}^{c}} \to 0$ by (\ref{chap4-eq1.19}). By the
definition of $\rho_{m}^{c}$ (as a convergent choice of $\rho$) we
have 
$$
(G_{m}, w_{m}) \to 0 \text{ as } m \to + \infty
$$
which means that $\rho_{m}$ is also a convergent choice of $\rho$.

Finally, since the choices of $\rho_{m}, w_{m}$ are both convergent for the minimization problem without constraints for $J_{\circ}$ we conclude using the results of Chapter \ref{chap3} that $u_{m} \to \widetilde{u}$ where $\widetilde{u}$ is the global minimum for $J_{\circ}$ (which exists and is unique by results of Chapter \ref{chap2}, Theorem \ref{chap2-thm2.1} of Section \ref{chap2-sec2} ). Thus we have
$$
J_{\circ} (\widetilde{u}) \leq J_{\circ} (u) < \ell \leq J_{\circ} (u_{m})
$$
$$
\text{ and } J_{\circ} (u_{m}) \to J_{\circ} (\widetilde{u})
$$
which is impossible and hence the case (2) cannot therefore occur.

This proves the theorem completely.

We shall conclude this section with some remarks.

\begin{remark}\label{chap4-rem1.1}
A special case of our algorithm was given a long time ago by Franck and Wolfe \cite{key17} in the absence of the constraints $J_{i}$ which we have linearized. More precisely they considered the following problem:

Let $J_{\circ}$ be a convex quadratic functional on a Hilbert space $V$ and $K$ be a\pageoriginale closed convex subset with non-empty interior. Then the problem is to give an algorithm for finding a minimizing sequence $u_{m}$ for
$$
u \epsilon K, J_{\circ} (u) = \inf\limits_{v \epsilon K} J_{\circ}(v).
$$
\end{remark}

The corresponding linear programming problem in this case will be the following:
\begin{equation*}
\begin{cases}
& U_{m} = K_{m} = \{(z, \sigma) \epsilon K \times \mathbb{R} (G_{\circ}^{m}, z-u_{m}) + \sigma \leq 0\},\\
& \text{ To find } (z_{m}, \sigma_{m}) \epsilon K_{m} \text{ such that } \sigma_{m} = \max_{(z, \sigma) \epsilon K_{m}} \sigma.
\end{cases}
\end{equation*}

Since $K$ itself can be assumed bounded using hypothesis $(HJ)_{\circ} (1)$ there is no need to introduce the bounded set $V$. When $z = z_{m}$ we have
$$
(G_{\circ}^{m}, z_{m} -u_{m}) + \sigma \leq (G_{\circ}^{m}, z_{m}-u_{m}) + \sigma_{m} \leq 0 \quad \forall \sigma \epsilon \mathbb{R}
$$
$$
\text{ i.e. } \min (G_{\circ}^{m}, z_{m}-u_{m}) + \sigma < 0.
$$

The algorithm given by Franck and Wolfe was the first convex programming algorithm in the literature.

\begin{remark}\label{chap4-rem1.2}
Our algorithm is a special case of a more general method known as Feasible direction method found by Zoutendjik \cite{key52}.
\end{remark}

\begin{remark}\label{chap4-rem1.3}
We can repeat our method to give a slightly different algorithm in the choice of $z_{m}$ as follows. We modify the set $U_{m}$ used in the linear programming problem $(P_{m})$ by introducing certain parameters $\gamma_{\circ}, \gamma_{1}, \cdots, \gamma_{k}$ with $\sigma$. More precisely, we replace (\ref{chap4-prob1.1}) by
\begin{equation*}
\begin{cases}
& z-u_{m} \epsilon \mathscr{V}\\
& (G_{\circ}^{m}, z-u_{m}) + \gamma_{\circ} \sigma \leq 0, \text{ and }\\
& J_{i}^{m} + (G_{i}^{m}, z-u_{m}) + \gamma_{i} \sigma \leq 0 \text{ for } i=1, \cdots , k,\tag*{$(1.1)'$}\label{chap4-eq1.1'}
\end{cases}
\end{equation*}
where $\gamma_{\circ}, \gamma_{1}, \cdots, \gamma_{k}$ are certain suitably chosen parameters. This modified algorithm\pageoriginale is useful when the curvature of the set $U$ is small.
\end{remark}

\begin{remark}\label{chap4-rem1.4}
  Suppose, in pur problem \ref{chap4-prob1.1}, some contraint $J_{i}$ is such that $J_{i}(u_{m}) = J_{i}^{m}$ is ``sufficiently negative'' at some stage of the iteration (i.e. for some $m \geq 0$). Since $J_{i}$ is regular then $J_{i}(v) \leq 0$ in a sufficiently small'' ball with centre at $u_{m}$. This can be seen explicitely using Taylor's formula. Thus we can ignore the constraint $J_{i}$ in the formulation of our problem i.e. in the definition of the set $U$.
\end{remark}

\begin{remark}\label{chap4-rem1.5}
The algorithm described in this section is not often used for minimizing problems arising from partial differential equation because the linear programming problem to be solved at each stage will be very large in this case. Hence our method will be expensive for numerical calculations for problems in partial diffeerential equation.
\end{remark}

\section{Centre Method}\label{chap4-sec2}
In this section we shall briefly sketch another algorithm to construct minimizing sequences for the minimizing problem for convex functionals on a finite dimensional space under constraints defined by a finite number of concave functionals. However we shall not prove the convergence of this algorithm. The main idea here is that at each step of the iteration we reduce the problem with constraints to one of a non-linear programming without contraints. An advantage with this method is that we do not use any regularity properties (i.e. existence of derivatives) of the functionals involved.

Let $V = \mathbb{R}^{r}$ and let
$$
J_{i} : \mathbb{R}^{r} \to \mathbb{R}, i = 1, \cdots, k,
$$
be continuous concave functionals (i.e. $-J_{i}$ are convex functionals). We define a set $U$ by
$$
U = \{v | v \epsilon \mathbb{R}^{r}, J_{i}(v) \geq 0 \text{ for all } i = 1, \cdots, k\}.
$$\pageoriginale

Since $-J_{i}$ are convex as in the previous section we see immediatly that $U$ is a convex set.

Suppose given a functional $J_{\circ} : \mathbb{R}^{r} \to \mathbb{R}$ satisfying:
\begin{enumerate}
\item[(1)] $J_{\circ}$ is continuous,\\
\item[(2)] $J_{\circ}$ is strictly convex and\\
\item[(3)] $J_{\circ}(v) \to + \infty$ as $||v|| \to + \infty$.
\end{enumerate}

We consider the following 
\begin{problem}\label{chap4-prob2.1}
To find $u \epsilon U$ such that
$$
J_{\circ}(u) \leq J_{\circ}(v) \text{ for all } v \epsilon U.
$$
\end{problem}

As usual, in view of the hypothesis (3) on $J_{\circ}$, we may without loss of generality assume that $U$ is bounded. We can describe the algorithm as follows.

Let $u_{\circ} \epsilon U$ be an initial point, arbitrarily fixed in $U$.

We shall find in our algorithm a sequence of triplets $(u_{m}, u'_{m}, \ell_{m})$ where for each $m \geq 0, u_{m}, u'_{m} \epsilon U$ and $\ell_{m}$ is a sequence of real numbers such that $\ell_{m} \geq \ell_{m+1} \; \forall_{m}$ and $\ell_{m} \geq J_{\circ} (u'_{m})$.

We take at the beginning of the algorithm the triple $(u_{\circ}, u'_{\circ}, \ell_{\circ})$ where $u'_{\circ} = u_{\circ}, \ell_{\circ} = J_{\circ} (u_{\circ})$

Suppose we have determined $(u_{m}, u'_{m}, \ell_{m})$. To determine the next triplet $(u_{m+1}, u'_{m+1}, \ell_{m+1})$ we proceed in the following manner.

Consider the subset $U_{m}$ of $U$ given by
\begin{equation*}
U_{m} = \{v | V \epsilon U, J_{\circ} (v) \leq \ell_{m}\}.\tag{2.1}\label{chap4-eq2.1}
\end{equation*}

Since $J_{\circ}$ is convex and continuous it follows immediately that $U_{m}$ is a bounded convex closed set in $\mathbb{R}^{r}$. Hence $U_{m}$ is a compact convex set in $\mathbb{R}^{r}$.

We\pageoriginale define a function $\varphi_{m} : \mathbb{R}^{r} \to \mathbb{R}$ by setting.
\begin{equation*}
\varphi_{m} (v) = (\ell_{m} - J_{\circ} (v)) \prod_{i=1}^{k} J_{i} (v).\tag{2.2}\label{chap4-eq2.2}
\end{equation*}

The continuity of the functionals $J_{\circ}, J_{1}, \cdots, J_{k}$ immediatly imply that $\varphi_{m}$ is also a continuous function. Moreover, $\varphi_{m}$ has the properties of distance from the boundary of $U_{m}$. i.e. 
\begin{enumerate}
\item[(i)] $\varphi_{m} (v) \geq 0$ for $v \epsilon U_{m}$.

\item[(ii)] $\varphi_{m}(v) = 0$ if $v$ belongs to the boundary of $U_{m}$. i.e. For any $v$ on any one of the $(k+1)$ -level surfaces defined by the equations
\end{enumerate}

$$
 J_{\circ} (v) = \ell_{m}, J_{1}(v) = 0, \cdots, J_{k}(v) = 0
$$
 we have
$$
  \varphi_{m}(v) = 0.
$$

Now since $U_{m}$ is a compact convex set in $\mathbb{R}^{r}$ and $\varphi_{m}$ is continuous it attains a maximum in $U_{m}$. $J_{\circ}$ being strictly convex this maximum is unique as can easily be checked.

We take $u_{m+1}$ as the solution of the maximizing problem:

\setcounter{uproblem}{1}
\begin{uproblem}\label{chap4-uprob2.2}
 $u_{m+1} \epsilon U_{m}$ such that $\varphi_{m} (u_{m+1}) \geq \varphi_{m} (v), \forall v \epsilon U_{m}$.

Now suppose $u'_{m} \epsilon U_{m}$ so that $J_{\circ} (u'_{m}) \leq \ell_{m}$. This is true by assumption at the beginning of the algorithm (i.e. when $m = 0$). Hence $\varphi_{m} (u'_{m}) \geq 0$. We take a point $u'_{m+1}$ such that
\begin{equation*}
u'_{m+1} \epsilon U_{m} \text{ and } J_{\circ} (u'_{m+1}) \leq J_{\circ} (u_{m+1}).\tag{2.3}\label{chap4-eq2.3}
\end{equation*}
\end{uproblem}

It is clear that such a point exists since we can take $u'_{m+1} = u_{m+1}$. However we shall choose $u_{m+1}$ as follows: Consider the line $\Lambda (u'_{m}, u_{m+1})$ joining $u'_{m}$ and $u_{m+1}$. We take for $u'_{m+1}$ the point in $U_{m}$ such that
\begin{equation*}
\begin{cases}
& u'_{m+1} \epsilon \lambda (u'_{m}. u_{m+1}) \cap \partial U_{m},\\
& \text{and } J_{\circ} (u'_{m+1}) \leq J_{\circ} (u_{m+1}).\tag{2.4}\label{chap4-eq2.4}
\end{cases}
\end{equation*}\pageoriginale

Now we have onlyu to choose $\ell_{m+1}$. For this, let $r_{m}$ be a sequence of real numbers such that
\begin{equation*}
0 < \alpha \leq r_{m} \leq 1, \text{ where } \alpha > 0 \text{ is a fixed constant.}\tag{2.5}\label{chap4-eq2.5}
\end{equation*}

We fix such a sequence arbitrarily in the beginning of the algorithm. We define $\ell_{m+1}$ by
\begin{equation*}
\ell_{m+1} = \ell_{m} - r_{m} (\ell_{m} - J_{\circ} (u'_{m+1})).\tag{2.6}\label{chap4-eq2.6}
\end{equation*}

It is clear that $\ell_{m+1} \leq \ell_{m}$ and that $\ell_{m+1} \geq J_{\circ} (u'_{m+1})$. Thus we can state our algotrithm as follows:

\medskip
\noindent{\textbf{Algorithm.}} Let $u_{\circ} \epsilon U$ be an arbitrarily fixed initial point. We determine a sequence of triplets $(u_{m}, u'_{m}, \ell_{m})$ starting from $(u_{\circ}, u_{\circ}, J_{\circ}(u_{\circ}))$ as follows: Let $(u_{m}, u'_{m}, \ell_{m})$ be given. Than $(u_{m+1}, u'_{m+1}, \ell_{m+1})$ is given by
\begin{enumerate}
\item[(a)] $u_{m+1} \epsilon U_{m}$ is the unique solution of the Problem \ref{chap4-uprob2.2}$_{m}$.

\item[(b)] $u'_{m+1} \epsilon U_{m}$ is given by (\ref{chap4-eq2.4}).

\item[(c)] $\ell_{m+1}$ is determined by (\ref{chap4-eq2.6}).
\end{enumerate}

Once again we can prove the convergence of this algorithm.

\begin{remark}\label{chap4-rem2.1}
The maximization problem \ref{chap4-uprob2.2}$_{m}$ at each step of the iteration is a non-linear programming problem without constraints. For the soultion of such a problem we can use any of the algorithms described in Chapter \ref{chap3}.
\end{remark}

\begin{remark}\label{chap4-rem2.2}
Since the function $\varphi_{m}$ which is maximized at each step has the properties of a distance function from the boundary of the domian $U_{m}$ and is $\geq 0$ in $U_{m}, \varphi_{m} > 0$ in $\overset{\circ}{U}_{m}$ and $\varphi_{m} = 0$ on $U_{m}$ the maximum is attained in the interior $\overset{\circ}{U}_{m}$ of $U_{m}$.\pageoriginale This is the reason for the nomenclature of the algorithm as the Centre method. (See also \cite{key45}).
\end{remark}

\begin{remark}\label{chap4-rem2.3}
The algorithm of the centre method was first given by Huard \cite{key25} and it was improved later on, in particular, by Tr\'{e}moli\'{e}res \cite{key45}.
\end{remark}

\begin{remark}\label{chap4-rem2.4}
This method is once again not usded for functionals $J_{\circ}$ arising from problems for partial differential equations.
\end{remark}

\section{Method of Gradient and Prohection}\label{chap4-sec3}
We shall describe here a fairly simple type of algorithm for the minimization probelm for a regular convex functional on a closed convex subset of a Hilbert space. In this method we suppose that it is easy to find numerically projections onto closed convex subsets. At each step to construct the next iterate first we use a gradient method, as developed in Chapter \ref{chap3}, for the minimization problem without constraints and then we project on to the given convex set. ``In the dual problem'' which we shall study in Chapter \ref{chap5} it is numerically easy to compute projections onto closed convex subsets and hence this method will be used there for a probelm for which the convex set is defined by certain constraints which we shall call dual constraints.

Let $K$ be a closed convex subset of a Hilbert space $V$ and $J : V \to \mathbb{R}$ be a functional on $V$. We make the following hypothesis on $K$ and $J$.
\begin{enumerate}
\item[(H1)] $K$ is a bounded closed convex set in $V$.

\item[(H2)] $J$ is regular in $V$: $J$ is twice $G$-differentiable everywhere in $V$ and has a gradient $G(u)$ and hessian $H(u)$ everywhere in $V$. Moreover, there exists a constant $M > 0$ such that
$$
||H(u)|| \leq M, \forall u \epsilon K.
$$

\item[(H3)] $H$ is uniformly coercive on $K$: there exists a constant $\alpha > 0$ such that
$$
(H(u) \varphi, \varphi) \geq \alpha ||\varphi||^{2}, \forall \varphi \epsilon V \text{ and } u \epsilon K.
$$\pageoriginale
\end{enumerate}

We note that the hypothesis of bounededness in $(H1)$ can be replaced by
\begin{equation*}
J(v) \to + \infty \text{ as } ||v|| \to + \infty.\tag*{$(H1)'$}\label{chap4-eqH1'}
\end{equation*}

Then we can fix a $u_{\circ} \epsilon K$ arbitrarily and restict our attention to the bounded closed convex set
$$
K \cap \{v | v \epsilon V ; J(v) \leq J(u_{\circ})\}.
$$

The hypothesis $(H3)$ implies that $J$ is strongly convex. The hypothesis $(H2)$ implies that the gradient $G(u)$ is uniformly Lipschitz continuous on $K$ and we have
\begin{equation*}
|| G(u) - G(v) || \leq M ||u-v||, \forall u, v \epsilon K.\tag{3.1}\label{chap4-eq3.1}
\end{equation*}

We now consider the problem :

\begin{problem}\label{chap4-prob3.1}
To find $u \epsilon K$ such that $J(u) \leq J(v)$, $\forall v \epsilon K$.
\end{problem}

\medskip
\noindent{\textbf{Algorithm.}} Let $u_{\circ} \in K$ be an arbitrarily fixed initial point of the algorithm and let $P : V \to K$ be the projection of $V$ onto the bounded closed convex set $K$.

Suppose $u_{m}$ is determined in the algorithm. The we define, for $\rho > 0$,
\begin{equation*}
u_{m+1} = P(u_{m} - \rho G(u_{m})).\tag{3.2}\label{chap4-eq3.2}
\end{equation*}

Then we have the following
\begin{theorem}\label{chap4-thm3.1}
Under the hypothesis $(H1) - (H3)$ the Problem (\ref{chap4-prob3.1}) has a unique solution $u$ and $u_{m} \to u$ as $m \to + \infty$.

This follows by a simple application of contraction mapping theorem.
\end{theorem}

\begin{proof}
Consider the mapping of $K$ into itself defined by
\begin{equation*}
T_{\rho} : K \ni u \mapsto P(u - \rho G(u) \epsilon K, \rho > 0.\tag{3.3}\label{chap4-eq3.3}
\end{equation*}
\end{proof}

Suppose\pageoriginale this mapping $T_{\rho}$ has a fixed point $w$. i.e. 
$$
w \epsilon K \text{ and satisfies } w = P(w - \rho G(w)).
$$

Then we have seen that such a $w$ is characterized as a solution of the variational inequality :
\begin{equation*}
w \epsilon K ; (w-(w - \rho G(w)), v-w) \geq 0, \forall v \epsilon K.\tag{3.4}\label{chap4-eq3.4}
\end{equation*}

Then (\ref{chap4-eq3.4}) is nothing but saying that
\begin{equation*}
w \epsilon K ; (G(w), v-w) \geq 0, \forall v \epsilon K.\tag*{$(3.4)'$}\label{chap4-eq3.4'}
\end{equation*}

Then by Theorem \ref{chap2-thm2.2} of Section \ref{chap2-sec2}, Chapter \ref{chap2} $w$ is a solution of the minimization Problem \ref{chap2-prob3.1} and conversely. In other words, Problem \ref{chap2-prob3.1} is equivalent to the following

\begin{dashprob}\label{chap4-dashprob3.1'}%%% 3.1'
To find a fixed points of the mapping $T_{\rho} : K \to K$. i.e. To find $w \in K$ such that $w = P(w-\rho G (w)).$

We shall now show that this Problem (\ref{chap4-dashprob3.1'})$'$ has a unique solution for $\rho > 0$ sufficiently small. For this we show that $T_{\rho}$ is a strict contraction for $\rho > 0$ sufficiently small: there exists a constant $\gamma, 0 < \gamma < 1$ such that, for $\rho > 0$ small enough,
$$
|| P(u - \rho G(u)) - P(v - \rho G(u)) || \leq \gamma ||u-v||, \forall u, v \epsilon K.
$$
\end{dashprob}

In fact, if $\rho > 0$ is any number then we have
$$
|| P(u - \rho G(u)) - P(v - \rho G(v)) ||^{2} \leq || (u - \rho G(u)) - (v - \rho G(v)) ||^{2}
$$
since $|| P || \leq 1$. The right hand side here is equal to
{\fontsize{10}{12}\selectfont
$$
|| u - v - \rho (G(u) - G(v)) ||^{2} = || u-v ||^{2} - 2\rho (G(u) - G(v), u-v) + \rho^{2} ||G(u) - G(v) ||^{2} 
$$}

Here we can write by Taylor's formula
$$
(G(u) - G(v), u-v) = (\overline{H}(u-v), u-v)
$$\pageoriginale
where $\overline{H} = H(v+\theta(u-v))$ for some $0 < \theta < 1$. Since $K$ is convex, $u, v \epsilon K$, $v + \theta(u-v) \epsilon K$ and then by uniform coercivity of $H$ on $K$ (i.e by H3)
$$
(H(u-v), u-v) \geq \alpha ||u-v||^{2} \forall u, v \epsilon K.
$$

This together with the Lipschitz continuity (\ref{chap4-eq3.1}) of $G$ gives
{\fontsize{10}{12}\selectfont
\begin{align*}
|| P(u - \rho G(u)) - P(v-\rho G(v)) ||^{2} & \leq ||u-v||^{2} -2\rho \alpha ||u-v||^{2} + M^{2} \rho^{2} ||u-v||^{2}.\\
& = ||u-v||^{2} (1-2 \rho \alpha + M^{2} \rho^{2}).
\end{align*}}

Now if we choose $\rho$ such that
\begin{equation*}
0 < \rho < 2\alpha / M^{2}\tag{3.5}\label{chap4-eq3.5}
\end{equation*}
it follows that $(1-2 \rho \alpha + M^{2} \rho^{2}) = \gamma^{2} < 1$.

Then by contraction mapping theorem applied to $T_{\rho}$ proves that there is a unique solution of the Problem (\ref{chap4-dashprob3.1'})$'$.

Finally to show that $u_{m} \to u$ as $m \to + \infty$, we take such a $\rho > 0$ sufficiently small i.e. $\rho > 0$ satisfying (\ref{chap4-eq3.5}). Now if $u_{m+1}$ is defined iteratively by the algorithm (\ref{chap4-eq3.2}) and $u$ is the unique solution of the Problem \ref{chap4-prob3.1} (or equivalently of the Problem (\ref{chap4-dashprob3.1'})$'$) then,
\begin{align*}
|| u_{m+1} - u || & = || P(u_{m} - \rho G(u_{m})) - P(u - \rho G(u)) ||\\ 
& = \leq \gamma || u_{m} - u ||
\end{align*}
so that we get
$$
|| u_{m+1} - u || \leq \gamma^{m} || u_{\circ} - u ||.
$$

Since $0 < \gamma < 1$ it follows immediatly from this that $u_{m} \to u$ as $m \to + \infty$. 

This proves the theorem completely.

Now\pageoriginale the convergence of the algorithm can be proved using the results of Chapter \ref{chap3}. (See Rosen \cite{key39}, \cite{key40}).

We also remark that if $V = K$ and hypothesis $(H1)'$, $(H2)$ and $(H3)$ are satisfied for bounded sets of $V$ then we get the gradirnt method of Chapter \ref{chap3}.

\section{Minimization in Product Spaces}\label{chap4-sec4}
In this section we shall be concerned with the probelm of optimization with or without constraints by Gauss-Seidel or more generally, by relaxation methods. The classical Gauss-Seidel method is used for solutions of linear equations in finite dimensional spaces. The main idea of optimization described here is to reduce by an iterative procedure the problem of minimizing a functional on a product space (with or without constraints) to a sequence of minimization problems in the factor spaces. Thus the methods of earlier sections can be used to obtain approximations to the solution of the problem on the product space.

The method described here follows that of the paper of C\'{e}a and Glowinski \cite{key9}, and generalizes earlier methods due to various authors.

We shall given algorithms for the construction of approximating sequences and prove that they converge to the solution of the optimization problem. One important feature is that we do not necessarily assume that the functionals to be minimized are $G$-differentiable.

\subsection{Statement of the problem}\label{chap4-subsec4.1}
The optimization problem in a product space can be formulated as follows: Let
\begin{enumerate}
\item[(i)] $V_{i} (i = 1, \cdots, N)$ be vector spaces over $\mathbb{R}$ and let
$$
V = \prod_{i=1}^{N} V_{i}
$$\pageoriginale
(dim $V_{i}$ are arbitrary).

\item[(ii)] $K$ be a convex subset of $V$ of the form $K = \prod_{i=1}^{N} K_{i}$ where each $K_{i}$ is a (non-empty) convex subset of $V_{i}(i = 1, \cdots, N)$. Suppose given a functional $J : V \to \mathbb{R}$. Consider the optimization problem:
\begin{equation*}
\begin{cases}
& \text{ To find } u \epsilon K \text{ such that}\\
& J(u) \leq J(v) \text{ for all } v \epsilon K.\tag{4.1}\label{chap4-eq4.1}
\end{cases}
\end{equation*}
\end{enumerate}

For this problem we describe two algorithms which reduce the problem to a sequence of N problems at each step, each of which is a minimization problem successively in $K_{i} (i = 1, \cdots, N)$. Let us denote a point $v \epsilon V$ by its coordinates as
$$
v = (v_{1}, \cdots, v_{N}), v_{i} \epsilon V_{i}.
$$

\medskip
\noindent{\textbf{Algorithm 4.1.}} (Gauss-Seidel method with constraints).
\begin{enumerate}
\item[(1)] Let $u^{\circ} = (u_{1}^{\circ}, \cdots, u_{N}^{\circ})$ be an arbitrary point in $K$.

\item[(2)] Suppose $u^{n} \epsilon K$ is already determined. Then we
  shall determine $u^{n+1}$ in $N$ steps by successively computing its
  components $u_{i}^{n+1}\break (i=1), \cdots , N$. 
\end{enumerate}

Assume $u_{j}^{n+1} \epsilon K_{j}$ is determined for all $j < i$. Then we determine $u_{i}^{n+1}$ as the solution of the minimization problem:
\begin{equation*}
\begin{cases}
& u_{i}^{n+1} \epsilon K_{i} \text{ such that }\\
& J(u_{1}^{n+1}, \cdots, u_{i-1}^{n+1}, u_{i}^{n+1}, u_{i+1}^{n}, \cdots, u_{N}^{n})\\
& \leq J(u_{1}^{n+1}, \cdots, u_{i-1}^{n+1}, v_{i}, u_{i+1}^{n}, \cdots , u_{N}^{n}) \text{ for all } v_{i} \epsilon K_{i}\tag{4.2}\label{chap4-eq4.2}
\end{cases}
\end{equation*}

In order to simplify the writing it is convenient to introduce the following notation.

\medskip
\noindent{\textbf{Notation.}}\pageoriginale Denote by $K_{i}^{n+1} (i = 1, \cdots, N)$ the subset of $K$: 
\begin{equation*}
K_{i}^{n+1} = \{v \epsilon K | v = (u_{1}^{n+1}, \cdots, u_{i-1}^{n+1}, v_{i}, u_{i+1}^{n}, \cdots, u_{N}^{n}), v_{i} \epsilon K_{i}\}.\tag{4.3}\label{chap4-eq4.3}
\end{equation*}
and
\begin{equation*}
\begin{cases}
& \widetilde{u}_{o}^{n+1} = u^{n}\\
& \widetilde{u}_{i}^{n+1} = (u_{1}^{n+1}, \cdots, u_{i-1}^{n+1}, u_{i}^{n+1}, u_{i+1}^{n}, \cdots, u_{N}^{n}).\tag{4.4}\label{chap4-eq4.4}
\end{cases}
\end{equation*}

With this notation we can write (\ref{chap4-eq4.2}) as follows:
\begin{equation*}
\begin{cases}
& \text{ To find } \widetilde{u}_{i}^{n+1} \epsilon K_{i}^{n+1} \text{ such that }\\
& J(\widetilde{u}_{i}^{n+1}) \leq J(v) \text{ for all } v \epsilon K_{i}^{n+1}.\tag*{$(4.2)'$}\label{chap4-eq4.2'}
\end{cases}
\end{equation*}

\medskip
\noindent{\textbf{Algorithm (4.2)}} (Relaxation method by blocks). We introduce numbers $w_{i}$ with $0 < w_{i} < 2 (i = 1, 2, \cdots, N)$.
\begin{enumerate}
\item[(1)] Let $u^{\circ} \epsilon K$ be arbitrarily chosen.
\item[(2)] Assume $u^{n} \epsilon K$ is known. Then $u^{n+1} \epsilon K$ is determined in N successive steps as follows: Suppose $u_{j}^{n+1} \epsilon K_{j}$ is determined for all $j < i$. Then $u_{i}^{n+1}$ is determined in two substeps:
\begin{equation*}
\begin{cases}
& \text{ To find } u_{i}^{n+\frac{1}{2}} \epsilon V_{i} \text{ such that }\\
& J(u_{1}^{n+1}, \cdots, u_{i-1}^{n+1}, u_{i}^{n+\frac{1}{2}}, u_{i+1}^{n}, \cdots, u_{N}^{n})\\
& \leq J(u_{1}^{n+1}, \cdots, u_{i-1}^{n+1}, v_{i}, u_{i+1}^{n}, \cdots, u_{N}^{n}) \text{ for all } v_{i} \epsilon V_{i}.\tag{4.5}\label{chap4-eq4.5}
\end{cases}
\end{equation*}
\end{enumerate}

Then we define
\begin{equation*}
u_{i}^{n+1} = P_{i} (u_{i}^{n} + w_{i} (u_{i}^{n+\frac{1}{2}} - u_{i}^{n}))\tag{4.6}\label{chap4-eq4.6}
\end{equation*}
where

\medskip
\noindent
(4.7) ~ $P_{i} : V_{i} \to K_{i}$  is the projection onto $K_{i}$ with respect to a suitable inner product which we shall specify later.


\begin{remark}\label{chap4-rem4.1}
The numbers $w_{i} \epsilon (0, 2)$ are called parameteres of relaxation. In the classical relaxation method each $w_{i} = w$, a fixed number $\epsilon (0, 2)$ and $V_{i} = K_{i}$. Hence for the classical relaxation method
\begin{equation*}
u_{i}^{n+1} = u_{i}^{n} + w(u_{i}^{n+\frac{1}{2}}-u_{i}^{n}).\tag{4.8}\label{chap4-eq4.8}
\end{equation*}
\end{remark}\pageoriginale

\subsection{Minimization with Constraints of Convex Functionals on Products of Reflexive Banach Spaces}\label{chap4-subsec4.2}
Here we shall introduce all the necessary hypothesis on the functional $J$ to be minimized. We consider $J$ to consist of a differentiable part $J_{\circ}$ and a non-differentiable part $J_{1}$ and we make separate hypothesis on $J_{\circ}$ and $J_{1}$.

Let $V_{i}(i = 1, \cdots, N)$ be reflexive Banach spaces and $V = \prod_{i=1}^{N} V_{i}$. The duality pairing $(\cdot ,  \cdot)_{V' \times V}$ will simply be denoted by $(\cdot , \cdot)$, then norm in $V$ by $|| \cdot ||$ and the dual norm in $V'$ by $|| \cdot ||_{*}$. Let $K_{i}$ be nonempty closed convex subsets of $V_{i}$ and $K = \prod_{i=1}^{N} K_{i}$. Then clearly $K$ is also a noneempty closed convex subset of $V$.

Let $J_{\circ} : V \to \mathbb{R}$ be a functional satisfying the following hypothesis:
\begin{enumerate}
\item[(H1)] $J_{\circ}$ is $G$-differentiable and admits a gradient $G_{\circ}$.

\item[(H2)] $J_{\circ}$ is convex in the following sense: If, for any $M > 0$, $B_{M}$ denotes the ball $\{v \epsilon V ; ||v|| \leq M\}$, then there exists a mapping
$$
T_{M} : B_{M} \times B_{M} \to \mathbb{R} 
$$
such that (\ref{chap4-eq4.9}) and (\ref{chap4-eq4.10}) hold:
\begin{equation*}
\begin{cases}
& J_{\circ}(v) \geq J_{\circ} (u) + (G_{\circ}(u), v-u) + T_{M} (u, v),\\
& T_{M}(u, v) \geq 0 \text{ for all } u, v \epsilon B_{M},\\
& T_{M} (u, v) > 0 \text{ for all } u, v \epsilon B_{M} \text{ with } u \neq v.\tag{4.9}\label{chap4-eq4.9}
\end{cases}
\end{equation*}
\begin{equation*}
\begin{cases}
& \text{ If } (u_{n}, v_{n})_{n} \text{ is a sequence in } B_{M} \times B_{M} \text{ such that }\\
& T_{M} (u_{n}, v_{n}) \to 0 \text{ as } n \to + \infty \text{ ther }\\
& ||u_{n} - v_{n}|| \to 0 \text{ as } n \to + \infty.
\end{cases}\tag{4.10}\label{chap4-eq4.10}
\end{equation*}\pageoriginale

\begin{remark}\label{chap4-rem4.2}
If $J_{\circ}$ is twice $G$-diffferentiable then we have
$$
T_{M} (u, v) = \frac{1}{2} J''_{\circ} (u + \theta(v-u), v-u, v-u) \text{ for some } 0 < \theta < 1.
$$
Then the hypothesis (\ref{chap4-eq4.9}) and (\ref{chap4-eq4.10}) can be restated in terms of $J''_{\circ}$. In particular, if $J_{\circ}$ admits a Hessian $H$ and if for every $M > 0$ there exists a constant $\alpha_{M} > 0$ such that
$$
(H(u)\varphi, \varphi) \geq \alpha_{M} ||\varphi||^{2} \text{ for all } \varphi \epsilon V \text{ and } u \epsilon B_{M} 
$$
then the two conditions (\ref{chap4-eq4.9}) and (\ref{chap4-eq4.10}) are satisfied.
\end{remark}
\item[(H3)] {\em Continuity of the gradient $G_{\circ}$ of $J_{\circ}$}.
\begin{equation*}
\begin{cases}
& \text{ If } (u_{n}, v_{n})_{n} \text{ is a sequence in } B_{M} \times B_{M} \text{ such that }\\
& ||u_{n}-v_{n}|| \to \text{ as } n \to + \infty \text{ then}\\
& ||G(u_{n}) - G(v_{n})||_{*} \to 0 \text{ as } n \to + \infty.\tag{4.11}\label{chap4-eq4.11}
\end{cases}
\end{equation*}
Next we consider the non-differentiable part $J_{1}$ of $J$. Let $J_{1} : V \to \mathbb{R}$ be a functional of the form
\begin{equation*}
J_{1} (v) = \sum_{i=1}^{N} J_{1, i} (v_{i}), v = (v_{1}, \cdots , v_{n}) \epsilon V\tag{4.12}\label{chap4-eq4.12}
\end{equation*}
where the functionals
$$
J_{1, i} : V_{i} \to \mathbb{R} (i = 1, \cdots , N)
$$
satisfy the hypothesis:
\item[(H4)] $J_{1, i}$ is a weakly lower semi-continuous convex functional on $V_{i}$.

We\pageoriginale define
\begin{equation*}
J = J_{\circ} + J_{1}.\tag{4.13}\label{chap4-eq4.13}
\end{equation*}
Finally we assume that $J$ satisfies the hypothesis:

\item[(H5)] $J(v) \to + \infty$ \text{ as } $||v|| \to + \infty$.
We now consider the minimization problem:
\begin{equation*}
\begin{cases}
& \text{ To find } u \epsilon K \text{ such that }\\
& J(u) \leq J(v) \text{ for all } v \epsilon K.\tag{4.14}\label{chap4-eq4.14}
\end{cases}
\end{equation*}
\end{enumerate}

\subsection{Main Results}\label{chap4-subsec4.3}
The main theorem of this section can now be stated as:

\begin{theorem}\label{chap4-thm4.1}
Under the hypothesis $(H1), \cdots , (H5)$ we have the following:
\begin{enumerate}
\item[(1)] The problem (\ref{chap4-eq4.14}) has a unique solution $u \epsilon K$ and the unique soultion is characterized by
\begin{equation*}
\begin{cases}
& u \epsilon K \text{ such that }\\
& G_{\circ}(u), v-u) + J_{1}(v) - J_{1}(u) \geq 0 \text{ for all } v \epsilon K.\tag{4.15}\label{chap4-eq4.15}
\end{cases}
\end{equation*}

\item[(2)] The sequence $u^{n}$ determined by the algorithm (\ref{chap4-eq4.1}) converges strongly to $u$ in $V$.
\end{enumerate}
\end{theorem}

\begin{proof}
We shall divide the proof into several steps.

\setcounter{step}{0}
\begin{step}\label{chap4-step1}
{\bf (Proof of (1)).} The first part of the theorem is an immediate consequence of the Theorem (\ref{chap2-thm1.1}) and (\ref{chap2-thm2.3}) of Chapter \ref{chap2}. In fact, $K$ is a closed non-empty convex subset of a reflexive Banach space $V$. By Hypothesis $(H2)$, $J$ is strictly convex since, for any $v, u \epsilon V$, we have
\begin{align*}
J_{\circ}(v) & \geq J_{\circ}(u) + (G_{\circ}(u), v-u) + T_{M}(v, u)\\
& > J_{\circ}(u) + (G_{\circ}(u), v-u) \qquad \text{ if } v \neq u,
\end{align*}
and\pageoriginale hence strictly convex, while $J_{1}(v)$ is convex so that for any $v_{1}, v_{2} \epsilon V$ and $\theta \epsilon [0, 1]$ we have
\begin{align*}
J(\theta v_{1} + (1-\theta)v_{2}) & = J_{\circ} (\theta v_{1} + (1-\theta)v_{2}) + J_{1}(\theta v_{1} + (1-\theta)v_{2})\\
& < \theta J_{\circ}(v_{1}) + (1-\theta) J_{\circ}(v_{2}) + \theta J_{1} (v_{1}) + (1-\theta)J_{1}(v_{2})\\
& = \theta J(v_{1}) + (1 - \theta)J(v_{2}).
\end{align*}

Next $J$ is weakly lower semi-continuous in $V$: In fact, since $J_{\circ}$ has a gradient $G_{\circ}$ the mapping
$$
\varphi \mapsto J'_{\circ}(u, \varphi) = (G_{\circ}(u), \varphi)
$$
is continuous linear and hence, by Proposition \ref{chap1-prop4.1} of Chapter \ref{chap1}, $J_{\circ}$ is weakly lower semi-continuous. On the other hand, by $(H4)$ $J_{1}$ is weakly lower semi-continuous which proves the assertion. Then Theorem (\ref{chap2-thm1.1}) of Chapter \ref{chap2} implies that states that $u$ is characterized by (\ref{chap4-eq4.15}).

We have therefore onlu to prove (2) of the statement. We shall prove the convergence of the algorithm in the following sequence of steps.
\end{step}

\begin{step}\label{chap4-step2}
At each stage of the algorithm the subproblem of determining $\widetilde{u}_{i}^{n+1}$ has a solution. In fact $K_{i}^{n+1}$ is againd a non-empty closed convex subset of $V$. Moreover, again as in Step \ref{chap4-step1}, $J$ satisfies all the hypothesis of Theorem (\ref{chap2-thm1.1}) of Chapter \ref{chap2} and (\ref{chap4-eq2.3}) of Chapter \ref{chap2}. Hence there exists a unique solution of the problem (\ref{chap4-eq4.14}) and this soution $\widetilde{u}_{i}^{n+1}$ is characterized by
\begin{align*}
\begin{cases}
~ \widetilde{u}_{i}^{n+1} \epsilon K,\\
~ (G_{\circ} (\widetilde{u}_{i}^{n+1}), v-\widetilde{u}_{i}^{n+1}) + J_{1, i} (v_{i}) - J_{1, i} (\widetilde{u}_{i}^{n+1}) \geq 0
\end{cases}\tag{4.16}\label{chap4-eq4.16}
\end{align*}
since 
$$
J_{1} (v) - J_{1} (\widetilde{u}_{i}^{n+1}) = \sum_{j=1}^{N} (J_{1, j} (v_{j}) - J_{1,j}(\widetilde{u}_{i,j}^{n+1})) = J_{1, i} (v_{i}) - J_{1, i} (u_{i}^{n+1}).
$$\pageoriginale
\end{step}

\begin{step}\label{chap4-step3}
{\bf $J(u^{n})$ is decresing.} We know that $\widetilde{u}_{i-1}^{n+1} \epsilon K_{i}^{n+1}$ for $i = 1, \cdots, N$ and on taking $v = \widetilde{u}_{i-1}^{n+1}$ in \ref{chap4-eq4.2'} we get
$$ 
 J(\widetilde{u}_{i}^{n+1}) \leq J(\widetilde{u}_{i-1}^{n+1}).  
$$

using this successively we find that
$$
J(\widetilde{u}_{i}^{n+1}) \leq J(\widetilde{u}_{i-1}^{n+1}) \leq \cdots \leq J(\widetilde{u}_{\circ}^{n+1}) = J(u^{n})
$$
and similarly
$$
J(u^{n+1}) = J(\widetilde{u}_{N}^{n+1}) \leq \cdots \leq J(\widetilde{u}_{i}^{n+1}).
$$

These two togrther imply that
$$
J(u^{n+1}) \leq J(u^{n}) \text{ ofr all } n = 0, 1, 2, \cdots
$$
which proves that the sequence $J(u^{n})$ is decreasing. In particular it is bounded above:
$$
J(u^{n}) \leq J(u^{\circ}) \text{ ofr all } n \geq 1.
$$

Since $u \epsilon K$ is the unique absolute minimum for $J$ given by Step (\ref{chap4-step1}) we have
$$
J(u) \leq J(u^{n}) \leq J(u^{\circ}) \text{ for all } n \geq 1.
$$

On the other hand, by Hypothesis $(H5)$ we see that $||u^{n}||$, $||\widetilde{u}_{i}^{n+1}||$ form bounded sequences. Thus there exists a constant $M > 0$ such that
\begin{equation*}
|| u^{n} || + ||\widetilde{u}_{i}^{n+1} || + ||u|| \leq M \text{ for all } n \geq 1 \text{ and all } 1 \leq i \leq N.\tag{4.17}\label{chap4-eq4.17}
\end{equation*}

Since 
$$
J(u) \leq J(u^{n+1}) \leq J(u^{n})
$$
it\pageoriginale also follows that
\begin{equation*}
J(u^{n}) - J(u^{n+1}) \to 0 \text{ as } n \to + \infty.\tag{4.18}\label{chap4-eq4.18}
\end{equation*}
\end{step}

\begin{step}\label{chap4-step4}
 We shall that $u^{n} - u^{n+1} \to 0$ as $n \to + \infty$. For this, by theconvexity hypothesis $(H2)$ of $J_{\circ}$ applied to $u = \widetilde{u}_{i}^{n+1}$ and $v = \widetilde{u}_{i-1}^{n+1}$ we get
$$
J_{\circ}(\widetilde{u}_{i-1}^{n+1}) \geq J_{\circ} (\widetilde{u}_{i}^{n+1}) + (G_{\circ}(\widetilde{u}_{i}^{n+1}), \widetilde{u}_{i-1}^{n+1} - \widetilde{u}_{i}^{n+1}) + T_{M} (\widetilde{u}_{i}^{n+1}, \widetilde{u}_{i-1}^{n+1} )
$$
where $M > 0$ is determined by (\ref{chap4-eq4.17}) in Step (\ref{chap4-step3}). From this we find
\begin{align*}
J(\widetilde{u}_{i-1}^{n+1}) & \geq J(\widetilde{u}_{i}^{n+1}) + [(G_{\circ} (\widetilde{u}_{i}^{n+1}), \widetilde{u}_{i-1}^{n+1} - \widetilde{u}_{i}^{n+1}) + J_{1} (\widetilde{u}_{i-1}^{n+1}) - J_{1} (\widetilde{u}_{i}^{n+1})]\\
& + T_{M} (\widetilde{u}_{i}^{n+1}, \widetilde{u}_{i-1}^{n+1}).
\end{align*}

Here by the characterization (\ref{chap4-eq4.16}) of $\widetilde{u}_{i}^{n+1} \epsilon K_{i}^{n+1}$ as the solution subproblem we see that the terms in the brackets $[ \cdots ] \geq 0$ and hence
$$
J(\widetilde{u}_{i-1}^{n+1}) \geq J(\widetilde{u}_{i}^{n+1}) + T_{M} (\widetilde{u}_{i}^{n+1}, \widetilde{u}_{i-1}^{n+1}) \text{ for all } i = 1, \cdots, N.
$$

Adding there inequalities for $i = 1, \cdots, N$ we obtain
\begin{align*}
J(\widetilde{u}_{\circ}^{n+1}) = J(u^{n}) & \geq J(\widetilde{u}_{N}^{n+1}) + \sum_{i} T_{M} (\widetilde{u}_{i}^{n+1}, \widetilde{u}_{i-1}^{n+1})\\
& = J(u^{n+1}) + \sum_{i} T_{M} (\widetilde{u}_{i}^{n+1}, \widetilde{u}_{i-1}^{n+1}),
\end{align*}
that is,
$$
J(u^{n}) - J(u^{n+1}) \geq \sum_{i} T_{M} (\widetilde{u}_{i}^{n+1}, \widetilde{u}_{i-1}^{n+1}).
$$

Here the left side tends to 0 as $n \to \infty$ and each term in the sum on the right side is non-negative by (\ref{chap4-eq4.9}) of Hypothesis $(H2)$ so that 
$$
T_{M} (\widetilde{u}_{i}^{n+1}, \widetilde{u}_{i-1}^{n+1}) \to 0 \text{ as } n \to + \infty \text{ for all } i = 1, \cdots, N.
$$

In view of (\ref{chap4-eq4.10}) of Hypothesis $(H2)$ it follows that
\begin{equation*}
\begin{cases}
& || \widetilde{u}_{i}^{n+1} - \widetilde{u}_{i-1}^{n+1} || \ \to 0 \text{ as } n \to + \infty \text{ for all } i = 1, \cdots, N \text{ and }\\
& || u^{n+1} - u^{n} |\ \to 0 \text{ as } n \to + \infty\tag{4.19}\label{chap4-eq4.19} 
\end{cases}
\end{equation*}\pageoriginale
which proves the required assertion.
\end{step}

\begin{step}\label{chap4-step5}
{\bf Convergence of the algorithm.} Using the convexity Hypothesis $(H2)$ of $J_{\circ}$ with $u$ and $v$ interchanged we get
\begin{align*}
& J_{\circ}(v) \geq J_{\circ}(u) + (G_{\circ}(u), v-u) + T_{M}(u, v)\\
& J_{\circ}(u) \geq J_{\circ}(v) + (G_{\circ}(v), v-u) + T_{M}(v, u)
\end{align*}
which on adding give
\begin{equation*}
\begin{cases}
& (G_{\circ}(v) - G_{\circ}(u), v-u) \geq R_{M} (v, u)\\
& \text{ where }\\
& R_{M} (v, u) = T_{M}(u, v) + T_{M} (v, u).\tag{4.20}\label{chap4-eq4.20}
\end{cases}
\end{equation*}

Taking for $u$ the unique solution of the problem (\ref{chap4-eq4.14}) and $v = u^{n+1}$ we obtain
$$
(G_{\circ} (u^{n+1}) - G_{\circ}(u), u^{n+1}-u) \geq R_{M} (u, u^{n+1})
$$ 
from which we get
\begin{equation*}
\begin{cases}
& (G_{\circ}(u^{n+1}), u^{n+1} - u) + J_{1} (u^{n+1}) - J_{1}(u)\\
& \geq [(G_{\circ}(u), u^{n+1}-u) + J_{1}(u^{n+1})-J_{1}(u)] + R_{M} (u, u^{n+1})\\
& \geq R_{M} (u, u^{n+1})\tag{4.21}\label{chap4-eq4.21}
\end{cases} 
\end{equation*}
since $u$ is characterized by (\ref{chap4-eq4.15}). Introducting the notation
$$
w_{i}^{n+1} = \widetilde{u}_{i}^{n+1} + (0, \cdots, 0, u_{i} - u_{i}^{n+1}, 0, \cdots, 0)
$$
we have
\begin{equation*}
\begin{cases}
& w_{i}^{n+1} = (u_{1}^{n+1}, \cdots, u_{i-1}^{n+1}, u_{i}, u_{i+1}^{n}, \cdots, u_{N}^{n}) \epsilon K_{i}^{n+1}\\
& \sum_{i} (w_{i}^{n+1} - \widetilde{u}_{i}^{n+1}) = (u-u^{n+1}).\tag{4.22}\label{chap4-eq4.22}
\end{cases}
\end{equation*}

Now\pageoriginale we use the fact that $J_{1}(v) = \sum_{i} J_{1, i} (v_{i})$ to get
$$
J_{1} (u^{n+1}) - J_{1}(u) = \sum_{i} (J_{1, i}(u_{i}^{n+1}) - J_{1, i} (u_{i})),
$$
which is the same as
\begin{equation*}
J_{1} (u^{n+1}) -J_{1}(u) = \sum_{i} (J_{1}(\widetilde{u}_{i}^{n+1}) - J_{1}(w_{i}^{n++1})).\tag{4.23}\label{chap4-eq4.23}
\end{equation*}

Substituting (\ref{chap4-eq4.22}) and (\ref{chap4-eq4.23}) in (\ref{chap4-eq4.21}) we have
\begin{equation*}
\begin{cases}
& \sum_{i} [(G_{\circ}(u^{n+1}), \widetilde{u}_{i}^{n+1} - w_{i}^{n+1}) + J_{1} (\widetilde{u}_{i}^{n+1}) - J_{1} (w_{i}^{n+1})]\\
& \geq R_{M} (u, u^{n+1}).
\end{cases}
\end{equation*}

This can be rewritten as
\begin{align*}
& \sum_{i} (G_{\circ}(u^{n+1}) - G_{\circ}(\widetilde{u}_{i}^{n+1}), \widetilde{u}_{i}^{n+1} - w_{i}^{n+1})\\
& \sum_{i} [(G_{\circ} (\widetilde{u}_{i}^{n+1}), w_{i}^{n+1} - \widetilde{u}_{i}^{n+1}) + J_{1}(w_{i}^{n+1})- J_{1} (\widetilde{u}_{i}^{n+1})] + R_{M} (u, u^{n+1}).
\end{align*}

But again by the characterization (\ref{chap4-eq4.16}) of the solution $\widetilde{u}_{i}^{n+1} \epsilon K_{i}^{n+1}$ of the sub-problem (\ref{chap4-eq4.14}) the terms in the square brackets and hence their sum is non negative (to see this we take $v = w_{i}^{n+1} \epsilon K_{i}^{n+1}$). Thus
\begin{equation*}
\sum_{i} (G_{\circ} (u^{n+1}) - G_{\circ} (\widetilde{u}_{i}^{n+1}), \widetilde{u}_{i}^{n+1} - w_{i}^{n+1}) \geq R_{M} (u, u^{n+1}).\tag{4.24}\label{chap4-eq4.24}
\end{equation*}
Here we have
$$
||\widetilde{u}^{n+1}_i - w^{n+1}_i ||_V = ||u_i - u^{n+1}_i||_{V_i} \leq || u|| + || \widetilde{u}_i^{n+1}|| \leq M. 
$$
By Cauchy-Schwarz inequality we have
$$
|(G_{\circ} (u^{n+1}) - G_{\circ}(\widetilde{u}_{i}^{n+1}), \widetilde{u}_{i}^{n+1} - w_{i}^{n+1})| \leq M || G_{\circ} (u^{n+1}) - G_{\circ} (\widetilde{u}_{i}^{n+1})||_*.
$$ 

Now since
$$
|| u^{n+1} - \widetilde{u}_{i}^{n+1} || = || \widetilde{u}_{N}^{n+1} - \widetilde{u}_{i}^{n+1} || \leq \sum_{j=i+1}^{N} || \widetilde{u}_{j}^{n+1} - \widetilde{u}_{j-1}^{n+1} ||
$$
which\pageoriginale tends to 0 by (\ref{chap4-eq4.19}) and since $G_{\circ}$ satisfies the continuity hypothesis (\ref{chap4-eq4.11}) of $(H3)$ it follows that
$$
R_{M} (u, u^{n+1}) \to 0 \text{ as } n \to \infty.
$$

This by the definition of $R_{M} (u, v)$ implies that
$$
T_{M} (u, u^{n+1}) \to 0 \text{ as } n \to \infty.
$$

Finally, by the property (\ref{chap4-eq4.10}) to $T_{M} (u, v)$ in Hypothesis $(H2)$ we conclude that 
$$
|| u-u^{n+1} || \to 0 \text{ as } n \to \infty.
$$
\end{step}

This completes the proof of the theorem.
\end{proof}

\begin{remark}\label{chap4-rem4.3}
If the convex set $K$ is bounded then the Hypothesis $(H5)$ is superfluous since the existence of the constant $M > 0$ in (\ref{chap4-eq4.17}) is then automatically assured since $u, u^{n}, \widetilde{u}_{i}^{n+1} \epsilon K$ for all $n \geq 1$ and $i = 1, \cdots, N$.
\end{remark}

\subsection{Some Applications : Differentiable and Non-Differatiable Functionals in Finite Dimensions}\label{chap4-subsec4.4}
We shall conclude this section with a few examples as applications of our main result (Theorem \ref{chap4-thm4.1}) without going into the details of the proofs. To begin with have the following:

\begin{theorem}\label{chap4-thm4.2}
(Case of differentaible functionals on the finite dimensional spaces).

Let $J_{\circ} : V = \mathbb{R}^{p} \to \mathbb{R}$ be a functional satisfying the Hypothesis:
\begin{enumerate}
\item[(K1)] $J_{\circ} \epsilon C^{1} (\mathbb{R}^{p}, \mathbb{R})$
\item[(K2)] $J_{\circ}$ is strictly convex
\item[(K3)] $J_{\circ}(v) \to + \infty$ as $|| v || \to + \infty$.
\end{enumerate}
Then the assertion of the Theorem (\ref{chap4-thm4.1}) hold with $J = J_{\circ}$.
\end{theorem}

It is immediate that the Hypothesis $(H1)$ and $(H3)$ are satisfied. Since $J_{1} \equiv 0$, $(H4)$ and $(H5)$ are also satisfied. There remains only to prove that the Hypothesis\pageoriginale $(H2)$ of the convexity of $J_{\circ}$ holds. For a proof of this we refer to the paper of C\'{e}a and Glowinski \cite{key9}. (See also Glowinski \cite{key18}, \cite{key19}).

\begin{remark}\label{chap4-rem4.4}
Suppose $p = \sum\limits_{i}^{N} p_{i}$ be a partition of p. Then in the above theorem we can take $V_{i} = \mathbb{R}^{p_{i}}$ so that $V = \prod\limits_{i=1}^{N} V_{i}$. We also have the
\end{remark}

\begin{theorem}\label{chap4-thm4.3}
(Case of non-differentiable functions on finite dimensional spaces - Cea and Glowinski). Let $V_{i} = \mathbb{R}^{p_{i}} (i = 1, \cdots, N)$ and $V = \mathbb{R}^{p} (p = \sum\limits_{i=1}^{N} p_{i})$. Suppose $J_{\circ} : V \to \mathbb{R}$ satisfies the hypothesis (K1), (K2) and (K3) pf Theorem (\ref{chap4-thm4.2}) above and $J_{1} : V \to \mathbb{R}$ be another functional of the form $J_{1} (v) = \sum\limits_{i=1}^{N} J_{1, i} (v_{i})$ where the functionals $J_{1, i} : V_{i} \to \mathbb{R}$ satisfy the Hypothesis below:

$(K4) J_{1, i}$ is a non-negative, convex and continuous functional on\break $\mathbb{R}^{p_{i}} = V_{i} (i = 1, \cdots, N)$.
\end{theorem}

Then the functional
$$
J = J_{\circ} + J_{1}
$$
satisfies all the Hypothesis of Theorem (\ref{chap4-thm4.1}) and hence the algorithm (\ref{chap4-eq4.1}) is (strongly) convergent in $V = \mathbb{R}^{p}$.

We shall now give a few examples of functional $J_{1}$ which satisfy (K4).
\begin{example}\label{chap4-exam4.1}
We take $J_{1, i} (v_{i}) = \alpha_{i} |\ell_{i} (v_{i})|$ where
\begin{enumerate}
\item[(i)] $\alpha_{i} \geq 0$ are fixed numbers
\item[(ii)] $\ell_{i} : V_{i} = \mathbb{R}^{p_{i}} \to \mathbb{R}$ is a continuous linear functional for each $i = 1, \cdots, N.$
\end{enumerate}
\end{example}

In particular, if $p_{i} = 1(i = 1,\cdots, N)$ and hence $p = N$ we can take
$$
J_{1, i}(v_{i}) = \alpha_{i} |v_{i}|,
$$
and
$$
J_{1}(v) = \sum_{i=1}^{N} \alpha_{i} |v_{i}|.
$$

This\pageoriginale case was treated earlier by Auslander \cite{key53} who proved that the algorithm for $u^{n}$ converges to the solution of the minimization problem in this case.

\begin{example}\label{chap4-exam4.2}
We take
$$
J_{1, i}(v_{i}) = \alpha_{i} [\ell_{i} (v_{i})^{+}]
$$
where
\begin{enumerate}
\item[(i)] $\alpha_{i} \geq 0$ are fixed numbers,

\item[(ii)] $\ell_{i} : V_{i} \to \mathbb{R}$ are continuous linear forms on $\mathbb{R}^{p_{i}}$, and we have used the standard notation:
\begin{equation*}
\ell_{i}(v_{i})^{+} = 
\begin{cases}
& \ell_{i}(v_{i}) \text{ when } \ell_{i}(v_{i}) \geq 0\\
& 0 \text{ when } \ell_{i} (v_{i}) < 0.
\end{cases}
\end{equation*}
\end{enumerate}
\end{example}

\begin{example}\label{chap4-exam4.3}
We take
$$
J_{1, i}(v_{i}) = \alpha_{i} ||v_{i}||_{\mathbb{R}^{p_{i}}}
$$
where
$$
|| v_{i} ||_{\mathbb{R}^{p_{i}}} = \left(\sum_{j=1}^{p_{i}} |v_{i, j}|^{2} \right)^{\frac{1}{2}}.
$$
\end{example}

\subsection[Minimization of Quadratic Functionals on
  Hilbert...]{Minimization of Quadratic Functionals on
  Hilbert\hfil\break Spaces-Relaxation Method by
  Blocks}\label{chap4-subsec4.5} 
Here we shall be concerned with the problem of minimization of quadra\-tic funcitonals on convex subsets of a product of Hilbert spaces. This is one of the most used methods for problems associated with partial differential equations. We shall describe an algorithm and prove the convergence of the approximations (obtained by this algorithm) to the solution of the minimization problem under consideration.

{\em Statement of the problem.} Let $V_{i}(i = 1, 2, \cdots N)$ be Hilbert spaces, the inner products and the norms are respectively denoted by $(( \cdot))_{i}$ and $|| \cdot ||_{i}$. On the product\pageoriginale space we define the natural inner product and norm by
\begin{equation*}
\begin{cases}
& ((u, v)) = \sum\limits_{i=1}^{N} ((u_{i}, v_{i}))_{i},\\
& || u || = \left(\sum\limits_{i=1}^{N} ||u_{i}||_{i}^{2} \right)^{\frac{1}{2}},\\
& u = (u_{1}, \cdots, u_{n}), v = (v_{1}, \cdots, v_{n}) \epsilon V,\tag{4.25}\label{chap4-eq4.25}
\end{cases}
\end{equation*}
for which $V$ becomes a Hilbert space. Let $K$ be a closed convex subset of $V$ of the form
\begin{equation*}
\begin{cases}
& K = \prod_{i=1}^{N} K_{i} \text{ where }\\
& K_{i} \text{ is a closed convex nonempty subset of } V_{i} (1 \leq i \leq N).\tag{4.26}\label{chap4-eq4.26}
\end{cases}
\end{equation*}

Let $J : V \to \mathbb{R}$ be a functional of the form
\begin{equation*}
J(v) = \frac{1}{2} a(v, v) - L(v)\tag{4.27}\label{chap4-eq4.27}
\end{equation*}
where $a(\cdot , \cdot)$ is a bilinear, symmetric, bicontinuous, $V$-coercive form on $V$:

There exist constants $M > 0$ and $\alpha > 0$ such that
\begin{equation*}
\begin{cases}
& |a(u, v)| \leq M ||u||_{V} ||v||_{V} \qquad\text{ for all } u, v \epsilon V,\\
& a(u, u) \geq \alpha ||u||_{V}^{2} \qquad\text{for all } u \epsilon V, \text{ and }\\
& a(u, v) = a(v, u)\tag{4.28}\label{chap4-eq4.28}
\end{cases}
\end{equation*}

Moreover, $L : V \to \mathbb{R}$ is a continuous linear functional on $V$. Consider the optimization problem :
\begin{equation*}
\begin{cases}
& \text{ To find } u \epsilon K \text{ such that }\\
& J(u) \leq J(v) \text{ for all } v \epsilon K.\tag{4.29}\label{chap4-eq4.29}
\end{cases}
\end{equation*}

Then we know by Theorem \ref{chap2-thm3.1} of Chapter \ref{chap2} that under the assumptions made on $V$, $K$ and $J$ the optimization problem (\ref{chap4-eq4.29}) has a unique solution whihc is characterized by the variational inequality
\begin{equation*}
\begin{cases}
& u \epsilon K.\\
& a(u, v - u) - L(v-u) \geq 0 \text{ for all } v \epsilon K.\tag{4.30}\label{chap4-eq4.30}
\end{cases}
\end{equation*}\pageoriginale

\subsection{Algorithm (4.2) of the Relaxation Method - Details}\label{chap4-subsec4.6}
In order to give an algorithm for the solution of the problem (\ref{chap4-eq4.29}) we obtain the following in view of the product Hilbert space structure of $V$. First of all, we observe that the bilinear form $a(\cdot , \cdot)$ give rise to bilinear forms
\begin{equation*}
a_{ij} : V_{i} \times V_{j} \to \mathbb{R}\tag{4.31}\label{chap4-eq4.31}
\end{equation*}
such that
\begin{equation*}
a(u, v) = \sum_{i, j = 1}^{N} a_{ij} (v_{i}, v_{j}).
\end{equation*}

In fact, for any $v_{i} \epsilon V_{i}$ if we set $v^{i}$ to be the element of $V$ having components $(v^{i})_{j} = 0$ for $j \neq 1$ and $(v^{i})_{i} = v_{i}$, we define
\begin{equation*}
a_{ij}(v_{i}, v_{j}) = a(v^{i}, v^{j}).\tag{4.33}\label{chap4-eq4.33}
\end{equation*}

It is the clear that the properties (\ref{chap4-eq4.28}) of $a(\cdot . \cdot)$ immediately imply the following properties of $a_{ij} (\cdot , \cdot)$:
\begin{equation*}
\begin{cases}
& a_{ij} \text{ is bicontinuous :} |a_{ij}(v_{i}, v_{j})| \leq M ||v_{i}||_{i} ||v_{j}||_{j}.\\
& a_{ij} (v_{i}, v_{j}) = a_{ji} (v_{j}, v_{i})\\
& a_{ii} \text{ is } V_{i}-\text{coercive }: a_{ii} (v_{i}, v_{i}) = a(v^{i}, v^{i}) \geq \alpha ||v^{i}||^{2} = \alpha ||v_{i||_{i}}^{2}\\
& \text{for all } v_i \in V_{i}, \; v_{j} \epsilon V_{j}
\end{cases} \tag{4.34}\label{chap4-eq4.34}
\end{equation*}

Using the bicontinuity of the bilinear forms $a_{ij}( \cdot , \cdot )$ together with Riesz-representation theorem, we can find
\begin{align*}
& A_{ij} \epsilon \mathscr{L} (V_{i}, V_{j}) \text{ suich that}\\
& a_{ij} (v_{i}, v_{j}) = (A_{ij} v_{i}, v_{j})_{V'_{j} \times V_{j}}\tag{4.35}\label{chap4-eq4.35}
\end{align*}
where\pageoriginale $(\cdot , \cdot)_{V'_{j} \times V_{j}}$ denotes the duality pairinig between $V_{j}$ and its dual $V'_{j}$ (which is canonically isomorphic to $V_{j}$). The properties (\ref{chap4-eq4.34}) can equivalently be stated in the following form:
\begin{equation*}
\begin{cases}
& ||A_{ij}||_{\mathscr{L}(V_{i}, V_{j})} \leq M,\\
& A_{ij} = A_{ij}^{*} , A_{ii} \text{ are self adjoint}\\
& (A_{ii} v_{i}, v_{i})_{V'_{i} \times V_{i}} \geq \alpha||v_{i}||_{i}^{2} \text{ for all } v_{i} \epsilon V_{i}.\tag*{$(4.34)'$}\label{chap4-eq4.34'}
\end{cases}
\end{equation*}

By lax-Milgram lemma $A_{ii}$ are invertible and $A_{ii}^{-1} \epsilon \mathscr{L} (V_{i}, V_{i})$.

In a similar way, we find the forms L defines continuous linear functionals $L_{i} : V_{i} \to \mathbb{R}$ such that
\begin{equation*}
\begin{cases}
& L_{i}(v_{i}) = L(v^{i}) \text{ for all } v_{i} \epsilon V_{i}\\
& L(v) = \sum_{i=1}^{N} L_{i} (v_{i}) \text{ for all } v \epsilon V.
\end{cases}
\end{equation*}

Again by Riesz-representation theorem there exist $F_{i} \epsilon V_{i}$ such that
$$
L_{i} (v_{i}) = ((F_{i}, v_{i}))_{i} \text{ for all } v_{i} \epsilon V_{i}
$$
so that we can write
\begin{equation*}
L(v) = \sum_{i=1}^{N} ((F_{i}, v_{i}))_{i}.\tag{4.36}\label{chap4-eq4.36}
\end{equation*}

As an immediate consequence of the properties of the bilinear forms $a_{ii}(\cdot , \cdot)$ on $V_{i}$ we can introduce a new inner product on $V_{i}$ by
\begin{equation*}
[u_{i}, v_{i}]_{V_{i}} = a_{ii} (u_{i}, v_{i}).\tag{4.37}\label{chap4-eq4.37}
\end{equation*}
which defines an equivalent norm which we shall denote by $||| \cdot |||_{i}$ (we can use Lax-Milgram lemma) on $V_{i}$.

{\em We shall denote by $P_{i}$ the projection of $V_{i}$ onto the closed convex subset $K_{i}$ with respect to the inner product $[\cdot , \cdot]_{i}$}.

We\pageoriginale are now in a position to describe the algorithm for the relaxation method with projection. (See also \cite{key19}).

\medskip
\noindent{\textit{Algorithm 4.2. - Relaxation with Projection by Blocks.}}

Let $w_{i} (i = 1, \cdots, N)$ be a fixed set of real numbers such that $0 < w_{i} < 2$.
\begin{enumerate}
\item[(1)] Let $u^{\circ} = (u_{1}^{\circ}, \cdots , u_{N}^{\circ}) \epsilon K$ be arbitrary.
\item[(2)] Suppose $u^{n} \epsilon K$ is already determined. We determine $u^{n+1} \epsilon K$ in N successive steps as follows: Suppose, $u_{j}^{n+1} \epsilon K$ are already found for $j < i$.
\end{enumerate}

Then we take
\begin{equation*}
\begin{cases}
& u_{i}^{n+1} = P_{i} (u_{i}^{n} - w_{i} A_{ii}^{i} (\sum\limits_{j < i} A_{ij} u_{j}^{n+1} + \sum\limits_{j \geq i} A_{ij} u_{j}^{n} - F_{i}))\\
& i = 1, \cdots, N.
\end{cases}\tag{4.38}\label{chap4-eq4.38}
\end{equation*}


\begin{remark}\label{chap4-rem4.5}
In applications, the boundary value problems associated with elliptic partial differential operators will be set in appropriate\break Sobolev spaces $H^{m}(\Omega)$ on some (bounded) open set $\Omega$ in Euclidean space. After discretization (say, by suitable finite elemnt approximations) we are led to problems in finite dimensional subspaces of $H^{m} (\Omega)$ which increase to $H^{m} (\Omega)$. In such a discretization $A_{ii}$ and $A_{ij}$ will be matrices with the properties \ref{chap4-eq4.34'} described above.
\end{remark}

\subsection{Convergence of the Algorithm}\label{chap4-subsec4.7}
As usual we shall prove that the algorithm converges to the solution of the minimization problem (\ref{chap4-eq4.29}) in a sequence of steps in the following. We shall begin with

\medskip
\noindent{\textbf{Step 1. $J(u^{n})$ is a decreasing sequence.}} For this we write
\begin{align*}
J(u^{n}) - J(u^{n+1}) & = J(\widetilde{u}_{\circ}^{n+1}) - J(\widetilde{u}_{N}^{n+1})\tag{4.39}\label{chap4-eq4.39}\\
& = \sum_{i=1}^{N} (J(\widetilde{u}_{i-1}^{n+1}) - J(\widetilde{u}_{i}^{n+1}))
\end{align*}
and\pageoriginale show that each term in tha last sum is non-negqtive. We observe here that
\begin{equation*}
\begin{cases}
& \widetilde{u}_{i-1}^{n+1} = (u_{1}^{n+1}, \cdots, u_{i-1}^{n+1}, u_{i}^{n}, u_{i+1}^{n}, \cdots , u_{N}^{n})\\
& \widetilde{u}_{i}^{n+1} = (u_{1}^{n+1}, \cdots, u_{i-1}^{n+1}, u_{i}^{n+1}, u_{i+1}^{n}, \cdots, u_{N}^{n}).\tag{4.40} \label{chap4-eq4.40}
\end{cases}
\end{equation*}

Setting, for each $i = 1, \cdots, N$,
\begin{equation*}
\begin{cases}
g_{i} & = -\frac{1}{2} \sum\limits_{j < i} A_{ij} u_{j}^{n+1} + \frac{1}{2} \sum\limits_{j > i} A_{ij} u_{j}^{n} + f_{i}\\
j_{i} (v_{i}) & = \frac{1}{2} (A_{ii} v_{i}, v_{i}) - (g_{i}, v_{i})\tag{4.41}\label{chap4-eq4.41}
\end{cases}
\end{equation*}
we immediately see that
\begin{equation*}
J(\widetilde{u}_{i-1}^{n+1}) - J(\widetilde{u}_{i}^{n+1}) = j_{i}(u_{i}^{n}) - j_{i} (u_{i}^{n+1}). \tag{4.42} \label{chap4-eq4.42}
\end{equation*}

Hence it is enough to show that the right hand side of (\ref{chap4-eq4.42}) is non-negative. In fact, we shall prove the following

\begin{proposition}\label{chap4-prop4.1}
For each $i, 1 \leq i \leq N$, we have
\begin{equation*}
j_{i} (u_{i}^{n}) - j_{i} (u_{i}^{n+1}) \geq \dfrac{2-w_{i}}{2w_{i}} ||| u_{i}^{n} - u_{i}^{n+1} |||. \tag{4.43}\label{chap4-eq4.43}
\end{equation*}
\end{proposition}

The proof will be based on some simple lemmas:

\medskip
\noindent{\textbf{Step 2. Two lemmas.}} Let $H$ be a Hilbert space and $C$ be a non-empty closed convex subset of $H$. Consider a quadratic functional $j : H \to \mathbb{R}$ of the form 
\begin{equation*}
j(v) = \frac{1}{2} b(v, v) - (g, v)\tag{4.44}\label{chap4-eq4.44}
\end{equation*}
where
\begin{equation*}
\begin{cases}
& b(\cdot , \cdot) \text{ is a symmetric, bicontinuous, $H$-coercive }\\
& \text{ bilinear form on $H$ and } g \epsilon H.
\end{cases} \tag{4.45}\label{chap4-eq4.45}
\end{equation*}

Then we know by Theorem \ref{chap2-thm3.1} of Chapter \ref{chap2} that the minimization problem
\begin{equation*}
\begin{cases}
& \text{ To find } u \epsilon C \text{ such that }\\
& j(u) \leq j(v) \text{ for all } v \epsilon C\tag{4.46}\label{chap4-eq4.46}
\end{cases}
\end{equation*}
has\pageoriginale a unique solution. On the other hand, the hypothesis on $b(\cdot , \cdot)$ imply that we can write
\begin{equation*}
\begin{cases}
& b(u, v) = v(Bv) \text{ for all } u, v \epsilon H\\
& \text{ and }\\
& B \epsilon \mathscr{L} (H, H), B = B^{*} \text{ exists and belongs to } (H, H)
\end{cases}
\end{equation*}

Moreover,
\begin{equation*}
[u, v] = b(u, v) = (u, Bv)\tag{4.48}\label{chap4-eq4.48}
\end{equation*}
defines an inner product on $H$ such that
\begin{equation*}
u \mapsto u = [u, u]^{\frac{1}{2}}\tag{4.49}\label{chap4-eq4.49}
\end{equation*}
is an equivalent norm in $H$. Then we have the

\begin{lemma}\label{chap4-lem4.1}
If $u \epsilon C$ is the unique solution of the problem (\ref{chap4-eq4.46}) and if $P : H \to C$ denotes the projection onto $C$ with respect to the inner product $[\cdot , \cdot]$ then
\begin{equation*}
u = P(B^{-1} g).\tag{4.50}\label{chap4-eq4.50}
\end{equation*}
\end{lemma}

\begin{proof}
We also know that the solution of the problem (\ref{chap4-eq4.46}) is characterized by the variational inequality
\begin{equation*}
\begin{cases}
& u \epsilon C,\\
& b(u, v-u) \geq (g, v-u) \text{ for all } v \epsilon C.\tag{4.51}\label{chap4-eq4.51}
\end{cases}
\end{equation*}
\end{proof}

Since we can write
\begin{equation*}
(g, v-u) = (B B^{-1} g. v-u) = b(B^{-1} g, v-u)\tag{4.52}\label{chap4-eq4.52}
\end{equation*}
this variational inequality can be rewritten in the form
\begin{equation*}
\begin{cases}
& u \epsilon C,\\
& [u - B^{-1} g, v-u] = b(u-B^{-1} g, v-u) \geq 0 \text{ for all } v \epsilon C.\tag*{$(4.51)'$}\label{chap4-eq4.51'}
\end{cases}
\end{equation*}\pageoriginale

But it is a well known fact that this new variational inequality characterizes the projection $P(B^{-1} g)$ with respect to the inner product $[\cdot , \cdot]$ (For a proof, see for instance Stampacchia \cite{key44}).

\begin{lemma}\label{chap4-lem4.2}
Let $u_{\circ} \epsilon C$. If $u_{1}$ is defined by
\begin{equation*}
u_{1} = P(u_{\circ} + w(B^{-1} g-u_{\circ})), w > 0.\tag{4.53}\label{chap4-eq4.53}
\end{equation*}
where P is the projection $H \to C$ with respect to $[\cdot , \cdot]$ then
\begin{equation*}
j(u_{\circ}) - j(u_{1}) \geq \dfrac{2-w}{2w} ||| u_{\circ} - u_{1} |||^{2}.\tag{4.54}\label{chap4-eq4.54}
\end{equation*} 
\end{lemma}

\begin{proof}
If $v_{1}, v_{2} \epsilon H$ then we have
\begin{align*}
j(v_{1}) -j(v_{2}) & = \frac{1}{2} \{b(v_{1}, v_{1}) -b(v_{2}, v_{2})\} - \{(g, v_{1}) - (g, v_{2})\}\\
                  & = \frac{1}{2} \{b(v_{1}, v_{1}) -b(v_{2}, v_{2})\} - (BB^{-1} g, v_{1} - v_{2})\\
                  & = \frac{1}{2} \{b(v_{1}, v_{1}) -b(v_{2}, v_{2})\} - b(B^{-1} g, v_{1} - v_{2})\\
                  & = \frac{1}{2} \{b(v_{1} - B^{-1} g, v_{1} - B^{-1} g) - b(v_{2}-B^{-1} g, v_{2} - B^{-1} g)\}\\
                  & = \frac{1}{2} (||| v_{1} - B^{-1} g |||^{2} - ||| v_{2} -B^{-1} g |||^{2}).
\end{align*}
\end{proof}

Since we can write
$$
u_{1} - B^{-1} g = (u_{\circ} - B^{-1} g) + (u_{1} - u_{\circ})
$$
we find
\begin{equation*}
||| u_{\circ} - B^{-1} g |||^{2} = ||| u_{1} - B^{-1} g |||^{2} - ||| u_{1} - u_{\circ} |||^{2} + [u_{\circ} - B^{-1} g, u_{1} - u_{0}] \tag{4.55}\label{chap4-eq4.55}
\end{equation*}

But on the other hand, by definition of $u_{1}$ as the projection it follows that
$$
[u_{\circ} + w(B^{-1} g - u_{0}) - u_{1}, u_{\circ} - u_{1}] \leq 0
$$
and hence
$$
||| u_{\circ} - u_{1} |||^{2} \leq w [u_{\circ} - B^{-1} g, u_{\circ} - u_{1}].
$$\pageoriginale

Substituting this in the above identity (\ref{chap4-eq4.55}) we get
\begin{align*}
||| u_{\circ} - B^{-1} g |||^{2} - ||| u_{1} - B^{-1} g |||^{2} & \geq (2-w) [u_{\circ} - B^{-1} g, u_{\circ} - u_{1}]\\
& \geq \dfrac{2-w}{2w} |||u_{\circ}-u_{1}|||^{2},
\end{align*}
which is precisely the required estimate (\ref{chap4-eq4.54}).


\medskip
\noindent{\textbf{Step 3. Proof of the Proposition (4.1).}} It is enough to take
$$
H = V_{i}, C = K_{i}, b(\cdot , \cdot) = a_{ii} (\cdot , \cdot), P = P_{i} = Proj \{V_{i} \to K_{i}\}
$$
and
$$
u_{i}^{n} = u_{\circ}, u_{i}^{n+1} = u_{1}
$$
in Lemma \ref{chap4-lem4.2}.

\begin{corollary}\label{chap4-coro4.1}
We have, for each $n \geq 0$,
\begin{equation*}
J(u^{n}) - J(u^{n+1}) \geq \sum_{i=1}^{N} \dfrac{2-w_{i}}{2w_{i}} ||| u_{i}^{n+1} - u_{i}^{n} |||_{i}^{2}.\tag{4.56}\label{chap4-eq4.56}
\end{equation*}
\end{corollary}

\begin{proposition}\label{chap4-prop4.2}
If $0 < w_{i} < 2$  for all  $i = 1, \cdots, N$  then 
\begin{equation*}
\begin{cases}
& J(u^{n}) \geq J(u^{n+1}) \text{ for all $n$ and }\\
& u^{n} - u^{n+1} \to 0 \text{ strongly in $V$ as } n \to \infty.\tag{4.57}\label{chap4-eq4.57}
\end{cases}
\end{equation*}
\end{proposition}

\begin{proof}
The fact that $J(u^{n})$ is a decreasing sequence follows immediately from the Corollary (\ref{chap4-coro4.1}). Moreover, $J(u^{n}) \geq J(u)$. for all $n$, where $u$ is the (unique) absolute minimum of $J$ in $K$. Hence,
$$
J(u^{n}) - J(u^{n+1}) \to 0 \text{ as } n \to \infty.
$$
\end{proof}

Once again using the Corollary (\ref{chap4-coro4.1}) and the fact that $2 - w_{i} > 0$ for each $i$ it follows that
$$
||| u_{i}^{n+1} - u_{i}^{n} |||_{i} \to 0 \text{ as } n \to \infty.
$$\pageoriginale

Since $||| \cdot |||_{i}$ and $|| \cdot ||_{i}$ are equivalent norms on $V_{i}$ we find that
$$
||u_{i}^{n} - u_{i}^{n+1}||_{i} \to 0 \text{ as } n \to \infty
$$
and therefore
$$
|| u^{n} - u^{n+1} || = \left(\sum ||u_{i}^{n} - u_{i}^{n+1}||_{i}^{2} \right)^{\frac{1}{2}} \to 0
$$
which proves the assertion.

\medskip
\noindent{\textbf{Step 4. Convergence of $u^{n}$.}} We hve the following result.

\begin{theorem}\label{chap4-thm4.4}
If $0 < w_{i} < 2$ for all $i = 1, \cdots, N$ and if $u^{n}$ is the sequence defined by the Algotihm (\ref{chap4-eq4.2}) then
\begin{equation*}
u^{n} \to u \text{ strongly in } V.\tag{4.58}\label{chap4-eq4.58}
\end{equation*}
\end{theorem}

\begin{proof}
By $V$-coercivity of the bilinear form $a(\cdot , \cdot)$ we have
\begin{align*}
\alpha || u^{n+1} - u ||^{2} & \leq a(u^{n+1} - u, u^{n+1} - u)\\
&  = a(u^{n+1}, u^{n+1}-u) - (f, u^{n+1} - u)\\
& - \{a(u, u^{n+1} - u) - (f, u^{n+1} - u)\}.
\end{align*}
\end{proof}

Here $u^{n+1} - u \epsilon K$ and $u$ is characterized by the variational inequality (\ref{chap4-eq4.30}) so that
$$
a(u, u^{n+1}-u) - (f, u^{n+1}-u) \geq 0
$$
and we obtain
\begin{equation*}
\alpha||u^{n+1} - u||^{2} \leq a(u^{n+1}, u^{n+1}-u) - (f, u^{n+1}-u),\tag{4.59} \label{chap4-eq4.59}
\end{equation*}

We can also wirte (\ref{chap4-eq4.59}) in terms of the operators $A_{ij}$ as
\begin{equation*}
\alpha ||u^{n+1}-u||^{2} \leq \sum_{i} ((\sum_{j} A_{ij} u_{j}^{n+1} -f_{i}, u_{i}^{n+1}-u_{i}))_{i}.\tag*{$(4.59)'$}\label{chap4-eq4.59'}
\end{equation*}

Consider\pageoriginale the minimization problem
\begin{equation*}
\begin{cases}
& \overline{u}_{i}^{n+1} \epsilon K_{i} \text{ such that }\\
& j_{i} (\overline{u}_{i}^{n+1}) \leq j_{i} (v_{i}) \text{ for all } v_{i} \epsilon K_{i} \text{ where }\\
& j_{i}(v_{i}) = J(u_{1}^{n+1}, \cdots , u_{i-1}^{n+1}, v_{i}, u_{i+1}^{n}, \cdots, u_{N}^{n}).\tag{4.60}\label{chap4-eq4.60}
\end{cases}
\end{equation*}

We notice that the definition of the functional $v_{i} \mapsto j_{i}(v_{i})$ coincides with the definition (\ref{chap4-eq4.41}). The unique solution of the problem (\ref{chap4-eq4.60}) (which exists by Theorem \ref{chap2-thm3.1} of Chapter \ref{chap2}) is characterized (in view of the Lemma (\ref{chap4-lem4.1})) by
\begin{equation*}
\overline{u}_{i}^{n+1} = P_{i}(A_{ii}^{-1} g_{i}) = P_{i}(A_{ii}^{-1}(f_{i} - \sum\limits_{j<i} A_{ij} u_{j}^{n+1} -\sum\limits_{j>1} A_{ij} u_{j}^{n}))\tag{4.61}\label{chap4-eq4.61}
\end{equation*}
or equivalent by the variational inequality:
\begin{equation*}
\begin{cases}
& (A_{ii} \overline{u}_{i}^{n+1} -g_{i}, v_{i} - \overline{u}_{i}^{n+1}) \geq 0 \text{ for all } v_{i} \epsilon K_{i}\\
& \overline{u}_{i}^{n+1} \epsilon K_{i}.
\end{cases}
\end{equation*}

This is, we have
\begin{equation*}
\begin{cases}
& (A_{ii} \overline{u}_{i}^{n+1} + \sum\limits_{j<1} A_{ij} u_{j}^{n+1} + \sum\limits_{j>i} A_{ij} u_{j}^{n} -f_{i}, v_{i}-\overline{u}_{i}^{n+1}) \geq 0 \text{ for all } v_{i} \epsilon K_{i}\\
& \overline{u}_{i}^{n+1} \epsilon K_{i}.
\end{cases} \tag{4.62} \label{chap4-eq4.62}
\end{equation*}

We can now write the right hand side of \ref{chap4-eq4.59'} as a sum
\begin{equation*}
I_{1} + I_{2} + I_{2} + I_{4}\tag*{$(4.59)''$}
\end{equation*}
where
\begin{equation*}
\begin{cases}
& I_{1} = \sum\limits_{i} ((A_{ii}(u_{i}^{n+1}-\overline{u}_{i}^{n+1}), u_{i}^{n+1} -u_{i}))_{i},\\
& I_{2} = \sum\limits_{i} ((\sum\limits_{j>1} A_{ij} (u_{j}^{n+1}-u_{j}^{n}), u_{i}^{n+1}-u_{i}))_{i},\\
& I_{3} = \sum\limits_{i} ((\sum\limits_{j<i} A_{ij} u_{j}^{n+1} + A_{ii} \overline{u}_{i}^{n+1} + \sum\limits_{j>1} A_{ij} u_{j}^{n} - f_{i}, u_{i}^{n+1} - \overline{u}_{i}^{n+1}))_{i},\\
& I_{4} = \sum\limits_{i} ((\sum\limits_{j<i} A_{ij} u_{j}^{n+1} + A_{ii} \overline{u}_{i}^{n+1} + \sum\limits_{j>i} A_{ij} u_{j}^{n} - f_{i}, \overline{u}_{i}^{n+1} - u_{i}))_{i}.\tag{4.63}\label{chap4-eq4.63}
\end{cases}
\end{equation*}

First\pageoriginale of all, (by \ref{chap4-eq4.62}), $I_{4} \leq 0$ and hence
\begin{equation*}
\alpha ||u^{n+1}-u||^{2} \leq I_{1} + I_{2} + I_{3}.\tag{4.64}\label{chap4-eq4.64}
\end{equation*}

We shall estimate each one of $I_{1}, I_{2}, I_{3}$ as follows: Since
$A_{ij} \epsilon \mathscr{L}\break (V_{i}, V_{j})$ we set 
\begin{equation*}
M_{1} = \max_{1\leq i, j\leq N} || A_{ij} ||_{\mathscr{L}(V_{i}, V_{j})}\tag{4.65}\label{chap4-eq4.65}
\end{equation*}

We also know that $|| u_{i}^{n} ||, || \overline{u}_{i}^{n}||$ and hence $||u^{n}||, ||\overline{u}^{n}||$ are bounded sequences. For otherwise, $j_{i}(u_{i}^{n})$ and $j_{i}(\overline{u}_{i}^{n})$ would tend to $+ \infty$ as $n \to \infty$. But we know that they are bounded above by $J(u^{\circ})$. So let
\begin{equation*}
M_{2} = \max_{1 \leq i \leq N} (\sup_{n} ||u_{i}^{n}||, \sup_{n}||\overline{u}_{i}^{n}||).\tag{4.66}\label{chap4-eq4.66}
\end{equation*}

The, by Cauchy-Schwarz inequality, we get
\begin{align*}
|I_{1}| & \leq (\sum_{i} ||u_{i}^{n+1} - u_{i}||_{i}^{2})^{\frac{1}{2}} (\sum_{i} ||A_{ii}||^{2}_{\mathscr{L}(V_{i}, V_{i})} ||u_{i}^{n+1} -\overline{u}_{i}^{n+1}||^{2})^{\frac{1}{2}}\\
& = M_{1} (M_{2} + ||u||) ||u^{n+1} - \overline{u}^{n+1}||
\end{align*}
and similarly we have
\begin{align*}
& |I_{2}| \leq M_{1} (M_{2} + ||u||) ||u^{n+1} - \overline{u}^{n+1} ||\\
& |I_{3}| \leq M_{1} (M_{2} + ||f||) ||u^{n+1} - \overline{u}^{n+1} ||.
\end{align*}

These estimates together with (\ref{chap4-eq4.64}) give
\begin{equation*}
\alpha ||u^{n+1} - u||^{2} \leq 3M_{1}(M_{2} + ||u|| + ||f||) ||u^{n+1}-\overline{u}^{n+1}||\tag{4.67}\label{chap4-eq4.67}
\end{equation*}
and hence it is enough to prove that
\begin{equation*}
|| u^{n+1} - \overline{u}^{n+1} || \to 0 \text{ as } n \to \infty.\tag{4.68}\label{chap3-eq4.68}
\end{equation*}

For this purpose, since $w_{i} > 0$ we can multiply the variational inequality (\ref{chap4-eq4.62}) by $w_{i}$ and then we can rewrite it as
{\fontsize{10}{12}\selectfont
\begin{equation*}
((A_{ii} \overline{u}_{i}^{n+1} - \{A_{ii} \overline{u}_{i}^{n+1} - w_{i} (\sum_{j<i} A_{ij} u^{n+1}_j + A_{ii} \bar{u}^{n+1}_i + \sum_{j>i} A_{ij} u_{j}^{n} - f_{i})\}, v_{i} - \overline{u}_{i}^{n+1})) \geq 0. \tag*{$(4.62)'$}\label{chap4-eq4.62'}
\end{equation*}}\pageoriginale

Once again using the fact that this variational inequality characterizes the projection $P_{i} : V_{i} \to K_{i}$ we see that
\begin{equation*}
\overline{u}_{i}^{n+1} = P_{i} \{(1-w_{i})\overline{u}_{i}^{n+1} - A_{ii}^{-1} (\sum_{j<i} A_{ij} u_{j}^{n+1} + \sum_{j>i} A_{ij} u_{j}^{n} - f_{i})\}.\tag{4.69}\label{chap4-eq4.69}
\end{equation*}

By (\ref{chap4-eq4.38}) we also have
$$
u_{i}^{n+1} = P_{i} \{(1-w_{i}) u_{i}^{n} - A_{ii}^{-1} (\sum_{j<1} A_{ij} u_{j}^{n+1} + \sum_{j>1} A_{ij} u_{j}^{n} - f_{i})\}.
$$

Substracting one from the other and using the fact that the projection are contractions we obtain
\begin{equation*}
||| \overline{u}_{i}^{n+1} - u_{i}^{n+1} |||_{i} \leq |1-w_{i}| ||| \overline{u}_{i}^{n+1} - u_{i}^{n} ||| \leq ||| \overline{u}_{i}^{n+1} - u_{i}^{n} |||_{i}\tag{4.70}\label{chap4-eq4.70}
\end{equation*}
since $0 < w_{i} < 2$ if and only if $0 < |1-w_{i}| < 1$. Now by triangle inequality we have
\begin{align*}
||| u_{i}^{n} - u_{i}^{n+1} ||| & \geq ||| u_{i}^{n} - \overline{u}_{i}^{n+1} |||_{i} - ||| \overline{u}_{i}^{n+1} - u_{i}^{n+1} |||_{i}\\
& \geq (1 - |1-w_{i}|) |||\overline{u}_{i}^{n+1} - u_{i}^{n}|||_{i}\\
& \geq (1 - |1-w_{i}|) ||| \overline{u}_{i}^{n+1} - u_{i}^{n+1} |||_{i}.
\end{align*}

But here, by (\ref{chap4-eq4.57}), we know that
$$
||| u_{i}^{n} - u_{i}^{n+1} |||_{i} \to 0 \text{ as } n \to \infty.
$$
and since $1 - |1-w_{i}| > 0$ it follwos that
$$
||| \overline{u}_{i}^{n+1} - u_{i}^{n+1} |||_{i} \to 0 
$$
which is the required assertion.

\begin{remark}\label{chap4-rem4.6}
The Theorem (\ref{chap4-thm4.4}) above on convergence of the relaxation method generalizes a result of Cryer \cite{key10} and of a classical result of Varge \cite{key50} in finite dimensional case but withour constraints.
\end{remark}

\begin{remark}\label{chap4-rem4.7}
In\pageoriginale this section we have introduced the parameters $w_{i}$ of relaxation. The algorithm described is said to be of over relaxation type (resp. relaxation, or under relaxation) with projection when $w_{i} > 1$ (resp. $w_{i} = 1$ or $0 < w_{i} < 1$) for all $i = 1, \cdots, N$.
\end{remark}

\subsection[Some Examples - Relaxation Method in Finite...]{Some
  Examples - Relaxation Method in Finite\hfil\break Dimensional
  Spaces}\label{chap4-subsec4.8} 

Let $V_{i} = \mathbb{R} (i = 1, \cdots , N)$ and $V = \prod_{i=1}^{N} V_{i} = \mathbb{R}^{N}$. Let A be a symmetric, positive definite $(n \times n)$ -matrix such that there is a constant $\alpha > 0$ with
\begin{equation*}
(Av, v)_{\mathbb{R}^{N}}  \geq \alpha ||v||_{\mathbb{R}^{N}}^{2} \text{ for all } v \epsilon \mathbb{R}^{N}.\tag{4.71}\label{chap4-eq4.71}
\end{equation*}

Consider the quadratic functional $J : \mathbb{R}^{N} \to \mathbb{R}$ of the form
\begin{equation*}
J(v) = \frac{1}{2} (Av, v)_{\mathbb{R}^{N}} - (f, v)_{\mathbb{R}^{N}}, f \epsilon \mathbb{R}^{N}.\tag{4.72}\label{chap4-eq4.72}
\end{equation*}

We consider the optimization probel for $J$.

\begin{example}\label{chap4-exam4.4}
(Optimization without constraints).
\begin{equation*}
\begin{cases}
& \text{ To find } u \epsilon \mathbb{R}^{N} \text{ such that }\\
& J(u) \leq J(v) \text{ for all } v \epsilon \mathbb{R}^{N}\tag{4.73}\label{chap4-eq4.73}
\end{cases}
\end{equation*}
If we write the matrix $A$ as $A = (a_{ij})$ then
\begin{equation*}
J(v) = \frac{1}{2} \sum_{i, j=1}^{N} a_{ij} v_{j} v_{i} - \sum_{i=1}^{N} f_{i} v_{i}, v = (v_{1}, \cdots , v_{N}) \epsilon \mathbb{R}^{N}.\tag{4.74}\label{chap4-eq4.74} 
\end{equation*}
\end{example}

We find then that the components of grad $j$ are 
$$
(grad J(v))_{i} = (Av-f)_{i} = ( \sum_{j=1}^{N} a_{ij} v_{j} - f_{i}), i = 1, \cdots, N. 
$$

If $u \epsilon \mathbb{R}^{N}$ is the (unique) solution of (\ref{chap4-eq4.73}) then grad $J(u) = 0$. That is,
\begin{equation*}
\begin{cases}
& u = (u_{1} \cdots, u_{n})\\
& \sum_{j=1}^{N} a_{ij} u_{j} = f_{i}, i = 1, \cdots , N.
\end{cases}
\end{equation*}

To\pageoriginale describe the algorithm (if we take $w_{i} = 1$ for all $i = 1, \cdots , N$) to construct $u^{k+1}$ from $u^{k}$ we find $u_{i}^{k+1}$ as the solution of the equation
$$
\sum_{j<1} a_{ij} u_{j}^{k+1} + a_{ii} u_{i}^{k+1} + \sum_{j>1} a_{ij} u_{j}^{k} = f_{i}.
$$

Since $a_{ii} > \alpha > 0$ we have
\begin{equation*}
u_{i}^{k+1} = a_{ii}^{-1} [f_{i} - \sum_{j<i} a_{ij} u_{j}^{k+1} - \sum_{j>i} a_{ij} u_{j}^{k}],\tag{4.75}\label{chap4-eq4.75}
\end{equation*}
and thus we obtain the algorithm of the classical Gauss-Seidel methods in finite dimensional spaces.

More generally, introducting a parameter $w(0 < w < 2)$ of relaxation we obtain the following algorithm:
\begin{equation*}
\begin{cases}
& u_{i}^{k+\frac{1}{2}} = a_{ii}^{-1} [f_{i} - \sum\limits_{j<i} a_{ij} u_{j}^{k+1} - \sum\limits_{j>i} a_{ij} u_{j}^{k}]\\
& u_{i}^{k+1} = u_{i}^{k} - w(u_{i}^{k+\frac{1}{2}} - u_{i}^{k})\tag{4.76}\label{chap4-eq4.76}
\end{cases}
\end{equation*}

\begin{example}\label{chap4-exam4.5}
(Optimization with constraints in finite dimensional\break spaces).

Let $V_{i}, V$ and $J$ be as in Exampl (\ref{chap4-exam4.4}). We take for the convex set $K$ the following set: Let $I_{\circ}, I_{1}$ be a partition of the set $\{1, 2, \cdots, N\}$. That is 
$$
I_{\circ} \cap I_{1} = \phi \text{ and } \{1, 2, \cdots , N\} = I_{\circ} \cup I_{1}.
$$
\end{example}

Define
\begin{equation*}
\begin{cases}
& K_{i} = \{v_{i} \epsilon \mathbb{R}; v_{i} \geq 0\} \text{ for all } i \epsilon I_{\circ} \text{ and }\\
& K_{i} = \mathbb{R} \text{ for all } i \epsilon I_{1}\tag{4.77}\label{chap4-eq4.77} 
\end{cases}
\end{equation*}
and hence
\begin{equation*}
K = \{v \epsilon \mathbb{R}^{N} ; v = (v_{1}, \cdots , v_{N}) \text{ such that } v_{i} \geq 0 \text{ for } i \epsilon I_{\circ}\}\tag{4.78}\label{chap4-eq4.78}
\end{equation*}

As in the previous case, suppose $u^{k}$ are known, Assume that $u_{j}^{k+1}$ are found for\pageoriginale all $j < i$. We find $u_{i}^{k+1}$ in there substeps as follows: We define $u_{i}^{K+1/3}$ as the unique solution of the linear equation obtained by requiring the gradient to vanish at the minimum : more precisely,
\begin{equation*}
u_{i}^{k+1/3} = a_{ii}^{-1} [f_{i} - \sum_{j<i} a_{ij} u_{j}^{k+1} - \sum_{j>i} a_{ij} u_{j}^{k}].\tag{4.79}\label{chap4-eq4.79}
\end{equation*}

The we set
\begin{equation*}
\begin{cases}
& u_{i}^{k+2/3} = u_{i}^{k} - w (u_{i}^{k+1/3} - u_{i}^{k})\\
& u_{i}^{k+1} = P_{i} (u_{i}^{k+2/3})\tag{4.80}\label{chap4-eq4.80}
\end{cases}
\end{equation*}
where $P_{i}$ is the projection of $V_{i}$ onto $K_{i}$ with  respect to the inner product
$$
[u_{i}, v_{i}] = a_{ii} (u_{i}, v_{i}) = a_{ii} u_{i} v-{i}. 
$$

Since $a_{ii} > 0$ and $K_{i}$ are defined by (\ref{chap4-eq4.74}) $P_{i}$ coincides with the projection of $V_{i}$ onto $K_{i}$ with respect to the standard inner product on $\mathbb{R}$. Hence we have
\begin{equation*}
P_{i}(u_{i}^{k+2/3}) = 
\begin{cases}
& 0 \text{ if } u_{i}^{k+2/3} \leq 0 \text{ and } i \epsilon I_{\circ}\\
& u_{i}^{k+2/3} \text{ in all other cases} .\tag{4.81}\label{chap4-eq4.81}
\end{cases}
\end{equation*}

\begin{example}\label{chap4-exam4.6}
Let $V = \mathbb{R}^{N} = \mathbb{R}^{1} \times \mathbb{R}^{N-1}, K = K_{1} \times K_{2}$ with $K_{1} = \mathbb{R}^{1}$ and
$$
K_{2} = \{v \epsilon \mathbb{R}^{N-1} ; g(v) \leq 0\},
$$
where $g : \mathbb{R}^{N-1} \to \mathbb{R}$ is a given smooth functional on $\mathbb{R}^{N-1}$. Let $J : V \to \mathbb{R}$ be a functional of the form (\ref{chap4-eq4.74}). We can use again an algorithm of the above type. In order to give an algorithm for the construction of the projection $P_{2}$ of $V = \mathbb{R}^{N-1}$ onto $K_{2}$ we can use any one of the standard methods described in earlier section as, for instance, the method of descent.
\end{example}

\subsection{Example in Infinite Dimensional Hilbert Spaces - Optimization with Constraints in Sobolev Spaces}\pageoriginale\label{chap4-subsec4.9}

We shall only mention briefly a few examples, without going into any details, of optimization problems in the typical saces of infinite dimensions which are of interest to linear partial differential equation, namely the Sobolev spaces $H^{1} (\Omega), H_{\circ}^{1} (\Omega)$ which occur naturally in various variational elliptic problems of second order.

\begin{example}\label{chap4-exam4.7}
Let $\Omega$ be a bounded open set in $\mathbb{R}^{n}$ with smoth boundary $\Gamma$.

Consider the closed convex subset $K_{\circ}$ in $H^{1}(\Omega)$ given by
\begin{equation*}
K_{\circ} = \{v ; v \epsilon H^{1} (\Omega), \gamma_{\circ} v \geq 0 \text{ a. e. on } \Gamma\},\tag{4.82}\label{chap4-eq4.82}
\end{equation*}
and the quadratic functional $J_{\circ} : H^{1} (\Omega) \to \mathbb{R}$ defined by
\begin{equation*}
J_{\circ} (v) = \frac{1}{2} ||v||_{H^{1} (\Omega)}^{2} -(f, v)_{L^{2} (\Omega)}.\tag{4.83}\label{chap4-eq4.83}
\end{equation*}

Then we have the optimization problem
\begin{equation*}
\begin{cases}
& \text{ To find } u \epsilon K_{\circ} \text{ such that }\\
& J_{\circ} (u) \leq J_{\circ} (v) \text{ for all } v \epsilon K_{\circ}\tag{4.84}\label{chap4-eq4.84}
\end{cases}
\end{equation*}
\end{example}

Usually we use the method of over relaxation for this problem.

\begin{example}\label{chap4-exam4.8}
Let $\Omega$ be a simply connected bounded open set in the plane $\mathbb{R}^{2}$.

Consider
\begin{equation*}
 K_{1}  = \{v \epsilon H_{\circ}^{1} (\Omega); | \text{grad } v(x)| \leq 1 \text{ a. e. in } \Omega\} \text{ and }\tag{4.85}\label{chap4-eq4.85}
\end{equation*}
\begin{equation*}
\begin{cases}
& J(v) = \frac{1}{2} \int_{\Omega} |grad v|^{2} dx - C \int_{\Omega} v dx\\
& \text{ where $C$ is a constant} > 0.\tag{4.86}\label{chap4-eq4.86}
\end{cases}
\end{equation*}
\end{example}

The existence and uniqueness of the solution to the minimization problem:
\begin{equation*}
\begin{cases}
& \text{ To find } u \epsilon K_{1} \text{ such that}\\
& J(u) \leq J(v) \text{ for all } v \epsilon K_{1}\tag{4.87}\label{chap4-eq4.87}
\end{cases}
\end{equation*}
is\pageoriginale classical and its properties have been studied in the paper of Brezis and Stampacchia \cite{key4} and some others. It was also shown by Brezis and Sibony \cite{key2} that the solution of (\ref{chap4-eq4.87}) is also the solution of the problem
\begin{equation*}
\begin{cases}
& \text{To fing } u \epsilon K_{2} \text{ such that }\\
& J(u) \leq J(v) \text{ for all } v \epsilon K_{2}, \text{ where }\\
& K_{2} = \{v \epsilon H_{\circ}^{1} (\Omega) ; |v(x)| \leq d(x, \Gamma) \text{ a.e. in } \Omega\},\\
& d(x, \Gamma) \text{ being the distance of } x \in \Omega \text{ to the boundary } \Gamma \text{ of } \Omega.\tag{4.88}\label{chap4-eq4.88}
\end{cases}
\end{equation*}
The method of relaxation described earlier has been used to solve the problem (\ref{chap4-eq4.88}) numerically by C\'{e}a and Glowinski \cite{key8,key9}. We also remark that the problem (\ref{chap4-eq4.87}) is a problem of elasto-palsticity where $\Omega$ denotes the cross section of a cylindrical bar whose boundary is $\Gamma$ and which is made of an elastic material which is perfectly plastic. For details of the numerical analysis of this probel we refer the reader to the paper of Cea and Glowinski quoted above.
 
