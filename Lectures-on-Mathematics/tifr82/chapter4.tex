\chapter{A General Study of Even Moments}\label{c4}

\section{The Error Term for the $2k^{\rm th}$ Moment}\label{c4:sec4.1}

In\pageoriginale This Chapter we are going to study the asymptotic evaluation of the
integral
\begin{equation}
  I_k(T) : = \int\limits_0^T \left|\zeta \left(\frac{1}{2} +
  it\right)\right|^{2k} dt\label{c4:eq4.1} 
\end{equation}
when $k \geq 1$ is a fixed integer. This is a problem that occupies a
central position in zeta-function theory. Upper bounds for $I_k (T)$
have numerous applications, for example in zero-density theorems for
$\zeta (s)$ and various divisor problems. In Chapter \ref{c2} and
Chapter \ref{c3} 
we studied extensively the function $E(T)$, defined by
$$
I_1 (T) = \int\limits_0^T \left|\zeta \left(\frac{1}{2} + it \right)
\right|^2 dt= T\left(\log \frac{T}{2 \pi} + 2 \gamma -1 \right) + E(T),
$$
and in Chapter \ref{c5} we shall investigate the asymptotic formula for $I_2
(T)$. There we shall present the recent work of Y. Motohashi, who
succeeded in obtaining a sharp asymptotic formula for a weighted
integral connected with $I_2 (T)$. A classical result of A.E. Ingham
states that
\begin{equation}
  I_2 (T) = \int\limits_0^T \left|\zeta \left(\frac{1}{2} + it
  \right)\right|^4 
  dt = \frac{T}{2\pi^2} \log^4 T + O(T \log ^3 T). \label{c4:eq4.2}
\end{equation}

Ingham's proof of \eqref{c4:eq4.2} was difficult, and \eqref{c4:eq4.2}
remained the best result of its kind for more than half a century. It
should be remarked that precise asymptotic formulas for 
$$
\int\limits_0^\infty e^{- \delta t} \left|\left(\frac{1}{2} + it
\right) \right|^2 dt,\quad \int\limits_0^T e^{- \delta t}
\left|\left(\frac{1}{2} + it \right) \right|^4 dt \quad (\delta \to O+)
$$
were\pageoriginale obtained by H. Kober \cite{Kober1} and F.V. Atkinson
\cite{Atkinson1}, respectively. So far no one has succeeded in deriving from
these formulas correspondingly sharp results for $I_1(T)$. In 1979
D.R. Heath Brown \cite{Heath-Brown3} substantially improved \eqref{c4:eq4.2} by
showing that
\begin{equation}
  I_2 (T)  = \int\limits_O^T \left|\zeta \left(\frac{1}{2} + it
  \right)\right|^4 dt =T \sum_{j=0}^4 a _j \log^j T + E_2 (T)\label{c4:eq4.3}
\end{equation}
with
\begin{equation}
  a_4 = 1/(2 \pi^2), a_3  = a\left(4 \gamma -1-\log(2 \pi)- 12 \zeta' (2)
  \pi^{-2} \pi^{-2}\right)\label{c4:eq4.4}
\end{equation}
and
\begin{equation}
  E_2(T)  \ll T^{7/8+\epsilon}.\label{c4:eq4.5}
\end{equation}

The remaining $a_j$'s in \eqref{c4:eq4.3} can be also written down
explicitly, but are not of such a simple form as $a_4$ and
$a_3$. Heath-Brown's method of proof, which will be briefly discussed
in Section \ref{c4:sec4.7}, rests on evaluating a certain weighted
integral rather than evaluating $I_2 (T)$ directly. This type of
technique, used also in Chapter \ref{c5}, is becoming prominent in analytic
number theorey. It usually gives good upper bound estimates for the
error terms in question. Its disadvantage is that it rarely produces
an explicit expression (such as Atkinson's formula for $E(T)$ does)
for the error term. Recently N.I. Zavorotnyi improved \eqref{c4:eq4.5}
by showing that
\begin{equation}
  E_2 (T) \ll T^{2/3+\epsilon}. \label{c4:eq4.6}
\end{equation}

The proof makes heavy use of spectral theory of automorphic functions
and N.V. Kuznetsov's ``trace formula''. Earlier H. Iwaniec \cite{Iwaniec1}
proved 
$$
\int\limits_T^{T+G} \left|\zeta \left(\frac{1}{2} + it
\right)\right|^4 dt \ll  GT^\epsilon \;  \left(T^{2/3} \leq G\leq T\right)
$$ 
by a related technique involving results on sums of Kloosterman
sums. Iwaniec's result was reproved by M. Jutila \cite{Jutila10}, \cite{Jutila11},
who used a more classical approach, based on transformation formulas
involving Dirichlet polynomials with the divisor function $d(n)$. The
upper bound in \eqref{c4:eq4.6} will be improved in Chapter \ref{c5}, where
we shall show that $T^\epsilon$ may be replaced by a suitable
log-power. Any improvement of the\pageoriginale exponent $2/3$
necessitates non-trivial estimates for exponential sums with the
quantities $\alpha_j H^3_j (\frac{1}{2})$ from the theory of
automorphic $L$-functions.

When $k \geq 3$ no asymptotic formulas for $I_k (T)$ are known at
present. Even upper bounds of the form
$$
I_k (T) \ll _{k, \epsilon} T^{1+\epsilon}
$$
would be of great interest, with many applications. The best known
unconditional bound for $2 \leq k \leq 6$ is, apart from $T^\epsilon$
which can be replaced by log-factors,
\begin{equation}
  I_k(T) \ll_\epsilon T^{1+ \epsilon + \frac{1}{4} (k-2)}. \label{c4:eq4.7}
\end{equation}

Concerning lower bounds for $I_k (T)$ we already mentioned (see\break
\eqref{c1:eq1.34}) that
\begin{equation}
  I_k (T) \ll_k T(\log T)^{k^2}, \label{c4:eq4.8}
\end{equation}
which is a result of K. Ramachandra. Presumably this lower bound is
closer to the true order of magnitude than the upper bound in
\eqref{c4:eq4.7}. Therefore it seems to make sense to define, for any
fixed integer $k \geq 1$,
\begin{equation}
  E_k (T) : = \int\limits_0^T \left|\zeta \left(\frac{1}{2} + it
  \right)\right|^{2k} dt - TP_{k^2} (\log T), \label{c4:eq4.9}
\end{equation}
where for some suitable constants $a_{j, k}$
\begin{equation}
  P_{k^2} (y) = \sum_{j=0}^{k^2} a_{j, k}y^j. \label{c4:eq4.10}
\end{equation}

Thus $E_1 (T) = E(T)$, $P_1 (y) = y+ 2 \gamma -1- \log (2 \pi)$, $P_4
(y)$ is given by \eqref{c4:eq4.3} and \eqref{c4:eq4.4}. However, in
the case of general $k$ one can only hope that $E_k (T) = o(T)$ as $T
\to \infty$ will be proved in the foreseeable future for any $k \geq
3$. Another problem is what are the values (explicit expressions) for
the constants $a_{j, k}$ in \eqref{c4:eq4.10}. In fact, heretofore it
has not been easy to define properly (even on heuristic grounds) the
value of
\begin{equation}
  c(k) = a_{k^2, k}= \lim\limits_{T \to \infty} \left(T \log^{k^2}T\right)^{-1}
  \int\limits_0^T \left|\zeta \left(\frac{1}{2} + it \right)\right|^{2k}
  dt, \label{c4:eq4.11} 
\end{equation}
provided\pageoriginale that the existence of the limit is assumed. Even assuming
unproved hypotheses, such as the Riemann hypothesis or the Lindel\"of
hypothesis, it does not seem easy to define $c(k)$. I believe that, if
$c(k)$ exists, then for all integers $k \geq 1$
{\fontsize{10pt}{12pt}\selectfont
\begin{equation}
  c(k) = 2\left(\frac{k}{2} \right)^{k^2} \frac{1}{\Gamma (k^2 + 1)}
  \prod_p \left\{\left(1- \frac{1}{p} \right)^{k^2}
  \left(\sum_{j=0}^\infty \left(\frac{\Gamma (k+j)}{j! \Gamma (k)}
  \right)^2  p^{-j} \right)\right\}. \label{c4:eq4.12}
\end{equation}}

This formula gives $c(1) =1$, $c(2) = 1/(2 \pi^2)$, which are the
correct values. A conditional lower bound for $I_k (T)$ is given by
Theorem \ref{c1:thm1.5}. Note that we have $c(k) = 2
\left(\frac{1}{2}k \right)^{k^2} c'_{k'}$ where $c'_k$ is the constant
defined by \eqref{c1:eq1.36}.

We proceed now to establish some general properties of $E_k (T)$,
defined by \eqref{c4:eq4.9}. We have $E_k \in C^\infty (O, \infty)$
with 
\begin{align*}
  E'_k (T) & = \left|\zeta \left(\frac{1}{2} + iT \right)\right|^{2k} -
  \left(P_{k^2}(\log T) + P'_{k^2} (\log T)\right)\\
  & = Z^{2k}(T) - \left(P_{k^2}(\log T) + P'_{k^2} (\log T)\right),
\end{align*}
where as usual
\begin{align*}
  Z(t) & = \chi^{-1/2} \left(\frac{1}{2} + it \right) \zeta
  \left(\frac{1}{2} + it \right),\\
  \chi (s) & = \frac{\zeta(s)}{\zeta (1-s)} = 2^s \pi^{s-1} \Gamma
  (1-s) \sin \left(\frac{\pi s}{2} \right).
\end{align*}

For $r \geq 2$ we obtain
$$
E_k^{(r)} (T) = (Z^{2k}(T))^{(r-1)} + O_{r, \epsilon} (T^{\epsilon-1}),
$$
where the $(r-1)^{\rm st}$ derivative can be easily found by an application
of Leibniz's rule and the Riemann-Siegel formula for $Z(T)$. 

One can use $E_k (T)$ to obtain bounds for $\zeta \left(\frac{1}{2} +
it \right)$. Using Theorem \ref{c1:thm1.2} with $\delta=1$ we obtain 
\begin{align*}
  \left|\zeta \left(\frac{1}{2}+ iT \right)\right|^{2k} & \ll \log T \left( 1+
  \int\limits_{T-1}^{T+1} \left|\zeta \left(\frac{1}{2} + it
  \right)\right|^{2k}dt\right)\\
  & = \log T \left\{ 1+ \left(t P_{k^2} (\log t)\right)\Bigg|_{T-1}^{T+1}+ E_k
  (T+1) - E_k (T-1)\right\}.
\end{align*}

Therefore\pageoriginale we obtain
\begin{lemma}\label{c4:lem4.1}
  For $k \geq 1$ a fixed integer we have
  \begin{equation}
    \zeta \left(\frac{1}{2} + iT\right) \ll (\log T)^{(k^2 +1)/(2k)}+
    \left(\log T \max_{t \in [T - 1, T+1]}|E_k (t) |
    \right)^{1/2k}.\label{c4:eq4.13} 
  \end{equation}
\end{lemma}

Note that Lemma \ref{c4:lem4.1} in conjunction with \eqref{c4:eq4.6}
gives 
\begin{equation}
  \zeta \left(\frac{1}{2} + iT \right) \ll T^{1/6+
    \epsilon}, \label{c4:eq4.14} 
\end{equation}
which is essentially the classical bound of Hardy-Littlewood.

The function $E_k(T)$ can grow fast (near the points where $\zeta
\left(\frac{1}{2}+ iT \right)$ is large), but it can decrease only
relatively slowly. Namely, for $0 \leq x \leq T$ one obtains, by
integrating the expression for $E'_k (t)$,
\begin{multline*}
  O \leq \int\limits_T^{T + x} ~\left|\zeta\left(\frac{1}{2} + it
  \right)\right|^{2k} dt = \int\limits_T^{T+x} \left(P_{k^2} (\log t) +
  P'_{k^2} (\log t)\right)dt\\
  + E_k (T + x)- E_k (T) \leq C_k x \log ^{k^2}T + E_k (T+x) - E_k (T)
\end{multline*}
for a suitable constant $C_k > 0$. Thus
$$
E_k (T) \leq E_k (T+ x) + C_k x \log^{k^2}T, 
$$
and integrating this inequality over $x$ from $0$ to $H(0 < H \leq T)$
we obtain 
$$
E_k (T) \leq H^{-1} \int\limits_T^{T+H} E_k (t) dt + C_k H \log^{k^2}T
\quad (0 < H \leq T).
$$

Similarly we obtain a lower bound for $E_k (T)$. The above discussion
is contained in 

\begin{lemma}\label{c4:lem4.2}
  For a suitable $C_k > 0$ and any fixed integer $k \geq 1$
  \begin{eqnarray}
    & E_k (T) \leq E_k (T+ x)+ C_k x \log ^{k^2} T &  (0 \leq x
    \leq T),\label{c4:eq4.15}\\
    & E_k (T)  \geq E_k (T-x) - C_k x \log ^{k^2} T &  (0 \leq x
    \leq T),\label{c4:eq4.16}\\
    & E_k (T)  \leq H^{-1}  \int\limits_T^{T+H} E_k (t) dt + C_k H
    \log ^{k^2} T &  (0 < H \leq T),\label{c4:eq4.17}\\
    & E_k (T)  \geq H^{-1}  \int\limits^T_{T+H} E_k (t) dt -C_k H
    \log ^{k^2} T & (0 < H \leq T).\label{c4:eq4.18}
  \end{eqnarray}
\end{lemma}

The\pageoriginale Lindel\"of hypothesis that $\zeta \left(\frac{1}{2}
+ it \right) \ll t^\epsilon$ has many equivalent formulations. A
simple connection between the Lindel\"of hypothesis and the
differences $E_k (T + H)- E_k (T)$ is contained in 

\begin{thm}\label{c4:thm4.1} 
  Let $k > 1$ be a fixed integer. The Lindel\"of hypothesis is
  equivalent to the statement that
\begin{equation}
E_k (T+H) - E_k (T)  = o  (|H|T^\epsilon) \quad (0 < |H|\leq
 T)\label{c4:eq4.19} 
\end{equation}
or
\begin{equation}
E_k(T)  = H^{-1} \int\limits_T^{T+H} E_k (t) dt + o(|H|
    T^\epsilon) \quad (0 < |H|\leq T).\label{c4:eq4.20}
\end{equation}
\end{thm}

\begin{proof}
  Let the Lindel\"of hypothesis be true. Then $\zeta (\frac{1}{2} +
  it)\ll t^{\epsilon_1}$ for any $\epsilon_1 >0$ and consequently
{\fontsize{10pt}{12pt}\selectfont
  \begin{align*}
    E_k (T+H) - E_k (T) & = \int\limits_T^{T+H} \left|
    \left(\frac{1}{2} + it \right) \right|^{2k} dt -
    \int\limits_T^{T+H} \left(P_{k^2} (\log t) + P'_{k^2} (\log t)\right) dt\\
    & \ll |H| \left(T^{2 \epsilon_1 k} + \log^{k^2} T \right) \ll |H|T^\epsilon
  \end{align*}}
  for $\epsilon = 2 \epsilon_1 k$. Conversely, if \eqref{c4:eq4.19}
  holds, then by following the proof of Lemma \ref{c4:lem4.1} we have
  \begin{align*}
    \left|\zeta\left(\frac{1}{2} + iT \right)\right|^{2k} & \ll \log T
    \left\{1+ t P_{k^2} (\log t) \Bigg|_{T-1}^{T+1} + e_k (T-1) - E_k
    (T-1) \right\}\\
    & \ll (\log T)^{k^2+1} + T^\epsilon \log T \ll T^{2 \epsilon},
  \end{align*}
which\pageoriginale gives $\zeta (\frac{1}{2} + iT) \ll_\epsilon
T^\epsilon$. Further we have
$$
E_k (T) - H^{-1} \int\limits_T^{T+H} E_k (t) dt = H^{-1}
\int\limits_T^{T+H} \left(E_k (T) - E_k (t)\right)dt,
$$
so that \eqref{c4:eq4.19} implies \eqref{c4:eq4.20}. Finally, if
\eqref{c4:eq4.20} holds then we use it once with $H$, and once with
$T$ replaced by $T+ H$ and $H$ by $-H$ to get \eqref{c4:eq4.19}. 
\end{proof}

Concerning the true order of magnitude of $E_k(T)I$ conjecture that
for $k \geq 1$ fixed and any $\epsilon > 0$
\begin{equation}
  E_k (T) = O_\epsilon (T^{1/4 k+\epsilon}) \label{c4:eq4.21}
\end{equation}
and for $1 \leq k \leq 4$
\begin{equation}
  E_k (T) = \Omega (T^{1/4k}),\label{c4:eq4.22}
\end{equation}
the last result being true for $k=1$ and $k=2$ (see Theorem
\ref{c5:thm5.7}). The reason I am imposing the restriction $1 \leq k
\leq 4$ in \eqref{c4:eq4.22} is that if \eqref{c4:eq4.22} holds for
$k=5$, then this would \textit{disprove} the Lindel\"of hypothesis
which trivially implies $E_k (T) \ll_\epsilon T^{1+
  \epsilon}$. Anyway, the result $E_2(T) = \Omega(T^{1/2})$ of Theorem
\ref{c5:thm5.7} clearly shows that it is reasonable to expect that
$E_k(T)$ gets bigger as $k$ increases, and that in some sense the
eighth moment will be a ``turning point'' in the theory of $I_k(T)$.

\section{The Approximate Functional Equation}\label{c4:sec4.2}

We pass now to the derivation of an approximate functional equation
for $\zeta^k(s)$ that will prove to be very useful in the evaluation
of $I_k(T)$. There are many approximate functional equations in the
literature. For example, one has
{\fontsize{10pt}{12pt}\selectfont
\begin{equation}
  \zeta (1/2 + it)= \sum_{n \leq (t/2 \pi)^{1/2}} n^{-1/2 -it}
  + \chi \left(\frac{1}{2} + it \right) \sum_{n \leq (t/2\pi)^{1/2}}
  n^{-1/2 + it}+ O(t^{-1/4}),\label{c4:eq4.23}
\end{equation}}
which is weakened form of the classical Riemann-Siegel formula, and 
Motohashi's result
$$
\zeta^2 (1/2 + it)= \sum_{n \leq t/2 \pi} d(n) n^{-1/2-it} + \chi^2
\left(\frac{1}{2} + it \right) \sum_{n \leq t/2 \pi} d(n) n^{-1/2 +
  it} + O(t^{-1/6}).
$$

If\pageoriginale we multiply these equations by $\chi^{-1/2}
  \left(\frac{1}{2} + it \right)$ and $\chi^{-1} \left(\frac{1}{2} +
  it \right)$, respectively, we see that they have s symmetric form,
  in the sense that one main term in each of them is the conjugate of
  the other main term. However, the error term in \eqref{c4:eq4.23} is
  best possible, and the other error term is also too large for the
  evaluation of the general $I_k (T)$. What we seek is a smoothed
  approximate functional equation, which is symmetric like
  \eqref{c4:eq4.23}, but contains a fairly small error term. Such an
  expression can be squared and integrated termwise, with the aim of
  evaluating the integral $I_k(T)$. Before we formulate our main
  result, we shall prove a lemma which guarantees the existence of the
  type of smoothing function that we need. This is 

\begin{lemma}\label{c4:lem4.3}
  Let $b >1$ be a fixed constant. There exists a real-valued function
  $\rho (x)$ such that 
  \begin{enumerate}[\rm (i)]
    \item $\rho(x) \in C^\infty (0, \infty)$, 

      \item $\rho (x) + \rho \left(\frac{1}{x}\right) =1$ for $x>0$,

        \item $\rho (x) =0$ for $x \geq b$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Let us define for $\alpha > \beta > 0$
  $$
  \varphi (t) = \exp\left((t^2 -\beta^2)^{-1} \right)
  \left(\int\limits_{-\beta}^\beta \exp\left((u^2 -\beta^2)^{-1}\right)du\right)^{-1}
  $$
  if $|t|< \beta$, and put $\varphi(t) =0$ if $|t|\geq \beta$, and let
  $$
  f(x) : = \int\limits_{x- \alpha}^{x + \alpha} \varphi (t) dt =
  \int\limits_{-\infty}^{x} \left(\varphi (t+ \alpha)- \varphi (t-\alpha)\right)dt.
  $$

Then $\varphi (t) \in C (- \infty, \infty)$, $\varphi(t) \geq 0$ for
all $t$, and from the definition of $\varphi$ and $f$ it follows that
$f(x) \in C^\infty (-\infty, \infty)$, $f(x) \geq 0$ for all $x$ and 
$$
f(x)= \begin{cases}
  0 & \text{if}~ |x|\geq \alpha+ \beta,\\
  1 & \text{if}~ |x| < \alpha - \beta.
\end{cases}
$$

Moreover\pageoriginale if $\varphi(t)$ is even, then $f(x)$ is also
even. Now choose $\alpha= \frac{1}{2} (1+b)$, $\beta = \frac{1}{2}
(b-1)$. then 
$$
f(x)= \begin{cases}
  0 & \text{if}~ x \geq b,\\
  1 & \text{if}~ 0 \leq x \leq 1.
\end{cases}
$$

Set
\begin{equation}
  \rho (x) : = \frac{1}{2} \left(1+ f(x)- f\left(\frac{1}{x}
  \right) \right). \label{c4:eq4.24} 
\end{equation}

The property $\rho (x) \in C^\infty (O, \infty)$, which is i) of Lemma
\eqref{c4:eq4.3}, is obvious. 

Next 
$$
\rho (x) + \rho \left( \frac{1}{x}\right)= \frac{1}{2} \left(1+ f(x) -
f\left(\frac{1}{x} \right)\right) + \frac{1}{2} \left(1+ f\left(\frac{1}{x}
\right)- f(x) \right)=1,
$$ 
which establishes ii) of the lemma. Lastly, if $x \geq b$,
$$
\rho (x) = \frac{1}{2} \left(1- f\left(\frac{1}{x} \right) \right)=0
$$
since $1/x\leq 1/b< 1$ and $f(x) =1$ for $0 \leq x \leq 1$. Thus
$\rho(x)$, defined by \eqref{c4:eq4.24}, satisfies the desired
requirements i) - iii), and the lemma is proved.
\end{proof}

The main result in this section is 
\begin{thm}\label{c4:thm4.2}
  For $1/2\leq \sigma < 1$ fixed $1 \ll x, y \ll t^k$, $s= \sigma +
  it$, $xy = \left( \frac{t}{2 \pi}\right)^k$, $t \geq t_0$ and $k
  \geq 1$ a fixed integer we have
  \begin{align}
    \zeta^k (s) & = \sum_{n=1}^\infty \rho \left(\frac{n}{x} \right)
    d_k (n) n^{-s} + \chi^k(s) \sum_{n=1}^\infty \rho
    \left(\frac{n}{y} \right) d_k (n) n^{s-1}\label{c4:eq4.25}\\
    &\quad+ O \left(t^{\frac{k(1-\sigma)}{3}-1} \right)+
    O\left(t^{k(1/2-\sigma)-2} y^\sigma \log^{k-1}t\right),\notag
  \end{align}
  where $\rho(x)$ is a function satisfying the conditions of Lemma
  \eqref{c4:eq4.3}. furthermore, if $c> 1$ is a fixed constant, then
  \begin{align}
    \zeta^k (s) & = \sum_{n=1}^\infty \rho \left(\frac{n}{x}
    \right)d_k (n) n^{-s} + \sum_{n=1}^\infty \rho
    \left(\frac{x}{n}\right)\rho \left(\frac{n}{cx} \right)d_k (n)
    n^{-s}\label{c4:eq4.26}\\
    &\quad + \chi^k (s) \sum_{n=1}^\infty \rho \left( \frac{n}{y}\right)
    \rho \left(\frac{nc}{y} \right)d_k (n) n^{s-1} + O
    \left(t^{\frac{k (1- \sigma)}{3} -1} \right)\notag \\
    &\quad + O \left(t^{k (1/2 -\sigma)^{-2}} y^\sigma \log^{k-1}t \right).\notag
  \end{align}
\end{thm}

\begin{proof}
  Note\pageoriginale that in the most important special case when $s=
  \frac{1}{2} + it$, $x= y= \left(\frac{t}{2 \pi} \right)^{1/2k}$ it
  follows that 
  \begin{align}
    \zeta^k (s) & = \sum_{n=1}^\infty \rho \left(\frac{n}{x}
    \right)d_k (n) n^{-s} + \chi^k (s) \sum_{n=1}^\infty \rho
    \left(\frac{n}{x} \right) d_k (n) n^{s-1}\label{c4:eq4.27}\\
    & \quad+ O\left(t^{k/6-1}+ t^{1/4 k-2} \log^{k-1}t\right),\notag \\ 
    \zeta^k (s) & = \sum_{n=1}^{\infty} \rho
    \left(\frac{n}{x} \right)d_k (n) n^{-s} + \sum_{n=1}^\infty \rho
    \left(\frac{x}{n} \right) \rho \left(\frac{n}{cx}\right) d_k (n)
    n^{-s} + \chi^k (s)\label{c4:eq4.28}\\
    &\quad \sum_{n=1}^\infty \rho\left(\frac{n}{x}
    \right) \rho \left(\frac{nc}{x}\right)d_k (n) n^{s-1} +
    O\left(t^{k/6-1}+ t^{1/4 k-2} \log^{k-1}t\right).\notag
  \end{align}
  Thus \eqref{c4:eq4.27} for $k=1$ is a smoothed variant of
  \eqref{c4:eq4.23} (since all the series are in fact finite sums),
  only it contains an error term for $k=1$ which is only $O(t^{5/6})$.


Our method of proof is based on the use of Mellin transforms.\break Namely,
let
\begin{equation}
  R(s) : = \int\limits_0^\infty \rho (x) x^{s-1} dx \label{c4:eq4.29}
\end{equation}
denote the Mellin transform of $\rho (x)$. This is a regular function
for $\re s> 0$, but it has analytic continuation to the whole complex
plane. Its only singularity is a simple pole at $s=0$ with residue
1. For $\re s>0$
$$
R(s) = \int\limits_0^b \rho(x) x^{s-1}dx = \frac{x^2}{s} \rho (x)
\Bigg|_0^b- \int\limits_0^b \frac{x^s}{s} \rho' (x) dx = \frac{1}{s}
\int\limits_0^\infty \rho' (x) x^s dx,
$$
and the last integral is an analytic function for $\re s > -1$. Since
$$
- \int\limits_0^b \rho' (x) dx =1,
$$
the residue of $R(s)$ at $s=0$ equals 1. In general, repeated
integration by parts gives, for $N \geq 0$ an integer
\begin{equation}
  R(s) = \frac{(-1)^{N+1}}{s(s+1) \ldots (s+N-1)(s+N)}\int\limits_0^b
  \rho^{(N+1)}(x) x^{s+N} dx.\label{c4:eq4.30} 
\end{equation}

Taking\pageoriginale $N$ sufficiently large it is seen that \eqref{c4:eq4.30}
provides analytic continuation of $R(s)$ to the whole complex
plane. For any $N$, and $\sigma$ in a fixed strip $(s = \sigma + it)$
\begin{equation}
  R(s) \ll_N |t|^{-N} \quad (|t|\to \infty).\label{c4:eq4.31}
\end{equation}

In fact, \eqref{c4:eq4.30} shows that $R(s)$ is regular at all points
except $s=0$, since for $N \geq 1$
$$
\int\limits_0^b \rho^{(N+1)}(x) dx= \rho^{(N)}(b) - \rho^{(N)}(0) =0.
$$

For $-1 < \re s < 1$ and $s \neq 0$ we have
$$
R(s) =- \frac{1}{s} \int\limits_0^b \rho'(x) x^s dx = -\frac{1}{s}
\int\limits_0^\infty \rho' (x) x^s dx = -\frac{1}{s}
\int\limits_0^\infty \rho' \left(\frac{1}{t}\right)t^{-s-2}dt
$$
after change of variable $x=1/t$. But
$$
\rho(t) + \rho \left(\frac{1}{t} \right) = 1,\quad \rho' (t) - t^{-2} \rho'
\left(\frac{1}{t} \right)=0,\quad \rho' \left(\frac{1}{t} \right) = t^2
\rho' (t).
$$

Hence 
$$
R(-s) = \frac{1}{s} \int\limits_O^\infty \rho' \left(\frac{1}{t}
\right) t^{s-2} dt = \frac{1}{s} \int\limits_0^\infty \rho' (t) t^s dt
= -R(s)
$$
for $-1 < \re s< 1$, and then by analytic continuation for all $s$
\begin{equation}
  R(-s) = -R (s) \label{c4:eq4.32}
\end{equation}

Conversely, from \eqref{c4:eq4.32} by the inverse Mellin transform
formula we can deduce the functional equation $\rho (x) + \rho
(1/x)=1$. The fact that $R(s)$ is an odd function plays an important
role in the proof of Theorem \ref{c4:thm4.2}. 
\end{proof}


Having established the necessary analytic properties of $R(s)$ we pass
to the proof, supposing $s= \sigma+ it$, $1/2 \leq \sigma < 1$, $t
\geq t_0$, $d = \re z > 1- \sigma$. Then for any integer $n \geq 1$ we
have from \eqref{c4:eq4.29}, by the inversion formula for Mellin
transforms
$$
\rho \left( \frac{n}{x}\right)= \frac{1}{2 \pi i} \int\limits_{d- i
  \infty}^{d+ i \infty} R(z) \left(\frac{x}{n} \right)^z dz.
$$ 

By the absolute convergence of the series for $\zeta^k(s)$ this gives
$$
\sum_{n=1}^\infty \rho\left(\frac{n}{x} \right)d_k (n) n^{-s} =
\frac{1}{2\pi i} \int\limits_{d- i \infty}^{d+ i \infty} R(z) x^z
\zeta^k (s+ z)dz.
$$

We\pageoriginale shift the line of integration to $\re z =- d$, passing the poles of
the integrand at $z=1-s$ and $z=0$. Using \eqref{c4:eq4.31} it is seen
that the first residue is $O(t^{-A})$ for any fixed $A >0$. Hence by
the residue theorem and the functional equation $\zeta (w) = \chi (w)
\zeta (1-w)$ we have
\begin{align*}
  & \sum_{n=1}^\infty \rho\left(\frac{n}{x} \right)d_k (n) n^{-s} =
  \zeta^k (s) + O(t^{-A}) + \frac{1}{2 \pi i} \int\limits_{-d+i
    \infty}^{-d - i \infty} R(z) x^{z} \zeta^k (s+z) dz\\
  & = \zeta^k (s) + O(t^{-A}) + \frac{1}{2 \pi i} \int\limits_{-d + i
    \infty}^{-d - i \infty} R(z) x^{z}\chi^k (s+z) \zeta^k (1- s
  -z)dz\\
  & = \zeta^k (s) + O(t^{-A}) - \frac{1}{2 \pi i} \int\limits_{d+ i
    \infty}^{d- i \infty} R(-w) x^{-w} \chi^k (s-w) \zeta^k (1-
  s+w)dw\\
 & = \zeta^k (s) + O(t^{-A})- \frac{1}{2 \pi i} \int\limits_{d- i
    \infty}^{d+ i \infty} R(w) y^w T^{-w} \chi^k (s-w) \zeta^k (1-
  s+w) dw.
\end{align*}

Here we supposed $1 \ll x, y \ll t^k$ and $xy =T$, where
\begin{equation}
  \log T = - k \frac{\chi'(1/2+ it)}{\chi(1/2+ it)}.\label{c4:eq4.33} 
\end{equation}

Since 
\begin{equation}
  \chi(s) = \frac{\zeta(s)}{\zeta(1-s)} = \pi^{s- \frac{1}{2}}
  \frac{\Gamma (1/2 -1/2s)}{\Gamma (1/2 s)},\label{c4:eq4.34}
\end{equation}
logarithmic differentiation and Stirling's formula for the gamma
function give
\begin{equation}
  \frac{\chi' (1/2 + it)}{\chi(1/2 + it)}=- \log t + \log (2 \pi) +
  O(t^{-2}).\label{c4:eq4.35} 
\end{equation}

Hence 
\begin{equation}
  T= \left(\frac{t}{2 \pi}\right)^k (1+ O (t^{-2})).\label{c4:eq4.36} 
\end{equation}

In fact, \eqref{c4:eq4.34} gives
$$  
\frac{\chi' (s)}{\chi(s)} = \log \pi  - \frac{\Gamma' (\frac{1}{2}-
  \frac{1}{2} s)}{2 \Gamma (\frac{1}{2} - \frac{1}{2} s)} -
\frac{\Gamma' (\frac{1}{2}s)}{2 \Gamma (\frac{1}{2}s)},
$$
and using 
$$ 
\frac{\Gamma'(s)}{\Gamma (s)} = \log s - \frac{1}{2s} + O
\left(\frac{1}{t^2}\right),
$$ 
which\pageoriginale is just a first approximation to a full asymptotic expansion of
Stirling, it follows that
$$ 
\frac{\chi' (s)}{\chi(s)} = \log (2 \pi) + \frac{1}{s(1-s)} -\frac{1}{2} \log s (1-s) + 
O   \left(\frac{1}{2} \right).
$$

If $s= \frac{1}{2} + it$, then 
$$
\frac{1}{2} \log s(1-s)= \frac{1}{2} \log \left(\frac{1}{4} + t^2
\right) = \log t + O \left(\frac{1}{t^2} \right)
$$
and \eqref{c4:eq4.35} follows. For $s= \sigma + it$ we obtain
similarly
\begin{equation}
  \frac{\chi'(\sigma+ it)}{\chi (\sigma + it)} =- \log t + \log (2 \pi)
  + O\left( \frac{1}{t}\right).\label{c4:eq4.37}
\end{equation}

We shall derive first the approximate functional equation with $x$,
$y$ satisfying $xy=T$, and then replace this by the condition $xy =
\left(\frac{t}{2 \pi} \right)^k$, estimating the error term
trivially. This is the method due originally to Hardy and Littlewood,
who used it to derive the classical approximate functional equations
for $\zeta (s)$ and $\zeta^2 (s)$. Therefore so far we have shown that
\begin{align}
  \zeta^k (s) & = \sum_{n=1}^\infty \rho\left(\frac{n}{x} \right)d_k (n)
  n^{-s} + O(t^{-A})\notag\\ 
  & \quad+ \frac{1}{2 \pi i} \int\limits_{d- i \infty}^{d+
  i \infty} R(w) \left( \frac{y}{T}\right)^w \chi^k (s-w) \zeta^k (1-
  s+w)dw.\label{c4:eq4.38}
\end{align}

We choose $d$ to satisfy $d> \sigma$, so that the series for $\zeta^k$
in \eqref{c4:eq4.38} is absolutely convergent and may be integrated
termwise. Since $R(w) \ll |v|^{-A}$ for any fixed $A> 0$ if $v= \im
w$, it is seen that the portion of the integral in \eqref{c4:eq4.38}
for which $|v|\geq t^\epsilon$ makes a negligible
contribution. Suppose now $N$ is a (large) fixed integer. For $|v|\leq
t^\epsilon$
{\fontsize{10}{12}\selectfont
\begin{multline*}
  T^{-w} \chi^k (s -w)= \\
  \exp \left\{kw \frac{\chi' (1/2 + it)}{\chi (1/2 + it)} + k \log \chi(s)-  kw
  \frac{\chi' (s)}{\chi(s)} + k \sum_{j=2}^N \frac{(-w)^j}{j!}
  \frac{d^j}{ds^j} (\log \chi (s)) \right\}\\ 
  (1+ O (t^{\epsilon -N}))
\end{multline*}}
because $\frac{d^j}{ds^j} (\log \chi (s)) \ll_j t^{- j+1}$ for $j \geq
2$. Using \eqref{c4:eq4.35} and \eqref{c4:eq4.37}\pageoriginale we
infer that 
$$
\exp \{ kw\ldots \} = \chi^k (s) (1+ G(w, s))
$$
with $G(w,s) \ll t^{\epsilon -1}$ for $|v|\leq t^\epsilon$. For
$\delta > 0$ and $N$ sufficiently large we obtain
\begin{align*}
&  \frac{1}{2 \pi i} \int\limits_{d- i \infty}^{d+ i \infty} R(w) y^{w}
  T^{-w} \chi^k (s- w)\zeta^k (1- s+w)dw\\ 
& = O(t^{-A})
 + \frac{1}{2 \pi i} \int\limits_{d- i \infty}^{d+ i \infty} R(w)y^w
  \chi^k (s) \zeta^k (1- s+w)dw\\ 
&\quad  + \frac{1}{2 \pi i}
  \int\limits_{\delta- i \infty}^{\delta _ i \infty} R(w) y^w \chi^k
  (s) \zeta^k (1- s+ w) G(w, s) dw.
\end{align*}

We have, since $d> \sigma$,
\begin{align*}
  &\frac{1}{2 \pi i}  \int\limits_{d- i \infty}^{d+ i \infty} R(w) y^w
  \chi^k (s) \zeta^k (1- s+ w)dw\\
  &\quad = \chi^k (s) \sum_{n=1}^\infty d_k (n) n^{s-1} \left\{\frac{1}{2
    \pi i} \int\limits_{d- i \infty}^{d+ i \infty} R(w)
  \left(\frac{n}{y} \right)^{-w} dw \right\}\\
  &\quad = \chi^k (s) \sum_{n=1}^\infty d_k (n) \rho \left(\frac{y}{n}
  \right) n^{s-1}.
\end{align*}

For $\delta > 0$ sufficiently small
{\fontsize{10pt}{12pt}\selectfont
\begin{align}
  & \frac{1}{2 \pi i} \int\limits_{\delta -i \infty}^{\delta+ i \infty}
  R(w) y^w \chi^k (s) \zeta^k (1- s+ w) G (w, s)dw \label{c4:eq4.39}\\
  & \ll t^{-A} + \left| \int\limits_{\; \; |\im w |\leq t^\epsilon, \re w =
    \delta} R(w)y^w \zeta^k (s-w) (\chi^k (s) \chi^k(1- s+w)) G(w,s)
  dw\right|\notag\\
  & \ll t^{-A} + t^{2 \epsilon-1} \int\limits_{-t^\epsilon}^{t^\epsilon}
  |\zeta (\sigma + it - \delta + iv)|^k dv \ll t^{-A} + t^{k \mu
    (\sigma) + \epsilon_1 -1}\notag
\end{align}}
where $\epsilon_1 \to 0$ as $\epsilon \to 0$, and where as in Chapter
\ref{c1} we set 
$$
\mu (\sigma) = \limsup_{t \to \infty} \frac{\log |\zeta (\sigma+
  it)|}{\log t}.
$$

For\pageoriginale our purposes the standard bound
\begin{equation}
  \mu (\sigma) < \frac{1- \sigma}{3} \quad \left(\frac{1}{2} \leq
  \sigma < 1 \right)\label{c4:eq4.40}
\end{equation}
will suffice. This comes from $\mu \left(\frac{1}{2} \right) <
\frac{1}{6}$, $\mu (1) =0$ and convexity, and the use of the recent
bounds for $\mu(1/2)$ (see \eqref{c1:eq1.28}) would lead to small
improvements. Thus, if $\epsilon = \epsilon (k)$ is sufficiently small
{\fontsize{10pt}{12pt}\selectfont
\begin{equation}
  \zeta^k (s) = \sum_{n=1}^\infty \rho \left(\frac{n}{x} \right)d_k
  (n) n^{-s} + \chi^k (s) \sum_{n=1}^\infty \rho \left(\frac{n}{y}\right) d_k
  (n) n^{s-1} + O \left(t^{\frac{k(1- \sigma)}{3}-1}\right)\label{c4:eq4.41}
\end{equation}}

It is the use of \eqref{c4:eq4.40} that required the range
$\frac{1}{2} \leq \sigma < 1$. We could have considered also the range
$0 < \sigma \leq \frac{1}{2}$. The analysis is, of course, quite
similar but instead of \eqref{c4:eq4.40} we would use
$$
\mu (\sigma) \leq \frac{1}{2} - \frac{2 \sigma}{3} \quad (0\leq \sigma
\leq \frac{1}{2}).
$$

This means that we would have obtained \eqref{c4:eq4.41} with
$O(t^{k(1- \sigma)/3-1})$ replaced by $O\left(t^{k \left(\frac{1}{2} -
  \frac{2 \sigma}{3}\right)+ \epsilon-1}\right)$. As the final step of
the proof of \eqref{c4:eq4.25} we replace $y= Tx^{-1}$ by $Y$, where
$Y= x^{-1} \left(\frac{t}{2\pi} \right)^k$. Then, for $n \ll y$, we
have
\begin{align*}
 & Y - y  = O(t^{k-2} x^{-1}),\\
&  \rho \left(\frac{n}{y} \right) - \rho \left(\frac{n}{Y} \right) 
  \ll \frac{|Y-y|n}{y^2} \ll t^{k-2} x^{-1} y^{-1} \ll t^{-2}.
\end{align*}

Since $\rho \left(\frac{n}{y} \right)=0$ for $n \geq$ by this means
that if in \eqref{c4:eq4.41} we replace $y$ by $Y$ the total error is 
$$
\ll t^{k (1/2 - \sigma)} \sum_{n \leq by} t^{-2} d_k (n) n^{\sigma-1}
\ll t^{t(1/2 - \sigma)- 2}Y^\sigma \log^{k-1}t.
$$

Writing then $y$ for $Y$ we obtain \eqref{c4:eq4.25}.

To prove \eqref{c4:eq4.26} we proved analogously, taking $\alpha,
\beta > 0$, $\beta > 1 - \sigma + \alpha$. Then for $c> 1$ fixed and
any fixed $A> 0$
{\fontsize{9}{11}\selectfont
\begin{align*}
  &\sum_{n=1}^\infty d_k (n) \rho \left(\frac{x}{n} \right) \rho
  \left(\frac{n}{cx} \right)n^{-s}
   = \frac{1}{(2 \pi i)^2} \int\limits^{\alpha + i \infty}_{\alpha - i \infty} \int\limits_{\beta - i \infty}^{\beta + i
    \infty} R(w) R(z) c^z x^{z-w} \zeta^k (s + z - w)dz dw\\
 & = \frac{1}{2 \pi i} \int\limits_{\alpha- i \infty}^{\alpha+ i
    \infty} R(w)x^{-w} \left\{\zeta^k (s-w) + O(t^{-A})+ \frac{1}{2
    \pi i} \int\limits_{- \beta - i \infty}^{-\beta + i \infty} x^z
  c^z R(z) \zeta^k (s+ z-w)dz \right\}dw\\
 & = \frac{1}{2 \pi i} \int\limits_{\alpha - i \infty}^{\alpha + i
    \infty} R(w) x^{-w} \chi^k (s- w) \zeta^k (1- s+ w)dw +
  O(t^{-A})\\
  & - \frac{1}{(2 \pi i)^2} \int\limits_{\alpha - i \infty}^{\alpha +
    i\infty} \int\limits_{\beta + i\infty}^{\beta - i \infty} R(w)
  R(-z) x^{- w-z}c^{-z} \chi^k (s- z -w) \zeta^k (1-s + z+w) dz dw,
\end{align*}}
where\pageoriginale we used the residue theorem and the functional equation for
$\zeta (s)$. Now we treat the above integrals as in the previous case,
using $R(-z)= - R (z)$, choosing suitably $\alpha$ and $\beta$ and
integrating termwise the series for $\zeta^k$. We also replace $\chi^k
(s-w)$ and $\chi^k (s- z -w)$ by $\chi^k (s)$ plus an error term, as
was done before. In this way we obtain 
\begin{align}
  & \sum_{n=1}^\infty d_k (n) \rho \left(\frac{x}{n}\right) \rho
  \left(\frac{n}{cx}  \right) n^{-s}\label{c4:eq4.42}\\
  & = \chi^k (s) \sum_{n=1}^\infty d_k (n) \rho \left(\frac{n}{y}
  \right)n^{s-1} - \chi^k (s) \sum_{n=1}^\infty d_k (n) \rho
  \left(\frac{n}{y} \right) \rho \left(\frac{nc}{y} \right) n^{s-1}\notag\\
  &\quad + O \left(t^{\frac{k(1-\sigma)}{3}-1} \right) + O\left(t^{k(\frac{1}{2} - \sigma) - 2} y^{\sigma} \log^{k-1} t\right) \notag
\end{align}
if $1 \ll x, y \ll t^k$, $xy = \left(\frac{t}{2 \pi}\right)^k$, $s =
\sigma + it$, $\frac{1}{2} \leq \sigma <1$. Combining
\eqref{c4:eq4.42} with \eqref{c4:eq4.25} we obtain \eqref{c4:eq4.26},
so that Theorem \ref{c4:thm4.2} is completely proved. 

\section{Evaluation of $I_k(T)$}\label{c4:sec4.3}

In this section we proved to evaluate asymptotically $I_k (T)$,
defined by \eqref{c4:eq4.1}, when $k \geq 1$ is a fixed integer. The
approach is in principle general, but the error terms that will be
obtained can\pageoriginale be $O(T^{1+ \epsilon})$ only for $k \leq
4$. Thus a weakened form of the eighth moment is in fact the limit of
the method. Of course, we still seem to be far from proving $I_4
(T)\ll T^{1+ \epsilon}$ (the proof of N.V. Kuznetsov \cite{Kuznetsov5} is not
complete). On the other hand our method, which is a variation of the
method used by N. Zavorotnyi to evaluate $I_2 (T)$, leaves hope for
evaluating successfully $I_3 (T)$. To follow Zavorotnyi more closely
we make a formal change of notation in Theorem \ref{c4:thm4.2} by
setting
$$
\nu (x) : = \rho \left(\frac{1}{x} \right),
$$
where $\rho$ is the function explicitly constructed in Lemma
\ref{c4:lem4.3}. Then $\nu (x) \in C^\infty (O, \infty)$, $\nu (x) +
\nu \left(\frac{1}{x} \right)=1$, $\nu (x) =0$ for $x \leq 1/b$, $\nu
(x)=1$ for $x \geq b$ for a fixed $b > 1$, $\nu (x)$ is monotonically
increasing in $(b^{-1} , b)$ and $\nu'$, $\nu''$ are piecewise
monotonic. In Theorem \ref{c4:thm4.2} we set $s= \frac{1}{2} + it$,
$x= y = \left(\frac{t}{2 \pi} \right)^k$, obtaining
\begin{align}
  \zeta^k(s)  &= \sum_{n=1}^\infty d_k (n) \nu \left(\frac{x}{n}\right)n^{-s} +
  \sum_{n=1}^\infty d_k (n) \nu \left(\frac{x}{n} \right)n^{s-1} + O
  (R_k (t)),\label{c4:eq4.43}\\
   \zeta^k (s)  &= O(R_k (t))+\sum_{n-1}^\infty d_k (n) \nu
  \left(\frac{x}{n}\right) n^{-s}\label{c4:eq4.44}\\
  & \quad + \sum_{n=1}^\infty d_k (n) \nu
  \left(\frac{n}{k}\right) \nu \left(\frac{cx}{n} \right) n^{-s}  + \chi^k
  (s) \sum_{n=1}^\infty d_k (n) \nu \left( \frac{x}{n}\right) \nu \left(
  \frac{x}{cn}\right) n^{s-1},\notag
\end{align}
where
\begin{equation}
 R_k (t)  = t^{\frac{k}{6}-1} + t^{\frac{k}{4}-2} \log^{k-1}
  t.\label{c4:eq4.45} 
\end{equation}

The case $k=2$ of these equations was used by Zavorotnyi, who had
however the weaker error term $R_2 (t) = t^{-1/2} \log t$, which was
still sufficiently small for the evaluation of $I_2 (T)$. 

As is common in analytic number theory, it is often not easy to
evaluate directly an integral, but it is more expedient to consider a
weighted (or smoothed) version of it. This approach was used by
Heath-Brown, Zavorotnyi and Motohashi (see Chapter \ref{c5}) in their
evaluation of $I_2 (T)$, so it is natural that we shall proceed
here\pageoriginale in a similar vein. The smoothing functions used in
the evaluation of $I_k (T)$ are introduced similarly as the function
$\rho$ in Lemma \ref{c4:lem4.3}. Let, for $|t|<1$,
$$
\beta (t) : = \exp \left( \frac{1}{t^2-1}\right)\cdot
\left(\int\limits_{-1}^1 \exp \left(\frac{1}{u^2-1}\right) \right)^{-1}
$$
and $\beta(t)=0$ for $|t|\geq 1$. Define then
$$
\alpha (t) : = \int\limits_{t-1}^{t+1} \beta (u) du.
$$

By construction $\int\limits_{-\infty}^{\infty} \beta (u) du =1$, and
it follows that $\alpha(t) = \alpha (-t)$, $\alpha (t) \in C^\infty
(0, \infty)$, $\alpha(t)=1$ for $|t|\leq 1$, $\alpha(t)=0$ for
$|t|\geq 2$, and $\alpha(t)$ is decreasing for $1 < t < 2$. Henceforth
assume $T$ is large and let the parameter $T_0$ satisfy $T^\epsilon
\ll T_0 \ll T^{1- \epsilon}$, and define
{\fontsize{10pt}{12pt}\selectfont
$$
\ob{f} (t) = \alpha \left(\frac{2 | t - \frac{1}{2} (3T)| + 2 T_0
  -T}{2T_0} \right),\quad \ub{f} (t) = \alpha \left(\frac{2|t - \frac{1}{2}
  (3T)| + 4T_0 -T}{2T_0}\right). 
$$}

The functions $\ob{f}$ and $\ub{f}$ are constructed in such a way
that, for any $t$,
$$
0 \leq \ub{f} (t) \leq X_T (t) \leq \ob{f} (t)
$$
if $\chi_T (t)$ denotes the characteristic function of the interval
$[T, 2T]$. Thus we have
\begin{equation}
  I_{k, \ub{f}}(T) \leq I_k (2T) - I_k (T) \leq I_{k, \ob{f}}
  (T),\label{c4:eq4.46} 
\end{equation}
where
{\fontsize{10pt}{12pt}\selectfont
\begin{equation}
  I_{k, \ub{f}}(T)= \int\limits_0^\infty \ub{f} (t) |\zeta(1/2 +
  it)|^{2k}dt, I_{k, \ob{f}} (T) = \int\limits_0^T \ob{f} (t) |\zeta
  (1/2 + it)|^{2k}dt.\label{c4:eq4.47}
\end{equation}}

Therefore if we can prove
\begin{equation}
  I_{k, f}(T) = t P_{k^2} (\log t)\big|_T^{2T} + O(T^{\gamma_k +
    \epsilon})\label{c4:eq4.48} 
\end{equation}
with a suitable $0 < \gamma_k \leq 1$, where henceforth $f$ will
denote either $\ub{f}$, then in view of \eqref{c4:eq4.46} we obtain 
\begin{equation}
  I_k (T) = \int\limits_0^T |\zeta (1/2 + it)|^{2k} dt = TP_{k^2}(\log
  T) + O(T^{\gamma +k+\epsilon})\label{c4:eq4.49}
\end{equation}
on\pageoriginale replacing $T$ by $T 2^{-j}$ and summing over $j \geq
1$. We can reinterpret our current knowledge on $I_k (T)$ by saying
that
$$ 
\frac{1}{4} \leq \gamma_1 \leq \frac{7}{22} ,\quad \frac{1}{2} \leq
\gamma_2 \leq \frac{2}{3},\quad  \gamma_k \leq 1 +
\frac{k-2}{4}~\text{ for }~ 3 \leq k
\leq 6.
$$

The problem of evaluating $I_k(T)$ is therefore reduced to the
(somewhat less difficult) problem of the evaluation of $I_{k, f} (T)$,
where $f$ is either $\ub{f}$ or $\ob{f}$. For any integer $g \geq 1$
$$
\ob{f}^{(r)} (t) = sgn^r \left(t- \frac{3}{2} T\right)\cdot
\alpha^{(r)} \left(\frac{2 |t - \frac{1}{2} (3T)| + 2 T_0 - T}{2T_0}
\right) T_0^{-r},  
$$
and an analogous formula also holds for $\ub{f}^{(r)}(t)$. This gives
\begin{equation}
  f^{(r)} (t) \ll _r T_0^{-r} \qquad (r= 1, 2,
  \ldots),\label{c4:eq4.50} 
\end{equation}
which is an important property of smoothing functions. It provides the
means for the regulation of the length of exponential sums (Dirichlet
series) that will appear in the sequel. A general disadvantage of this
type of approach is that it is difficult to obtain an explicit formula
for $E_k(T)$ when $k \geq 2$. Hopes for obtaining analogues of
Atkinson's formula for $E_1 (T)= E(T)$ in the case of general
$E_k(T)$, may be unrealistic. Namely, Theorem \ref{c5:thm5.1} shows
that the explicit formula for the integral in question does not
contain any divisor function, but instead it contains quantities from
the spectral theory of automorphic functions. 

%raghu 
Let now
\begin{equation}
  \sum(t) : = \sum_{m=1}^\infty \nu \left(\frac{x}{m}\right) d_k (m) m^{-1/2-
  it}\cdot\left(x = \left(\frac{t}{2 \pi} \right)^{1/2k} \right). \label{c4:eq4.51}
\end{equation}

Then \eqref{c4:eq4.43} yields
$$
\zeta^k (1/2 + it) = \sum(t) + \chi^k (1/2 +it)\ob{\sum}(t) + O(R_k (t)),
$$
hence taking conjugates one obtains
$$
\zeta^k (1/2 -it)= \ob{\sum} (t) \chi^k (1/2-it) \sum (t) + O(R_k (t)).
$$

From\pageoriginale the functional equation one obtains $\chi(s)
\chi(1-s)=1$, hence multiplying the above expressions we obtain 
\begin{align*}
  |\zeta (1/2 + it)|^{2k} &= 2 |\sum (t)|^2 + 2 \re \left(\chi^k (1/2 +
  it)  \ob{\sum}^2 (t)\right)\\
&\quad + O \left(R_k (t) | \sum (t)|\right)+ O(R^2_k (t)).
\end{align*}

Now we multiply the last relation by $f(t)$ and integrate over $t$
from $T/2$ to $(5T)/2$. Since the support of $f$ is contained in
$[T/2, (5T)/2]$ we have
{\fontsize{10}{12}\selectfont
\begin{align*}
\frac{1}{2} I_{k, f}(T) &= \int_0^\infty f(t) |\sum (t)|^2
  dt + \re \left(\int\limits_0^\infty f(t) \chi^k (1/2 + it)
  \ob{\sum}^2 (t) dt\right)\\
&\quad  + O \left(\int\limits_{1/2 T}^{1/2 (5T)} R_k (t) |\sum (t)|dt
  \right) + \left(\int\limits_{1/2 T}^{1/2 (5T)} R_k^2 (t) dt \right).
\end{align*}}

We can proceed now by using
\begin{equation}
  R_k (t) = t^{\frac{k}{6}-1} + t^{\frac{k}{4}-2} \log
  ^{k-1}t\label{c4:eq4.52} 
\end{equation}
and applying the Cauchy-Schwarz inequality to the first $O$-term. We
shall restrict $k$ to $1 \leq k \leq 4$, although we could easily
carry out the subsequent analysis for general $k$. The reason for this
is that in the sequel the error terms for $k \geq 5$ will be greater
than $T^{1+ \eta_k}$ for some $\eta_k> 0$, hence there is no hope of
getting an asymptotic formula for $I_k(T)$ by this method. However,
instead of using the pointwise estimate \eqref{c4:eq4.52}, a sharper
result will be obtained by using \eqref{c4:eq4.39} with $\sigma =
1/2$, namely
\begin{equation}
  R_k (t) \ll t^{1/4 k-2} \log^{k-1} t + t^{2 \epsilon -1}
  \int\limits_{-t^\epsilon}^{t^\epsilon} | \zeta (1/2 + it - \delta +
  iv)|^k dv.\label{c4:eq4.53} 
\end{equation}
we have 
$$
\int\limits_{1/2T}^{1/2 (5T)} |\zeta (1/2 + it)|^k dt \ll T \log^4T
\quad (1 \leq k \leq 4),
$$
and by standard methods (Perron's inversion formula) it follows that
$$
\sum(t) \ll T^{k \mu (1/2)+ \epsilon}
$$

Thus\pageoriginale in view of $\mu (1/2)< 1/6$ we obtain 
\begin{align}
  \frac{1}{2} I_{k, f}(T) & = \int\limits_0^\infty f(t)| \sum (t) |^2
  dt\label{c4:eq4.54} \\
  & + \re \left\{\int\limits_0^\infty f(t) \chi^k \left(1/2 + it \right)
  \bar{\sum}^2 (t) dt \right\} + O (T^{k/6}),\notag
\end{align}
which shows that the error term coming from the error term in our
approximate functional equation for $\zeta^k (s)$ is fairly
small. However, in the sequel we shall encounter larger error terms
which hinder the possibility of obtaining a good asymptotic evaluation
of $I_k (T)$ when $k \geq 5$. In \eqref{c4:eq4.54} write
$$
\chi^k \cdot \ob{\sum}^2 = \left(\chi^k \ob{\sum} \right)\cdot \ob{\sum}
$$
and evaluate the factor in brackets by subtracting \eqref{c4:eq4.44}
from \eqref{c4:eq4.43}. We obtain
\begin{align*}
&  \int\limits_0^\infty f(t) \chi^k (1/2 + it) \ob{\sum}^2 (t) dt
  =\sum_{m, n=1}^\infty d_k (m) d_k (n) (mn)^{-1/2} \\
&   \int\limits_0^\infty f(t) \nu \left(\frac{x}{m} \right) \nu
  \left(\frac{n}{x} \right) \nu \left(\frac{cx}{n} \right)
  \left(\frac{m}{n} \right)^{it} dt + O(T ^{k/6})+ \sum_{m,
    n=1}^\infty d_k (m) d_k (n) (mn)^{-1/2}\\
&   \int\limits_0^\infty f(t) e^{ki (t \log \frac{2\pi}{t} + t +
    \frac{1}{4}\pi)} (1+ g_k (t)) \nu \left(\frac{x}{m} \right) \nu
  \left(\frac{x}{n} \right)\nu \left(\frac{x}{cn} \right) (mn)^{it} dt,
\end{align*}
where $g_k (t) \ll 1/t$. The last bound is a consequence of the
asymptotic expansion
$$
\chi(1/2 +it)=e^{i \left(t \log \frac{2 \pi}{t} + t + \frac{1}{4}\pi
  \right)} \left(1 - \frac{i}{24 t} + \frac{A_2}{t^2} + \ldots + O
\left(\frac{1}{t^N} \right) \right),
$$
which follows, for any fixed integer $N \geq 2$, from Stirling's
formula for the gamma-function. Therefore the method used to estimate
\begin{align}
  \sum_{m, n=1}^\infty d_k (m) d_k (n)(mn)^{-1/2}
  & \int\limits_0^\infty f(t) e^{ki \left(t \log \frac{2 \pi}{t} + t +
    \frac{1}{4} \pi\right)}\label{c4:eq4.55} \\
  &\nu \left(\frac{x}{m} \right) \nu
  \left(\frac{x}{n}  \right) \nu \left(\frac{x}{cn} \right)
  (mn)^{it}dt\notag
\end{align}
will give a bound of a lower order of magnitude (by a factor or $T$)
for the corresponding integral containing $g_k(t)$. In the integral
in \eqref{c4:eq4.55}, which will be denoted by $I(m, n)$, we make the
substitution $t= 2 \pi u(mn)^{1/k}$. \pageoriginale Since $x = (t/2
\pi)^{1/2k}$ we have
\begin{align*}
  I (m, n): & =  2 \pi e^{\frac{k i \pi}{4}} (mn)^{\frac{4}{k}}
  \int\limits_0^\infty  f(2 \pi u (mn)^{\frac{4}{k}}) \nu
  \left(u^{\frac{k}{2}}\sqrt{\frac{n}{m}} \right)\\ 
  & \qquad \nu
  \left(u^{\frac{k}{2}}C^{-1} \sqrt{\frac{m}{n}}\right) e^{2 \pi k i
    (mn)^{1/k} (-u \log u + \mu)}du\\
  & = 2 \pi e^{1/4 k i \pi} (mn)^{1/k} \int\limits_A^B H(t; m, n)e^{2
    \pi ki (mn)(-t \log t + t)}dt 
\end{align*}
with
{\fontsize{9}{11}\selectfont
\begin{align*}
  H (t; m,n) : & = f (2 \pi t (mn)^{1/k}) \nu \left(t^{1/2 k}
  \left(\frac{n}{m}\right)^{1/2}\right) \nu \left(t^{1/2k} \left(\frac{m}{n}
  \right)^{1/2} \right)  \nu \left(t^{1/2k} c^{-1} \left(\frac{m}{n}
  \right)^{1/2} \right),\\
  A: & = \max \left(\frac{T \mp T_0}{2 \pi (mn)^{1/k}},
  \left(\frac{m}{n}\right)^{1/k} b^{-2/k}, \left(
  \frac{c}{b}\right)^{2/k} \left( \frac{n}{m}\right)^{1/k} \right)
  \leq t \leq B: = \frac{2T \pm T_0}{2 \pi (mn)^{1/k}},
\end{align*}}
where upper signs refer to $\ob{f}$, and lower signs to $\ub{f}$. If 
$$
F(t) = - t \log t + t,
$$
then 
$$
|F'(t)| = \log t \geq \log  (1+ \delta) \gg \delta
$$
for a given $O > \delta < 1$ when $\left(\frac{m}{n} \right)^{1/k}
b^{-2/k} \geq 1+ \delta$. On the other hand, if $\left(\frac{m}{n}
\right) b^{-2/k}< 1 + \delta$, then
$$
\left(\frac{c}{b}\right)^{2/k} \left(\frac{n}{m}\right)^{1/k}
>\left(\frac{c}{b} \right)^{2/k_b - 2/k} (1+\delta)^{-1} \geq 1+ \delta
$$
for 
\begin{equation}
  c= b^2 (1+ 2 \delta)^k. \label{c4:eq4.56}
\end{equation}

Thus $|F'(t)|= \log t \gg \delta$ for $A \leq t \leq B$, provided that
\eqref{c4:eq4.56} holds. Since $H(A; m,n)= H(B;m, n)=0$, integration
by parts yields
\begin{align*}
&  \int\limits_A^B H (t; m, n)  e\left(k(mn)^{1/k} F(t)\right) dt\\ 
  &\quad =-
  \frac{(mn)^{-1/k}}{2\pi ki} \int\limits_A^B \frac{H(t; m, n)}{\log
    t} \frac{d}{dt} \left\{e \left(k (mn)^{1/k} F(t)\right) \right\}\\
  &\quad = \frac{(mn)^{-1/k}}{2 \pi ki} \int\limits_A^B \left(\frac{H'(t;
    m,n)}{\log t}- \frac{H (t; m, n)}{t \log^2 t} \right) e\left(k
  (mn)^{1/k} f(t)\right)dt.
\end{align*}

Now we use the standard estimate (see Lemma \ref{c2:lem2.1}) for
exponential integrals\pageoriginale to show that the portion of the
last integral containing $H(t; m, n)$ is $\ll
(mn)^{-2/k_T-1}$. Consider next, in obvious notation,
\begin{align}
H' (t; m, n) &= 2 \pi (mn)^{1/k}f' \left(2\pi (mn)^{1/k}t\right)
  \nu(\cdot) \nu (\cdot) \nu (\cdot)\notag\\
&\quad + \sum f(\cdot) \frac{d \nu
    (\cdot)}{dt} \nu (\cdot) \nu (\cdot).  \label{c4:eq4.57} 
\end{align}

One has $\nu'(x) =0$ for $x \leq 1/b$ or $x \geq b$, so that the terms
coming from $\sum f(\cdot) \frac{d \nu (\cdot)}{dt} \ldots $ in
\eqref{c4:eq4.57} vanish, because of the conditions 
$$
\frac{1}{b} \leq t^{1/2k} \left(\frac{m}{n} \right)^{1/2} \leq b
~\text{ or }~ \frac{1}{b} \leq t^{1/2k} \left(\frac{n}{m} \right)^{1/2}
\leq b,
$$ 
which cannot hold in view of $t \asymp T$, $1 \leq m, n \ll
T^{1/2k}$. Taking into account that $f' (t) \ll 1/T_0$ and using again
Lemma \eqref{c2:lem2.1} it follows that the expression in
\eqref{c4:eq4.55} equals
\begin{align*}
  \sum_{m, n=1}^{\infty} I(m, n) d_k (n) (mn)^{-1/2} & \ll T_0^{-1}
  \sum_{m \ll T^{1/2k}} \sum_{n \ll T^{1/2k}} d_k (m) d_k (n)
  (mn)^{-1/2}\\
  & \ll T^{1/2k} T_0^{-1} (\log T)^{2k -2}.
\end{align*}

This gives 
\begin{align}
&  \frac{1}{2} I_{k, f}(T)   = \sum_{\substack{m, n=1\\1-\delta \leq
      \frac{m}{n} \leq 1+ \delta}} d_k (m) d_k (n) (mn)^{-1/2} \re
  \left\{\int\limits_O^\infty f(t) \nu \left( \frac{x}{m}\right) \nu
    \left(\frac{x}{n} \right)^{it}dt\right\}\label{c4:eq4.58} \\
&\hspace{1.6cm}    + O(T^{k/6})+ O\left(T^{1/2 k+\epsilon} T_0^{-1}\right) +
    \sum\limits^\infty_{\substack{m, n = 1 \\ 1- \delta \leq \frac{m}{n} \leq 1 +
        \delta}} d_k (m) d_k (n) (mn)^{-1/2}\notag\\
&     \re \left\{\int\limits_0^\infty f(t) \nu \left(\frac{x}{m}
    \right) \nu \left(\frac{n}{x} \right)\nu \left(
    \frac{m}{n}\right)^{it} dt\right\} + \sum\limits^\infty_{\substack{m,
        n=1\\\frac{m}{n} < 1- \delta, \frac{m}{n} > 1+ \delta}} d_k
    (m) d_k (n) (mn)^{-1/2}\notag\\
&    \left[ \re \left\{\int\limits_0^\infty f(t) \left(\nu
      \left(\frac{x}{m} \right) \nu \left(\frac{x}{n} \right)+ \nu
      \left(\frac{x}{m} \right) \nu \left(\frac{n}{x} \right)\nu
      \left(\frac{cx}{n} \right)\right) \left(\frac{m}{n}
      \right)^{it} dt \right\}\right]. \notag
\end{align}

We recall that we have at out disposal the parameters $T_0$ and
$\delta$\pageoriginale such that $T^\epsilon \ll T_0 \ll T^{1-
  \epsilon}$ and $0 < \delta < 1$. In the last sum in
\eqref{c4:eq4.57} we have $\log(m/n)\gg \delta$. We integrate the
integral in that sum by parts, setting
$$
u = f(t), dv = \left(\nu \left(\frac{x}{m} \right) \nu
\left(\frac{x}{n} \right)+ \nu \left(\frac{x}{m}\right) \nu
\left(\frac{n}{x}\right) \nu \left(\frac{cx}{n}\right) \right)
\left(\frac{m}{n}\right)^{it} dt, 
$$   
and after integration we use the second mean value theorem for
integrals to remove the $\nu$-function. Since $f'(t) \ll 1/T_0$, on
applying again Lemma \ref{c2:lem2.1} it is seen that the last sum in
\eqref{c4:eq4.58} is $O(T^{1/2 k+ \epsilon}T^{-1}_0)$.

It remains to simplify the remaining two sums in \eqref{c4:eq4.58},
and it is at this point that we shall make use of the parameter
$\delta$. Namely, it will be shown that for $\delta$ sufficiently
small $\nu \left(\frac{cx}{n} \right)=1$ if \eqref{c4:eq4.56}
holds. This happens for $\frac{cx}{n} \geq b$, or $t \geq 2 \pi
(nb/c)^{2/k}$. The integral in question is non-zero for $x/m\geq 1/b$,
giving $t \geq 2 \pi \left( \frac{m}{b}\right)^{2/k}$. Since in both
sums $m \geq (1- \delta)n$, we have
$$ 
t \geq 2 \pi \left(\frac{m}{b} \right)^{2/k} \geq 2\pi
\left(\frac{n}{b} \right)^{2/k} (1-\delta)^{2/k} \geq 2 \pi
\left(\frac{nb}{c} \right)^{2/k} 
$$ 
for
$$
(1- \delta)c \geq b^2.
$$

In view of \eqref{c4:eq4.56} this condition reduces to showing that
$$
(1+ 2 \delta)^k (1- \delta) \geq 1.
$$

The last inequality is certainly true for $0 < \delta < \frac{1}{2}$,
since $(1+ 2 \delta)^k \geq 1+ 2 \delta$. Therefore, in the second
integral in \eqref{c4:eq4.58}, $\nu \left( \frac{cx}{n}\right)$ may be
omitted for $0 < \delta < \frac{1}{2}$. Using $\nu \left(\frac{x}{n}
\right) + \nu \left( \frac{n}{x}\right)=1$ we finally obtain

\begin{thm}\label{c4:thm4.3}
  for $1 <  k \leq 4$ an integer, $0 < \delta < \frac{1}{2}$,
  $T^\epsilon \ll T_0 \ll T^{1- \epsilon}$ and $x= \left(\frac{t}{2
    \pi} \right)^{1/2k}$ we have
  {\fontsize{10}{12}\selectfont
  \begin{align}
    \frac{1}{2} I_{k, f} (T) & = \sum_{n=1}^\infty d_k^2 (n) n^{-1}
    \re \left\{\int\limits_0^\infty f(t) \nu
    \left(\frac{x}{n}\right)dt \right\} + O(T^{k/6}) + O\left(T^{1/2k +
      \epsilon} T_0^{-1}\right)\label{c4:eq4.59}\\
    &\quad + \sum_{\substack{m, n=1\\m \neq n, 1- \delta \leq \frac{m}{n}
        \leq 1+ \delta}} d_k (m) d_k (n) (mn)^{-1/2} \re
    \left\{\int\limits_0^\infty f(t) \nu \left(\frac{x}{m}
    \right)\left(\frac{m}{n} \right)^{it} dt\right\}.\notag
  \end{align}}
\end{thm}

The\pageoriginale above formulas may be considered as the starting point for the
evaluation of $I_k (T)$. Two sums will emerge, of which the first is
$TR_{k^2} (\log T)$ plus an error term. This will be shown in the next
section, where $R_{k^2}(y)$ is a polynomial of degree $k^2$ in $y$
whose first two coefficients do not depend on $\nu$, but the others
do. The other (double) sum, presumably of order lower by a factor of
$\log^2 T$ than the first one, is much more complicated. This sum
represents the major difficulty in the evaluation of $I_k(T)$, and so
far can be evaluated asymptotically only for $k=1$ and $k=2$. Since
the final (expected) formula \eqref{c4:eq4.9} (with $E_k (T)= o(T)$ as
$T \to \infty$) does not contain in its main term the $\nu$-function,
it is natural to expect that the coefficients in both sums containing
the $\nu$-function will eventually cancel each other. This is because
the $\nu$-function does not pertain intrinsically to the problem of
evaluating asymptotically $I_k (T)$, and can be in fact chosen with a
reasonable degree of arbitrariness.

\section{Evaluation of the First Main Term}\label{c4:sec4.4}

The title of this section refers to the evaluation of the sum 
\begin{equation}
  S_k (T) : = \sum_{n=1}^\infty d_k^2 (n) n^{-1} \re
  \left\{\int\limits_0^\infty f(t) \nu \left(\frac{x}{n} \right)dt
  \right\} \label{c4:eq4.60}
\end{equation}
which figures in \eqref{c4:eq4.59}. This sum can be precisely
evaluated. We shall show that, for $1 \leq k \leq 4$,
\begin{equation}
  S_k (T) = (t Q_{k^2} (\log t))\bigg|_T^{2T} + (tH_{k, \nu} (\log
  t))\bigg|_{T}^{2T} + O(T^\epsilon T_0) + O(T^{\eta_k +
    \epsilon}), \label{c4:eq4.61} 
\end{equation}
where\pageoriginale $\eta_1 = 0$, $\eta_k = \frac{1}{2}$ for $2 \leq k
\leq 4$. In \eqref{c4:eq4.61} $H_{k, \nu} (y)$ is a polynomial of
degree $k^2 -2$ in $y$ whose coefficients depend on $k$ and the
function $\nu$, while $Q_{k^2}(y)$ is a polynomial of degree $k^2$ in
$y$ whose coefficients depend only on $k$, and may be explicitly
evaluated. Before this is done, observe that the error terms
$O(T^\epsilon T_0)$ and $O(T^{1/2 k + \epsilon}T_0^{-1})$ in
\eqref{c4:eq4.59} and \eqref{c4:eq4.61} set the limit to the upper
bound on $E_k (T)$ as 
$$
E_k (T) = O\left(T^{1/4 k+\epsilon}\right)
$$
on choosing $T_0 = T^{1/4k}$, $1 \leq k \leq 4$. This is the
conjectural bound mentioned in \eqref{c4:eq4.21}, and the above
discussion provides some heuristic reasons for its validity. Coupled
with \eqref{c4:eq4.22}, it means that heuristically we have a fairly
good idea of what the true order of magnitude of $E_k(T)$ should be
for $1 \leq k \leq 4$. We are certainly at present far from proving
the above upper bound for any $k$, which is quite strong, since by
Lemma \ref{c4:eq4.1} it implies $\mu(1/2) \leq 1/8$.

To prove \eqref{c4:eq4.61} we define first, for $\re s> 1$ and $k \geq
1$ a fixed integer 
$$
F_k (s) : = \sum_{n=1}^\infty d_k^2 (n) n^{-s}.
$$

Since 
$$
d_k (p^\alpha) = \frac{k(k+1)\ldots (\alpha+ k-1)}{\alpha!} =
\frac{\Gamma (k+ \alpha)}{\alpha! \Gamma (k)},
$$
we have
$$
F_1 (s) = \zeta (s), F_2(s) = \frac{\zeta^4 (s)}{\zeta (2s)},
$$
\begin{align*}
  F_k (s) & = \zeta^{k^2} (s) \prod_p \left(1 - p^{-s}\right)^{k^2}
  \left(1+ k^2 p^{-s} 
    + \frac{k^2 (k+1)^2}{4} p^{-2s}+ \ldots\right)\\
    & = \zeta^{k^2}(s) \prod_p \left( 1- \frac{k^2 (k-1)^2}{4} p^{-2
      s}\right) + O_{k}(p^{3 \sigma}).
\end{align*}

The Dirichlet series defined by the last product converges absolutely
for $\re s \geq \frac{1}{2} + \epsilon$ and any fixed $\epsilon > 0$
(the O -term vanishes for $k=2$), so that the last formula provides
analytic continuation of $F_k (s)$ to $\re s \geq \frac{1}{2} +
\epsilon$. 

Further\pageoriginale we introduce the function
$$
P(s) = \int\limits_0^\infty \nu (x) x^{-s} dx = \frac{1}{s-1}
\int\limits_0^\infty \nu' (x) x^{1-s} dx \ll _A |\im s|^{-A}
$$
for any fixed $A>0$ if $\re$ $s$ lies in any fixed strip, the last bound
being obtained by successive integrations by parts. $P(s)$ is regular
in the complex plane, and has only a simple pole at $s=1$ with residue
$ 1 \left( = \int\limits_0^\infty f(x) x^{s-1}dx\right) $
is the Mellin transform of $f$, then $\hat{f}$ is regular (since the
support of $f$ does not contain zero). By the Mellin inversion formula
$$
f(x) = \frac{1}{2 \pi i} \int\limits_{c- i \infty}^{c+ i \infty}
\hat{f} (s) x^{-s} ds \qquad (c> 0).
$$

In \eqref{c4:eq4.60} we make the substitution $t= 2 \pi (nu)^{2/k}$,
$dt = \frac{4\pi}{k} n ^{2/k}u^{2/k-1}$. Then for $c> 1$
{\fontsize{8}{10}\selectfont
\begin{align}
  S_k (T) & = \frac{4 \pi}{k} \sum_{n=1}^\infty d_k^2 (n) n^{- (1-
    \frac{2}{k})} \re \left\{\int\limits_0^\infty f\left(2 \pi
  (nu)^{2/k}\right)\nu (u) u^{\frac{2}{k}-1} du \right\}\label{c4:eq4.62}\\
  & = \frac{4 \pi}{k} \sum_{n=1}^\infty d_k^2 (n) n^{- \left(1-
    \frac{2}{k}\right)} \re \left\{\frac{1}{2 \pi i} \int\limits_{c- i
  \infty}^{c+ i \infty} \int\limits_0^\infty  \hat{f} (s) \left(2
  \pi(nx)^{2/k} \right)^{-s}  x^{2/x-1} \nu (x) dx ds\right\}\notag\\
  & = \re \left\{\frac{4 \pi}{k} \frac{1}{2 \pi i} \int\limits_{c- i
    \infty}^{c+ i \infty} \hat{f} (s) (2 \pi)^{-s} \left(\int_0^\infty
  \nu (x) x^{2/k (1-s)-1}dx\right) \left(\sum_{n=1}^\infty d_k^2 (n)
  n^{- \left(1+ \frac{2}{k} (s-1) \right)} \right)ds \right\}\notag\\
  & = \re \left\{\frac{2}{k} \cdot \frac{1}{2 \pi i} \int\limits_{c- i
  \infty}^{c+ i \infty} \hat{f} (s) (2 \pi)^{1-s} F_k \left(1+
  \frac{2}{k} (s-1) \right) P \left(1+ \frac{2}{k} (s-1) \right)ds
  \right\},\notag 
\end{align}}
where the interchange of summation  and integration is justified by
absolute convergence. We have
\begin{align}
  \frac{d^r}{ds^r} \hat{f} (s) & = \int\limits_0^\infty f(x) x^{s-1}
  \log^r x \,dx \quad (r= 0,1, 2, \ldots),\notag\\
  \hat{f} (s) & = \int\limits_{T/2}^{5T/2} f(x) x^{s-1} dx \ll
  \int\limits_{T/2}^{5T/2} x^{\sigma-1} dx \ll T^\sigma.\label{c4:eq4.63} 
\end{align}

Moreover,\pageoriginale if $r \geq 1$ is an integer and $\re$ $s$ lies
in a fixed 
strip, then integration by parts and \eqref{c4:eq4.60} give
$$
\hat{f} (s) = \frac{(-1)^r}{s(s+1) \ldots (s+ r-1)}
\int\limits_0^\infty f^{(r)} (x) x^{s+r-1} dx \ll |\im s|^{-r}
T_0^{-r} T^{\sigma+r}. 
$$

In the last integral in \eqref{c4:eq4.62} we move the line of
integration to $\re s = \epsilon$ if $k=1$, and to $\re s =
\frac{1}{2} + \epsilon$ if $k \geq 2$. We encounter the pole of the
integrand of order $k^2 +1$ at $s=1$. By the residue theorem we have
{\fontsize{10}{12}\selectfont
$$
S_k (T) = \frac{2}{k} \re \left\{ \mathop{\res}_{s=1} \hat{f} (s) (2
\pi)^{1-s} F_k \left(1+ \frac{2}{k} (s-1) \right) P\left(1+
\frac{2}{k} (s-1) \right) \right\} + O(T^{\eta_k+\epsilon})
$$}
with $\eta_1=0$ for $k=1$ and $\eta_k = \frac{1}{2}$ for $2 \leq k
\leq 4$. Here we used the properties of $F_k (s)$, \eqref{c4:eq4.63}
and $P(s) \ll_A |\im s|^{-A}$. To evaluate the residue at $s=1$, we
shall use the following power series expansions near $s=1$:
\begin{align}
  (2 \pi)^{1-s} & = 1+ a_1 (s-1)+ a_2 (s-1)^2 + \cdots \left(a_j
  =\frac{(-1)^j(\log 2 \pi)^j}{j!} \right), \notag\\
  F_k (s) & = \frac{d_{-k^2}(k)}{(s-1)^{k^2}} + \cdots +
  \frac{d_{-1}(k)}{s-1} + d_0 (k) + d_1 (k) (s-1) + \cdots , \notag\\
  \hat{f} (s) & = \sum_{j=0}^\infty c_j (s-1)^j, \label{c4:eq4.64}
\end{align}
{\fontsize{9}{11}\selectfont
\begin{align*}
  c_j & = \frac{1}{j!} \hat{f}^{(j)}(1) = \frac{1}{j!}
  \int\limits_0^\infty f(x) \log^j x dx = \frac{1}{j!}
  \left(\int\limits_T^{2T} \log^j x dx + O(T_0 \log^j T) \right)\notag\\
  & = \frac{1}{j!} \left(t \log^j t+ t \sum_{\ell=1}^j (-1)^\ell j
  (j-1) \ldots (j - \ell +1)\log^{j-\ell} t\right) \Bigg|_T^{2T} +
  O(T_0 \log ^j T),\notag\\
  P(s) & = \frac{1}{s-1} + \sum^\infty_{j=0} h_j (\nu) (s-1)^j,\notag
\end{align*}}
with\pageoriginale $h_{2j}(\nu)=0$ for $j= 0, 1, 2, \ldots$. The last
assertion follows because
$$
h_j (\nu) = \frac{(-1)^{j+1}}{(j+1)!} \int\limits_0^\infty \nu' (x)
\log^{j+1} x\, dx,
$$
and using $\nu(x) + \nu \left(\frac{1}{x} \right)=1$ we have $\nu' (x)
- x^{-2} \nu' \left(\frac{1}{x}\right)=0$, which gives 
\begin{align*}
  \int\limits_0^\infty \nu' (x) \log^{2n-1} x \, dx  =
  \int\limits_0^1 \nu' (x) \log^{2n-1} x\, dx + \int\limits_1^\infty
  \nu' (x) \log ^{2n-1} x \, dx\\
   = \int\limits_0^1 \nu' (x) \log^{2n-1} x\, dx - \int\limits_0^1
  \nu' \left(\frac{1}{x}\right) x^{-2} \log^{2n-1} x\, dx = 0
\end{align*}
for $n=1, 2, \ldots$. Near $s=1$ we thus have the expansion
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
  & \frac{2}{k} \hat{f} (s) (2 \pi)^{1-s} F_k \left(1+ \frac{2}{k} (s-1)
  \right) P\left(1+ \frac{2}{k} (s-1) \right)\\
  & =\frac{2}{k} \left(1+ a_1 (s-1) + a_2 (s-1)^2 + \cdots \right)
  \left(c_0 + c_1 (s-1) 
  + c_2 (s-1)^2 + \cdots\right)\\
  & \times \left(\frac{1}{\frac{2}{k} (s-1)} + \frac{2}{k} h_1 (\nu)
  (s-1) + \left(\frac{2}{k} \right)^3 h_3 (\nu) (s-1)^3+ \cdots
  \right)\\
  & \times \left( \frac{d_{-k^2}(k)}{\left(\frac{2}{k}\right)^{k^2}
    (s-1)^{k^2}} + \cdots + \frac{d_{-1} (k)}{\frac{2}{k} (s-1)} + d_0
  (k) + d_1 (k) \frac{2}{k} (s-1) + \cdots \right)\\
  &= \left\{ c_0 t (c_1 + c_0a_1) (s-1) + (c_2 + c_1a_1 + c_0 a_2)
  (s-1)^2 + \cdots\right. \\
  & \hspace{3cm}\left. + \left(c_{k2} + c_{k^2-1}a_1 + \cdots + c_0 a_{k2}\right)
  (s-1)^{k^2}+ \cdots \right\}\\
  & \times \left\{\frac{d_{-k^2}(k)}{\left(\frac{2}{k}\right)^{k^2}
    (s-1)^{k^2 +1}} + \frac{d_{- (k^2 -1)}(k)}{\left(\frac{2}{k}
    \right)^{k^2-1} (s-1)^{k^2}}+ \sum_{j= -(k^2 -1)}^\infty e_j (s-1)^j \right\}
\end{align*}}
with suitable coefficients $e_j = e_j (k, \nu)$ depending on $k$ and
the function $\nu$. The residue in the formula for $S_k (T)$ is the
coefficient of $(s-1)^{-1}$ in the above power series expansion. It
will be a linear combination of the $c_j$'s with $j$ running from 0 to
$k^2$. The $e_j$'s (which depend on $\nu$) will appear in the
expression for the residue as the coefficients of the $c_j$'s for $j
\leq k^2- 2$. In view of \eqref{c4:eq4.64} we obtain then the main in
\eqref{c4:eq4.61}. If $Q_{k^2}(y)$ in \eqref{c4:eq4.61} is
written\pageoriginale as 
$$
Q_{k^2} (y) = \sum_{j=0}^{k^2} A_j (k) y^{k^2-j},
$$
then the above power series expansion enables us to calculate
explicitly $A_0 (k)$ and $A_1 (k)$. We have
\begin{align*}
  A_0 (k) & = (k^2!) \left(\frac{2}{k}\right)^{-k^2} d_{-k^2} (k) =
  (k^2!)^{-1} \left(\frac{k}{2} \right)^{k^2} \lim\limits_{s \to 1+0} (s-1)^{k^2}
  F_k (s)\\
  & = (k^2!)^{-1} \left(\frac{k}{2} \right)^{k^2} \lim\limits_{s \to
    1+0} \left(\zeta(s)^{-k^2} \sum_{n=1}^\infty d_k^2 (n)
  n^{-s}\right)\\
  & = \Gamma^{-1} (k^2 +1) \left(\frac{k}{2} \right)^{k^2} \prod_p
  \left\{\left(1- \frac{1}{p}\right)^{k^2} \left(\sum_{j=0}^\infty
  \left(\frac{\Gamma (j+k)}{j! \Gamma(k)} \right) p^{-j} \right) \right\}.
\end{align*}

Note that $A_0 (k)$ is the coefficient of $(\log T)^{k^2}$ in the
asymptotic formula for $\sum\limits_{n \leq (T/2 \pi)^{\frac{1}{2}k}} d_k^2 (n)
n^{-1}$. I conjecture that for $k= 3,4$
\begin{equation}
  I_k (T) = 2S_k (T)+ O\left(T \log^{k^2- 2}T\right)\label{c4:eq4.65}
\end{equation}
holds, and \eqref{c4:eq4.65} is in fact true for $k=1,2$ (when, of
course, much more is known). Hence the expression for $A_0(k)$ leads
to the conjectural value \eqref{c4:eq4.12} for $c(k)$, namely
\begin{align*}
  C(k) & = \lim\limits_{T \to \infty} \left(T \log^{k^2} T\right)^{-1}
  \int\limits_0^T |\zeta (1/2 + it)|^{2k} dt\\
  & = 2 \left( \frac{k}{2}\right)^{k^2} \frac{1}{\Gamma (k^2 +1)}
  \prod_p \left\{\left(1- \frac{1}{p} \right)^{k^2} \left(
  \sum_{j=0}^\infty \left(\frac{\Gamma
    (k+j)^2}{j! \Gamma (k)}\right)^2 p^{-j} \right)\right\}.
\end{align*}

Other coefficients $A_j (k)$ can be also calculated, although the
calculations turn out to be quite tedious as $j$ increases. I shall
only evaluate here $A_1 (k)$ explicitly. It is the coefficient of $t
\log^{k^2-1}t \bigg|_T^{2T}$ in 
$$
\left(\frac{k}{2}\right)^{k^2} \left(c_{k^2} + c_{k^2-1} a_1
\right)d_{-k^2} (k) + \left(\frac{k}{2} \right)^{k^2-1} c_{k^2 -1}d_{-
(k^2-1)} (k).
$$

Thus\pageoriginale in view of \eqref{c4:eq4.64} we obtain
\begin{align}
  A_1 (k) & = \left(\frac{k}{2} \right)^{k^2} \left(-
  \frac{k^2}{(k^2)!} - \frac{\log (2 \pi)}{(k^2 -1)!} \right) d_{-k^2}
  (k) + \left(\frac{k}{2} \right)^{k^2-1} \frac{1}{(k^2 -1)} d_{- (k^2
    -1)}(k)\label{c4:eq4.66}\\
  & = \left(\frac{k}{2} \right)^{k^2-1} \cdot \frac{1}{(k^2-1)!}
  \left\{- \frac{k}{2} d_{-k^2}(k) (1+ \log (2\pi)) + d_{- (k^2 -1)}(k)
  \right\}.\notag 
\end{align}

For $k=2$
$$ 
d_{-4}(2) = \lim\limits_{s \to 1+0} (s-1)^4 \frac{\zeta^4
   (s)}{\zeta(2)} = \frac{1}{\zeta(2)} = \frac{6}{\pi^2}.
$$

Thus
$$
c(2) = \lim\limits_{T \to \infty} (T \log^4 T)^{-1} \int\limits_0^T |
\zeta (1/2 + it)|^4 dt = 2 \cdot \frac{1}{4!} \cdot 6 \pi^{-2} =
\frac{1}{2\pi^2}. 
$$

Further, near $s=1$,
\begin{align*}
  F_2 (s) & = \frac{\zeta^4 (s)}{\zeta(2 s)} = \left( \frac{1}{s-1} +
  \gamma + \gamma_1 (s-1) + \cdots \right)^4 \\
  & \qquad\left(\frac{1}{\zeta(2)}
  - \frac{2 \zeta'(2)}{\zeta^2 (2)} (s-1) + \ldots \right)\\
  & = d_{-4} (2) (s-1)^{-4} + d_{-3} (2) (s-1)^{-3} + \cdots
\end{align*}
with
$$
d_{-3} (2) = \frac{4 \gamma}{\zeta (2)} - \frac{2 \zeta'(2)}{\zeta^2 (2)}.
$$

Hence \eqref{c4:eq4.66} for $k=2$ yields
\begin{align*}
  A_1 (2) & = \frac{1}{6} \left(- \frac{(1+ \log (2 \pi))}{\zeta(2)}
   + \frac{4 \gamma - 12\zeta' (2) \pi^{-2}}{\zeta(2)} \right)\\[5pt]
   & = \pi^{-2} \left(4 \gamma -1 - \log (2 \pi) - 12 \zeta' (2)
   \pi^{-2}\right), 
\end{align*}
which coincides with \eqref{c4:eq4.4}, since for $a_3$ given by
\eqref{c4:eq4.3} we have $a_3 = 2A_1(2)$. 

To find a more explicit form of the expression for $A_1 (k)$, as given
by \eqref{c4:eq4.66}, we proceed as follows. Write
\begin{equation*}
  F_k (s)  = \zeta^{k^2} (s) G_k (s) \qquad (\re s > 1),
\end{equation*}
where
\begin{equation}
  G_k (s) = \prod_p \left\{(1- p^{-s})^{k^2} \left(\sum_{j=0}^{\infty}
  \left(\frac{\Gamma (k+j)}{j! \Gamma (k)} \right)^2 p^{-js}\right)
  \right\} \ \left(\re s > \frac{1}{2}\right).\label{c4:eq4.67} 
\end{equation}

Near\pageoriginale $s=1$ we have the Laurent expansion
\begin{align*}
  F_k (s) & = d_{-k^2} (k) (s-1)^{-k^2} + d_{- (k^2-1)} (k)
  (s-1)^{-(k^2 -1)} + \cdots\\
  & = \left(\frac{1}{s-1} + \gamma + \gamma_1 (s-1) + \cdots
  \right)^{k^2} \left(G_k (1) + G'_k (1) (s-1)+ \cdots\right).
\end{align*}
\indent
This yields 
\begin{align*}
  d_{-k^2} (k) = G_k (1) & = \prod_p \left\{\left(1-
  \frac{1}{p}\right)^{k^2} \left(\sum_{j=0}^\infty
  \left(\frac{\Gamma(k+j)}{j! \Gamma (k)} \right)^2 p^{-j} \right)
  \right\}.\\
  d_{k^2 -1}(k) & = \gamma k^2 G_k (1) + G'_k (1).
\end{align*}

Hence we obtain
\begin{thm}\label{c4:thm4.4}
  If $Q_{k^2} (y)= \sum\limits_{j=0}^{k^2} A_j (k) y^{k^2-j}$ is the
  polynomial in \eqref{c4:eq4.61}, we obtain 
  \begin{align*}
    A_0 (k) & = \frac{1}{(k^2)!} \left(\frac{k}{2}\right)^{k^2} G_k
    (1),\\
    A_1 (k) & = \frac{1}{(k^2 -1)!} \left(\frac{k}{2} \right)^{k^2-1}
    \left\{ \left( \gamma k^2 - \frac{k}{2} (1+ \log (2 \pi))\right)
    G_k (1) + G'_k (1) \right\},
  \end{align*}
  where $G_k (s)$ is given by \eqref{c4:eq4.67}.
\end{thm}

Note that the conjecture \eqref{c4:eq4.65} may be formulated more
explicitly as $(k=3,4)$
{\fontsize{10pt}{12pt}\selectfont
$$
\int\limits_0^T  |\zeta (1/2 + it)|^{2k} dt = T \left(2A_0 (k) \log^{k^2} T
+ 2A_1 (k) \log^{k^2-1} T + O (\log^{k^2 -2}T)\right).
$$}

A. Odlyzko has kindly calculated the values
$$
2 A_o (3) = 1.04502424 \times 10^{-5}, 2 A_0 (4) = 1.34488711 \times 10^{-12}.
$$

\section{The Mean Square Formula}\label{c4:sec4.5}

The mean square formula
$$
I_1 (T) = \int\limits_0^T |\zeta(1/2 +it)|^2 dt = T \left(\log
\frac{T}{2\pi} + 2 \gamma -1 \right) + E(T)
$$
and\pageoriginale results concerning $E(T)$ were discussed
extensively in Chapter \ref{c2} and Chapter \ref{c3}. In this section
we shall show 
how bounds for $E(T)$ may be obtained from Theorem \ref{c4:thm4.3},
which for $k=1$ gives
\begin{align*}
  \frac{1}{2} I_{1, f} (T) &= \sum_{n=1}^\infty \frac{1}{n} \re
  \left(\int\limits_0^\infty f(t) \nu \left(\frac{x}{n} \right)dt
  \right) + o(T^{1/6}) + o\left(T^{1/2 + \epsilon} T^{-1}_0\right)\\
&\quad  + \sum_{\substack{m, n=1; m \neq n\\ 1 - \delta \leq m/n \leq 1+
      \delta}} (mn)^{-1/2} \re \left\{\int\limits_0^\infty f(t) \nu
  \left(\frac{x}{m}\right) \left(\frac{m}{n} \right)^{it}dt \right\} ,
\end{align*}
where $x= \left(\frac{t}{2 \pi} \right)^{1/2}$, and $0 < \delta <
\frac{1}{2}$. From \eqref{c4:eq4.60} and \eqref{c4:eq4.62} we have
\begin{align}
  S_1 (T) & = \sum_{n=1}^T \frac{1}{n} \re \left(\int_0^\infty f(t)
  \nu \left(\frac{x}{n} \right)dt \right)\label{c4:eq4.68}\\
  & = 2 \re \left\{\mathop{\res}_{s=1} \hat{f} (s) (2 \pi)^{1-s} \zeta
  (1+ 2 (s-1)) P (1+2(s-1))\right\} + O (T^\epsilon).\notag
\end{align}
\indent
Near $s=1$
\begin{align*}
&  2 \hat{f} (s) (2 \pi)^{1-s} \zeta (1+2 (s-1)) P(1+2(s-1))\\
&  = 2\left(c_0 + c_1 (s-1) + c_2 (s-1)^2 + \cdots\right) (1+ a_1 (s-1) + a_2
  (s-1)^2 + \cdots\\
&  \times \left(\frac{1}{2 (s-1)} + \gamma + 2 \gamma_1 (s-1) + \cdots
  \right) \left(\frac{1}{2(s-1)} + 2h_1 (s-1) + \cdots \right).
\end{align*}

The residue at $s=1$ therefore equals
$$
\frac{1}{2} \left(c_90 a_1 + c_1 + 2\gamma c_0\right).
$$

But 
\begin{gather*}
a_1 =- \log (2\pi), c_0 = t\bigg|_T^{2T} + O (T_0),\\
c_1 = \int\limits_T^{2T} \log t dt + O(T_0 \log T) = (t \log t-t)
  \bigg|_T^{2T} + O(T_0 \log T).
\end{gather*}

Hence \eqref{c4:eq4.68} becomes
\begin{equation}
  S_1 (T) = \frac{1}{2} \left(t \log \frac{t}{2 \pi} + (2 \gamma -1)t
  \right) \Bigg|_T^{2T} + O(T^\epsilon) + O(T_0 \log T).\label{c4:eq4.69}
\end{equation}

In\pageoriginale view of \eqref{c4:eq4.61} it is seen that the above expression
(with a suitable choice for $T_0$) will indeed furnish the main term
in the asymptotic formula for $I_1 (T)$. Therefore it remains to
estimate the double sum
\begin{align}
  \sum (T, \delta) : & = \sum_{\substack{m, n = 1; m\neq n\\ 1-
    \delta \leq \frac{m}{n} \leq 1+ \delta}} (mn)^{-1/2}
  \int\limits_0^\infty f(t) \nu \left(\frac{x}{m}\right)
  \left(\frac{m}{n}\right)^{it} dt\label{c4:eq4.70}\\
  & = \sum_{\substack{m, n=1; m\neq n\\1- \delta \leq \frac{m}{n} \leq
  1 + \delta}} (mn)^{-1/2} J(T; m, n),\notag
\end{align}
where both $m, n$ take at take at most $2 \sqrt{T}$ values, and 
\begin{equation}
  J(T; m, n): = \int\limits_0^\infty f(t) \nu \left(\frac{x}{m} \right)
  \left(\frac{m}{n} \right)^{it} dt.\label{c4:eq4.71}
\end{equation}

Integrating by parts the integral in \eqref{c4:eq4.71} we obtain 
\begin{multline*}
  J(T; m, n) = - \int\limits_0^\infty f' (t)
  \left(\int\limits_0^\infty \nu \left(m^{-1} \sqrt{\frac{u_1}{2\pi}}
  \right)\left(\frac{m}{n} \right)^{iu_1} du_1 \right) dt  = \ldots
  =\\
  = (-1)^R \int\limits_0^\infty f^{(R)} (t) \left(\int\limits_0^t
  \underbrace{\cdots}_R \int\limits_0^{u_1} \nu \left(m^{-1}
  \sqrt{\frac{u_1}{2\pi}} \right) \left(\frac{m}{n} \right)^{iu_1}
  du_1 \cdots du_R \right)dt.
\end{multline*}

We evaluate the $R$-fold integral by using $\int c^{it} dt = c^{it}/(i
\log c)$, after removing the $\nu$-function from the innermost
integral by the second mean value theorem for integrals. Hence this
integral is $\ll_R |\log \frac{m}{n}|^{-R}$. Moreover, since
$f^{(R)}(t) \ll_R T_0^{-R}$, we obtain uniformly in $m$ and $n$
$$
J(T; m, n) \quad \ll_R \quad TT_0^R \big|\log \frac{m}{n}\big|^{-R}.
$$

To use this bound for $J$ we write the sum in \eqref{c4:eq4.70} ad
\begin{align*}
  \sum (T, \delta) & = \sum_{\substack{m, n =1; m \neq n\\ 1- \delta
      \leq \frac{m}{n} \leq 1+ \delta, |m-n| > T^{1/2+ \epsilon}
      T_0^{-1}}}^\infty + \sum^\infty_{\substack{m,n =1 ; m \neq
      n\\ 1- \delta \leq \frac{m}{n} \leq 1+ \delta, |m-n| \leq
      T^{1/2+ \epsilon}T_0^{-1}}}\\
  & = \sideset{}{_1}\sum + \sideset{}{_2}\sum,
\end{align*}
say,\pageoriginale where $\epsilon > 0$ is an arbitrarily small, but
fixed number. We obtain 
$$
\sideset{}{_1}\sum \ll_R   \sum_{\substack{m, n \leq 2 \sqrt{T}; m\neq
    n\\ 1- \delta \leq \frac{m}{n} \leq 1+ \delta; |m-n| \geq T^{1/2+
\epsilon}T_0^{-1}}} (mn)^{-1/2} |\log \frac{m}{n}|^{-R} TT_0^{-R}.
$$

Since $1- \delta \leq \frac{m}{n} \leq 1+ \delta$ and $0 < \delta <
\frac{1}{2}$, we have
$$
\log \frac{m}{n} = \log \left(1+ \frac{m-n}{n} \right) \gg_\delta
\frac{|m-n|}{n} \geq T^{1/2+ \epsilon} T^{-1}_0 n^{-1}.
$$ 

Therefore 
\begin{align*}
  \sideset{}{_1}\sum & \ll _R \sum_{m, n \leq 2 T^{1/2}} TT^{-R}_0
  (mn)^{-1/2} n^R T^{-1/2 R - \epsilon R}T^R_0\\
  & \ll_R T^{1- \epsilon R} \sum_{m \leq 2T^{1/2}} \sum_{n \leq
    T^{1/2}} n^{- 1/2} \ll T^{3/2 - \epsilon R} \ll 1
\end{align*}
if we choose $R = [3/(2\epsilon)]+1$. In other words, the smoothing
function $f$, with its salient property that $f^{(R)} (t) \ll_R
T_0^{-R}$, has the effect of making the contribution of $\sum_{1}$
negligible What remains is the ``shortened'' sum $\sum_3$, which
becomes, on writing $m- n=r$,
$$
{\sum}_2 = \sum_{\substack{0 < |r| \leq T^{1/2 + \epsilon}
    T_0^{-1}\\r \geq 2 \sqrt{T}, - \delta n \leq r \leq \delta n}}
n^{-1/2} (n+r)^{-1/2} \int\limits_0^\infty f(t) \nu
\left(\frac{x}{n+r} \right)\left(1+ \frac{r}{n} \right)^{it} dt.
$$

We suppose $T^\epsilon \ll T_0 \ll T^{1/2}$, since we do not wish the
sum over $r$ to be empty. To estimate $\sum_2$ write
\begin{align*}
  \int\limits_0^\infty f(t) \nu \left(\frac{x}{n+r}\right) \left(1+
  \frac{r}{n}\right)^{it} dt & = \int\limits_{T+ T_0}^{2T-T_0} f(t)
  \nu \left(\frac{x}{n+r} \right) \left(\frac{r}{n} \right)^{it} dt +
  I_2\\
  & = I_1 + I_2,
\end{align*}
say, where in $I_2$ the intervals of integration are $\ll T_0$ in
length. In $I_1$ we have $f(t)=1$, and the $\nu$-function is removed
by the second mean value theorem for integrals. We use
$$
\int\limits_A^B \left(\frac{m}{n}\right)^{it} dt = \frac{(m/n)^{iB} -
  (m/n)^{iA}}{i \log (m/n)}
$$
and\pageoriginale perform summation over $n$. Since $m=n +r$ we
encounter sums of the type 
\begin{equation}
  \sum_{n \leq 2 T^{1/2}} n^{-1} \left(\log \left(1+ \frac{r}{n} \right) \right)^{-1} \exp \left(i
  \tau \log \left(1+ \frac{r}{n} \right) \right)\label{c4:eq4.72} 
\end{equation}
with $\tau \asymp T$. If $(\chi, \lambda)$ is an exponent pair, then
for $N \ll T^{1/2}$
\begin{equation}
  \sum_{N < n \leq N' \leq 2N} \exp \left(i \tau \log \left(1+ \frac{r}{n}\right) \right) \ll
  \left(\frac{Tr}{N^2} \right)^\chi N^\lambda,\label{c4:eq4.73}
\end{equation}
and since $\log \left(1+ \frac{r}{n} \right) \sim \frac{r}{n}$ it follows by
partial summation that the sum in \eqref{c4:eq4.72} is 
$$
\ll r^{\chi -1} T^x T^{1/2} (\lambda- 2 x) = r^{x-1}T^{1/2 \lambda}.
$$

Summation over $r$ shows then that the contribution of $I_1$ is 
$$
\ll \sum_{1 \leq r \leq  T^{1/2+ \epsilon}T_0^{-1}} 
r^{x-1}T^{1/2\lambda} \ll T^{\epsilon + 1/2} (\chi+ \lambda) T^{-x}_0.
$$

The contribution of $I_2$ is obtained not by integration, but by
estimation, first over $n$ using again \eqref{c4:eq4.73}, then over
$r$ under the integral sign. Trivial estimation of the integral as
$O(T_0)$ times the maximum of the sums shows that this contribution
will be $\ll T^{\epsilon + 1/2(\chi+\lambda)T_0^{-x}}$, just as in
the previous case. Combining the preceding estimates we have, for
$T^\epsilon \ll T_0 \ll T^{1/2}$,
$$
E(T) \ll T^\epsilon \left(T_0+ T^{1/2} T^{-1}_0 + T^{1/2 (x+
  \lambda)}T_0^{-x} \right). 
$$

We choose now $T_0 = T^{(x+\lambda)/(2 x+2)}$. Since $1/4 \leq
(x+\lambda) /(2x+2)\leq 1/2$ for any $(x, \lambda)$, we have
$T^{1/4} \leq T_0 \leq T^{1/2}$ and therefore
\begin{equation}
  E(T) \ll \frac{x+ \lambda}{T^{2(x+1)}}+   \epsilon. \label{c4:eq4.74}  
\end{equation}

This is the same upper bound that was obtained in Section
\ref{c2:sec2.7} from Atkinson's formula for $E(T)$ by an averaging
process. However, we already remarked that it seems difficult to
obtain an explicit formula\pageoriginale (such as Atkinson's) for
$E(T)$ from Theorem \ref{c4:thm4.3}.

There is yet another approach to the estimation of $E(T)$, which gives
essentially the same bound as \eqref{c4:eq4.74}. It involves the use
of an explicit formula for $E(T)$, due to R. Balasubramanian
\cite{Balasubramanian1}. He integrated the classical Riemann-Siegel formula and
obtained
{\fontsize{9pt}{11pt}\selectfont
\begin{equation}
 E(T)  = 
   2 \sum_{n \leq K} \sum_{m \leq K, m \neq n}  \left\{\frac{\sin
    (T \log \frac{m}{n})}{(mn)^{1/2} \log \frac{m}{n}} + \frac{\sin (2
    \theta - T \log mn)}{(mn)^{1/2} (2 \theta' - \log mn)}
  \right\}+ O(\log^2 T),\label{c4:eq4.75}
\end{equation}}
where $K= K(T)= \left( \frac{T}{2 \pi}\right)^{1/2}$, $\theta =
\theta(T) = \frac{T}{2} \log \frac{T}{2 \pi}- \frac{T}{2} -
\frac{\pi}{8}$. As already seen in Chapter \ref{c2}, from the definition of
$E(T)$ it follows that 
$$
E(T) \leq E(T+ u) + C_1 u \log T \quad (C_1 > 0, 0\leq u \leq T).
$$

Set $u=t+ GL$, multiply by $exp (-t^2 G^{-2})$ and integrate over $t$
from $-GL$ to $GL$, where $L= \log T$. We obtain 
$$
\int\limits_{-GL}^{GL} E(T) e^{-(t/G)^2}dt \leq \int\limits_{-GL}^{GL}
E(T + t + GL) e^{- (t/G)^2}dt+ CG^2 L^2,
$$
whence for $1 \ll G \ll T^{1/2}$ and a suitable $C> 0$
\begin{equation}
  E(T) \leq \left(\sqrt{\pi}G\right)^{-1} \int\limits_{-GL}^{GL} E(T+ t + GL)
  e^{- (t/G)^2}dt + CGL^2,\label{c4:eq4.76} 
\end{equation}
and similarly
\begin{equation}
  E(T) \geq \left(\sqrt{\pi}G\right)^{-1} \int\limits_{-GL}^{GL} E(T+ t - GL)
  e^{-(t/G)^2}dt - CGL^2.\label{c4:eq4.77}
\end{equation}

If $E(T)> 0$ we use \eqref{c4:eq4.76}, otherwise \eqref{c4:eq4.77},
and since the analysis is similar in both cases we may concentrate
on \eqref{c4:eq4.76}. The parameter $G$ may be restricted to the range
$T^{1/4} \leq G \leq T^{1/3}$ since $E(T) = \Omega (T^{1/4})$ and
$E(T) = O(T^{1/3})$. We insert \eqref{c4:eq4.75} in the integral
in \eqref{c4:eq4.76}, with $T$ replaced by $T+t+GL$. The first step is
to replace $K(T + t + GL)$ by $K(T)$. In doing this we make an error
which is $0(GL)$. Then we integrate Balasubramanian's formula by using
\begin{equation}
  \int\limits_{-\infty}^\infty e^{-Bx^2} \sin (Ax + C)dx =
  \sqrt{\frac{\pi}{B}} e^{\frac{A^2}{4B}} \sin C \quad (\re B >
  0). \label{c4:eq4.78} 
\end{equation}

We\pageoriginale set $\tau = T + GL$ and use
\begin{align*}
&  \sin (2 \theta (\tau +t) - (\tau+ t) \log mn)\\
&  = \sin \left(\tau \log \frac{\tau}{2 \pi mn} - \tau + t \log
  \frac{\tau}{2 \pi mn}- \frac{\pi}{4} \right) + O\left(G^2 L^2
  T^{-1}\right),\\ 
&  (2 \theta' - \log mn)^{-1} = \left(\log \frac{\tau}{2\pi mn}
  \right)^{-1} + O \left(GL \left(\log \frac{T}{2 \pi
    mn}\right)^{-2}\right), 
\end{align*}
which follows by Taylor's formula after some simplifying. The total
contribution of the error terms will be again $O(GL)$. Thus
using \eqref{c4:eq4.78} we obtain, for $T^{1/4} \leq G \leq T^{1/3}$,
$\tau = T + GL$, $L = \log T$,
{\fontsize{9pt}{11pt}\selectfont
\begin{align}
& E(T)\leq CGL^2 + 2 \sum_{n \leq \sqrt{\frac{T}{2T}}} \sum_{m \leq
  \sqrt{\frac{T}{2 T}}, m \neq n}\notag\\ 
&\frac{1}{\sqrt{mn}}\left\{ \frac{\sin \left(J \log
\frac{n}{m}\right)}{\log \frac{n}{m}} e^{-\frac{1}{4} G^2 \log^2
  \frac{n}{m}}
 +\frac{\sin \left(J \log \frac{\tau}{2 T mn} - 
        J - \frac{\pi}{4}\right)}{\log \frac{J}{2 \pi mn}}
      e^{-\frac{1}{4} G^2 \log^2 \frac{J}{2 \pi mn}}\right\}.\label{c4:eq4.79}
\end{align}}

The bound for $E(T)$ given by \eqref{c4:eq4.79} allows an easy
application of exponent pairs (and also of two-dimensional techniques
for the estimation of exponential sums). The sum over $n$ can be split
into $O(\log T)$ subsums in which $K_1 < n \leq 2K_1 (\ll T^{1/2})$,
and then we may suppose $1/2 K_1 < m \leq 1/2 (5K_1)$, for otherwise
the exponential factors are very small. By this and by symmetry we may
set $m= n+r$, $1 \leq r \leq 1/2 K_1$, the contribution of $r$ for
which $r\geq K_1 G^{-1} L$ being negligible by the rapid decay of the
exponential function (the contribution of $K_1 \leq 2 GL^{-1}$ is also
negligible). Thus by using partial summation we get
{\fontsize{10}{12}\selectfont
\begin{align*}
  E(T) & \ll GL^2 + L \max\limits_{K_1 \ll T^{1/2}} \sum_{r \leq K_1
    G^{-1}L} r^{-1} \max_{K_1 \leq K'_1 \leq K''\leq 2 K_1} \left|
    \sum_{K'_1 < n \leq K''_1} \exp \left(i \tau \log \left(1+
    \frac{r}{n} \right) \right)\right|\\
    & \ll GL^2 + L \max\limits_{K_1 \ll T^{1/2}} \sum_{r \leq K_1
      G^{-1}L} r^{-1} \left(\frac{Tr^\chi}{K^2_1} \right) K_1^\lambda,
\end{align*}}
on\pageoriginale estimating the first sine terms in \eqref{c4:eq4.79},
and the other 
sine terms will give at the end the same upper bound. Thus
\begin{align*}
  E(T) & \ll GL^2 +L \max\limits_{K_1 \ll T^{1/2}} T^x K_1^{\lambda -
  2x}  (K_1 G^{-1} L)^x\\
  & \ll L^2 \left(G + T^{1/2 (x+ \lambda)}G^{-x}\right).
\end{align*}

Taking $G=T^{(x + \lambda)/(2 x+2)}$ we obtain
\begin{equation}
  E(T) \ll T^{\frac{ x + \lambda}{2 (x+1)}} \log^2
  T.\label{c4:eq4.80} 
\end{equation}

This is the same as \eqref{c4:eq4.74}, only $T^\epsilon$ is now
replaced by the sharper $\log^2T$. The reason for this is the use of
the specific smoothing factor $\exp (-t^2 G^{-2})$, which decays very
fast, and by \eqref{c4:eq4.78} enables one to evaluate explicitly the
integrals in question. The had work is, in this case, contained in
Balasubramanian's formula \eqref{c4:eq4.75} for $E(T)$. In the
previous approach, which was the evaluation of $I_1 (T)$ via $I_{1, f}
(T)$, the smoothing factor $f(\cdot)$ was built in from the very
beginning of proof. This was useful, since it saved some hard work,
but as pointed out before, it resulted in the fact that the could not
get an explicit formula for $E(T)$ itself, but just an upper bound.

\section{The Fourth Power Moment}\label{c4:sec4.6}

In Chapter \ref{c5} we shall study extensively the asymptotic evaluation of 
$$
I_2 (T) = \int\limits_0^T |\zeta (1/2 + it)|^4 dt 
$$
by the powerful methods developed by Y. Motohashi. In this section we
wish to investigate $I_2 (T)$ by using Theorem \ref{c4:thm4.3}, and
indicate how the bounds $E_2 (T) = O(T^{7/8+\epsilon})$ and $E_2 (T)=
O(T^{2/8+\epsilon})$, due to Hewth-Brown\pageoriginale and Zavorotnyi,
respectively, may be derived. So far we have pushed Zavorotnyi's
method to the extent of establishing Theorem \ref{c4:thm4.3} for $k
\leq 4$, and not only for $k \leq 2$, as was done by Zavorotnyi. Now
we shall use first Heath-Brown's approach in conjunction with Theorem
\ref{c4:thm4.3}, and then we shall briefly sketch the salient points
of Zavorotnyi's method. In view of \eqref{c4:eq4.61} we need only to
evaluate the sum
{\fontsize{9}{11}\selectfont
$$ 
 S(T)= S(T; \delta, f): = 
  \sum^\infty_{\substack{m, n=1; m\neq n\\1- \delta\leq
      m/n \leq 1+ \delta}} d(m) d(n) (mn)^{-1/2} \re \left\{\int\limits_0^T f(t)
  \nu \left(\frac{t}{2 \pi m} \right) \left(\frac{m}{n}\right)^{it}dt\right\}.
$$}

This sum is split into three subsums, according to the conditions: 

(i) $m > n, m/n \leq 1+ \delta$, ii) $m < n, n/m \leq 1+ \delta$, iii)
$1+ \delta < n/m \leq \frac{1}{1- \delta}$, and the subsums are
denoted by $S_i(T)$, $S_{ii}(T)$, $S_{iii}(T)$,
respectively. Integrating by parts, as in the proof of Theorem
\ref{c4:thm4.3}, and using $f'(t) \ll T_{-1}$, we obtain
$$
S_{iii} (T) \ll T^{1+\epsilon} T^{-1}_0.
$$

In $S_i (T)$ we set $m=n +r$, and in $S_{ii}(T) n = m+r$. Changing
then $m$ into $n$ in $S_{ii}(T)$ we obtain, for $0< \delta < 1/2$, 
\begin{multline*}
  S(T) = O \left(T^{1+ \epsilon}T^{-1}_0\right) + \sum^\infty_{n=1} \sum_{1 \leq
    r \leq \delta n} d(n) d(n+r) (n (n+r))^{-1/2}\\
  \int\limits_0^\infty f(t) \left(\nu \left(\frac{t}{2\pi (n+r)} \right) +
  \nu \left(\frac{t}{2 \pi n}\right) \right) \cos \left(t \log
  \left(1+ \frac{r}{n} \right) \right)dt.
\end{multline*}

Note that $n \leq bT$ in the range of summation of $n$ in view of the
properties of $\nu(x)$. By \eqref{c4:eq4.57} we see that the terms for
which $n \leq T_0$ contribute
$$
\ll \sum_{n \leq T_0} \sum_{r \leq \delta n} \frac{d(n)d(n+r)}{n}\cdot
\frac{n}{r} \ll T^\epsilon T_0.
$$

Now choose $0 < \delta < 1/2$ fixed, $\epsilon > 0$ arbitrarily small
but fixed, and $T_1= T_1 (\epsilon, \delta)$ so large that
$T^{\epsilon}T^{-1}_0 \leq \delta$ for $T \geq T_1$. For $r >
T^\epsilon T^{-1}_0 n$ we have $\log \left(1+ \frac{r}{n} \right) \ll
T^\epsilon T^{-1}_0$. Thus integrating sufficiently\pageoriginale many
times by parts it is seen that the contribution of the terms for which
$r > T^{\epsilon} T^{-1}_0 n$ to $S(T)$ is negligible. We have
\begin{align}
&  S(T) = O (T^\epsilon T_0) + O(T^{1+ \epsilon}T^{-1}_0) +
  \sum_{n \geq T_0} \sum_{r \leq T^\epsilon T^{-1}_0
    \pi}\hspace{2cm} \label{c4:eq4.81}\\
& \frac{d(n) d(n+r)}{(n (n+r))^{1/2}} \int\limits_0^\infty f(t)
  \left(\nu \left(\frac{t}{2\pi (n+r)} \right)+ \nu \left(\frac{t}{2
    \pi n} \right) \right)  \cos \left(t \log \left(1+ \frac{r}{n}
  \right)\right)dt.\notag 
\end{align}

Next, for $\nu (\cdot)$ in the above sum we have, by Taylor's formula, 
$$
\nu \left(\frac{t}{2\pi (n+r)} \right) = \nu \left(\frac{t}{2 \pi n}
\right) - \frac{tr}{2 \pi n^2} \nu' \left(\frac{t}{2 \pi n} \right) +
O\left(\frac{t^2 r^2}{n^4} \right).
$$

The contribution of the $O$-term to $S(T)$ is trivially 
\begin{align*}
  & \ll \sum_{n \geq T_0} \sum_{r \leq T^\epsilon T^{-1}_0n}
  \frac{d(n) d(n+r)}{(n (n+r))^{1/2}} \cdot \frac{T^3t^2}{n^4}\\[5pt]
    & \ll T^{3+\epsilon} \sum_{n \geq T_0} n^{-5} T^{3 \epsilon}
    T^{-3}_0 n^3 \ll T^{3+ 4 \epsilon} T_0^{-4}. 
\end{align*}

Therefore \eqref{c4:eq4.81} becomes
\begin{align}
  S(T) & = 2 \sum_{n \geq T_0} \sum_{r \leq T^\epsilon T_0^{-1}n}
  \frac{d(n) d(n+r)}{(n (n+r))^{1/2}}\label{c4:eq4.82}\\
  &\quad \int\limits_0^\infty f(t) \nu \left(\frac{t}{2 \pi n} \right) \cos
  \left(t \log \left(1 + \frac{r}{n} \right) \right)dt-\sum_{n \geq
    T_0} \frac{1}{2 \pi n^2} \sum_{r \leq T^\epsilon T^{-1}_0 n}\notag\\
  &\quad \frac{rd(n) d(n+r)}{(n(n+r))^{1/2}} \int\limits_0^\infty f(t) t
   \nu' \left(\frac{t}{2 \pi n} \right) 
   \cos \left(t \log\left(1+ \frac{r}{n} \right) \right)dt \notag\\
   &\quad + O\left(T^{1+\epsilon} T^{-1}_0\right) + O(T^\epsilon T_0)
   + O\left(T^{3+ 4 \epsilon} T^{-4}_0\right).\notag
\end{align}

The contribution of the second double sum in \eqref{c4:eq4.82} is, on
applying Lemma \ref{c2:lem2.1},
$$
\ll T^\epsilon \sum_{T_0 \leq n \leq bT} n^{-2} \sum_{r \leq
  T^\epsilon T^{-1}_0 n} \frac{r}{n} \cdot T \cdot \frac{n}{r} \ll
T^{1+3 \epsilon} T^{-1}_0.
$$

Write\pageoriginale now 
$$
(n+r)^{-1/2}= n^{-1/2} - \frac{1}{2} rn^{-3/2} + O\left(r^2 n^{-5/2}\right).  
$$

The contribution of the terms $-r /(2n^{3/2})$ to $S(T)$ will be 
$$
O\left(T^{1+3\epsilon} T_0^{-1}\right)
$$ 
by Lemma \ref{c2:lem2.1}. The 0-term will contribute
\begin{align*}
&  \ll T^\epsilon \sum_{T_0 \leq n \leq  bT}n^{-1} \sum_{r \leq T^\epsilon
  T_0^{-1}n} Tr^2 n^{-5/2} \ll T^{1+\epsilon} \sum_{T_0 \leq n \leq
  bT} n^{-7/2} T^{3 \epsilon}T^{-3}_0 n^3\\[5pt]
&  \ll T^{3/2 + 4 \epsilon} T^{-3}_0.
\end{align*}

Therefore
{\fontsize{10}{12}\selectfont
\begin{align}
&  S(T)  = O\left\{T^{4 \epsilon} \left(TT_0^{-1} + T_0 + T^3 T^{-4}_0 +
  T^{3/2} T_0^{-3}\right) \right\}\label{c4:eq4.83}\\
&\quad  + 2 \sum_{T_0 \leq n \leq bT} \sum_{r \leq T^\epsilon T^{-1}_0n}
  d(n) d(n+r)n^{-1} \int\limits_0^\infty f(t) \nu \left(\frac{t}{2 \pi
    n}\right) \cos \left(t \log \left(1+ \frac{r}{n}\right) \right) dt.\notag
\end{align}}

We may further simplify \eqref{c4:eq4.83} by using
$$
\cos \left(t \log \left(1+ \frac{r}{n}\right) \right) = \cos
\left(\frac{tr}{n} \right) + \frac{tr^2}{2n^2} \sin \left(\frac{tr}{n}
\right) + O \left(\frac{t^2 r^4}{n^4} \right).
$$

The contribution of the error term is 
$$
\ll \sum_{T_0 \leq n \leq bT} T^\epsilon n^{-1} \sum_{r \leq
  T^\epsilon T_0^{-1} n} T^3 r^4 n^{-4} \ll T^{4+ 6 \epsilon} T_0^{-5}. 
$$

The contribution of the sine terms will be, by Lemma \ref{c2:lem2.1},
$$
O(T^{2 + 4\epsilon} T^{-2}_0).
$$ 

Hence we have
{\fontsize{10pt}{12pt}\selectfont
\begin{equation}
  S(T) = \sum (T) + O\left\{ T^{6 \epsilon} \left(T_0 + TT_0^{-1} + T^2
  T_0^{-2} + T^3 T_0^{-4} + T^4 T^{-5}_0\right)\right\}, \label{c4:eq4.84}
\end{equation}}
where
{\fontsize{10pt}{12pt}\selectfont
\begin{equation}
  \sum(T) : = 2 \sum_{T_0 \leq n \leq b T} \sum_{r \leq T^\epsilon
    T_0^{-1}} \frac{d(n) d(n+r)}{n} \int\limits_0^\infty f(t) \nu
  \left(\frac{t}{2 \pi n} \right) \cos \left(\frac{tr}{n}
  \right)dt.\label{c4:eq4.85} 
\end{equation}}

Writing $A_r = \max (T_0, rT_0 T^{-\epsilon})$, we may change the
order of summation in $\sum (T)$ and obtain 
{\fontsize{10pt}{12pt}\selectfont
\begin{equation}
  \sum(T) = 2 \sum_{\gamma \leq b T^{1+ \epsilon} T_0^{-1}} \sum_{A_r
    \leq n \leq b T} \frac{d(n) d (n+r)}{n} \int\limits_0^\infty f(t)
  \nu \left(\frac{t}{2 \pi n} \right) \cos \left(\frac{tr}{n}\right)
  dt.\label{c4:eq4.86}
\end{equation}}

It\pageoriginale would be good if the above sum could be evaluated as
a double (exponential) sum with divisor coefficients. This,
unfortunately, does not seem possible at the moment. One can, however,
perform summation over $n$ and then over $r$. Some loss is bound to
occur in this approach, used both by Heath-Brown and Zavorotnyi. The
former shows that
$$
\sum_{n \leq x} d(n) d(n+r) = m (x, r) + E(x, r),
$$
where for some absolute constants $c_{ij}$
$$
m (x, r) = \sum_{i=0}^2 c_i (r) x (\log x)^i, c_i (r) = \sum^2_{j=0}
c_{ij} \sum_{j=0}^2 c_{ij} \sum_{d \mid r} d^{-1} (\log d)^j,
$$
and uniformly for $1 \leq r \leq x^{5/6}$
\begin{equation}
  E(x, r) \ll x^{5/6+\epsilon}, \label{c4:eq4.87}
\end{equation}
while uniformly for $1 \leq r \leq X^{3/4}$
\begin{equation}
  \int\limits_X^{2X} E^2 (x, r) dx \ll X^{5/2+
    \epsilon}. \label{c4:eq4.88} 
\end{equation}

The estimates \eqref{c4:eq4.87} and \eqref{c4:eq4.88} are of
independent interest. They are obtained by techniques from analytic
number theory and T. Estermann's estimate \cite{Estermann3}
\begin{equation}
  |S (u, v; q)| \leq d(q) q^{1/2} (u, v, q)^{1/2}\label{c4:eq4.89}
\end{equation}
for the Kloosterman sum $(e(y) = \exp (2 \pi i y))$
\begin{equation}
  S(u, v; q)= \sum_{n \leq q, (n, q)=1, nn' \equiv 1 \pmod{q}}
  e \left(\frac{un+ vn'}{q} \right).
\end{equation}

In \eqref{c4:eq4.86} we write the sum as 
\begin{align*}
  F(T, r) : &= \sum_{A_r \leq n \leq bT} d(n) d(n+r) h (n, r),\\
  h (x, r): & = \frac{1}{x} \int\limits_0^\infty f(t) \nu
  \left(\frac{t}{2 \pi x} \right) \cos \left(\frac{tr}{x}\right)dt.
\end{align*}

Hence
\begin{align*}
  F(T, r) & = \int\limits_{A_r}^{bT} h(x,r)d \{ m (x, r)+ E (x, r\}\\
  & = \int\limits_{A_r}^{bT} m' (x, r) h (x, r) dx + E (x, r) h (x, r)
  \Bigg|_{A_r}^{bT} - \int\limits_{A_r}^{bT} h' (x, r) E (x, r)dx.
\end{align*}

By\pageoriginale Lemma \ref{c2:lem2.1} we have $h(x, r)= O(1/r)$
uniformly in $r$. Thus the total contribution of the integrated terms
to $S(T)$ is $\ll T^{5/6+\epsilon}$, the condition $r\leq T^{5/6}$
being trivial if we suppose that $T_0 \geq T^{1/2}$. Next
\begin{align*}
&\frac{dh(x, r)}{dx}  = -x^{-2} \int\limits_0^\infty f(t) \nu
  \left(\frac{t}{2 \pi x}\right) \cos \left(\frac{tr}{x} \right) dt\\
& - x^{-3} \int\limits_0^\infty f(t) \frac{t}{2 \pi} \nu'
  \left(\frac{t}{2 \pi x} \right) \cos \left(\frac{tr}{x} \right)dt +
  x^{-3} \int\limits_0^\infty f(t) tr\nu \left(\frac{t}{2 \pi x}
  \right) \sin \left(\frac{tr}{x} \right) dt\\
&\ll (rx)^{-1} + Tr (rx^2){-1} \ll Tx^2.
\end{align*}

Using \eqref{c4:eq4.88} (the condition $r \ll T^{3/4}$ holds again for
$T_0 \geq T^{1/2}$) we obtain, for $A_r \leq Y \leq bT$,
\begin{align*}
  \int\limits_Y^{2Y} h' (x, r) E (x, r)dx & \ll T
  \left(\int\limits_Y^{2Y} x^{-4} dx \right)^{1/2}
  \left(\int\limits_Y^{2Y} E^2 (x, r)dx \right)^{1/2}\\[5pt]
  & \ll TY^{-3/2} Y^{5/4 + \epsilon} \ll TY^{\epsilon-1/4}.
\end{align*}

Hence
\begin{align*}
  \sum_{1 \leq r \leq bT^{1+ \epsilon}T_0^{-1}} &\int\limits_{A_r}^{bT}
  h' (x, r) E (x, r)dx \ll \sum_{1 \leq r \leq bT^{1+ \epsilon}
    T_0^{-1}} T^{1+ \epsilon}A_r^{-1/4}\\
  & \ll \sum_{r \leq T^\epsilon} T^{1+ \epsilon}T_0^{-1/4} +
  \sum_{T^\epsilon < r \leq bT ^{1+\epsilon}T_0^{-1}} T^{1+ \epsilon}
  T_0^{-1/4}r^{-1/4}\\
  & \ll T^{1+2 \epsilon}T_0^{- 1/4} + T^{7/4+\epsilon}T_0^{-1}.
\end{align*}

It is the contribution of the last term above which is large, and sets
the limit of Heath-Brown's method to $E_2(T) \ll T^{7/8+\epsilon}$, on
choosing $T_0 = T^{7/8}$. With this choice of $T_0$ we have
$$ 
\sum(T) = 2 \sum_{1 \leq r \leq b T^{1+ \epsilon}T_0^{-1}}
\int\limits_{A_r}^{bT} m' (x, r) h (x, r) dx + O \left(T^{7/8+\epsilon}\right), 
$$
with\pageoriginale
\begin{align*}
  m' (x, r) & = \left(c_0 (r) + c_1 (r)\right) + \left(c_1(r) + 2 c_2
  (r)\right) \log x + c_2 (r) \log^2 x\\[5pt]
  & = d_0 (r) + d_1 (r) \log x + d_2 (r) \log^2 x,
\end{align*}
say. Thus 
$$
\sum (T) = 2 \sum^2_{j=0} \sum_{1 \leq r \leq bT^{1+ \epsilon}
  T_0^{-1}} d_j (r) \int\limits_{A_r}^{bT} h (x, r) \log^j x \, dx +
O\left(T^{7/8+\epsilon}\right).  
$$

By the second mean value theorem for integrals it is seen that $h(x,
r)$ equals at most four expressions of the form
$$ 
\frac{1}{r} \sin (\tau_1 rx^{-1}) \nu (\tau_2/(2\pi x)) \qquad
\left(\frac{T}{2} \leq \tau_1, \tau_2 \leq \frac{5T}{2} \right),
$$
each multiplied by a factor containing the $f$-function. Hence by
Lemma \ref{c2:lem2.1} 
$$
\int\limits_0^{A_r} h (x, r) \log^j x \, dx \ll T^{\epsilon
  -1}r^{-2}A_r^2 \ll T^{2\epsilon-1} T^2_0,
$$
and so 
$$
\sum_{1 \leq r \leq bT^{1+ \epsilon}T^{-1}_0} d_j (r)
\int\limits_0^{A_r} h(x, r) \log^j x\, dx \ll T^{2\epsilon -1} T^2_0
T^{1+ \epsilon} T_0^{-1} = T^{3 \epsilon}T_0.
$$

Thus 
$$
\sum(T) =2 \sum_{j=0}^2 \sum_{1 \leq r \leq bT^{1+ \epsilon} T_0^{-1}}
d_j(r) \int\limits_0^{bT} h(x, r) \log^j x\, dx + O(T^{7/8+\epsilon}).
$$

Now write
{\fontsize{10}{12}\selectfont
$$
h (x, r) = \frac{1}{x} \int\limits_T^{2T} \nu \left( \frac{t}{2 \pi
  x}\right) \cos \left(\frac{tr}{x} \right) dt + \frac{1}{x}
\left(\int\limits_{T- T_0}^T  + \int\limits_{2T}^{2T +T_0}\right) f(t)
\nu \left(\frac{t}{2 \pi x} \right) \cos \left(\frac{tr}{x} \right)dt
$$}
in case $f(t) = \ob{f} (t)$, and the case when $f(t) = \ub{f} (t)$ is
analogous. The contribution of the last two integrals is estimated in
a similar way. By using Lemma \ref{c2:lem2.1} we have
\begin{align*}
  \sum_{r \leq bT^{1+ \epsilon}T_0^{-1}} & d_j (r) \int\limits_0^{bT}
  \int\limits_{T- T_0}^T f(t) \nu \left(\frac{t}{2 \pi x}
  \right)x^{-1} \cos \left(\frac{tr}{x} \right) dt\, dx\\[5pt]
  & = \sum_{r \leq bT^{1+ \epsilon}T_0^{-1}} d_j (r) \int^T_{T-T_0}
  f(t) \left(\int_0^{bT} \nu \left(\frac{t}{2\pi x} \right) x^{-1}
  \cos \left(\frac{tr}{x} \right)dx\right) dt\\[5pt]
  & \ll T_0 \sum_{r \leq bT^{1+ \epsilon} T_0^{-1}} d_j (r) r^{-1}T_0,
\end{align*}
which\pageoriginale is negligible. The same bound holds for
$\int\limits_{2T}^{2T + T_0}$. Hence combining this estimate with
\eqref{c4:eq4.84}-\eqref{c4:eq4.86} we infer that, for $T_0 =
T^{7/8}$, 
\begin{align}
  S(T) & = 0(T^\epsilon T_0) + 2 \sum_{j=0}^2 \sum_{1 \leq r \leq
    bT^{1+ \epsilon}T_0^{-1}}\label{c4:eq4.91}\\
  &\quad d_j (r) \int\limits_0^{bT} \, \int\limits_T^{2T} x^{-1} \nu
  \left(\frac{t}{2 \pi x} \right) \cos \left(\frac{tr}{x} \right)
  \log^j x \, dt\, dx.\notag
\end{align}

We have
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
  \int\limits_T^{2T} \nu \left(\frac{t}{2 \pi x} \right) \cos
  \left(\frac{tr}{x} \right) dt &= \frac{x}{r} \nu \left(\frac{t}{2 \pi
    x} \right) \sin \left(\frac{tr}{x} \right) \Bigg|_T^{2T}
-\frac{x}{r} \int\limits_T^{2T} \frac{1}{2 \pi x} \nu' \left(\frac{t}{2
    \pi x}\right) \sin \left(\frac{tr}{x} \right)dt.
\end{align*}}

Hence
\begin{align*}
  S(T) & = 2 \sum_{j=0}^2 \sum_{1 \leq r \leq bT^{1+ \epsilon}T_0^{-1}}
  d_j (r) r^{-1} \left\{\int\limits_{0}^{bT} \sin \left(\frac{tr}{x}
  \right) \nu \left(\frac{t}{2 \pi x}\right)\log^j x \cdot dx
  \right\}\Bigg|_{T}^{2T} \\
  &\quad- \frac{1}{\pi} \sum^2_{j=0} \sum_{1 \leq r \leq bT^{1+
      \epsilon}T_0^{-1}} d_j(r) r^{-1}
  \int\limits_T^{2T}\int\limits_0^T x^{-1} \sin \left(\frac{tr}{x}
  \right) \nu' \left(\frac{t}{2 \pi x} \right)\\ 
  & \quad\log^j x\, dx\, dt+  O(T^\epsilon T_0).
\end{align*}

Note that $\nu (y)$, $\nu'(y)=0$ for $y \leq 1/b$, and also
$\nu'(y)=0$ for $y \geq b$. Hence $\int\limits_0^{bT}$ becomes
$\int\limits_0^{bt/2\pi}$ in the first sum above and $\int\limits_{t/2
\pi b}^{bt/2\pi}$ in the second sum. We make the change of variable
$t/(2 \pi x)=y$ to obtain
{\fontsize{9}{11}\selectfont
\begin{align*}
  S(T) & = 2 \sum_{j=0}^2 \sum_{1 \leq r \leq bT^{1+
      \epsilon}T_0^{-1}} d_j(r) r^{-1} \left\{\int\limits_{1/b}^\infty
  \sin (2 \pi yr) \nu (y) \frac{t}{2\pi} \log^j \left(\frac{t}{2\pi y}
  \right) \frac{dy}{y^2} \right\}\Bigg|_T^{2T}\\
  &\quad - \frac{1}{\pi} \sum_{j=0}^2 \sum_{r \leq bT^{1+\epsilon}T_0^{-1}}
  d_j (r) r^{-1} \int\limits_T^{2T}
  \left(\int\limits_{1/b}^{b} \nu'
  (y) \sin(2 \pi yr) \log^j \left(\frac{t}{2 \pi y} \right)
  \frac{dy}{y}\right)dt +  O(T^\epsilon T_0).
\end{align*}}

It\pageoriginale remains to write
\begin{align*}
  \log \frac{t}{2 \pi y} & = \log t - \log (2 \pi y),\\
  log^2 \frac{t}{2 \pi y} & = \log^2 t - 2 \log t \log (2 \pi y) +
  \log^2 (2 \pi y),
\end{align*}
observe that the integral
$$
\int\limits_{1/b}^\infty \sin (2 \pi y r) \nu (y) \log^j (2 \pi y)
\frac{dy}{y^2} 
$$
converges and that, by lemma \ref{c2:lem2.1}, it is uniformly
$O(1/r)$. Similar analysis holds for $\int\limits_{1/b}^b$. This
means that in the expression for $S(T)$ we can extend summation over
$r$ to $\infty$, making an error which is $O(T^{1+\epsilon}
T_0^{-1})$. In the second integral we can integrate $\log^j t$ from
$T$ to $2T$. This finally gives
\begin{equation}
  \label{c4:eq4.92}
  S(T) = \left(C_0 t \log ^2 t+ C_1 t \log  t+ C_2 t\right) \Bigg|_T^{2T} +
  O(T^{7/8+\epsilon}), 
\end{equation}
where each $C_i$ is a linear combination of integrals of the type
\begin{align*}
  & \sum_{r=1}^\infty e_j (r) r^{-1} \int\limits_{1/b}^\infty \sin (2
  \pi y r) \nu (y) \log^j (2 \pi y) \frac{dy}{y^2},\\
  & \sum_{r=1}^\infty f_j (r) r^{-1} \int\limits_{1/b}^b \sin (2\pi
  yr) \nu' (2 \pi yr) \nu' (y) \log^j (2 \pi y) \frac{dy}{y},
\end{align*}
with suitable $e_j (r)$, $f_j (r)$. When combined with
\eqref{c4:eq4.61} $(k=2)$, \eqref{c4:eq4.92} proves Heath-Brown's
result
\begin{equation}
  \label{c4:eq4.93}  E_2(T) \ll T^{7/8+\epsilon}.
\end{equation}

The only problem which remains is the technical one, namely to show
that $(t H_{2, \nu} (\log t))\bigg|_T^{2T}$ from \eqref{c4:eq4.61}
cancels with all terms containing the $\nu$-function in
\eqref{c4:eq4.92}. This can be achieved in two ways: first by
following Zavorotnyi \cite{Zavorotnyi1}, who actually shows that this must be
the case, i,e. that the terms with the $\nu$-function actually cancel
each other. This fact is a difficult part of Zavorotnyi's proof of
\eqref{c4:eq4.6}. Equally difficult, but feasible, is to show this
fact directly. We can\pageoriginale identify the coefficients $a_j$ in
\eqref{c4:eq4.3} in the final formula for $I_2 (T)$ by going through
Heath-Brown's proof. Essentially these will come from
$\int\limits_{1/b}^\infty$ if integration is from 1 to $\infty$, and
no $\nu$-function is present. But $\nu (y) =1$ for $y \geq b$ and in
the integral from 1 to $b$ write $\nu(y) =1+ (\nu (y)-1)$ and show
that $\nu (y) -1 =- \nu (1/y)$ can be cancelled with the corresponding
part from $1/b$ to $b$. The  integrals
$$
\int\limits_{1/b}^b \sin (2 \pi y r)\nu' (y) \log^j (2 \pi y) \frac{dy}{y}
$$ 
will be the ``true'' integrals containing the $\nu$-function. It can
be, however, shown that the coefficients of $H_{2, \nu}$ in
\eqref{c4:eq4.61} will contain exactly the coefficients representable
by these integrals. This requires a lot of additional (technical)
work, and will not be carried out explicitly here, since Zavorotnyi's
work already established that eventually all the terms containing the
$\nu$-function cancel each other. Naturally, Heath-Brown's proof of
\eqref{c4:eq4.93} is in several ways technically simpler than the
above one, since it does not use the $\nu$-function but smoothing with
the exponential factor $\exp (-t^2 G^{-2})$, which is easier to
handle.

N. Zavorotnyi's improvement of \eqref{c4:eq4.93}, namely
\eqref{c4:eq4.6}, is based on the convolution formula of
N.V. Kuznetsov \cite{Kuznetsov4}. This gives an explicit representation of 
\begin{align}
  \label{c4:eq4.94}
  & W_N (s, \gamma; w_0, w_1) = N^{s-1}\sum_{n=1}^\infty
  \tau_\nu (n)\\ 
  & \left\{ \sigma_{1-2s} (n-N)w_0 \left(\sqrt{\frac{n}{N}} \right) +
  \sigma_{1-2s} (n + N)w_1 \left(\sqrt{\frac{n}{N}} \right) \right\},\notag
\end{align}
where
$$
\sigma_a (n)= \sum_{d \mid n} d^a, \tau_\nu (m) = \mid m
\mid^{\nu-1/2} \sigma_{1-2 \nu}(m), \sigma_{1- 2 s}(0) = \zeta (2 s-1), 
$$
$w_0, w_1$ are sufficiently smooth functions with rapid decay. Crudely\break
speaking, Heath-Brown used \eqref{c4:eq4.89} as the pointwise estimate
of the absolute value of the Kloosterman sum, whereas Kuznetsov's
formula for \eqref{c4:eq4.94} involves an average of Kloosterman sums,
where a much larger\pageoriginale cancellation of terms occurs, and at
the end the sharp result \eqref{c4:eq4.6} is obtained. That massive
cancellation occurs in sums of Kloosterman sums may be seen from
N.V. Kuznetsov's bound 
\begin{equation}
  \label{c4:eq4.95}
  \sum_{c \leq T} S(m, n; c)c^{-1} \ll T^{1/6} (\log T)^{1/3}.
\end{equation}

Actually in application to the fourth moment Kuznetsov's formula
\eqref{c4:eq4.94} is used with $w_0 (x) =0$, $w_1 (x)= w_N(x) \in
C^\infty (0, \infty)$ and with $s, \nu \to 1/2$. This is written as 
\begin{align*}
   W_N \left(\frac{1}{2} , \frac{1}{2}; 0, w_N \right): & = N^{-1/2}
  \sum_{n=1}^\infty d(n) d(n+N)w_N \left(\sqrt{\frac{n}{N}} \right)\\
  & = Z_N^{(d)} \left(\frac{1}{2}, \frac{1}{2}; h_0 \right) +
  Z_{-N}^{(d)} \left(\frac{1}{2}, \frac{1}{2}; h_0- h_1 \right)\\
  & \quad  +Z_N^{(c)} \left(\frac{1}{2}, \frac{1}{2}; h_0- h_1 \right) +
  Z_N^{(p)} \left(\frac{1}{2}, \frac{1}{2}; h^* \right)+ G_N.
\end{align*}

The $Z_N$'s are explicit, but complicated expressions, involving the
Hecke series and spectral values of the corresponding Laplacian. These
functions will contribute to the error term (in the application to
$I_2 (T)$, $N\leq T^{1/3}$ with the choice $T_0 = T^{2/3}$). The main
term will arise, after a long and complicated calculation, from
\begin{align*}
  G_N &= \lim\limits_{\substack{s \to 1/2\\\nu\to 1/2}} \Big\{\zeta_N (s,
  \nu) V_N (1/2, \nu) +  \zeta_N (s, 1- \nu) V_N (1/2, 1-\nu)\\ 
  &\quad +
  \zeta_N (1- s, \nu) V_N (s, \nu)+ \zeta_N (1-s, 1-\nu) V_N (s,
  l-\nu) \Big\}
\end{align*}
where
\begin{align*}
  \zeta_N (s, \nu) & = \frac{\zeta(2 s)\zeta(2\nu)}{\zeta(2s+ 2\nu)}
  \tau_{s+\nu} (N),\\
  V_N (s, \nu) & = \int\limits_0^\infty (1+ x^2)^{1-2s}w_N (x) x^{2
    \nu} dx.
\end{align*}

The complete details of the proof are given by N. Zavorotnyi \cite{Zavorotnyi1},
and will not be reproduced here. Another reason for not giving the
details of proof is that in Chapter \ref{c5} spectral theory will be
extensively used in presenting Motohashi's formula for the fourth
moment (Theorem \ref{c5:thm5.1}), which is more powerful than
Heath-Brown's or Zavorotnyi's.

Also\pageoriginale in Section \ref{c5:sec5.3} more details on spectral
theory and hecke series may be found.

\section{The Sixth Power Moment}\label{c4:sec4.7}

We shall conclude this chapter by giving a discussion of $I_3 (T)$,
based on Theorem \ref{c4:thm4.3} and \eqref{c4:eq4.61}. This problem
is considerably more difficult than the problem of evaluation of $I_1
(T)$ or $I_2 (T)$, and it does not seen possible at this moment to
prove by any existing method even the weak upper bound $I_3 (T) \ll
T^{1+\epsilon}$, much less an asymptotic formula for $I_3 (T)$. As in
the case of $I_2(T)$ we define similarly (see \eqref{c4:eq4.59} with
$k=3$)
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
  S(T) & = \sum_{\substack{m \neq n; m,n=1\\1-\delta \leq m/n \leq 1+
      \delta}} d_3 (m) d_3 (n) (mn)^{-1/2}
\re
  \left\{\int\limits_0^\infty f(t) \nu \left(
  \left(\frac{t}{2\pi}\right)^{3/2}m^{-1} \right) \left(\frac{m}{n}
  \right)^{it}dt \right\}\\[5pt]
  & = S_i (T) + S_{ii}(T) + S_{iii}(T),
\end{align*}}
say. We have $0 < \delta < 1/2$, in $S_i(T) m> n$, $m/n \leq 1+
\delta$, in $S_{ii} (T)$ we have $m<n$, $n/m \leq 1+ \delta$, and in
$S_{iii}(T)$ $1+ \delta < m \leq 1/(1-\delta)$. The property that $\nu
(y) =0$ for $y \leq 1/b$ gives the condition $m, n \leq bT^{3/2}$ in
$S(T)$, since $1- \delta \leq \frac{m}{n} \leq 1+ \delta$. We
integrate $S_{iii}(T)$ by parts and use $f'(t) \ll T_0^{-1}$ to obtain 
$$
S_{iii} (T) \ll \sum_{m, n \leq bT^{3/2}} d_3 (m) d_3 (n) (mn)^{-1/2}
T_0^{-1} \ll T^{3/2} T_0^{-1} \log^4 T. 
$$

In $S_i (T)$ we set $m=n +r$, and in $S_{ii}(T)n= m+r$. Changing $m$
into $n$ in $S_{ii}(T)$ we obtain
\begin{align*}
  S(T) & = O(T^{3/2}T_0^{-1} \log^4 T)+ \sum_{n=1}^\infty \sum_{r \leq
    \delta n} d_3 (n) d_3 (n+r)n^{-1/2} (n+r)^{-\frac{1}{2}}\\
  &\quad \int\limits_0^\infty f(t) \left\{\nu
  \left(\frac{(t/2\pi)^{3/2}}{n+r} \right) + \nu
  \left(\frac{(t/2\pi)^{3/2}}{n} \right) \right\} \cos (t \log (1+
  \frac{r}{n}))dt. 
\end{align*}

As\pageoriginale in the case of $I_2 (T)$, the terms for which $n \leq
T_0$ by Lemma \ref{c2:lem2.1} make a contribution which is 
$$
\ll \sum_{n \leq T_0} \sum_{r \leq \delta n} \frac{d_3 (n) d_3
  (n+r)}{n} \frac{n}{r} \ll T^\epsilon T_0.
$$

Now choose a fixed $\delta$ such that $0 < \delta < \frac{1}{2},
\epsilon > 0$ arbitrarily small but fixed, and $T_1=T_1 (\delta,
\epsilon)$ so large that $T^\epsilon T_0^{-1} \leq \delta$ for $T \geq
T_1$. For $r > T^\epsilon T^{-1}_0n $ and $n > T_0$ we have $\log (1+
r/n) \gg T^\epsilon T_0^{-1}$. Thus integrating $R$ times by parts we
have 
\begin{align*}
&  \sum_{T_0 < n \leq bT^{3/2}}  ~\sum_{T^\epsilon T_0^{-1} n < r \leq
      \delta n} d_3 (n) d_3 (n+r) n^{-1/2} (n+r)^{-1/2} \\
& \int\limits_0^\infty   f(t) \left\{ \nu
  \left(\frac{(d/2\pi)^{3/2}}{n+r}\right) + \nu
  \left(\frac{(t/2\pi)^{3/2}}{n}\right)\right\} \cos \left(t \log
  \left( 1+ \frac{r}{n}\right) \right)dt\\
& \ll T^\epsilon \sum_{T_0 \leq n \leq bT^{3/2}} \frac{1}{n}
  \sum_{T^\epsilon T_0^{-1} n < r \leq \delta n} TT^{-R}_0  \left(\log
  \left(1+ \frac{r}{n} \right)\right)^{-R}\\
& \ll T^{1+\epsilon} \sum_{T_0 \leq n \leq bT^{3/2}} \frac{1}{n}
  T_0^{-R} \sum_{T^\epsilon T_0^{-1} n < r \leq \delta n} n^R r^{-R}\\
& \ll T^{1+ \epsilon}T_0^{-R} \sum_{n \leq bT^{3/2}} n^{R-1} (T^\epsilon
  T_0^{-1})^{1-R} \ll T^{1+\epsilon + \epsilon (1-R)}
  T_0^{-1}T^{3/2}\\
& \ll 1
\end{align*}
if $R= [5/(2\epsilon)+1]$, say. We thus have
\begin{align}
   S(T)  &= O(T^\epsilon T_0) + O(T^{3/2+\epsilon}
  T_0^{-1})\label{c4:eq4.96}\\[5pt] 
  &\quad +\sum_{T_0
  \leq n \leq bT^{3/2}}  \sum_{r \leq T^\epsilon T_0^{-1} n} d_3 (n)
  d_3 (n+r)n^{-1/2} (n+r)^{-1/2}\notag\\
   &\quad \int\limits_0^\infty f(t) \left\{ \nu \left(\frac{(
    t/2\pi)^{3/2}}{n+r} \right) + \nu
  \left(\frac{(t/2\pi)^{3/2}}{n}\right)\right\} \cos \left(t \log
  \left(1+ \frac{r}{n} \right)\right)dt.\notag
\end{align}

Now we use 
$$ 
\nu \left(\frac{(t/2\pi)^{3/2}}{n+r} \right) = \nu
\left(\frac{(t/2\pi)^{3/2}}{n}\right) - \left(\frac{t}{2 \pi}
\right)^{3/2} \frac{r}{n^2} \nu' \left(\frac{(t/2\pi)^{3/2}}{n}
\right) + O\left(\frac{t^3 r^2}{n^4} \right)
$$
and Lemma \ref{c2:lem2.1} to find that the total error terms coming
from the $O$-term and $\nu'$ are $\ll T^{4+ 4\epsilon}T^{-4}_0$ and
$T^{3/2+ 2 \epsilon} T^{-1}_0$, respectively. Hence for $T^{1/2} \leq
T_0 \ll T^{1- \epsilon}$ \eqref{c4:eq4.96} becomes
\begin{align}
  S(T)& = O (T^\epsilon T_0) + O\left(T^{3/2 + 2
    \epsilon}T_0^{-1}\right)+ O\left(T^{4+ 4 \epsilon}
  T_0^{-4}\right)\label{c4:eq4.97}\\ 
  & \quad +2 \sum_{T_0 \leq n \leq b T^{3/2}} \sum_{r \leq
    T^\epsilon T_0^{-1}\pi} d_3 (n) d_3 (n+r) n^{-1/2} (n+r)^{-1/2}\notag\\
  & \quad\quad \int\limits_0^\infty f(t) \nu
  \left(\frac{(t/2\pi)^{3/2}}{n} \right)  \cos \left(t \log \left(1+
  \frac{r}{n} \right) \right)dt. \notag 
\end{align}

The\pageoriginale last sum can be further simplified if we use
$$
(n+r)^{-1/2} = n^{-1/2} - \frac{1}{2} rn^{-3/2} +  O \left(r^2 n^{-5/2}\right).
$$

The contribution of the terms $-\frac{1}{2} r n^{3/2}$ to $S(T)$ will
be, by using Lemma \ref{c2:lem2.1},
$$
\ll T^\epsilon \sum_{T_0 \leq n \leq bT^{3/2}}n^{-2} \sum_{r \leq
  T^\epsilon T_0^{-1} n} r nr^{-1} \ll T^{3/2 + 2 \epsilon} T_0^{-1}.
$$

The $O$-term will contribute
\begin{align*}
  & \ll T^\epsilon \sum_{T_0 \leq n \leq bT^{3/2}} n^{-1} \sum_{r \leq
      T^\epsilon T_0^{-1} n} Tr^2 n^{-5/2}\\[5pt]
  & \ll T^{1+\epsilon} \sum_{T_0 \leq n \leq bT^{3/2}} n^{-1/2} T^{3
    \epsilon} T_0^{-3} \ll T^{7/4 + 4 \epsilon} T^{-3}_0  \ll T^{3/2 + 4 \epsilon} T_0^{-1}
\end{align*}
if $T_0 \geq T^{1/2}$. We further simplify \eqref{c4:eq4.97} by using 
$$
\cos \left( t \log \left(1+ \frac{r}{n} \right)\right) = \cos
\left(\frac{tr}{n} \right) + \frac{tr^2}{2n^2} \sin \left(\frac{tr}{n}
\right) + O \left(\frac{t^2 r^4}{4} \right).
$$

The contribution of the last error term is $\ll T^{9/2+ 6\epsilon}
T_0^{-5}$, and the sine term contributes $\ll T^{5/2 + 3
  \epsilon}T_0^{-2}$ if we again use Lemma \ref{c2:lem2.1}. For
$T^{1/2} \leq T_0 \ll T^{1- \epsilon}$ we have
$$
T^{9/2}T_0^{-5} \leq T^4 T_0^{-4},
$$
and we obtain 
$$
S(T) = \sum (T) + O \left\{T^{6\epsilon}\left(T_0 + T^{3/2}T_0^{-1} +
T^{5/2}T_0^{-2} + T^4 T_0^{-4}\right) \right\},
$$
where we set
\begin{multline*}
  \sum(T) =2 \sum_{T_0 \leq n \leq b T^{3/2}} \sum_{\gamma \leq
    T^\epsilon T_0^{-1}n} d_3 (n) d_3 (n+r)n^{-1}\\
  \int\limits_0^\infty f(t)\nu \left(\frac{(t/2\pi)^{3/2}}{n} \right)
  \cos \left(t \log \left(1+ \frac{r}{n} \right) \right) dt.
\end{multline*}

Therefore\pageoriginale if we collect all previous estimates we obtain

\begin{thm}\label{c4:thm4.5}
  For $T^{1/2} \leq T_0 \ll T^{1-\epsilon}$ there exist polynomials
  $Q_9 (y)$ and $H_{3, \nu}(y)$ of degree nine and seven,
  respectively, such that the coefficients of $H_{3, \nu}$ depend on
  the smoothing function $\nu (\cdot)$, while those of $Q_9$ do not,
  and
  \begin{align}
     I_{3,f} (T)&= \left(t Q_9 (\log t)\right) \Bigg|_T^{2T} +
     \left(tH_{3, \nu} 
    (\log t)\right) \Bigg|_T^{2T}\notag\\
 &\quad + O \left\{ T^{6\epsilon}  \left(T_0 + T^{3/2} T_0^{-1} +
    T^{5/2}T_0^{-2} + T^4 T_0^{-4}\right) \right\}\notag\\  
 &\quad +4 \sum_{T_0 \leq n \leq b T^{3/2}}  \sum_{1 \leq r \leq T^\epsilon
      T_0^{-1} n} d_3 (n) d_3 (n+r) n^{-1}\notag\\ 
 &\quad\times\int\limits_0^\infty f(t)
    \nu \left(\frac{(t/2\pi)^{3/2}}{n} \right) \cos \left(\frac{tr}{n}
    \right)dt. \label{c4:eq4.98}
  \end{align}
\end{thm}

Obviously, the best error term one can get here is $O(T^{5/6+
  \epsilon})$ with the choice $T_0 = T^{5/6}$. But the main difficulty
lies in the evaluation of the double sum in \eqref{c4:eq4.98}. I
expect the double sum in question to equal
$$
\left(t R_{3, \nu} (\log t) + t S_3 (\log t)\right)\Bigg|_T^{2T}
$$
plus an error term, where $R_{3, \nu}(y)$ is a polynomial of degree
seven in $y$ whose coefficients depend on $\nu$, and actually equals
$- H_{3, \nu} (y)$. The coefficients of the polynomial $S_3 (\log t)$,
of degree $\leq 7$ in $\log t$, should not depend on $\nu$. It is hard
to imagine that one could take advantage of the fact that the sum in
\eqref{c4:eq4.98} is a double sum, when the same situation was
difficult to exploit in the simpler case of $I_2 (T)$. One thing seems
clear: there is hope of getting $I_3 (T) \leq T^{1+ \epsilon}$ (weak
form of the sixth moment) by this method only if one can take
advantage in \eqref{c4:eq4.98} of special properties of the function
$d_3 (\cdot)$. Specifically, one should try to establish\pageoriginale
an asymptotic formula for the summatory function of $d_3 (n) d_3
(n+r)$, where $n \ll T^{3/2}$, $r \ll T^{1+\epsilon}$. Trivial
estimation of $\int\limits_0^\infty \ldots dt$ in \eqref{c4:eq4.98}
by Lemma \ref{c2:lem2.1} produces only the trivial bound
$$
I_3 (T) \ll T^{3/2+ \epsilon}.
$$

It should be of interest even to reprove
$$
I_3 (T) \ll T^{5/4+ \epsilon},
$$
which is (up to ``$\epsilon$'') the best currently known upper bound
for $I_3 (T)$. Finally, it was pointed out by Y. Motohashi that the
key to solving the sixth power moment problem lies probably in the use
of spectral theory of $SL (3, \mathbb{Z})$. Motohashi was led to this
assertion by analogy with the study of $I_2 (T)$. His powerful method
will be fully explained in Chapter \ref{c5}. In any case, the formula
\eqref{c4:eq4.98} may serve as the basis for an attack on the sixth
moment. The double sum appearing in it has the merit that the sum over
$r$ is ``short'', in the sense that the range is $\ll T^{3/2 +
  \epsilon}T_0^{-1}$, while is the range for $n$ is $\ll T^{3/2}$, and
heuristically some saving should result from this situation.

\newpage

\begin{center}
  \textbf{\LARGE Notes For Chapter 4}
\end{center}

\bigskip

A\pageoriginale proof of A.E. Ingham's classical result
\eqref{c4:eq4.2}, given in his work \cite{Ingham1}, is given in Chapter \ref{c5} of
Ivi\'c \cite{Ivic1}. The latter proof is based on the ideas of
K. Ramachandra \cite{Ramachandra1} and the use of the mean value theorem
\eqref{c1:eq1.15}.

The asymptotic formula of H. Kober \cite{Kober1} for $\int\limits_0^\infty
e^{-\delta t} \left| \zeta (\frac{1}{2} + it)\right|^2 dt$ is proved
in Chapter 7 of Titchmarsh \cite{Titchmarsh1}.

F.V. Atkinson proved, as $\delta \to 0 +$,
{\fontsize{9pt}{11pt}\selectfont
\begin{align}
\int\limits_0^\infty e^{- \delta t} \left|\zeta \left(\frac{1}{2} + it\right)
\right|^4 dt
&= \frac{1}{\delta} \left(A \log^4 \frac{1}{\delta} + B
  \log^3 \frac{1}{\delta} + C \log^2 \frac{1}{\delta} + D \log \log
  \frac{1}{\delta} + E \right)\notag\\ 
&\quad + O \left( \left(\frac{1}{\delta}
    \right)^{13/14+\epsilon} \right)  \label{c4:eq4.99}
\end{align}}
with suitable constants $a, B, C, D, E$. In particular, he obtained
$$
A= \frac{1}{2\pi^2},\quad B=- \frac{1}{\pi^2} \left(2 \log (2\pi) - 6 \gamma +
\frac{24 \zeta' (2)}{\pi^2}\right).
$$

His proof uses T. Estermann's formula \cite{Estermann2} for the sum 
$$
S(x, r): = \sum_{n \leq x} d(n) d(n+r);
$$
see Section \ref{c4:sec4.6} for Heath-Brown's results
\eqref{c4:eq4.87} and \eqref{c4:eq4.88}, proved in his paper
\cite{Heath-Brown3}. Observe that $A= a_4$ in \eqref{c4:eq4.4}, but $B \neq
a_3$. Atkinson remarked that improved estimates for $S (x, r)$,
already available in his time, would improve the exponent in the error
term in \eqref{c4:eq4.99} to $8/9 + \epsilon$, and further
improvements would result from the best known estimates for $S(x, r)$
(see Th. \ref{c4:thm4.1} of N.V. Kuznetsov \cite{Kuznetsov4}). Thus, through the use of
Estermann's formula for $S(x, r)$, Kloosterman sums appear in the
asymptotic formula for the integral in \eqref{c4:eq4.99}. In \cite{Estermann2}
Estermann acknowledges the influence of E. Hecke in the approach to
treat $S(x, r)$ by considering Dirichlet series of $d(n)
e\left(\frac{h}{k}n \right)$. Clearly one can see the genesis of
``Kloostermania'' here.

The work of N.I. Zavorotnyi \cite{Zavorotnyi1} in which he proves
\eqref{c4:eq4.6} exists in the form of a preprint, and to the best of
my knowledge it has not been published in a periodical.

For\pageoriginale N.V. Kuznetsov's convolution formula and related
results involving the use of spectral theory of automorphic functions
see his papers \cite{Kuznetsov1} - \cite{Kuznetsov5}. In \cite{Kuznetsov5} he  claims to have proved
\begin{equation}
  \int\limits_0^T \left|\zeta \left(\frac{1}{2} + it\right)\right|^8 dt \ll T(\log
  T)^B,\label{c4:eq4.100} 
\end{equation}
but although his paper contains some nice and deep ideas,
\eqref{c4:eq4.100} is not proved. Not only is the proof in the text
itself not complete, but Y. Motohashi kindly pointed out that
e.g. the change of triple summation in \eqref{c3:eq3.6} of \cite{Motohashi5}
needs certain conditions under which $\Phi_0 (x)$ does not seem to
satisfy the conditions stated in Theorem 4.

The function $E_k (T)$, defined by \eqref{c4:eq4.9}, should not be
confused with $E_\sigma (T)$ (defined by \eqref{c2:eq2.2} for
$\frac{1}{2} < \sigma < 1$), which was discussed in Chapter \ref{c2} and
Chapter \ref{c3}.

The definition of $c(k)$ in \eqref{c4:eq4.12} is made by using the
gamma - function, so that the expression for $c(k)$ makes sense even
when $k$ is not an integer, although in that case I have no conjecture
about the correct value of $c(k)$.

For some of the approximate functional equations for $\zeta^k (s)$ see\break
Chapter \ref{c4} of Ivi\'c \cite{Ivic1}. This contains also an account of the
Riemann-Siegel formula, for which one also see C.L. Siegel \cite{Siegel1}
and W. Gabcke \cite{Gabcke1}. For Y. Motohashi's analogue of the
Riemann-Siegel formula for $\zeta^2(s)$, see his workds \cite{Motohashi2} and
\cite{Motohashi4}. M. Jutila \cite{Jutila7}, \cite{Jutila8} also obtained interesting
results concerning the approximate functional equation for $\zeta^2
(s)$.

The smoothing function $\rho(x)$ of the type given by Lemma
\ref{c4:lem4.3} is used also by Zavorotnyi \cite{Zavorotnyi1}, but his work does
not give the construction of such a function, whereas Lemma
\ref{c4:lem4.3} does. Other authors, such as A. Good \cite{Good1},
\cite{Good2} or N.V. Kuznetsov \cite{Kuznetsov3}, made use of similar smoothing
functions.

The classical results of G.H. Hardy and J.E. Littlewood on the
approximate functuional equations for $\zeta(s)$ and $\zeta^2 (s)$ are
to be found in their papers \cite{Hardy and Littlewood1} and \cite{Hardy and Littlewood2}.

The rather awkward form of the approximate functional
equation\pageoriginale \eqref{c4:eq4.26} is used in the proof of
Theorem \ref{c4:thm4.3}. This is the reason why \eqref{c4:eq4.26} is
derived. 

For the Perron inversion formula for Dirichlet series, used in the
proof of \eqref{c4:eq4.54}, see the Appendix of Ivi\'c \cite{Ivic1} or
Lemma 3.12 of E.C. Titchmarsh \cite{Titchmarsh1}.

Several interesting results on mean values of $|\zeta (\frac{1}{2} +
it)|$ are obtained by J.B. Conrey and A. Ghosh \cite{Conrey and Ghosh1} - \cite{Conrey and Ghosh4} and
Conrey et al. \cite{Conrey and Ghosh1}. In particular, in \cite{Conrey and Ghosh4} Conrey and Ghosh
consider $c(k)$, as defined by \eqref{c4:eq4.11}, for integral and
non-integral values of $k >0$. Assuming that $c(k)$ exists they show
that
$$
c(k) \geq F_k \Gamma^{-1} (k^2 +1) \prod_p \left\{\left( 1-
\frac{1}{p}\right)^{k^2} \left(\sum_{j=0}^\infty \left(\frac{\Gamma
  (k+j)}{j! \Gamma (k)} \right)^2 p^{-j} \right) \right\}
$$
with specific values $F_3 = 10.13$, $F_4=205$, $F_5 = 3242$, $F_6=
28130$, and with even sharper conditional bounds. None of these lower
bounds contradict my conjectural value \eqref{c4:eq4.12} for $c(k)$.

The discussion on $E(T)$ in Section \ref{c4:sec4.5} complements
Chapter \ref{c2}. R. Balasubramanian's formula \cite{Balasubramanian1} is presented here to
show how a different smoothing technique, namely one with the
exponential function, can be also effectively used. The bound
\eqref{c4:eq4.79} seems to be particularly well-suited for the
application of two-dimensional techniques for the estimation of
exponential sums, but it seems unlikely that these techniques can
improve on the result of Heath-Brown and Huxley (Theorem
\ref{c2:thm2.7}) that $E(T) \ll T^{7/22+\epsilon}$. This is in
distinction with \eqref{c4:eq4.86}, where one does not see how to take
advantage of the fact that the exponential sum in question is two
dimensional.

If instead of Heath-Brown's results \eqref{c4:eq4.87} and
\eqref{c4:eq4.88} one uses Th. \ref{c4:thm4.1}  of N.V. Kuznetsov \cite{Kuznetsov4} for
$E(x, r)$, one can get $E_2 (T) \ll T^{5/7+\epsilon}$ directly by the
method used in the text. N.I. Zavorotnyi's approach \cite{Zavorotnyi1}, which we
also briefly discussed, is in the same vein, but it is more
sophisticated and leads to \eqref{c4:eq4.6}.

From\pageoriginale the discussion of $E(T)$ made in this chapter and
previously in Chapter \ref{c2}, and from the estimation of $E_2(T)$ in
Chapter \ref{c5}, it transpires that
\begin{equation}
  E_k (T) \ll_\epsilon T^{k/3 + \epsilon}\label{c4:eq4.101}
\end{equation}
can be proved for $k=1,2$ by trivial estimation. In fact, for $k=2$ no
non-trivial estimation of exponential sums with the quantities
$\alpha_j H_j^3 (\frac{1}{2})$ is known. The same situation is
expected when $k \geq 3$, so at most that we can heuristically hope
for is \eqref{c4:eq4.101} for $k=3$, which is of course the sixth
moment. But if \eqref{c4:eq4.101} holds for $k=3$, then for $k >3$ it
trivially holds by using the bound for $k=3$ and $\zeta (\frac{1}{2} +
it) \ll t^{1/6}$. A strong conjecture of A.I. Vinogradov \cite{Vinogradov3}
states that 
\begin{equation}
  \sum_{n \leq x}d_k (n) d_k (n+r) = x Q_{2k-2} (\log x; r) +
  0\left(x^{(k-1)/k}\right),\label{c4:eq4.102} 
\end{equation}
where $k \geq 2$ is fixed, $Q_{2k-2}$ is a polynomial in $\log x$ of
degree $2k-2$ whose coefficients depend on $r$, and $r$ lies in a
certain range (although this is not discussed by Vinogradov). The
asymptotic formula \eqref{c4:eq4.102} suggests that perhaps on could
have
$$
E_k (T) \ll_\epsilon T^{(k-1)k+\epsilon}\quad (k \geq 2),
$$
which is stronger than my conjecture \eqref{c4:eq4.21} for $k \geq
3$. Vinogradov's paper \cite{Vinogradov3} stresses the importance of spectral
theory of $SL (k; \mathbb{Z})$ in the problem of the evaluation of
$I_k (T)$.

Additive divisor problems, of which \eqref{c4:eq4.102} is an example,
are especially difficult when $r$ is not fixed, but may be depending
on $x$. When $r$ is fixed, a large literature exists on the asymptotic
formulas for sums of $d_k (n) d_m (n+r)$. For example, Y. Motohashi
\cite{Motohashi1} shows that for any fixed $k \geq 2$
\begin{equation}
  \sum_{n \leq x}d_k (n) d (n+1) = x \sum_{j=0}^k \xi_k (j)
  \log^{k-j}x + O \left(x \frac{(\log \log x)^{c(k)}}{\log x}
  \right) \label{c4:eq4.103} 
\end{equation}
with $c(k) \geq 0$ and constants $\xi_k (j)$ which may be effectively
calculated. When $k=2$ J.M. Deshouillers and H. Iwaniec \cite{Deshouillers and Iwaniec2} obtain
the\pageoriginale asymptotic formula with the error term
$0(x^{2/3+\epsilon})$, and show explicitly
\begin{align*}
  \xi_2 (0) & = 6 \pi^{-2}, \xi_2 (1) = \sum_{n=1}^\infty \mu (n)
  n^{-2} (4 \gamma -4 \log n-2),\\
  \xi_2 (2) & = 4 \sum_{n=1}^\infty \mu (n) n^{-2} \left\{(\gamma -
  \log n)(\gamma - \log n-1)+ 2 \right\}.
\end{align*}

Their technique, based on the use of Kuznetsov's trace formula, was
used by N.V. Kuznetsov himself in \cite{Kuznetsov4} to yield that, for $k=2$
the error term in \eqref{c4:eq4.103} is $O((x \log x)^{2/3})$. For
$k=3$ D.R. Heath-Brown \cite{Heath-Brown5} obtained \eqref{c4:eq4.103} with the
error term $O(x^{1-1/102+ \epsilon})$, while for $k \geq 4$ E. Fouvry
and G. Tenenbaum \cite{Fouvry and Tenenbaum1} have shown that the error term is 
$$
O\left(x \exp \left(- c_1 (k) \sqrt{\log x}\right)\right).
$$

The bound \eqref{c4:eq4.95} is proved by N.V. Kuznetsov \cite{Kuznetsov2}, while the proof of this result (with $(\log T)^{1/3}$ replaced by
$T^\epsilon$) has been alternatively obtained by D. Goldfeld and
P. Sarnak \cite{Goldfeld and Sarnak1}, whose method is simpler than Kuznetsov's. In fact,
Y.V. Linnik \cite{Linnik1} and A. Selberg \cite{Selberg2} independently conjectured
that, for any $\epsilon > 0$ and $T > (m, n)^{1/2+\epsilon}$, one has
$$
\sum_{c \leq T} c^{-1} S(m, n;c) \ll_\epsilon T^\epsilon.
$$

If true, this conjecture is close to being best possible, since M. Ram
Murthy \cite{Ram Murthy1} proved that, for some $C_1 > 0$,
$$
\sum_{c \leq T} c^{-1} S(m, n;c)= \Omega \left(\exp \left(\frac{C_1
  \log T}{\log \log T} \right) \right).
$$

As shown by Theorem \ref{c4:thm4.5}, sums of $d_3 (n) d_3 (n+r)$ play
a fundamental r\^ole in the study of $I_3 (T)$. They were studied,
together with analytic properties of the associated zeta-function
$$
Z_3 (s) = \sum_{n=1}^\infty d_3 (n) d_3 (n+r)n^{-s} \quad (\re s> 1),
$$
by A.I. Vinogradov and L. Tahtad\v zjan \cite{Vinogradov and Tahtadzjan1}. This topic is also
investigated by A.I. Vinogradov \cite{Vinogradov1}, \cite{Vinogradov2}.


