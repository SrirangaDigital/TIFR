\chapter{Elementary Theory}\label{c1}

\section{Basic Properties of $\zeta (s)$}\label{c1:s1}

THE\pageoriginale RIEMANN ZETA-FUNCTION $\zeta(s)$ is defined as 
\begin{equation}
\zeta(s) = \sum^\infty_{n=1} n^{-s}= \prod_p (1-p^{-s})^{-1} (\sigma =
\res > 1),\label{c1:eq1.1}
\end{equation}
where the product is over all primes $p$. For other values of the
complex variable $s= \sigma +$ it it is defined by analytic
continuation. It is regular for all values of $s$ except $s=1$, where
it has a simple pole with residue equal to 1. Analytic continuation of
$\zeta(s)$ for $\sigma > 0$ is given by 
\begin{equation}
  \zeta(s) = (1- 2^{1-s})^{-1} \sum^\infty_{n=1} (-1)^{n-1}
  n^{-s},\label{c1:eq1.2} 
\end{equation}
since the series is \eqref{c1:eq1.2} converges for $\re \;  s > 0$.

For $x> 1$ one has 
\begin{align*}
\sum_{n \leq x} n^{-s} & = \int^x_{1-0} u^{-s} d[u] = [x]x^{-s} + s
\int_1^x [u] u^{-s-1}du\\
& = 0 (x^{1- \sigma}) + s \int_1^x ([u] - u) u^{-s-1} du +
\frac{s}{s-1} - \frac{s x^{1-s}}{s-1}.
\end{align*}
 If $\sigma > 1$ and $x \to \infty$, it follows that
$$
\zeta (s) = \frac{s}{s-1} + s \int^\infty_1 ([u]-u) u^{-s-1} du.
$$

By using the customary notation $\psi (x) =x - [x] - 1/2$ this
relation can be written as 
\begin{equation}
  \zeta(s) = \frac{1}{s-1} + \frac{1}{2} - s \int^\infty_1 \psi (u)
  u^{-s-1}du. \label{c1:eq1.3}
\end{equation}

Since $\displaystyle{\int^{y+1}_y} \psi (u) du=0$ for any $y$,
integration by parts shows that \eqref{c1:eq1.3}\pageoriginale
provides the analytic 
continuation of $\zeta(s)$ to the half-plane $\sigma > -1$, and in
particular it follows that $\zeta (0)=- 1/2$. The Laurent expansion of
$\zeta(s)$ at $s=1$ has the form
\begin{equation}
  \zeta (s) = \frac{1}{s-1} + \gamma_0 + \gamma_1 (s-1) + \gamma_2
  (s-1)^2 + \cdots \label{c1:eq1.4}
\end{equation}
with
\begin{align}
  \gamma_k &= \frac{(-1)^{k+1}}{k!} \int^\infty_{1-0} x^{-1}(\log x)^k d \psi
  (x)\nonumber\\ 
&= \frac{(-1)^k}{k!} \lim\limits_{N \to \infty} \left( \sum\limits_{n\leq N}\frac{\log
    ^k n}{n} - \frac{\log^{k+1}N}{k+1} \right),\label{c1:eq1.5}
\end{align}
and in particular
\begin{align*}
  \gamma_0 &= \gamma = \lim\limits_{N \to \infty} \left(1+ \frac{1}{2} +
  \cdots + \frac{1}{N} - \log N \right) = \Gamma' (1)\\ 
  &=- \int^\infty_{0}
  e^{-x} \log x \, dx = 0.577\ldots
\end{align*}
is Euler's constant. To obtain \eqref{c1:eq1.4} and \eqref{c1:eq1.5},
write \eqref{c1:eq1.3} as 
$$
\zeta (s) = \frac{1}{s-1} = \int^\infty_{1-0} x^{-s} d \psi (x).
$$

Then 
\begin{align*}
  - \int^\infty_{1-0} x^{-s} d \psi (x) & =- \int^\infty_{1-0} x^{-1}
  e^{- (s-1)\log x} d \psi (x)\\ 
  &  = \int^\infty_{1-0} x^{-1}
  \sum^\infty_{k=0} \frac{(-1)^{k+1}}{k!} (\log x)^k (s-1)^k d \psi
  (x)\\
  & = \sum^\infty_{k=0} \left\{ \frac{(-1)^{k+1}}{k!}
  \int^\infty_{1-0} x^{-1} (\log x)^k d \psi (x)\right\} (s-1)^k.
\end{align*}

The inversion of summation and integration is justified by the fact
that, for $k \geq 1$, integration by parts gives
$$
\int^\infty_{1-0} x^{-1} \log^k x d \psi (x) = \int^\infty_{1-0}
x^{-2} \psi (x) (\log^k x- k \log^{k-1}x) dx,
$$
and the last integral is absolutely convergent.

To obtain analytic continuation of $\zeta(s)$ to the whole complex
plane it is most convenient to use the functional equation
\begin{equation}
  \zeta (s) = 2^s \pi^{s-1} \sin \left( \frac{\pi s}{2}\right) \Gamma
  (1-s) \zeta (1-s),\label{c1:eq1.6}
\end{equation}
which is valid for all $s$. Namely, for $\sigma < 0$
$$
-s \int^1_0 \psi (u) u ^{-s -1} du = \frac{1}{s-1} + \frac{1}{2},
$$
hence by \eqref{c1:eq1.3}
\begin{equation}
  \zeta (s) = -s \int^\infty_{0} \psi (u) u^{-s -1}du, \qquad (-1 <
  \sigma < 0). \label{c1:eq1.7}
\end{equation}

By\pageoriginale using the Fourier expansion 
$$
\psi (x) =- \sum^\infty_{n=1} \frac{\sin (2 n \pi x)}{n \pi},
$$
which is valid when $x$ is not an integer, we obtain for $s = \sigma$,
$-1< \sigma < 0$,
\begin{align*}
  \zeta (s) & = \frac{s}{\pi} \sum^\infty_{n=1} \frac{1}{n}
  \int^\infty_{0} u^{-s-1} \sin (2 n \pi u) du\\
  & = \frac{s}{\pi} \sum^\infty_{n=1} \frac{(2 n \pi)^s}{n} \im
  \left\{\int^\infty_0 \frac{e^{iy}}{y^{s+1}} dy \right\}\\
  & = \frac{s}{\pi} \sum^\infty_{n=1} (2 \pi)^s n^{s-1} \im
  \left\{i^{-s} \int^{-i \infty}_0 e^{-z} z^{-s-1}dz \right\}\\
  & = \frac{s}{\pi} (2 \pi)^s \zeta (1-s) \im \left\{
  e^{\frac{- \pi i s}{2}} \int^\infty_0 \frac{e^{-z}}{z^{s+1}}
    dz\right\}\\
    & = \frac{s}{\pi} (2 \pi)^s \zeta (1-s) \im (e^{- \frac{1}{2} \pi
      i s}) \Gamma (-s)\\
    & = \frac{(2 \pi)^s}{\pi} \zeta (1 -s) \sin \left(\frac{\pi
      s}{2}\right)\{ -s \Gamma (-s)\}\\
    & = 2^s \pi^{s-1} \sin \left( \frac{\pi s}{2}\right) \Gamma (1- s)
    \zeta (1-s).
\end{align*}

Here one can justify termwise integration by showing that
$$
\lim\limits_{y \to \infty} \sum^\infty_{n=1} \frac{1}{n}
\int^\infty_{y} \sin (2 n \pi u) u^{-s -1} du =0 \qquad (-1 < \sigma <
0), 
$$
which is easily established by performing an integration by
parts. Thus \eqref{c1:eq1.6} follows for $-1 < s = \sigma < 0$, and
for other values of $s$ it follows by analytic continuation.

One can also write \eqref{c1:eq1.6} as 
\begin{equation}
  \zeta (s) = \chi (s) \zeta (1- s), \chi (s) = (2 \pi)^s /\left(2 \Gamma
  (s)\cos \left(\frac{\pi s}{2} \right)\right).\label{c1:eq1.8}
\end{equation}

By using Stirling's formula for the gamma-function it follows that,
uniformly in $\sigma$,
\begin{equation}
\chi (s) = \left(\frac{2 \pi}{t} \right)^{\sigma + it -
  \frac{1}{2}} e^{i (t+ \frac{1}{4} \pi)} \left\{1 + 0 \left(\frac{1}{t}\right)
\right\} (0 < \sigma \leq 1, t \geq t_0 > 0)
\label{c1:eq1.9}
\end{equation}
and also
\begin{equation}
\chi \left(\frac{1}{2} + it\right) = \left(\frac{2 \pi}{t} \right)^{- it 
  \log (t/2\pi) + it  + \frac{1}{4} i \pi} \left\{1- \frac{i}{24
  t} + 0 \left(\frac{1}{t^2} \right)\right\} \; (t \geq t_0 > 0).\label{c1:eq1.10}
\end{equation}
\pageoriginale

\section{Elementary Mean Value Results}\label{c1:s2}

In general, mean value results concern the evaluation of the integral 
\begin{equation}
  \int^T_1 \big|\zeta (\sigma + it ) \big|^kdt \label{c1:eq1.11}
\end{equation}
as $T \to \infty$, where $\sigma$ and $k(> 0)$ are fixed. Since
$\zeta(s) \gg_\sigma 1$ when $\sigma > 1$, the case $\sigma > 1$ is
fairly easy to handle. By the functional equation \eqref{c1:eq1.8} and
\eqref{c1:eq1.9} it is seen that the range $\sigma < 1/2$ can be
essentially reduced to the range $\sigma \geq 1/2$. Thus the basic
cases of \eqref{c1:eq1.11} are $\sigma 1/2$ (``the critical line''),
$1/2< \sigma < 1$ (``the critical strip'') and $\sigma=1$. Naturally,
the case when $k$ in \eqref{c1:eq1.11} is not an integer is more
difficult , because $\zeta^k(s)$ may not be regular. For the time
being we shall suppose that $k \geq 1$ is an integer, and we also
remark that when $k=2N$ is even the problem is somewhat less
difficult, since $|\zeta (\sigma + it )|^{2N}= \zeta^N (\sigma+
it ) \zeta^N (\sigma - it )$.

In problems involving the evaluation of \eqref{c1:eq1.11} one often
encounters the general divisor function
$$
d_k (n) = \sum_{n= n_1 \ldots n_k} 1,
$$
which denotes the number of ways $n$ may be written as a product of
$k (\geq 2)$ fixed factors. In this notation $d(n) = d_2 (n)$ denotes
the number of all positive divisors of $n$. For $\re \; s > 1$
\begin{equation}
  \zeta^k (s) = \left(\sum^\infty_{n=1} n^{-s} \right)^k =
  \sum^\infty_{n=1} d_k (n) n^{-s}.\label{c1:eq1.12}
\end{equation}

Note that $d_k (n)$ is a multiplicative function of $n$ (meaning $d_k
(mn)= d_k (m) d_k (n)$ for coprime $m$ and $n$) and 
$$
d_k (p^\alpha)= \binom{\alpha + k-1}{k-1} = \frac{k(k+1)\ldots (\alpha
  + k-1)}{\alpha !}.
$$

For fixed $k$ we have $d_k (n) \ll_\epsilon$ $n^\epsilon$ for any
$\epsilon > 0$, which follows from the\pageoriginale stronger
inequality
\begin{equation}
d_k \leq  \exp (C(k) \log n/ \log \log n) \qquad (n \geq
2),\label{c1:eq1.13} 
\end{equation}
where $C(k)> 0$ is a suitable constant. One proves \eqref{c1:eq1.13}
by induction on $k$, since $d_k (n)= \sum\limits_{\delta|n} d_{k-1} (\delta)$
and \eqref{c1:eq1.13} is not difficult to establish for $k=2$. 

To obtain some basic mean-value formulas we shall use a simple
so-called ``approximate functional equation''. This name refers to
various formulas which express $\zeta (s)$ (or $\zeta^k(s)$) as a
number of finite sums involving the function $n^{-s}$. The approximate
functional equation of the simplest kind is 
\begin{equation}
  \zeta (s) = \sum_{n \leq x}n^{-s} + \frac{x^{1-s}}{s-1} + O
  (x^{-\sigma}),\label{c1:eq1.14} 
\end{equation}
and is valid for $0 < \sigma_0 \geq \sigma \leq 2$, $x \geq |t| /\pi$,
$s= \sigma + it $, where the 0-constant depends only on
$\sigma_0$. We shall also need a result for the evaluation of
integrals of Dirichlet polynomials, namely sums of the form
$\displaystyle{\sum_{n \leq N} a_nn^{it }}$, where $t$ is real
and the $a_n$'s are complex. The standard mean-value result for
Dirichlet polynomials, known as the Montgomery-Vaug\-han theorem, is the
asymptotic formula
\begin{equation}
  \int^T_0 \Bigg| \sum_{n \leq N}a_n n^{it } \Bigg|^2 dt = T
  \sum_{n \leq N} |a_n|^2+ O \left( \sum_{n \leq N} n|a_n
  |^2\right).\label{c1:eq1.15} 
\end{equation}

This holds for arbitrary complex numbers $a_1 , \ldots , a_N$, and
remains true if $N= \infty$, provided that the series on the
right-hand side of \eqref{c1:eq1.15} converge.

Now suppose $\frac{1}{2} T \geq t \geq T$ and choose $x=T$ in
\eqref{c1:eq1.14} to obtain, for $1/2< \sigma < 1$ fixed,
$$
\zeta(\sigma + it ) = \sum_{n \leq T} n^{- \sigma - it }+ R, 
$$
where $R \ll T^{- \sigma}$. Since $|\zeta (s)|^2= \zeta (s)
\ob{\zeta(s)}$, we obtain 
\begin{multline*}
  \int^T_{\frac{1}{2} T} |\zeta (\sigma + it )|^2 dt =
  \int^T_{\frac{1}{2} T} \Bigg|\sum_{n \leq T}n^{- \sigma -
    it } \Bigg|^2 dt + 0(T^{1-2 \sigma})\\
  + 2 \re \left\{\int^T_{\frac{1}{2} T} \sum_{n \leq T} n^{- \sigma -
    it }\ob{R} dt \right\}.
\end{multline*}

Using\pageoriginale \eqref{c1:eq1.15} we have
\begin{align}
  \int^T_{\frac{1}{2}T} \left| \sum_{n \leq T} n^{- \sigma -
    it } \right|^2 dt & = \frac{1}{2} T \sum_{n \geq T} n^{- 2
    \sigma}+ O \left( \sum_{n \leq T} n^{1 - 2 \sigma}\right)\label{c1:eq1.16} \\
  & = \frac{1}{2} \zeta (2 \sigma) T+ O (T^{2- 2 \sigma}).\notag
\end{align}

Trivially we have $\displaystyle{\sum_{n \leq T} n^{- \sigma -
    it } \ll T^{1 -\sigma}, R\ll T^{- \sigma}}$, hence 
$$
\int^T_{\frac{1}{2} T} \sum_{n \leq T} n^{- \sigma - it } \ob{R}
dt \ll T^{2- 2 \sigma}.
$$

In the case when $\sigma = \frac{1}{2}$ the analysis is similar, only
the right-hand side of \eqref{c1:eq1.16} becomes
$$
\frac{1}{2} T \sum_{n \leq T} n^{-1} + O \left(\sum_{n \leq T} 1
\right) = \frac{T}{2} \log \frac{T}{2} + O (T).
$$

Thus replacing $T$ by $T 2^{-j}$ in the formulas above $(j = 1, 2,
\ldots)$ and adding the results we obtain
$$
\int^T_{0} |\zeta (\sigma + it )|^2 dt = \zeta (2 \sigma)T + O
(T^{2- 2 \sigma})
$$
and 
$$
\int^T_{0} |\zeta (\frac{1}{2} + it )|^2 dt = T \log T + O (T).
$$

To obtain the mean square formula in the extreme case $\sigma =1$ we
use \eqref{c1:eq1.14} with $s=1 + it $, $1 \leq t \leq T$, $x =
T$. Then we have
$$
\zeta (1+ it ) = \sum_{n \leq T} n^{-1 -it } + \frac{T^{-
    it }}{it }+ O \left( \frac{1}{T}\right).
$$

We use this formula to obtain 
\begin{align}
\int^T_1 |\zeta (1+ it )|^2 dt & = \int^T_1 \left|\sum_{n \leq T}
n^{-1 - it }\right|^2 dt - 2 \re \left\{\frac{1}{i} \int^T_1
\sum_{n \geq T} n^{-1} \left(\frac{T}{n}\right)^{it }
\frac{dt}{t} \right\}\notag\\ 
& \hspace{2cm}+ O \left(\int^T_1 \left|\sum_{n \leq T} n^{-1- it }\right|
\frac{dt}{T} + O (1). \right)\label{c1:eq1.17}
\end{align}

Therefore\pageoriginale by \eqref{c1:eq1.15}
\begin{align}
  \int^T_1 \left| \sum_{n \leqq T} n^{-1 - it }\right|^2 dt &
  =(T-1) \sum_{n \leq T} n^{-2} + 0 \left( \sum_{n \leq T}
  n^{-1}\right)\notag\\
  & =\zeta (2) T + O (\log T),\label{c1:eq1.18}
\end{align}
and so by the Cauchy-Schwarz inequality
$$
\int^T_1 \left|\sum_{n \leq T} n^{-1 - it }\right| \frac{dt}{T}
= O (1).
$$

Finally, let $H$ be a parameter which satisfies $2 \leq H \leq
\frac{1}{2} T$. Then
\begin{align*}
  \int^T_1 \sum_{n \leq T}& n^{-1}
  \left(\frac{T}{n}^{it }\right) \frac{dt}{t}\\
  & = \sum_{n \leq
    T(1-1/H)} n^{-1} \left\{\frac{(T/n)^{it }}{it  \log
    (T/n)} \Big|^T_1 + \int^T_1 \frac{(T/n)^{it }}{it ^2
    \log (T/n)} dt \right\}\\
  & \hspace{3cm}+ O\left( \sum_{T(1-1/H < n\leq T)} n^{-1} \int^T_1
  \frac{dt}{t}\right)\\
  & \ll \sum_{n \leq T (1-1/H)} \frac{1}{n \log (T/n)}+ \log T
  \sum_{T(1-1 /H) < n \leq T} \frac{1}{n}\\
  & \ll \int^{T(1-1/H)} \frac{dx}{x \log (T/x)} + \frac{\log T}{H} + 1\\
  & = \int^T_{(1-1/H)^{-1}} \frac{du}{u \log u} + \frac{\log T}{H} + 1\\
  & \ll \log \log T - \log \log (1-1/H)^{-1}+ \frac{\log T}{H} +1\\
  & \ll \log \log T + \log H + \frac{\log T}{H} + 1 \ll \log \log T  
\end{align*}
for\pageoriginale $H= \log T$. In view of \eqref{c1:eq1.17} this shows
that 
\begin{equation}
  \int^{T}_1 |\zeta (1 + it )|^2 dt = \int^T_1 \left| \sum_{n
    \leq T} n^{-1- it } \right|^2 dt + O (\log \log T), \label{c1:eq1.19}
\end{equation}
and in conjunction with \eqref{c1:eq1.18} we obtain
$$
\int^T_1 |\zeta (1+ it )|^2 dt = \zeta(2) T + O (\log T).
$$

Hence we have proved
\begin{thm}\label{c1:thm1.1}
  For $1/2 < \sigma < 1$ fixed we have
  \begin{equation}
    \int^T_0 |\zeta (\sigma + it )|^2 dt = \zeta (2 \sigma) T+ O
    (T^{2- 2 \sigma}).\label{c1:eq1.20}
  \end{equation}
\end{thm}

Moreover 
\begin{equation}
  \int^T_0 |\zeta (\frac{1}{2}+ it )|^2 dt = T \log T + O
  (T)\label{c1:eq1.21} 
\end{equation}
and 
\begin{equation}
  \int^T_1 |\zeta (1 + it )|^2dt = \zeta (2) T + O (\log
  T).\label{c1:eq1.22}
\end{equation}

It should be remarked that the asymptotic formulas \eqref{c1:eq1.20},
\eqref{c1:eq1.21} and \eqref{c1:eq1.22} cannot be improved, that is,
the error terms appearing in them are in fact of the order $T^{2- 2
  \sigma}$, $T$ and $\log T$, respectively. But the first two formulas
in equation may be given in a much more precise form, which will be
the topic of our study in Chapter \ref{c2} and Chapter \ref{c3}. Note also that the
argument used in the proof of Theorem \ref{c1:thm1.1}yields easily, on using $d_k
(n) \ll_\epsilon n^\epsilon$,
$$
\int^T_1 |\zeta (\sigma + it )|^{2k}dt = \left(\sum^\infty_{n=1}
d^2_k (n) n^{-2 \sigma}\right) T + O (T^{2-2 \sigma})+ 0 (1) 
$$
for $\sigma > 1$ fixed and $k \geq 1$ a fixed integer.

\section{Bounds Over Short Intervals}\label{c1:s3}

We\pageoriginale begin with a useful result which shows that pointwise estimation of
$\zeta^k(s)$ may be replaced by estimation of the integral of $\zeta^k
(s)$ over a short interval, and the latter is in many cases easier to
carry out. This is 

\begin{thm}\label{c1:thm1.2}
  Let $1/2 \leq \sigma \leq 1$ be fixed and let $k\geq 1$ be a fixed
  integer. Then for any fixed constants $\delta$, $A > 0$
  \begin{equation}
    |\zeta (\sigma + it )|^k \ll (\log T) \int^\delta_{- \delta}
    |\zeta (\sigma + it )|^k dv + T^{-A}.\label{c1:eq1.23}
  \end{equation}
\end{thm}

\begin{proof}
  Let $B, C > 0$ denote constants to be chosen later, $r= [C \log T]$,
  $s= \sigma + it $, $T \geq T_0$ and $X = \exp (u_1+ \cdots +
  u_r)$. By the residue theorem we have
  \begin{equation}
    2 \pi i B^r \zeta^k (s)= \int^B_0\cdots \int^B_0 \int_{|w|=\delta}
    \zeta^k (s + w) X^{w_w-1} dw du_1 \ldots du_r.\label{c1:eq1.24}
  \end{equation}

We may clearly suppose that $0<\delta < 1/2$, so that on the
semicircle $|w| = \delta$, $\re \; w < 0$ we have $|e^{Bw} -1| \leq 2$ and
$\zeta (s+w)\ll T^{\frac{1}{2}}$ (because from \eqref{c1:eq1.14} we
trivially have $\zeta (1+ it ) \ll \log T$, and then by the
functional equation $\zeta (it )\ll T^{\frac{1}{2}} \log
  T$). Hence
\begin{align*}
  & \left|\int^B_0 \cdots \int_0^B \int\limits_{|w| = \delta, \re w < 0}
  \zeta^k (s+w) X^{w} w^{-1} dw du_1 \ldots du_r \right|\\
  &=  \left| ~\int\limits_{|w|= \delta, \re w < 0} \zeta^k (s+w)
  \int\limits_0^B  e^{wu_1} du_1 \ldots \int\limits^B_0 e^{wu_r} du_r
  \frac{dw}{w} \right|\\
  &= \left| ~\int\limits_{|w| =  \delta, \re w < 0} \zeta^k (s+w)
  \left( \frac{e^{Bw}-1}{w}\right)^r \frac{dw}{w} \right| \leq \pi
  T^{\frac{1}{2}k} \left(\frac{2}{\delta} \right)^r.
\end{align*}

On the other hand we obtain by Cauchy's theorem
\begin{align*}
  \int\limits_{|w|= \delta, \re w \geq 0} \zeta^k (s+w) X^w \frac{dw}{w}
  = \int\limits_{|w| = \delta, \re w \geq 0} \zeta^k (s+w)\frac{X^w -
    X^{-w}}{w} dw\\
  + \int\limits_{|w| = \delta, \re w \geq 0} \zeta^k (s+w)X^{-w}
  \frac{dw}{w}\\
  = \int\limits_{- i\delta}^{i \delta} \zeta^k (s + w) \frac{X^w -
    X^{-w}}{w} dw + \int\limits_{|w| = \delta, \re \geq 0} \zeta^k
  (s+w) X^{-w} \frac{dw}{w},
\end{align*}
since\pageoriginale $w^{-1} (X^w - X^{-w})$ is regular on the segment $[- i \delta, i
\delta]$. On the semicircle $|w| = \delta$, $\re w \geq 0$ we have
$|X^{-w}| \leq 1$, hence the total contribution of the last integral
above will be again in absolute value $\leq \pi T^{\frac{1}{2} k}
(2/\delta)^r$. Thus \eqref{c1:eq1.24} gives 
\begin{multline*}
|\zeta (\sigma + it )|^k \leq B^{-r} \int\limits^B_0 \cdots
\int\limits^B_0 \left| \int\limits_{- i \delta}^{i \delta} \zeta^k
(s+w) \frac{X^w - X^{-w}}{w} dw \right| du_1 \ldots du_r\\ 
+ 2 \pi T^{\frac{1}{2} k} \left(\frac{2}{\delta B} \right)^r.
\end{multline*}

For $w = iv$, $- \delta \leq v \leq \delta$ we have
\begin{multline*}
  \left|\frac{X^w - X^{-w}}{w}\right|  = 2 \log X \left| \frac{e^{i v\log X}-
    e^{- iv \log X}}{2 iv \log X}\right|= 2 \log X \left|\frac{\sin (v
    \log X)}{v \log X} \right|\\
  \leq 2 \log X = 2 (u_1 + \cdots + u_r) \leq 2 Br \leq \log T.
\end{multline*}

Taking $B= 4 \delta^{-1}$, $r= [C \log T]$ with $C= C (k, \delta , A)>
0$ a sufficiently large constant, we obtain
\begin{align*}
  |\zeta (\sigma + i T)|^k & \ll B^{-r} B^r \int\limits^\delta_{-
    \delta} |\zeta (\sigma + iT + iv)|^k \log T dv + T^{\frac{1}{2}k}
  2^{-r}\\
  & \ll (\log T) \int\limits^\delta_{- \delta} |\zeta (\sigma+ iT+
  iv)|^k dv + T^{-A},
\end{align*}
as asserted.

From Theorem \ref{c1:thm1.2} one sees immediately that the famous
Lindel\"of hypothesis that $\zeta (\frac{1}{2} + it )
\ll_\epsilon |t|^\epsilon$ is equivalent to the statement that
\begin{equation}
  \int\limits_{1}^T |\zeta\left(\frac{1}{2} + it \right)|^k dt \ll T^{1 +
    \epsilon} \label{c1:eq1.25}
\end{equation}
holds\pageoriginale for any fixed integer $k \geq 1$. That the
lindel\"of hypothesis implies \eqref{c1:eq1.25} is obvious, and by
\eqref{c1:eq1.23}
\begin{align*}
|\zeta \left(\frac{1}{2}+ iT\right)|^k &\ll \log  T \int\limits^{T+1}_{T-1} |\zeta
\left(\frac{1}{2} + iu\right)|^k du + T^{-A}\\ 
&\ll T^{1+\epsilon} \log T \log T \ll T^{1+ 2 \epsilon}
\end{align*}
if \eqref{c1:eq1.25} holds. Thus for any $\epsilon_1 > 0$
$$
\zeta (\frac{1}{2} + i T) \ll T^{\epsilon_1}
$$
if $k= [(1+ 2 \epsilon)/\epsilon_1]+1$ in the last bound above. At the
time of the writing of this text both the Lindel\"of hypothesis and
the stronger Riemann hypothesis (all complex zeros of $\zeta(s)$ lie
on the line $\sigma= 1/2$) are neither known to be true nor false. The
Lindel\"of hypothesis may be rephrased as $\mu (\sigma)=0$ for $\sigma
\geq 1/2$, where for any real $\sigma$ one defines
$$
\mu(\sigma) = \mathop{\lim\sup}_{t \to \infty} \frac{\log |\zeta
  (\sigma + it )|}{\log t},
$$
so that $\zeta (\sigma + it ) \ll t^{\mu (\sigma) + \epsilon}$
holds, but $\zeta(\sigma+ it ) \leq t^c$ does not hold if $c<
\mu (\sigma)$. It may be shown that the function $\mu (\sigma)$ is
convex downward and non-increasing. The last assertion is a
consequence of 
\end{proof}


\begin{thm}\label{c1:thm1.3}
  For $0 \leq \sigma_1 \leq \sigma_0 \leq \sigma_1 + \frac{1}{2} \leq
  \frac{3}{2}$, $t \geq t_0$ we have
  \begin{equation}
   \zeta (\sigma_0 + it ) \ll 1 + \mathop{\max}_{|v|\leq \log
     \log t} |\zeta (\sigma_1 + it + iv)|.\label{c1:eq1.26}
  \end{equation}
\end{thm}

\begin{proof}
  Let $\mathscr{D}$ be the rectangle with vertices $\sigma_1 +
  it  \pm i \log \log t$, $\frac{11}{10} + it  \pm i \log
  \log t$. The function
  $$
  f(s) = \zeta(s) \exp \left(- \cos \left(\frac{\pi}{3} (s - s_0)
  \right)\right), s_0 = \sigma_0 + it  
  $$
  is regular in the domain bounded by $\mathscr{D}$. Therefore by the
  maximum modulus principle
  $$
  |\zeta (s_0)| = e|f (s_0)|\leq e \mathop{\max}_{s \in \mathscr{D}}
  \left|\zeta (s) \exp \left(- \cos \left(\frac{\pi}{3} (s-s_0)
  \right) \right)\right|. 
  $$
\end{proof}

But\pageoriginale for $w= u+ iv (u, v$ real) we have
\begin{gather*}
  |\exp (- \cos w)|= \left|\exp \left(- \frac{1}{2} (e^{iw} + e^{-
    iw})\right)\right| \\
  \left|\exp \left( - \frac{1}{2} (e^{iu}e^{-v} + e^{-iu} e^v)\right)
  \right| = \exp (- \cos u \cdot ch\, v),
\end{gather*}
which decays like a second-order exponential as $|v| \to \infty$ if
$\cos u > 0$. If $s \in  \mathscr{D}$, then $|\re (s - s_0)| \leq
\frac{11}{10}$, hence 
$$
\cos \left(\frac{\pi}{3} (s- s_0) \right) \geq \cos \left(\frac{11
  \pi}{30} \right): A > 0, 
$$
and the maximum of $|f(s)|$ on the side of $\mathscr{D}$ with $\re s=
\frac{11}{10}$ is $O (1)$. On the horizontal sides of $\mathscr{D}$ we
have $|Im (s- s_0| = \log \log t$, and since trivially $\zeta (s) \ll
t$ we have that the maximum over these sides is
\begin{multline*}
  \ll t \exp \left(- A ch \left(\frac{\pi}{3} \log \log t \right)
  \right) \leq t \exp \left(\frac{A}{2} e^{(\pi \log \log t)/3}
  \right)\\
  = t \exp \left(- \frac{A}{2} \left(\log t\right)^{\pi/3} \right) = 0(1)
\end{multline*}
as $t \to \infty$. On the vertical side of $\mathscr{D}$ with $\re s =
\sigma_1$ the exponential factor is bounded, and \eqref{c1:eq1.26}
follows.

From \eqref{c1:eq1.14}, the functional equation and convexity it
follows that
\begin{equation}
\zeta (\sigma + it ) \ll
\begin{cases}
  1 & \text{for}~ \sigma \geq 2,\\
  \log t & \text{for} ~1 \leq \sigma \leq 2,\\
  t^{\frac{1}{2}(1- \sigma)} \log t & \text{for}~ 0 \leq \sigma \leq
  1,\\
  t^{\frac{1}{2} - \sigma} \log t & \text{for}~ \sigma \leq 0.
\end{cases}\label{c1:eq1.27}
\end{equation}

The bound in \eqref{c1:eq1.27} for $0 \leq \sigma \leq 1$ is not best
possible, and the true order of $\zeta (\sigma + it )$ (or the
value of $\mu(\sigma)$) is one of the deepest problems of
zeta-function theory. By using the functional equation one obtains
$\mu(\sigma) \leq \frac{1}{2} - \sigma + \mu (1- \sigma)$, so that the
most interesting range for $\sigma$ is $\frac{1}{2} \leq \sigma \leq
1$. The latest bounds for $\zeta (\frac{1}{2}+
it )$\pageoriginale are
\begin{align}
\mu \left(\frac{1}{2}\right) & \leq \frac{89}{560}  = 0.15892 \ldots ,
\mu \left(\frac{1}{2}\right) \leq \frac{17}{108} =
0.15740\ldots,\label{c1:eq1.28} \\
\mu \left( \frac{1}{2}\right) & \leq 89/570 =
0.15614\ldots\notag
\end{align}
due to N. Watt \cite{Watt1}, M.N. Huxley and G. Kolesnik \cite{Huxley and Kolesnik1} and 
M.N. Huxley \cite{Huxley4}, respectively. These are the last in a long
string of improvements obtained by the use of intricate techniques
from the theory of exponential sums.

We pass now to a lower bound result for mean values over short
intervals. As in the previous theorem we shall make use of the kernal
$\exp (- \cos w)$, which regulates the length of the interval in our
result.

\begin{thm}\label{c1:thm1.4}
  If $k \geq 1$ is a fixed integer, $\sigma \geq 1/2$ is fixed, $12
  \log \log T \leq Y \leq T$, $T \geq T_0$, then uniformly in $o$
  \begin{equation}
    \int\limits_{T-Y}^{T+Y} |\zeta (\sigma + it )|^k dt \gg
    Y.  \label{c1:eq1.29} 
  \end{equation}
\end{thm}

\begin{proof}
  Let $\sigma_1 = \sigma +2$, $s_1 = \sigma_1 + it $, $T -
  \frac{1}{2} Y \leq t \leq Y + \frac{1}{2} Y$.
\end{proof}

Then $\zeta (s_1) \gg 1$ and therefore 
\begin{equation}
\int\limits_{T- \frac{1}{2}Y}^{T + \frac{1}{2}Y} |\zeta (\sigma_1 +
it )|^k dt \gg Y.
\label{c1:eq1.30}
\end{equation}

Let now $\mathscr{E}$ be the rectangle with vertices $\sigma+ i T \pm
i Y$, $\sigma_2 + i T \pm i Y$ $(\sigma_2 = \sigma+3)$ and let $X$ be
a parameter which satisfies
$$
T^{-c} X \leq T^c
$$
for some $c >0$. The residue theorem gives
$$
e^{-1} \zeta^k (s_1)= \frac{1}{2 \pi i} \int\limits_{\mathscr{E}}
\frac{\zeta^k (w)}{w- s_1} \exp \left(- \cos \left(\frac{w- s_1}{3} \right)
  \right) X^{s_1-w} dw.
$$

On $\mathscr{E}$ we have $|\re ((w- s_1)/3)| \leq 1$, and on its
horizontal sides
$$
\left|Im \left(\frac{w- s_1}{3} \right) \right| \geq \frac{1}{3} \cdot
\frac{Y}{2}  \geq 2 \log \log T.
$$

Hence\pageoriginale if $w$ lies on the horizontal sides of
$\mathscr{E}$ we have  
\begin{multline*}
\left|\exp \left(- \cos\left(\frac{w-s_1}{3} \right) \right) \right|
\geq \exp \left(- \frac{\cos 1}{2} \exp (2 \log \log T) \right)\\ 
= \exp \left( - \frac{\cos 1}{2} (\log T)^2 \right).
\end{multline*}

Therefore the condition $T^{-c} \leq X \leq T^c$ ensures that, for a
suitable $c_1 > 0$,
\begin{multline*}
  \zeta^k (\sigma_1 + it ) \ll X^2 \int\limits_{T-Y}^{T+Y}
  |\zeta (\sigma + iv)|^k \exp \left(-c_1 e^{|v-t|/3}\right) dv\\
  + X^{-1} \int\limits_{T-Y}^{T+Y} \exp \left(-c_1 e^{|v-t|/3}\right)dv+ 0(1).
\end{multline*}

Integrating this estimate over $t$ and using \eqref{c1:eq1.30} we
obtain
\begin{align}
  Y & \ll X^2 \int\limits_{T-Y}^{T+Y} |\zeta (\sigma+ iv)|^k dv \left(
  \int\limits_{T- \frac{1}{2}Y}^{T+ \frac{1}{2}Y} \exp (-c_1
  e^{|v-t|/3})dt\right)\label{c1:eq1.31}\\
  & \hspace{2cm} + X^{-1} \int\limits_{T-Y}^{T+Y} dv
  \left(\int\limits_{T- \frac{1}{2}Y}^{T+ \frac{1}{2}Y} \exp (-c_1
  e^{|v-t|/3})dt \right)\notag\\
  & \ll X^2 \int\limits_{T-Y}^{T+Y} |\zeta (\sigma + iv)|^k dv+ X^{-1} Y.\notag
 \end{align}

Let now 
$$
I : = \int\limits_{T-Y}^{T+Y} |\zeta (\sigma + iv)|^k dv,
$$
and choose first $X= Y^\epsilon$. Then \eqref{c1:eq1.31} gives $I \gg
Y^{1-2 \epsilon}$, showing that I cannot be too small. Then we choose
  $X= Y^{1/3}I^{-1/3}$, so that in view of \eqref{c1:eq1.28} trivially
$$
T^{-k /18} \ll X \ll Y.
$$

With this choice of $X$ \eqref{c1:eq1.31} reduces to $Y \ll Y^{2/3}
Y^{1/3}$, and \eqref{c1:eq1.29} follows.

\section[Lower Bounds For Mean Values on...]{Lower Bounds For Mean
  Values on The Critical  Line}\label{c1:s4} 

The\pageoriginale lower bound of Theorem \ref{c1:thm1.4} is best
possible when $\sigma > 1/2$ (for $Y \gg T^{2-2\sigma}$ this follows
from \eqref{c1:eq1.20}). However, in the most important case when
$\sigma=1/2$, this bound is poorer by a log-factor than the expected
order of magnitude of the integral in question. It is conjectured that
for any fixed $k \geq 0$
\begin{equation}
I_k (T): = \int\limits_0^T \left|\zeta \left(\frac{1}{2} +
it \right)\right|^{2k}dt \sim c_k T (\log T)^{k^2} (T \to
\infty)\label{c1:eq1.32} 
\end{equation}
for some constant $c_k (> 0)$. So far this is known to hold only for
$k=0$ (the trivial case), $k=1$, $k=2$ with $c_0 =1$, $c_1=1$ and $c_2
= 1/(2\pi^2)$, respectively. For $k=1$ this follows from
\eqref{c1:eq1.21}, and for $k=2$ this is a consequence of
A.E. Ingham's classical result that
\begin{equation}
  I_2 (T)= \int^T_0 \left|\zeta \left(\frac{1}{2}+ it  \right)\right|^4 dt
  = \frac{T}{2 \pi^2} \log^4 T + O (T \log^3 T).\label{c1:eq1.33}
\end{equation}

For other values of $k$ it is impossible at present to prove
\eqref{c1:eq1.32} ($k= 1/2$ would be very interesting, for example),
and it seems difficult even to formulate a plausible conjectural value
of $c_k$ (this subject will be discussed more in chapter \ref{c4}). The lower
bound 
\begin{equation}
  I_k (T) \gg_k T (\log T )^{k^2}\label{c1:eq1.34}
\end{equation}
is known to hold for all rational $k \geq 0$, and for all real $k \geq
0$ if the Riemann hypothesis is true. The following theorem proves the
lower bound \eqref{c1:eq1.30} (under the Lindel\"of hypothesis) with
the explicit constant implied by the symbol $\gg$. This is 

\begin{thm}\label{c1:thm1.5}
  Assume the Lindel\"of hypothesis. If $k > 0$ is a fixed integer,
  then as $T \to \infty$ 
  \begin{equation}
    \int\limits_0^T \left|\zeta \left( \frac{1}{2} + it \right)\right|^{2k}
    dt \geq (c_k' + 0(1)) T (\log T)^{k^2}, \label{c1:eq1.35}
  \end{equation}
  where 
  \begin{equation}
    c_k' = \frac{1}{\Gamma(k^2+1)} \prod\limits_p \left((1- p^{-1})^{k^2}
    \sum^\infty_{m=0} \left(\frac{\Gamma (k+m)}{\Gamma (k) m!}
    \right)^2 p^{-m}\right)\label{c1:eq1.36}
  \end{equation}
\end{thm}

\begin{proof}
  Note\pageoriginale that $c_0 = c'_0$ and $c_1 = c'_1$. The proof will give
  unconditionally \eqref{c1:eq1.35} for $k < 2/\mu (1/2)$ (so that
  $\mu(1/2)=0$, the Lindel\"of hypothesis, gives the assertion of the
  theorem). Also, if the Riemann hypothesis is assumed then
  \eqref{c1:eq1.35} holds for all $k \geq 0$ (and not only for
  integers). For this reason it is expedient to use in
  \eqref{c1:eq1.36} the gamma-function notation.

  Let $a_k (s) = \displaystyle{\sum_{n \leq Y} d_k (n) n^{-s}}$, where
    $Y= Y(T)= o (T)$ will be chosen later. Using $|a-b|^2= |a|^2+
    |b|^2- 2 \re a \ob{b}$ one has
    \begin{align*}
      0 & \leq \int\limits^T_1 |\zeta^k (1/2+ it )- A_k (\frac{1}{2} + it ) |^2 +
       dt\\
      & = I_k (T) + O(1) + \int\limits^T_1 |A_k (1/2+ it )|^2
      dt\\ 
      & \hspace{4cm}-
      2 \re \left( \int\limits^T_1 \zeta^k (1/2 + it ) A_k (1/2 -
      it ) dt \right)
    \end{align*}

From \eqref{c1:eq1.15} we obtain
\begin{equation}
  \int\limits^T_1 |A_k (1/2 + it )|^2 dt = (T + O (Y))
  \sum\limits_{n \leq Y} d^2_k (n) n^{-1}.\label{c1:eq1.37}
\end{equation}

Consider now the rectangle with vertices $\frac{1}{2} + i$, $a+i$, $a+
iT$, $\frac{1}{2} + iT$, where $a=1+ 1/(\log T)$. Then by Cauchy's
theorem we have
\begin{align*}
& \int\limits_1^T \left(\frac{1}{2} + it   \right) A_k
\left(\frac{1}{2}- it \right) dt = \frac{1}{i} \int\limits_{a+i}^{a+ iT} \zeta^k (s) A_k (1-s) ds\\
&\hspace{2cm} + O \left( \int\limits_{\frac{1}{2}}^a  |\zeta (\delta + it)|^k \sum_{n
  \leq Y} d_k n^{\sigma-1} d \sigma\right)+ O(1).
\end{align*}

Now we use the Lindel\"of hypothesis in the form
$$
\zeta (\sigma + it) \ll t^{2 \epsilon (1- \sigma)} \log t \quad
\left(\frac{1}{2} \leq \sigma \leq 1, t \geq t_0\right),
$$
which follows from $\zeta \left(\frac{1}{2} + it \right) \ll
t^\epsilon$, $\zeta (1+ it)\ll \log t$ and convexity.\pageoriginale We
also recall the elementary estimate
$$
\sum\limits_{n \leq Y} d_k (n) \ll Y \log^{k-1}Y,
$$
so that by partial summation
\begin{align*}
  \int_{\frac{1}{2}}^a & \ll \mathop{\max}_{\frac{1}{2} \leq \sigma
    \leq 1} T^{2 k\epsilon (1 - \sigma)} Y^\sigma \log^{2k} T+ Y
  \log^{2k}T\\
  & \ll (T^{k \epsilon} Y^{\frac{1}{2}} + Y) \log^{2k}T \ll Y \log^{2k}T
\end{align*}
with the choice $\epsilon =1/(3k)$, since obviously $k \geq 1$ may be
assumed. Since 
$$
A_k (1-s) \ll A_k (1-a) = \sum_{n \leq Y} d_k (n) n^{a-1} \ll \sum_{n \leq Y} d_k (n), 
$$
we have by absolute convergence
\begin{align}
  \frac{1}{i} \int\limits_{a+i}^{a+ iT} & \zeta^k (s) A_k (1-s)ds =
  \sum_{m=1}^\infty d_k (m) \sum_{n \leq Y} d_k (n) n^{-1}
  \left\{\frac{1}{i} \int\limits^{a+ iT}_{a+i} \left(\frac{m}{n}
  \right)^{-s} ds\right\}\times\notag\\
  & (T-1) \sum_{n \leq Y} d^2_k (n) n^{-1} + O \left( \sum_{m \neq n,
    n \leq Y} \frac{d_k (m) d_k (n)}{|\log \frac{m}{n}| m^a
    n^{1-a}}\right). \label{c1:eq1.38}
\end{align}

To estimate the last error term we use the elementary inequality
$$
d_k (m) d_k (n) \leq \frac{1}{2} (d_k^2 (m) + d_k^2 (n))
$$
and distinguish between the cases $m\leq 2Y$ and $m > 2Y$. The total
contribution of the error term is then found to be
\begin{align*}
  & \ll \sum^\infty_{m=1} d_k^2 (m) m^{-a} \sum_{n \leq Y, n \neq m}
  \left|\log \frac{m}{n}\right|^{-1}+ \sum_{n \leq Y} d_k^2 (n) \sum_{m=1, m\neq
    n}^\infty m^{-a} \left|\log \frac{m}{n}\right|^{-1}\times\\
  &\qquad \sum^\infty_{m=1} d_k^2 (m) m^{-a} Y \log T + \sum_{n \leq Y}
  d_k^2 (n) \log^2 T.
\end{align*}

From \eqref{c1:eq1.37} and \eqref{c1:eq1.38} it follows then 
\begin{multline*}
  I_k (T) \geq T \sum_{n \leq Y} d_k^2 (n) n^{-1} + O \left(\sum_{n
    \leq Y} d_k^2 (n) \log^2 T \right)\\
  + O \left\{Y \left( \sum_{m \leq Y} d_k^2 (m) m^{-1}+ \sum^\infty_{m=1}
  d_k^2 (m) m^{-a} \log T + \log^{2k} T \right)\right\}.
\end{multline*}

To\pageoriginale finish the proof note that
$$
\zeta (a) \ll \frac{1}{a-1} +1 \qquad (a> 1)
$$
for $a=1+ 1/(\log T)$ gives
$$
\sum^\infty_{m=1}d_k^2 (m) m^{-a} \ll \zeta^{k^2} (a) \ll (\log T)^{k^2},
$$
and that we have
\begin{equation}
  \sum_{n \leq Y} d_k^2 (n) n^{-1} = (c'_k + O(1)) (\log Y)^{k^2}
  \qquad (y \to \infty) \label{c1:eq1.39}
\end{equation}
with $c'_k$ given by \eqref{c1:eq1.36}. Thus taking
$$
Y = T \exp \left(- \frac{\log T}{\log \log T} \right)
$$
we obtain the assertion of the theorem. For unconditional results we
use the bound
$$
\zeta (\sigma + it) \ll t^{(2 \mu (1/2)+ \epsilon)(1- \sigma)} \log t
\; (1/2 \leq \sigma \leq 1, t \geq t_0),
$$
while if the Riemann hypothesis is true we may use the bound 
\begin{equation}
  \zeta (\sigma + it) \ll \exp \left( \frac{A \log t}{\log \log
    t}\right) \quad \left(A > 0, \sigma \geq \frac{1}{2}, t \geq
  t_0\right).\label{c1:eq1.40}
\end{equation}

In the last case the appropriate choice for $Y$ is 
$$
Y= T \exp \left(- \frac{A_1 \log T}{\log \log T} \right)
$$
with $a_1 > A$. We conclude by noting that an unconditional proof of
\eqref{c1:eq1.35} will be given in Chapter \ref{c6}.
\end{proof}

\newpage

\begin{center}
  \textbf{\LARGE Notes For Chapter 1}
\end{center}

\medskip
For\pageoriginale the elementary theory of $\zeta(s)$ the reader is
referred to the first two chapters of E.C. Titchmarsh's classic
\cite{Titchmarsh1} and to the first chapter of the author's monograph \cite{Titchmarsh1}.

There are many ways to obtain the analytic continuation of $\zeta(s)$
outside the region $\sigma > 1$. One simple way (see T. Estermann
\cite{Estermann4}) is to write, for $\sigma> 1$,
$$
\zeta(s) = \sum_{n=1}^\infty n^{-s} = \sum^\infty_{n=1} \left(n^{-s} -
\int\limits_{n}^{n+1} u^{-s}du\right) + \frac{1}{s-1}
$$
and to observe that
$$
\left|n^{-s} - \int\limits_n^{n+1} u^{-s} du \right| = \left| s
\int\limits_n^{n+1} \int\limits_u^n z^{-s-1} dz du \right| \leq
|s|n^{- \sigma-1}.
$$

Hence the second series above converges absolutely for $\sigma > 0$,
and we obtain
$$
\zeta(s) = \sum^\infty_{n=1} \left(n^{-s}- \int\limits_n^{n+1} u^{-s}
du \right) + \frac{1}{s-1} \quad (\sigma > 0).
$$

One can formalize this approach (see R.  Balasubramanian and
K. Ramachandra \cite{Balasubramanian and Ramachandra2}) and show that
$$
\sum_{a\leq n < b} f(n) = \int\limits_a^b f(x) dx - \frac{1}{2}
\int\limits^1_0 \int\limits_0^1 \sum_{a \leq n < b} f' (n+ u^{1/2}v) du dv
$$
if $a< b$ are integers and $f(x) \in C^1 [a, b]$. By repeated
application of this summation formula one can obtain analytic
continuation of $\zeta(s)$ to $\mathbb{C}$, and also the approximate
functional equation \eqref{c1:eq1.14} for $x \geq \left(\frac{1}{2} +
\epsilon \right) |t|$.

The approximate functional equation \eqref{c1:eq1.14} is given as
Theorem 1.8 of Ivi\'c \cite{Ivic1} and as Theorem 4.11 is Titchmarsh \cite{Titchmarsh1}.

The\pageoriginale functional equation \eqref{c1:eq1.6} is, together
with the Euler product representation \eqref{c1:eq1.1}, one of the
fundamental features of $\zeta(s)$. There are seven proofs of
\eqref{c1:eq1.6} in Chapter \ref{c2} of Titchmarsh \cite{Titchmarsh1} (one of which is
given in our text), plus another one in the Notes for Chapter \ref{c1} via
the theory of Eisenstein series.

For fractional mean values, that is, the integral \eqref{c1:eq1.11}
when $k$ is not necessarily a natural number, the reader is referred
to Chapter \ref{c6}. The formulas \eqref{c6:eq6.4} - \eqref{c6:eq6.7} show
how \eqref{c1:eq1.12} can be made meaningful for an arbitrary complex
number $k$.

To obtain a precise form of \eqref{c1:eq1.13} when $k=2$, let $n=
p_1^{\alpha_1}\cdots p_r^{\alpha_r}$ be the canonical decomposition of
$n$. Since $p^\alpha$ has exactly $\alpha+1$ divisors for any prime
$p$ we have
$$
d(n) n^{- \delta}= \prod_{j=1}^r (\alpha_j + 1)p_j^{- \alpha_j \delta},
$$
where $\delta> 0$ will be suitably chosen. Now $(\alpha + 1)p^{- \alpha
\delta} \leq 1$ for $p \geq 2^{1/\delta}$ and
$$ 
2^{\alpha \delta}\left(1+ \frac{1}{\delta \log 2}\right) \geq (1+
\alpha \delta \log 2) \left(1+ \frac{1}{\delta \log 2} \right) \geq 1 + \alpha
$$ 
shows that 
$$
(\alpha + 1) p^{-\delta} \leq 1 + \frac{1}{\delta \log 2}
$$
for all primes $p$ and $\alpha \geq 1$. Hence
$$
d(n) n^{-\delta} \leq \left( 1+ \frac{1}{\delta \log 2}\right)^{\pi
  (2^{1/\delta})}, 
$$
where $\pi (x) (\sim x /\log x ~\text{as}~ x \to \infty)$ is the
number of primes not exceeding $x$. The choice
$$
\delta = \left(1+  \frac{C \log_3 n}{\log_2 n} \right) \frac{\log 2}{\log_2 n},
$$
with $C > 2$, $\log_2 n = \log \log n$, $\log_3 n = \log \log \log n$ gives after a simple calculation
$$
 d(n) \leq \exp \left\{\frac{\log 2 \log n}{\log_2 n}+ o \left(
\frac{\log n \log_3 n}{(\log_2 n)^2}\right) \right\}.
$$

As shown by S. Ramanujan \cite{Ramanujan1}, this inequality holds even without
$\log_3 n$ in the O-term, in which case it is actually best possible.

H.L. Montgomery and R.C. Vaughan \cite{Montgomery and Vaughan1}\pageoriginale proved \eqref{c1:eq1.15} from a version of the so-called Hilbert
inequality. The proof of \eqref{c1:eq1.15} was simplified by
K. Ramachandra \cite{Ramachandra4}, and his proof is essentially given in Chapter
\ref{c5} of A. Ivi\'c \cite{Ivic1}. See also the papers of S. Srinivasan \cite{Srinivasan1} and E. Preissmann \cite{Preissmann1}.

Recently R. Balasubramanian, A. Ivi\'c and K. Ramachandra \cite{Balasubramanian Ivic and Ramachandra1} investigated the function 
$$
R(T) : = \int\limits_1^T |\zeta (1+ it)|^2 dt - \zeta(2) T,
$$
where the lower bound of integration has to be positive because of the
pole of $\zeta (s)$ at $s=1$. They proved that
$$
R(T) = O(\log T),
$$
which is in fact \eqref{c1:eq1.22},
$$
\int\limits_1^T R(t) dt =- \pi T \log T + O (T \log \log T)
$$
and 
$$
\int\limits_1^T (R(t)+ \log t)^2 dt =O (T \log \log T)^4.
$$

From either of the last two results one can deduce that
$$
R(T) = \Omega_- (\log T),
$$
which justifies the claim in the text that the error term in
\eqref{c1:eq1.22} cannot be improved.

Theorem \ref{c1:thm1.2} is due to R. Balasubramanian and
K. Ramachandra \cite{Balasubramanian and Ramachandra3}, \cite{Balasubramanian and Ramachandra4}. It improves considerably on Lemma 7.1 of the author's work [1], which generalizes a
lemma of D.R. Heath-Brown \cite{Heath-Brown1}.

The convexity of the function $\mu (\sigma)$ follows e.g. from general
results on Dirichlet series. A direct proof is given by the author
[1] in Chapter 8.

Theorem \ref{c1:thm1.3} is an improved version of Lemma
1.3 of the author's work [1]. The improvement comes
from the use of the kernel function $\exp (- \cos w)$, which decays
like a second-order exponential. 

This\pageoriginale kernel function, in the alternative form $\exp
(\sin^2 w)$, was introduced and systematically used by K. Ramachandra
(see e.g. \cite{Ramachandra2} and \cite{Ramachandra6}). In Part I of \cite{Ramachandra6} Ramachandra
expresses the opinion that probably no function regular in a strip
exists, which decays faster than a second-order exponential. This is
indeed so, as was kindly pointed out to me by W.K. Hayman in a letter
of August 1990. Thus Ramachandra's kernel function $\exp (\sin^2 w)$
(or $\exp (- \cos w)$) is essentially best possible. This does not
imply that, for example, the range $|v|\leq \log \log t$ in
\eqref{c1:eq1.26} cannot be reduced, but it certainly cannot be
reduced by the method of proof given in the text.

Bounds for $\mu(\sigma)$ are extensively discussed in Chapter \ref{c5} of
Titchmarsh \cite{Titchmarsh1} and Chapter 7 of Ivi\'c \cite{Ivic1}. All of the
latest vounds for $\mu(1/2)$, given by \eqref{c1:eq1.28}, are based on
the powerful method introduced by E. Bombieri and H. Iwaniec \cite{Bombieri and Iwaniec1},
who proved $\mu(1/2)\leq 9/56= 0.16071 \ldots$. This method was also
used by H. Iwaniec and C.J. Mozzochi \cite{Iwaniec1} to prove that
$\delta(x)= O (x^{7/22+\epsilon})$, where
$$
\Delta (x) : = \sum_{n \leq x} d(n) - x (\log x + 2 \gamma -1)
$$
is the error term in the Dirichlet divisor problem. M.N. Huxley,
either alone \cite{Huxley2}, \cite{Huxley3}, \cite{Huxley4}, \cite{Huxley5}, for jointly with
N. Watt \cite{Watt1}, \cite{Huxley2}, successfully investigated exponential sums
via the Bombieri-Iwaniec method. Among other things, they succeeded
in obtaining new exponent pairs for the estimation of exponential sums
(for the definition and basic properties of exponent pairs see
S.W. Graham \cite{Graham1} and E. Kr\"atzel \cite{Kratzel1}).

M.N. Huxley kindly informed me that he just succeeded in proving $\mu
(\frac{1}{3}) \leq 89/570= 0.156140\ldots$. As in the previous
estimates of $\mu(\frac{1}{2})$ by the Bombieri-Iwaniec method, the
limit of the method, appears to be the value $3/20$. Huxley also provided me with the following summary of the salient ideas of the Bombieri-Iwaniec method, for which I am grateful. 

Like\pageoriginale the van der Corput method, the Bombieri-Iwaniec
method begins by dividing the sum
$$
S= \sum_{M \leq n \leq 2 M} e(f(m))
$$
of length $M$ into short sums of the same length N. The short sums are
estimated in absolute value, losing the possibility of cancellation
between different short sums. The best estimate obtainable from the
method must be $\Omega (MN^{- 1/2})$. The first idea is to approximate
$f(x)$ on each short interval by a polynomial with rational
coefficients. Suppose that $f^{(r)}(x)\ll TM^{-r}$ for $r= 2, 3, 4$
and $f^{(3)}(x) \gg TM^{-3}$. Then as $x$ runs through an interval of
length $N$, the derivative $1/2 f'' (x)$ runs through an interval of
length 
$$
\asymp NT /M^3 \asymp 1/R^2;
$$
this equation defines the parameter $R$. A rational number $a/q$ is
chosen in the interval. Let $m$ be the integer for which $1/2 f'' (m)$
is closest to $a/q$. Then $f(m +x)$ can be approximated by a
polynomial
$$
f(m) + (b+ x)q^{-1} x+ aq^{-1} x^2 + \mu x^3,
$$ 
where $b$ is an integer, $x$ and $\mu$ are real numbers.

The sum of length $N$ is transformed by Poisson summation first modulo
$q$, then in the variable $x$, to give an exponential sum in a new
variable $h$, whose length is proportional to $q$. Short sums with $q
\ll R^2/N$ are called major arcs. For this the sum over $h$ is
estimated trivially. Estimating all transformed sums trivially gives
the bound 
{\small $O(MT^\epsilon (NR^2)^{- 1/6})$},
corresponding to the
exponent pair $\left(\frac{1}{6}, \frac{2}{3} \right)$. The
exponential in the transformed sum is (for $q$ odd) essentially
$$
e\left(- \frac{\ob{4a}}{q} (h^2 - 2 bh) - \frac{2 h^{3/2}+ 3 x
  h^{1/2}}{(27 \mu q^3)^{1/2}} \right),
$$
since further terms will make a negligible contribution. The
expression in the exponential can be written as
$$
- \mathop{x}_\sim (h) \cdot \mathop{y}_\sim (a/q),
$$
where $\displaystyle{\mathop{x}_\sim(h)}$ is the vector $(h^2 , h, h^{3/2},
h^{1/2})$, and $\displaystyle{\mathop{y}_\sim(a/q)}$ has entries
involving\pageoriginale $\ob{4a}$, $b$, $q$, $x$ and $\mu$ from the
approximating polynomial on the minor arc ($\ob{4a}$ denotes the
multiplicative inverse of $4a$ modulo $q$)./ Bombieri and Iwaniec
interpreted these sums as analogues of those in the classical large
sieve, which are sums first over integers, then over rational
numbers. They devised an extremely general form of the large sieve.

The large sieve, applied to the transformed sums with $q$ large (minor
arcs) would show that the short sums have root mean square size
$O(N^{1/2})$ if two spacing problems could be settled. The first
spacing problem is to show that for some $r$ the vectors
$$
\mathop{x}_\sim(h_1)+ \mathop{x}_\sim(h_2)+ \cdots \mathop{x}_\sim(h_r)
$$
corresponding to different $r$-tuples of integers $(h_1 , \ldots ,
h_r)$ are usually separated by a certain distance. This is easy to
show of $r=3$. Bombieri and Iwaniec \cite{Bombieri and Iwaniec1} gave a proof  for $r=4$,
using analytic methods and ingenious induction. N. watt gave an
elementary proof for $r=4$, and a partial result for $r=5$. Huxley and
Kolesnik \cite{Huxley and Kolesnik1} obtained the full result for $r=5$, combining Watt's
elementary method with exponential sum techniques. The case $r=6$ is
open. For $r \geq 7$ the Dirichlet box principle shows that there must
be many near-coincident sums of seven
$\displaystyle{\mathop{x}_\sim(h)}$ vectors.

The second spacing problem is to show that the vectors
$\displaystyle{\mathop{y}_\sim}(a/q)$ corresponding to different minor
arcs are usually separated by a certain distance. Bombieri and Iwaniec
\cite{Bombieri and Iwaniec1} gave an argument for the case of Dirichlet series, showing
for certain ranges of the parameters that the vectors are separated in
their first and third entries. Huxley and Watt \cite{Huxley and Watt1} and
G. Kolesnik obtained arguments of the same power in the general
case. The parameter ranges could be extended if one could show that
vectors which coincide in their first and third entries are separated
in their second and fourth entries. Huxley (to appear) considered how
the entries of the vector $\displaystyle{\mathop{y}_\sim(a/q)}$ change
on adjacent minor arcs, and obtained a partial result in this
direction. 

Bombieri and Iwaniec got the estimate $O(MT^\epsilon (NR)^{-1/2})$ for
the originale sum S.N. Watt \cite{Watt1} sharpened this to $O (MT^\epsilon
(N^{11}R^9)^{-1/40})$,  Huxley and Kolesnik \cite{Huxley and Kolesnik1}\pageoriginale to
$O(MT^\epsilon (N^3 R^2)^{-1/10})$, and Huxley (to appear) to
$O(MT^\epsilon (N^{11} R^4)^{-1/30})$. The conjectured answer to the
second spacing problem could give $O(MT^\epsilon N^{- 1/2})$ even with
$r=3$ in the first spacing problem.

The method was adapted by H. Iwaniec and C.J. Mozzochi \cite{Iwaniec1} to
show that
$$
\sum_{H < h \leq 2H} \sum_{M< m \leq 2M} e(hf' (m))= O(HMT^\epsilon
(NR)^{-1/2}) 
$$ 
with the approximating polynomial $(b + x)q^{-1}h + 2 ah q^{-1} x+ 3
\mu h x^2$ in the notation used above. The first spacing problem is
different, but easier. These sums are related to the error terms in
the divisor and circle problems (see \eqref{c2:eq2.4}, discussion
after \eqref{c2:eq2.111} and Notes for Chapter \ref{c2}), which Iwaniec and
Mozzochi estimated as $O(x^{7/22+ \epsilon})$. Huxley \cite{Huxley2}
generalized the application to other lattice-point problems in two
dimensions. The improvement in the second spacing problem gives the
better bound $O(HMT^\epsilon (HN^3 R^2)^{-1/6})$, and the exponent 7/22
  in the divisor and circle problems can be improved to 23/73 (Huxley,
  to appear). The method can be also applied to the estimation of
  $E(T)$. This was done by Heath-Brown and Huxley \cite{Heath-Brown and Huxley1} (see
  Theorem \ref{c2:thm2.9}), who obtained the exponent 7/22 in this
  problem also.  In the case of $E(T)$ there is the extra condition
  $H^2 \ll NR$ which prevents one from using the improvement in the
  second spacing problem.

Theorem \ref{c1:thm1.4} improves Theorem 9.6 of Ivi\'c
\cite{Ivic1}. The method of proof is the same, only again the use of the
kernel function $\exp(- \cos w)$ improves the range for $Y$ from
$\log^{1+ \epsilon} T \leq Y \leq T$ to $12 \log \log T \leq Y \leq
T$. A result analogous to Theorem \ref{c1:thm1.4} holds for a large
class of Dirichlet series, since the proof uses very little from the
properties of $\zeta(s)$.

The lower bound \eqref{c1:eq1.34} when $k$ is an integer was proved
first by K. Ramachandra \cite{Ramachandra3}, who obtained many significant
results on lower bounds for power moments. For more about his results,
see the Notes for Chapter \ref{c6}.

Theorem 1.5 is due to J.B. Conrey and A. Ghosh \cite{Conrey and Ghosh1}. Their
research\pageoriginale is continued in \cite{Conrey and Ghosh3} and \cite{Conrey and Ghosh4}. I have
included the proof of Theorem \ref{c1:eq1.5} because of its
simplicity, since a sharper result (Theorem \ref{c6:thm6.5}), due to
Balasubramanian and Ramachandra, will be proved later.

The asymptotic formula \eqref{c1:eq1.39} follows by standard methods
of analytic number theory. Namely,
$$
c'_k = \lim\limits_{s \to 1+0} (s-1)^{k^2} F_k (s)/(k^2)!,
$$
where (see also Section \ref{c4:sec4.4}) for $\re s> 1$
$$ 
F_k (s) = \sum^\infty_{n=1} d^2_k (n) n^{-s} = \zeta^{k^2}(s) \prod_p
((1-p^{-s})^{k^2} \sum^\infty_{m=0} \left(\frac{\Gamma (k+m)}{m! \Gamma
  (k)} \right)^2 p^{-ms}. 
$$

Since $\displaystyle{\lim\limits_{s \to 1}} (s-1) \zeta (s) =1$, one
obtains from the above representation the value of $c'_k$ given by
\eqref{c1:eq1.36}. 

In what concerns unconditional bounds for $\zeta(s)$, the best known
bound in the vicinity of $\sigma=1$ is of the form
\begin{equation}
\zeta (\sigma + it) \ll |t|^{A(1- \sigma)^{3/2}} \log^{2/3} |t| \;\;  (t \geq
2, \frac{1}{2} \leq \sigma \leq 1).
\label{c1:eq1.41}
\end{equation}

This was proved (with $C= 100$) by H.-E. Richert \cite{Richert1} by using
Vinogradoc's method. In Chapter \ref{c6} of \cite{Vinogradov1} the author proved that
the estimate 
\begin{equation}
  \sum_{N < N \leq N' \leq 2N} n^{it} \ll N \exp \left(\frac{D \log^3
    N}{\log^2 t} \right) \quad (1 \ll N \leq t),\label{c1:eq1.42}
\end{equation}
which also follows from Vinogradov's method, implies \eqref{c1:eq1.41}
for $1- \eta \leq \sigma \leq 1$ (the relevant range for $\sigma$)
with $C= \frac{2}{3} (3D)^{-1/2}$. The estimate \eqref{c1:eq1.42} was
shown to hold with $D= 10^{-5}$, giving $C= 122$. Recently
E.I. Panteleeva \cite{Panteleeva1} improved the value of D  to $D=
\frac{1}{2976}$. Since $\frac{2}{3} \left(
\frac{2976}{3}\right)^{1/2}= 20.99735\ldots$, this proves
\eqref{c1:eq1.41} with $C= 21$, a fact which was also independently
obtained by K.M. Bartz \cite{Bartz1}.

On the other hand, the best conditional bound for $\zeta \left(\frac{1}{2}
+ it\right)$ under the Riemann hypothesis is \eqref{c1:eq1.40}, a
proof of which is to be found in Titchmarsh \cite{Titchmarsh1} or Ivi\'c
\cite{Ivic1} (thus by \eqref{c1:eq1.40}) it is seen that the Riemann
hypothesis implies the Lindel\"of hypothesis). An explicit value of
the constant A appearing in \eqref{c1:eq1.40}, namely $A=
0.46657\ldots$, was found recently by K. Ramachandra and
A. Sankaranarayanan \cite{Ramachandra and Sankaranarayanan1}.

