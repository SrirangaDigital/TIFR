
\chapter{Summation Formulae}\label{chap1}

THERE\pageoriginale IS AN extensive literature on various summation
formulae of the Voronoi type and on different ways to prove such
results (see \eg the series of papers by B.C. Berndt \cite{key3} and
his survey article \cite{key4}). We are going to need such identities
for the sums
$$
\sideset{}{'}\sum\limits_{a\leq n\leq b}b(n)e(nr)f(n),
$$
where $0<a<b, f\in C^1[a,b], r=h/k$, and $b(n)=d(n)\quad\text{or}\quad
a(n)$. The case $f(x)=1$ is actually the important one, for the
generalization is easily made by partial summation. So the basic
problem is to prove identities for the sums $D(x,r)$ and $A(x,r)$ (see
Notation for definitions). In view of their importance and interest,
we found it expedient to derive these identities from scratch, with a
minimum of background and effort.

Our argument proceeds via Riesz means $D_a(x,r)$ and $A_a(x,r)$ where
$a\geq 0$ is an integer. We follow A.L. Dixon and W.L. Ferrar
\cite{key6} with some simplifications. First, in \cite{key6} the more
general case when $a$ is not necessarily an integer was discussed, and
this leads to complications since the final result can be formulated
in terms of ordinary Bessel functions only if $a$ is an
integer. Secondly, it turned out that for $a=0$ the case $x\in
\mathbb{Z}$, which requires a lengthy separate treatment in
\cite{key6}, can actually be reduced to the case $x\notin \mathbb{Z}$
in a fairly simple way. 

To\pageoriginale get started with the proofs of the main results of
this chapter, we need information on the Dirichlet series $E(s,r)$ and
$\varphi(s,r)$, in particular their analytic continuations and
functional equations. The necessary facts are provided in \S\S ~
\ref{chap1:sec1.1} and \ref{chap1:sec1.2}.

Bessel functions emerge in the proofs of the summation formulae when
certain complex integrals involving the gamma function are
calculated. We could refer here to Watson \cite{key29} or Titchmarsh
\cite{key26}, but for convenience, in \S ~ \ref{chap1:sec1.4} , we
calculate these integrals directly by the theorem of residues.

In practice, it is useful to have besides the identities also
approximate and mean value results on $D(x,r)$ and $A(x,r)$, to be
given in \S ~ \ref{chap1:sec1.5}.

Identities for $D_a(x,r)$ and $A_a(x,r)$ are proved in \S\S~
\ref{chap1:sec1.6}--\ref{chap1:sec1.8}, first for $a\geq 1$ and then
for $a=0$. The general summation formulae are finally deduced in \S~
\ref{chap1:sec1.9}.

\section{The Function $E(s,r)$}\label{chap1:sec1.1}

The function 
\begin{equation}\label{chap1:eq1.1.1}
E(s,r)=\sum\limits_{n=1}^\infty d(n)e(nr)n^{-s}(\sigma > 1)
\end{equation}
where $r=h/k$, was investigated by T. Estermann \cite{key8}, who
proved the results of the following lemma. Our proofs are somewhat
different in details, for we are making systematic use of the Hurwitz
zeta-function $\zeta(s,a)$. 
\begin{lem}\label{chap1:lem1.1}
The function $E(s,h/k)$ can be continued analytically to a meromorphic
function, which is holomorphic in the whole complex plane up to a
double pole at $s=1$, satisfies the functional equation 
\begin{equation}
\begin{aligned}
E(s,h/k)& =2(2\pi)^{2s-2}\Gamma^2(1-s)k^{1-2s}\times\label{chap1:eq1.1.2}\\
& \times \{E(1-s,\bar{h}/k)-\cos(\pi s)E(1-s,\bar{h}/k)\},
\end{aligned}
\end{equation}\pageoriginale
and has at $s=1$ the Laurent expansion 
\begin{equation}\label{chap1:eq1.1.3}
E(s,h/k)=k^{-1}(s-1)^{-2}+k^{-1}(2\gamma -2\log k)(s-1)^{-1}+\cdots 
\end{equation}
Also,
\begin{equation}\label{chap1:eq1.1.4}
E(0,h/k)\ll k\log 2k.
\end{equation}
\end{lem}

\begin{proof}
The Dirichlet series \eqref{chap1:eq1.1.1} converges absolutely and
thus defines a holomorphic function in the half-plane $\sigma >1$. The
function $E(s,h/k)$ can be expressed in terms of the Hurwitz
zeta-function 
$$
\zeta(s,a) = \sum\limits_{n=0}^\infty (n+a)^{-s} \quad (\sigma >1,0<
a\leq 1).
$$
Indeed, for $\sigma > 1$ we have
\begin{align*}
E(s,h/k) &= \sum\limits_{m,n=1}^\infty e_k(mnh)(mn)^{-s}\\
&= \sum\limits_{\alpha,\beta =1}^k e_k(\alpha\beta h)
\underset{\underset{n\equiv\beta \pmod{k}}{m\equiv\alpha
    \pmod{k}}}{\sum} (mn)^{-s}\\ 
&= \sum\limits_{\alpha,\beta =1}^k e_k(\alpha\beta h)
\sum\limits_{\mu,\nu =0}^\infty ((\alpha +\mu k)(\beta +\nu k))^{-s},
\end{align*}
so that
\begin{equation}\label{chap1:eq1.1.5}
E(s,h/k)= k^{-2s}\sum\limits_{\alpha,\beta =1}^k e_k(\alpha\beta
h)\zeta(s,\alpha/k)\zeta(s,\beta/k).
\end{equation}

This holds, in the first place, for $\sigma >1$, but since
$\zeta(s,a)$ can be analytically\pageoriginale continued to a
meromorphic function which has a simple pole with residue 1 at $s=1$
as its only singularity (see \cite{key27}, p.~37), the equation
\eqref{chap1:eq1.1.5} gives an analytic continuation of $E(s,h/k)$ to
a meromorphic function. Moreover, its only possible pole, of order at
most 2, is $s=1$.

To study the behaviour of $E(s,h/k)$ near $s=1$, let us compare it
with the function
$$
k^{-2s}\zeta(s)\sum\limits_{\alpha,\beta =1}^k e_k(\alpha\beta
h)\zeta(s,\beta/k)=k^{1-2s} \zeta^2(s).
$$
The difference of these functions is by \eqref{chap1:eq1.1.5} equal to 
\begin{equation}\label{chap1:eq1.1.6}
k^{-2s}\sum\limits_{\alpha =1}^k\left(\sum\limits_{\beta =1}^k
e_k(\alpha\beta h)\zeta(s,\beta/k)\right)(\zeta(s,\alpha/k)-\zeta(s)).
\end{equation}

Here the factor $\zeta(s,\alpha/k)-\zeta(s)$ is holomorphic at $s=1$
for all $\alpha$, and vanishes for $\alpha =k$. Since the sum with
respect to $\beta$ is also holomorphic at $s=1$ for $\alpha\neq k$,
the function \eqref{chap1:eq1.1.6} is holomorphic at
$s=1$. Accordingly, the functions $E(s,h/k)$ and $k^{1-2s}\zeta^2(s)$
have the same principal part at $s=1$. Because 
$$
\zeta(s)=\frac{1}{s-1}+\cdots,
$$
this principal part is that given in \eqref{chap1:eq1.1.3}.

To prove the functional equation \eqref{chap1:eq1.1.2}, we utilize the
formula\break (\cite{key27}, equation (2.17.3))
\begin{equation}\label{chap1:eq1.1.7}
\zeta(s,a)=2(2\pi)^{s-1}\Gamma(1-s)\sum\limits_{m=1}^\infty \sin
(\frac{1}{2}\pi s+2\pi ma)m^{s-1}(\sigma <0).
\end{equation}

Then the equation \eqref{chap1:eq1.1.5} becomes 
\begin{align*}
E(s,h/k) &= -(2\pi)^{2s-2}\Gamma^2(1-s)k^{-2s}\times\\
&\times \sum\limits_{\alpha,\beta =1}^k e_k(\alpha\beta h)
\sum\limits_{m,n=1}^\infty \{e^{\pi is}e_k(m\alpha +n\beta)+e^{-\pi
is}e_k (-m\alpha -n\beta)\\
&-e_k(m\alpha -n\beta)-e_k(-m\alpha+n\beta)\}(mn)^{s-1}\quad(\sigma <
0). 
\end{align*}\pageoriginale

Note that 
\begin{equation*}
\sum\limits_{\alpha =1}^k e_k(\alpha\beta h\mp m\alpha)=
\begin{cases}
k & \text{if $\beta\equiv\pm m\bar{h}\pmod{k}$},\\
0 & \text{otherwise}
\end{cases}
\end{equation*}

The functional equation \eqref{chap1:eq1.1.2} now follows, first for
$\sigma < 0$, but by analytic continuation elsewhere also. 

For a proof of \eqref{chap1:eq1.1.4}, we derive for $E(0,h/k)$ an
expression in a closed form. By \eqref{chap1:eq1.1.5}, 
\begin{equation}\label{chap1:eq1.1.8}
E(0,h/k)=\sum\limits_{\alpha,\beta =1}^ke_k(\alpha\beta h)\zeta
(0,\alpha/k) \zeta(0,\beta/k).
\end{equation}

If $0< a<1$, then the series in \eqref{chap1:eq1.1.7} converges
uniformly and thus defines a continuous function for all real $s\leq
0$. Hence, by continuity, \eqref{chap1:eq1.1.7} remains valid also for
$s=0$ in this case. It follows that 
$$
\zeta(0,a)=\pi^{-1}\sum\limits_{m=1}^\infty \sin(2\pi ma)m^{-1}.
$$

But the series on the right equals $\pi(1/2-a)$ for $0< a<1$, whence 
\begin{equation}\label{chap1:eq1.1.9}
\zeta(0,a)=1/2 -a.
\end{equation}

Since $\zeta(0,1)=\zeta(0)=-1/2$, this holds for $a=1$ as well. Now,
by \eqref{chap1:eq1.1.8} and \eqref{chap1:eq1.1.9}
$$
E(0,h/k) = \sum\limits_{\alpha,\beta=1}^ke_k(\alpha\beta
h)(1/2-\alpha/k) (1/2-\beta/k).
$$

From\pageoriginale this it follows easily that
\begin{equation}\label{chap1:eq1.1.10} 
E(0,h/k) = -\frac{3}{4}k+k^{-2}
\sum\limits_{\alpha,\beta=1}^ke_k(\alpha\beta h)\alpha\beta.
\end{equation}

To estimate the double sum on the right, observe that if $1\leq\alpha
\leq k-1$ and $\beta$ runs over an arbitrary interval, then
$$
\left|\sum\limits_\beta e_k(\alpha\beta h)\right|\ll \parallel\alpha
h/k\parallel^{-1}.
$$

Thus, by partial summation,
\begin{gather*}
\left|\sum\limits_{\alpha=1}^{k-1}\sum\limits_{\beta=1}^k
e_k(\alpha\beta h)\alpha\beta\right|\ll k^2
\sum\limits_{\alpha=1}^{k-1} \parallel\alpha h/k\parallel^{-1}\\
\ll k^2\sum\limits_{1\leq\alpha\leq k/2} k/\alpha \ll k^3\log k,
\end{gather*}
and \eqref{chap1:eq1.1.4} follows from \eqref{chap1:eq1.1.10}.
\end{proof}

\section{The Function $\varphi(s,r)$}\label{chap1:sec1.2}

Let $H$ be the upper half-plane $\Iim \tau >0$. The mappings 
$$
\tau\to\frac{a\tau+b}{c\tau+d}
$$
where $\left(\begin{smallmatrix} a & b\\ c &
d \end{smallmatrix}\right)$ is an  integral matrix of determinant $1$,
take $H$ onto itself and constitute the (full) {\bf modular group}. A
function $f$ which is holomorphic in $H$ and not identically zero, is
a {\bf cusp form} of {\bf weight} $k$ for the modular group if 
\begin{equation}\label{chap1:eq1.2.1}
f\left(\frac{a\tau+b}{c\tau+d}\right)=(c\tau+d)^k f(\tau),\tau\in H
\end{equation}
for all mappings of the modular group, and moreover
\begin{equation}\label{chap1:eq1.2.2}
\lim\limits_{\Iim\tau \to\infty}f(\tau)=0.
\end{equation}

It\pageoriginale is well-known that $k$ is an even integer at
least 12, and that the dimension of the vector space of cusp forms of
weight $k$ is $[k/12]$ if $k\nequiv 2 \pmod{12}$ and
$[k/12]-1$ if $k \equiv 2 \pmod{12}$ (see \cite{key1}, \S
\S ~ 6.3 and 6.5).

A special case of \eqref{chap1:eq1.2.1} is $f(\tau+1)=f(\tau)$. Hence,
by periodicity, $f$ has a Fourier series, which by
\eqref{chap1:eq1.2.2} is necessarily of the form
\begin{equation}\label{chap1:eq1.2.3}
f(\tau)=\sum\limits_{n=1}^\infty a(n)e(n\tau).
\end{equation}

The numbers $a(n)$ are called the {\bf Fourier coefficients} of the
cusp form $f$. The case $k =12$ is of particular interest, for
then $a(n)=\tau(n)$, the Ramanujan function defined by
$$
\sum\limits_{n=1}^\propto \tau(n)x^n=x\prod\limits_{m=1}^\infty
(1-x^m)^{24}\,(|x|<1).
$$

We are going to need some information on the order of magnitude of the
Fourier coefficients $a(n)$. For most purposes, the classical mean
value theorem
\begin{equation}\label{chap1:eq1.2.4}
\sum\limits_{n\leq x}|a(n)|^2=Ax^k + o(x^{k-2/5})
\end{equation}
of R.A. Rankin \cite{key24} suffices, though sometimes it
will be convenient of necessary to refer to the estimate
\begin{equation}\label{chap1:eq1.2.5}
|a(n)|\leq n^{(k-1)/2}d(n).
\end{equation}

This was known as the Ramanujan-Petersson conjecture, untill it became
a theorem after having been proved by P. Deligne \cite{key5}. In
\eqref{chap1:eq1.2.5}, it should be understood that $f$ is a normalized
eigenform (\ie $a(1)=1$) of all Hecke operators $T(n)$, but this is
not an essential restriction, for a basis\pageoriginale of the vector
space of cusp forms of a given weight can be constructed of such
forms.

Now \eqref{chap1:eq1.2.4} implies that the estimate
\eqref{chap1:eq1.2.5}, and even more, is true in a mean sense, and
since we shall be dealing with expressions involving $a(n)$ for many
values of $n$, it will be usually enough to know the order of $a(n)$
on the average.

It follows easily from \eqref{chap1:eq1.2.4} that the Dirichlet series 
$$
\varphi(s)=\sum\limits_{n=1}^\infty a(n)n^{-s},
$$
and, more generally, the series
$$
\varphi(s,r)=\sum\limits_{n=1}^\infty a(n)e(nr)n^{-s},
$$
where $r=h/k$, converges absolutely and defines a holomorphic function
in the half-plane $\sigma >(k+1)/2$. It was shown by J.R. Wilton
\cite{key30}, in the case $a(n)=\tau(n)$, that $\varphi(s,r)$ can be
continued analytically to an entire function satisfying a functional
equation of the Riemann type. But his argument applies as such also in
the general case, and the result is as follows.
\begin{lem}\label{chap1:lem1.2}
The function $\varphi(s,h/k)$ can be continued analytically to an
entire function satisfying the functional equation
\begin{gather}
(k/2\pi)^s\Gamma(s)\varphi(s,h/k)\label{chap1:eq1.2.6}\\
=(-1)^{k/2}(k/2\pi)^{k-s}\Gamma(k-s)\varphi (k-s,
-\bar{h}/k).\notag 
\end{gather}
\end{lem}

\begin{proof}
Let 
$$
\tau=\frac{h}{k}+\frac{iz}{k}, \quad \tau'=-\frac{\bar{h}}{k}+
\frac{i}{zk}, 
$$
where\pageoriginale $Re~ z>0$. Then $\tau,\tau'\in H$, and we show
first that 
\begin{equation}\label{chap1:eq1.2.7}
f(\tau')=(-1)^{k/2}z^k f(\tau).
\end{equation}

The points $\tau$ and $\tau'$ are equivalent under the modular group,
for putting $a=\bar{h},b =(1-h\bar{h})/k,c =-k$, and $d=h$, we have
$ad-bc=1$ and 
$$
\frac{a\tau+b}{c\tau+d}=\tau'.
$$

Also,
$$
c\tau+d=-iz,
$$
so that \eqref{chap1:eq1.2.7} is a consequence of the relation
\eqref{chap1:eq1.2.1}. 

Now let $\sigma > (k+1)/2$. Then we have 
\begin{align*}
(k/2\pi)^s\Gamma(s)\varphi(s,h/k) &= \sum\limits_{n=1}^\infty
a(n)e_k(nh) \int\limits_0^\infty x^{s-1}e^{-2\pi nx/k}\,dx\\
&= \int\limits_0^\infty x^{s-1}f\left(\frac{h}{k}+\frac{ix}{k}
\right)\,dx. 
\end{align*}

Here the integral over (0,1) can be written by \eqref{chap1:eq1.2.7}
as 
$$
(-1)^{k/2}\int\limits_0^1x^{s-1-k} f\left(-
\frac{\bar{h}}{k}+ \frac{i}{xk}\right)\,dx=(-1)^{k/2}
\int\limits_1^\infty x^{k-1-s}f\left(-\frac{\bar{h}}{k}+
\frac{ix}{k}\right) \,dx.
$$

Hence
\begin{gather}
(k/2\pi)^s\Gamma(s)\varphi(s,h/k)\label{chap1:eq1.2.8}\\
=\int\limits_1^\infty \left\{x^{s-1}f\left(\frac{h}{k}+\frac{ix}{k}
\right)+(-1)^{k/2} x^{k-1-s}f\left(
-\frac{\bar{h}}{k}+\frac{ix}{k}\right) \right\}\,dx.\notag
\end{gather}

But the integral on the right defines an entire function of $s$, for,
by \eqref{chap1:eq1.2.3}, the function $f(\tau)$ decays exponentially
as $\Iim \tau$ tends to infinity. Thus \eqref{chap1:eq1.2.8} gives an
analytic continuation of $\varphi(s,h/k)$ to an entire
function. Moreover, it is immediately seen that the right
hand\pageoriginale side remains invariant under the transformation
$h/k\to -\bar{h}/k, s\to k -s \text{ if } k/2$ is even,
and changes its sign if $k/2$ is odd. Thus the functional
equation \eqref{chap1:eq1.2.6} holds in any case.
\end{proof}

\begin{REM*}
The special case $k=1$ of \eqref{chap1:eq1.2.6} amounts to Hecke's
functional equation
$$
(2\pi)^{-s}\Gamma(s)\varphi(s)=(-1)^{k/2}(2\pi)^{s-k}
\Gamma(k-s)\varphi(k-s).
$$
\end{REM*}

\section[Aysmptotic Formula]{Asymptotic Formulae for the Gamma
Function and Bessel Functions}\label{chap1:sec1.3}

The special functions that will occur in this text are the gamma
function $\Gamma(s)$ and the Bessel functions $J_n(z),Y_n(z),K_n(z)$
of nonnegative integral order $n$. By definition,
\begin{gather}
J_n(z) = \sum\limits_{k=0}^\infty \frac{(-1)^k(z/2)^{2k+n}}
{k!(n+k)!},\label{chap1:eq1.3.1}\\
Y_n(z)= -\pi^{-1}\sum\limits_{k=0}^{n-1} \frac{(n-k-1)!}{k!}
(z/2)^{2k-n}\label{chap1:eq1.3.2}\\
+\pi^{-1}\sum\limits_{k=0}^\infty \frac{(-1)^k(z/2)^{2k+n}} {k!(n+k)!}
(2\log(z/2)-\psi(k+1)-\psi(k+n+1)),\notag\\
\intertext{and}
K_n(z)=\frac{1}{2}\sum\limits_{k=0}^{n-1} \frac{(-1)^k(n-k-1)!}{k!}
(z/2)^{2k-n}\label{chap1:eq1.3.3}\\
+\frac{1}{2}(-1)^{n-1}\sum\limits_{k=0}^\infty \frac{(z-2)^{2k+n}}
{k!(n+k)!} 92\log(z/2)-\psi(k+1)-\psi(k+n+1)),\notag
\end{gather}
where
$$
\psi(z)=\frac{\Gamma'}{\Gamma}(z).
$$

In particular,
$$
\psi(1)= -\gamma,\psi(n+1)= -\gamma +\sum\limits_{k=1}^nk^{-1},
n=1,2,\ldots 
$$
Repeated\pageoriginale use will be made of Stirling's formula for
$\Gamma(s)$ and of the asymptotic formulae for Bessel
functions. Therefore we recall these well-konwn results here for the
convenience of further reference.

The following version of Stirling's formula is precise enough for our
purposes.
\begin{lem}\label{chap1:lem1.3}
Let $\delta < \pi$ be a fixed positive number. Then
\begin{equation}\label{chap1:eq1.3.4}
\Gamma(s)=\sqrt{2\pi}\exp\{s-1/2)\log s-s\}(1+o(|s|^{-1}))
\end{equation}
in the sector $|\arg s|\leq\pi-\delta,|s|\geq 1$. Also, in any fixed
strip $A_1\leq \sigma \leq A_2$ we have for $t\geq 1$
\begin{equation}\label{chap1:eq1.3.5}
\Gamma(s)=\sqrt{2\pi} t^{s-1/2}\exp(-\frac{1}{2}\pi t-it+ \frac{1}{2}
\pi(\sigma -1/2)i)(1+o(t^{-1})),
\end{equation}
and
\begin{equation}\label{chap1:eq1.3.6}
|\Gamma(s)|=\sqrt{2\pi} t^{\sigma -1/2} e^{-(\pi/2)t}(1+0(t^{-1})).
\end{equation}
\end{lem}

The asymptotic formulae for the functions $J_n(z), Y_n(z)$, and
$K_n(z)$ can be derived from the analogous results for Hankel
functions 
\begin{equation}\label{chap1:eq1.3.7}
H_n^{(j)}(z)=J_n(z)+(-1)^{j-1}iY_n(z), j=1,2.
\end{equation}

The variable $z$ is here restricted to the slit complex plane $z\neq
0, |\arg$ $z|<\pi$. Obviously,
\begin{align}
J_n(z) &= \frac{1}{2}\left( H_n^{(1)}(z)+H_n^{(2)}(z)
\right),\label{chap1:eq1.3.8}\\ 
Y_n(z) &= \frac{1}{2i}\left(H_n^{(1)}(z)-H_n^{(2)}(z)
\right).\label{chap1:eq1.3.9} 
\end{align}
The function $K_n(z)$ can also be written in terms of Hankel
functions, for (see \cite{key29}, p.~78) 
\begin{align}
K_n(z) &= \frac{\pi}{2}i^{n+1}H_n^{(1)}(iz)\quad\text{for}\quad -\pi <
\arg z<\pi/2,\label{chap1:eq1.3.10}\\
K_n(z) &= \frac{\pi}{2}i^{-n+1}H_n^{(2)}(-iz)\quad\text{for}\quad
\frac{\pi}{2} < \arg z < \pi.\label{chap1:eq1.3.11}
\end{align}\pageoriginale

The asymptotic formulae for Hankel functions are usually derived from
appropriate integral representations, and then the asymptotic
behaviour of $J_n, Y_n$, and $K_n$ can be determined by the relations
\eqref{chap1:eq1.3.8} - \eqref{chap1:eq1.3.11} (see \cite{key29},
\S\S~7.2, 7.21 and 7.23). The results are as follows.
\begin{lem}\label{chap1:lem1.4}
Let $\delta_1<\pi$ and $\delta_2$ be fixed positive numbers. Then in
the sector
\begin{gather}
|arg z|\leq\pi -\delta_1, |z|\geq\delta_2\label{chap1:eq1.3.12}\\ 
\intertext{we have}
H_n^{(j)}(z)=(2/\pi z)^{1/2} \exp\left((-1)^{j-1}i\left(z-\frac{1}{2}n\pi
-\frac{1}{4}\pi\right)\right)(1+g_j(z)),\label{chap1:eq1.3.13} 
\end{gather}
where the functions $g_j(z)$ are holomorphic in the slit complex plane
$z\neq 0, |\arg z|<\pi$, and satisfy
\begin{equation}\label{chap1:eq1.3.14}
|g_j(z)|\ll |z|^{-1}
\end{equation}
in the sector \eqref{chap1:eq1.3.12}. Also, for real $x\geq\delta_2$,
\begin{align}
J_n(x) &= (2/\pi x)^{1/2}\cos\left(x-\frac{1}{2}n\pi-\frac{1}{4}\pi
\right)+o(x^{-3/2}),\label{chap1:eq1.3.15}\\
Y_n(x) &= (2/\pi x)^{1/2}\sin\left(x-\frac{1}{2}n\pi-\frac{1}{4} \pi
\right)+o(x^{-3/2}),\label{chap1:eq1.3.16}
\end{align}
and
\begin{equation}\label{chap1:eq1.3.17}
K_n(x)=(\pi/2x)^{1/2}e^{-x}\left(1+o(x^{-1})\right).
\end{equation}
\end{lem}

Strictly speaking, the functions $g_j$ should actually be denoted by
$g_{j,n}$, say, because they depend on $n$ as well, but for
simplicity\pageoriginale we dropped the index $n$, which will always
be known from the context. 

\section{Evaluation of Some Complex Integrals}\label{chap1:sec1.4} 

Let $a$ be a nonnegative integer, $\sigma_1\geq -a/2, \sigma_2 < -a,
T>0$, and let $C_a$ be the contour joining the points $\sigma_1
-i\infty, \sigma_1-Ti, \sigma_2-Ti, \sigma_2+Ti, \sigma_1+Ti$, and
$\sigma_1+ i\infty$ by straight lines. Let $X>0,k$ a positive
integer, and $c$ a number such that 
$$
(k-a-1)/2\leq c<k.
$$

In the next two sections we are going to need the values of the
complex integrals
\begin{align}
I_1 &= \frac{1}{2\pi i}\int\limits_{C_a}\Gamma^2(1-s)X^s(s (s+1)\ldots
(s+a))^{-1} \,ds,\label{chap1:eq1.4.1}\\
I_2 &= \frac{1}{2\pi i}\int\limits_{C_a}\Gamma^2(1-s)\cos(\pi s)
X^s(s(s+1)\ldots(s+a))^{-1}\,ds,\label{chap1:eq1.4.2}\\
\intertext{and}
I_3 &= \frac{1}{2\pi i}\int\limits_{(c)}\Gamma(k-s)\Gamma^{-1}
(s)X^s(s(s+1)\ldots(s+a))^{-1}\,ds.\label{chap1:eq1.4.3}
\end{align}

\begin{lem}\label{chap1:lem1.5}
We have
\begin{align}
I_1 &= 2(-1)^{a+1}X^{(1-a)/2}K_{a+1}
(2X^{1/2}),\label{chap1:eq1.4.4}\\
I_2 &= \pi X^{(1-a)/2}Y_{a+1}(2X^{1/2}),\label{chap1:eq1.4.5}\\
\intertext{and}
I_3 &= X^{(k-a)/2}J_{k+a}(2X^{1/2}).\label{chap1:eq1.4.6} 
\end{align}
\end{lem}

\begin{proof}
For positive numbers $T_1$ and $T_2$ exceeding $T$, denote by
$C_a(T_1,\break T_2)$ that part of $C_a$ which lies in the strip $-T_1\leq
t\leq T_2$. The integrals $I_1$ and $I_2$ are understood as limits of
the corresponding integrals\pageoriginale over $C_a(T_1,T_2)$ as $T_1$
and $T_2$ tend to infinity independently. Similarly, $I_3$ is the
limit of the integral over the line segment $[c-iT_1,c+iT_2]$.

Let $N$ be a large positive integer, which is kept fixed for a
moment. Denote by $\Gamma(T_1,T_2;N)$ the closed contour joining the
points $N+1/2-iT_1$ and $N+1/2+iT_2$ with each other and with the
initial and end point of $C_a(T_1,T_2)$, respectively, or with the
points $c-iT_1$ and $c+iT_2$ in the case of $I_3$. Then, by the
theorem of residues, 
\begin{equation}\label{chap1:eq1.4.7}
\frac{1}{2\pi i}\int\limits_\Gamma(\ldots)\,ds = \sum \Res, 
\end{equation}
where $(\ldots)$ means the integrand of the respective $I_j$, whose
residues inside $\Gamma=\Gamma(T_1,T_2;N)$ are summed on the right.

By \eqref{chap1:eq1.3.6} and our assumptions on $\sigma_1$ and $c$,
the integrals over those horizontal parts of $\Gamma(T_1,T_2;N)$ lying
on the lines $t=-T_1$ and $t=T_2$ are seen to be $\ll(\log T_i)^{-1},
i=1,2$. Hence these integrals vanish in the limit as the $T_i$ tend to
infinity. Then the equation \eqref{chap1:eq1.4.7} becomes 
\begin{equation}\label{chap1:eq1.4.8}
-I_j+\frac{1}{2\pi i}\int\limits_{(N+1/2)}(\ldots)\,ds =\sum \Res. 
\end{equation}

Consider now the integrals over the line $\sigma=N+1/2$.

By a repeated application of the formula $\Gamma(s)=s^{-1}
\Gamma(s+1)$, the $\Gamma$-factors in the integrands can be expressed
in terms of $\Gamma(1/2+it)$. Then, by some simple estimations, we
find that the integrals in question vanish in the limit as $N$ tends
to infinity. Therefore \eqref{chap1:eq1.4.8} gives 
\begin{align}
I_J &= - \sum\limits_{k=0}^a \Res(\cdotp,-k)- \sum\limits_{k=1}^{\infty}
\Res (\cdotp,k), j =1,2,\label{chap1:eq1.4.9}\\
I_3 &= - \sum\limits_{k=k}^{\infty} \Res
(\cdotp,k),\label{chap1:eq1.4.10}  
\end{align}\pageoriginale
where the dot denotes the respective integrand.

Consider the integral $I_1$ first. Obviously
$$
\Res(\cdotp,-h)=(-1)^hh!((a-h)!)^{-1}X^{-h}\quad\text{for}\quad h=0,
1,\ldots, a.
$$
The sum of these can be written, on putting $k=a-h$, as 
\begin{equation}\label{chap1:eq1.4.11} 
(-1)^a(X^{1/2})^{1-a}\sum\limits_{k=0}^a
(-1)^k(a-k)!(k!)^{-1}(2X^{1/2}/2)^{2k-a-1}.
\end{equation}

The integrand has double poles at $s=1,2,\ldots,$ and the residue at
$k$ can be calculated, multiplying (for $s=k+\delta$) the expansions 
\begin{align*}
\Gamma^2(1-s) &=
\delta^{-2}\Gamma^2(1-\delta)(k-1+\delta)^{-2}(k-2+\delta)^{-2} \ldots
(1+\delta)^{-2}\\
&= \delta^{-2}((k-1)!)^{-2}(1-2\psi(k)\delta+\ldots),\\
(s(s+1) &\ldots (s+a))^{-1}=(k-1)!((k+a)!)^{-1}\\
& \hspace{3cm}(1-(\psi(k+a+1)-
\psi(k))\delta +\cdots),
\end{align*}
and
$$
X^s=X^k(1+\delta \log X+\cdots).
$$

We obtain
$$
\Res(\cdotp,k)=X^k((k+a)!(k-1)!)^{-1}(\log X-\psi(k+a+1) -\psi(k)),
k=1,2,\ldots 
$$
Hence, also taking into account \eqref{chap1:eq1.4.11} and
\eqref{chap1:eq1.3.3}, we may write the sum of residues as 
{\fontsize{10}{11}\selectfont
\begin{multline*}
2(-1)^a(X^{1/2})^{1-a}\left\{\frac{1}{2}\sum\limits_{k=0}^{(a+1)-1}
(-1)^k((a+1)-k-1)!(k!)^{-1}(2X^{1/2}/2)^{2k-(a+1)}\right.\\
+\frac{1}{2}(-1)^{(a+1)-1}\sum\limits_{k=0}^\infty
(k!(k+(a+1))!)^{-1}(2X^{1/2}/2)^{2k+(a+1)}.\\
\cdotp(2\log(2X^{1/2}/2)-\psi(k+1)-\psi(k+(a+1)+1))\left.\right\} =2(-1)^a
X^{(1-a)/2} K_{a+1}(2X^{1/2})\cdotp
\end{multline*}}
Now \eqref{chap1:eq1.4.4} follows from \eqref{chap1:eq1.4.9}.

The\pageoriginale residues of the integrands of $I_1$ and $I_2$ at
$s=k$ differ only by the sign $(-1)^k$. The series of residues of the
integrand of $I_2$ can be written in terms of the function $Y_{a+1}$,
and the assertion \eqref{chap1:eq1.4.5} follows by a calculation
similar to that above.

Finally, the residue of the integrand of $I_3$ at $h\geq k$ is 
$$
(-1)^{h-k}X^h((h+a)!(h-k)!)^{-1},
$$
and putting $k=h-k$ the sum of these terms can be arranged so as
to give
$$
-X^{(k-a)/2}J_{k+a}(2X^{1/2}).
$$
\end{proof}

This proves \eqref{chap1:eq1.4.6}.

\section[Approximate Formulae and...]{Approximate Formula and Mean
  Value\hfil\break Estimates for $D(x,r)$ and $A(x,r)$}\label{chap1:sec1.5}  

Our object in this section is to derive approximate formulae of the
Voro\-noi type for the exponential sums
$$
D(x,r)=\sideset{}{'}\sum\limits_{n\leq x}d(n)e(nr)
$$
and
$$
A(x,r)=\sideset{}{'}\sum\limits_{n\leq x}a(n)e(nr),
$$
and to apply these to the pointwise and mean square estimation of
$D(x,r)$ and $A(x,r)$. As before, $r=h/k$ is a rational number.

A model of a result like this is the following classical formul for
$D(x)=D(x,1)$: 
\begin{gather}
D(x)=(\log x+2\gamma-1)x\label{chap1:eq1.5.1}\\
+(\pi\sqrt{2})^{-1_x{\frac{1}{4}}}\sum\limits_{n\leq N}d(n)n^{-3/4}\cos
(4\pi\sqrt{nx}-\pi/4)+o(x^{1/2+\epsilon_N-1/2}),\notag
\end{gather}
where\pageoriginale $x\geq 1$ and $1\leq N\ll x$ (see \cite{key27},
p.~269). The corresponding formula for $D(x,r)$ will be of the form 
\begin{equation}\label{chap1:eq1.5.2}
D(x,h/k)=k^{-1}(\log x+2\gamma-1-2 \log k)x+E(0,h/k)+\Delta (x,h/k),
\end{equation}
where $\Delta(x,h/k)$ is an error term.

The next theorem reveals an analogy between $\Delta(x,r)$ and
$A(x,r)$. 
\begin{THM}\label{chap1:thm1.1}
For $x\geq 1, k\leq x$, and $1\leq N\ll x$ the equation
\eqref{chap1:eq1.5.2} holds with 
\begin{gather}
\Delta (x,h/k)=(\pi\sqrt{2})^{-1} k^{1/2} x^{1/4}\sum\limits_{n\leq N} d(n)
e_k(-n\bar{h})n^{-3/4}\cos(4\pi\sqrt{nx}/k-
\pi/4)\label{chap1:eq1.5.3}\\ 
+0(k x^{\frac{1}{2} +\epsilon_N-\frac{1}{2}}).\notag
\end{gather}

Also,
{\fontsize{10}{12}\selectfont
\begin{gather}
A(x,h/k)=(\pi\sqrt{2})^{-1}k^{\frac{1}{2}} x^{-\frac{1}{4}+
    \frac{k}{2}}\sum\limits_{n\leq N} 
a(n)e_k(-n\bar{h})n^{-1/4-k/2}\cos \left(\frac{4\pi\sqrt{nx}}{k}
-\frac{\pi}{4}\right)\label{chap1:eq1.5.4}\\
+0(kx^{k/2+\epsilon_N-1/2}).\notag
\end{gather}}
\end{THM}

\begin{proof}
Consider first the formula \eqref{chap1:eq1.5.3}. We follow the
argument of proof of \eqref{chap1:eq1.5.1} in \cite{key27},
pp.~266--269, with minor modifications. 

Let $\delta$ be a small positive number which will be kept fixed
throughout the proof. By Perron's formula,
\begin{equation}\label{chap1:eq1.5.5}
D(x,r)=\frac{1}{2\pi i}\int\limits_{1+\delta-iT}^{1+\delta+iT}
E(s,r)x^s s^{-1}\,ds+0(x^{1+\delta} T^{-1}),
\end{equation}
where $r=h/k$ and $T$ is a parameter such that 
\begin{equation}\label{chap1:eq1.5.6}
1\leq T\ll k^{-1}x.
\end{equation}

As\pageoriginale a preliminary for the next step, which consists of
moving the integration in \eqref{chap1:eq1.5.5} to the line $\sigma=
-\delta$, we need an estimate for $E(s,r)$ in the strip $-\delta\leq
\sigma \leq 1+\delta$ for $|t|\geq 1$.

The auxiliary function 
\begin{equation}\label{chap1:eq1.5.7}
\left(\frac{s-1}{s-2}\right)^2E(s,r)
\end{equation}
is holomorphic in the strip $-\delta\leq\sigma\leq 1+\delta$, and in
the part where $|t|\geq 1$ it is of the same order of magnitude as
$E(s,r)$. This function is bounded on the line $\sigma=1+\delta$, and
on the line $\sigma=-\delta$ it is 
$$
\ll (k(|t|+1))^{1+2\delta}
$$
by the functional equation \eqref{chap1:eq1.1.2} and the estimate
\eqref{chap1:eq1.3.6} of the gamma function. The convexity principle
now gives an estimate for the function \eqref{chap1:eq1.5.7}, and as a
consequence we obtain
\begin{equation}\label{chap1:eq1.5.8}
|E(s,r)|\ll(k|t|)^{1-\sigma+\delta}\quad\text{for}\quad -\delta\leq
\sigma \leq 1+\delta, |t|\geq 1.
\end{equation}

Let $C$ be the rectangular contour with vertices $1+\delta\pm iT$ and
$-\delta\pm iT$. By the theorem of residues, we have 
\begin{equation}\label{chap1:eq1.5.9}
\frac{1}{2\pi i}\int\limits_C E(s,r)x^ss^{-1}\,ds=k^{-1}(\log
x+2\gamma-1-2 \log k)x+E(0,r),
\end{equation}
where the expansion \eqref{chap1:eq1.1.3} has been used in the
calculation of the residue at $s=1$.

The integrals over the horizontal parts fo $C$ are $\ll
x^{1+\delta} T^{-1}$ by \eqref{chap1:eq1.5.8} and
\eqref{chap1:eq1.5.6}. Hence \eqref{chap1:eq1.5.2},
\eqref{chap1:eq1.5.5}, and \eqref{chap1:eq1.5.9} give together 
\begin{equation}\label{chap1:eq1.5.10}
\Delta(x,r)=\frac{1}{2\pi i}\int\limits_{\delta-iT}^{-\delta+iT}
E(s,r)x^s s^{-1}\,ds +o(x^{1+\delta} T^{-1}). 
\end{equation}

The functional equation \eqref{chap1:eq1.1.2} for $E(s,r)$ is now
applied. The term involving $E(1-s,\bar{h}/k)$ decreases rapidly as
$|t|$ increases and it\pageoriginale will be estimated as an error
term. Then for $\sigma =-\delta$, we obtain 
\begin{gather*}
E(s,r)=-2(2\pi)^{2s-2} \Gamma^2(1-s)k^{1-2s}\cos(\pi s)
\sum\limits_{n=1}^\infty d(n)e_k(-n\bar{h})n^{s-1}\\  
+o((k(|t|+1))^{1+2\delta}e^{-\pi|t|}).
\end{gather*}

The contribution of the error term to the integral in
\eqref{chap1:eq1.5.10} is 
$$
\ll k^{1+2\delta}x^{-\delta}\ll kx^\delta \ll x^{1+\delta}T^{-1}.
$$

Thus we have 
\begin{gather}
\Delta(x,r)=-\frac{1}{2}\pi^{-2}k\sum\limits_{n=1}^\infty
d(n)n^{-1}e_k(-n\bar{h})j_n+o(x^{1+\delta}
T^{-1}),\label{chap1:eq1.5.11}\\ 
\intertext{where}
j_n=\frac{1}{2\pi i}\int\limits_{-\delta-iT}^{-\delta+iT}\Gamma^2
(1-s)\cos(\pi s)(4\pi^2nxk^{-2})^s s^{-1} \,ds.\label{chap1:eq1.5.12} 
\end{gather}

At this stage we fix the parameter $T$, putting 
\begin{equation}\label{chap1:eq1.5.13}
T^2k^2(4\pi^2x)^{-1}=N+1/2,
\end{equation}
where $N$ is an integer such that $1\leq N\ll x$. It is immediately
seen that $T\ll k^{-1}x$. In order that the condition
\eqref{chap1:eq1.5.6} be satisfied, we should also have $T\geq 1$,
which presupposes that $N\gg k^2x^{-1}$. We may assume this, for
otherwise the assertion \eqref{chap1:eq1.5.3} holds for trivial
reasons. Indeed, if $1\leq N\ll k^2x^{-1}$, then \eqref{chap1:eq1.5.3}
is implied by the estimate $\Delta(x,h/k)\ll x^{1+\epsilon}$, which is
definitely true by \eqref{chap1:eq1.5.2} and \eqref{chap1:eq1.1.4}.

Next we dispose of the tail $n>N$ of the series in
\eqref{chap1:eq1.5.11}. The integral $j_n$ splits into three parts, in
which $t$ runs respectively over the intervals $[-T,-1],[-1,1]$, and
$[1,T]$. The second integral is clearly $\ll k^{2\delta}n^{-\delta}
x^{-\delta}$, and these terms contribute $\ll
kx^\delta$. The\pageoriginale first and third integrals are similar;
consider the third one, say $j_n'$.

By \eqref{chap1:eq1.3.5} we have for $-\delta\leq\sigma\leq\delta$ and
$t\geq 1$
\begin{gather}
\Gamma^2(1-s)\cos(\pi s)(4\pi^2nxk^{-2})^s s^{-1}\label{chap1:eq1.5.14}\\ 
=A(\sigma)t^{-2\sigma}(4\pi^2nxk^{-2})^\sigma e^{iF(t)}(1+O(t^{-1})),\notag 
\end{gather}
where $A(\sigma)$ is bounded and 
\begin{equation}\label{chap1:eq1.5.15}
F(t)= -2t\log t+2t+t\log(4\pi^2nxk^{-2}).
\end{equation}

Thus
\begin{equation}\label{chap1:eq1.5.16}
j_n'=Ak^{2\delta}n^{-\delta}x^{-\delta}\left(\int\limits_1^T
t^{2\delta}e^{iF(t)}\,dt+o(T^{2\delta})\right).
\end{equation}

The last integral is estimated by the following elementary lemma
(\cite{key27}, Lemma 4.3) on exponential integrals.
\end{proof}

\begin{lem}\label{chap1:lem1.6}
Let $F(x)$ and $G(x)$ be real functions in the interval $[a,b]$ where
$G(x)$ is continuous and $F(x)$ continuously differentiable. Suppose
that $G(x)/F'(x)$  is monotonic and $|F'(x)/G(x)|\geq m>0$. Then 
\begin{equation}\label{chap1:eq1.5.17}
|\int\limits_a^bG(x)e^{iF(x)}\,dx|\leq 4/m.
\end{equation}
\end{lem}

Now by \eqref{chap1:eq1.5.15} and \eqref{chap1:eq1.5.13} we have 
\begin{equation}\label{chap1:eq1.5.18}
F'(t)=\log(4\pi^2nxk^{-2}t^{-2})\geq\log \left(\frac{n}{N+1/2}\right) 
\end{equation}
for $1\leq t\leq T$, whence by \eqref{chap1:eq1.5.16} and
\eqref{chap1:eq1.5.17} 
$$
j_n'\ll k^{2\delta}n^{-\delta}T^{2\delta}x^{-\delta}\left(\left(\log\left(
\frac{n}{N+1/2}\right)\right)^{-1}+1\right).
$$

Thus 
\begin{gather*}
\sum\limits_{n\geq 2N}d(n)n^{-1}|j_n'|\ll N^\delta \ll x^\delta,\\
\text{and}\qquad \sum\limits_{N<n\leq 2N}d(n)n^{-1}|j_n'|\ll
\sum\limits_{1\leq m\leq N}d(N+m)m^{-1}\ll x^\delta.\qquad 
\end{gather*}\pageoriginale
Accordingly, in \eqref{chap1:eq1.5.11} the tail $n>N$ of the series
can be omitted with an error $\ll kx^\delta$, and taking into account
the choice \eqref{chap1:eq1.5.13} of $T$, we obtain 
\begin{equation}\label{chap1:eq1.5.19}
\Delta(x,r)=-\frac{1}{2}\pi^{-2}k\sum\limits_{n\leq N}d(n)n^{-1} e_k
(-n\bar{h})j_n+o(kx^{\frac{1}{2}+\delta} N^{\frac{1}{2}}).
\end{equation}

The remaining integrals $j_n$ will be calculated approximately by
Lem\-ma \ref{chap1:lem1.5}, and to this end we extend the path of
integration in \eqref{chap1:eq1.5.12} to the infinite broken line
through the points $\delta-i\infty, \delta-iT, -\delta-iT, -\delta-iT,
-\delta+iT, \delta+iT$ and $\delta+i\infty$, estimating the consequent
error when the $j_n$ in \eqref{chap1:eq1.5.19} are replaced by the new
integrals. 

First, by \eqref{chap1:eq1.5.14} and \eqref{chap1:eq1.5.13},
\begin{gather*}
\sum\limits_{n\leq N}d(n)n^{-1}\left|\int\limits_{-\delta+iT}
^{\delta+iT}(\cdots)\right|\ll \sum\limits_{n\leq N}d(n)n^{-1}
\int\limits_{-\delta}^\delta(n/N)^\sigma \,d\sigma\\
\ll N^\delta \sum\limits_{n\leq N}d(n)n^{-1-\delta}\ll x^\delta,
\end{gather*}
where $(\cdots)$ means the integrand of $j_n$. The same estimate holds
for the integrals over the line segment $[-\delta-iT, \delta-iT]$. 

Next, by \eqref{chap1:eq1.5.14}, \eqref{chap1:eq1.5.18}, and Lemma
\ref{chap1:lem1.6}, we have 
\begin{gather*}
\sum\limits_{n\leq N}d(n)n^{-1}\left|\int\limits_{\delta+iT}
^{\delta+i\infty}(\cdots)\right|\ll (k^{-2}x)^\delta
\sum\limits_{n\leq N}d(n)n^{-1+\delta}\\
\qquad\qquad\left| \int\limits_T^\infty
t^{-2\delta}\left(e^{iF(t)}+o(t^{-1})\right)dt\right|\\
\ll (k^{-2}T^{-2}x)^\delta \sum\limits_{n\leq N}d(n)n^{-1+\delta}
\left(\left(\log\left(\frac{N+1/2}{n}\right)\right)^{-1}+1\right)\\
\ll \sum\limits_{n\leq N/2}d(n)n^{-1}+\sum\limits_{N/2<n\leq N} d(n)
(N+1/2-n)^{-1}\ll x^\delta,
\end{gather*}
and\pageoriginale similarly for the integrals over
$[\delta-i\infty,\delta-iT]$.

These estimations show that \eqref{chap1:eq1.5.19} remains valid if
the $j_n$ are replaced by the modified integrals, which are of the
type $I_2$ in \eqref{chap1:eq1.4.2} for $a=0$ and $X=4\pi^2nxk^{-2}$,
and thus equal to 
$$
2\pi^2(nx)^{1/2}k^{-1}Y_1(4\pi\sqrt{nx}/k)
$$
by \eqref{chap1:eq1.4.5}. The assertion \eqref{chap1:eq1.5.3} now
follows when $Y_1$ is replaced by its expression \eqref{chap1:eq1.3.16}
(which holds trivially for $n=1$ with the error term $0(x^{-1})$ even
in the interval $(o,\delta_2)$).

The proof of \eqref{chap1:eq1.5.4} is quite similar. The starting
point is the equation
$$
A(x,r)=\frac{1}{2\pi i}\int\limits_{(k+1)/2+\delta-iT}^
{(k+1)/2+\delta+iT}\varphi(s,r)x^ss^{-1}\,ds+o(x^{(k+1)/2+\delta}
T^{-1}),
$$
where $1\leq T\ll k^{-1}x$. It should be noted that Deligne's estimate
\eqref{chap1:eq1.2.5} is needed here; otherwise the error term would
be bigger.

The integration is next shifted to the line segment
$[(k-1)/2-\delta-iT, (k-1)/2-\delta+iT]$ with arguments as
in the proof of \eqref{chap1:eq1.5.10}, except that now there are no
residue terms. Applying the functional equation \eqref{chap1:eq1.2.6}
of $\varphi(s,r)$, we obtain
\begin{align*}
& A(x,r)=(-1)^{k/2}(k/2\pi)^k \sum\limits_{n=1}^\infty
a(n)n^{-k}e_k(-\bar{h}n)\times\\
& \times \frac{1}{2\pi i}
\int\limits_{(k-1)/2-\delta-iT}^{(k-1)/2-\delta+iT} 
\Gamma(k-s)\Gamma(s)^{-1}(4\pi^2nxk^{-2})^ss^{-1}\,ds+o
(x^{(k+1)/2+\delta}T^{-1}).
\end{align*}

The parameter $T$ is chosen as in \eqref{chap1:eq1.5.13} again. Next
it is shown, as before, that the tail $n>N$ of the above series can be
omitted, and that in the remaining terms the integration can be
shifted to\pageoriginale the whole line
$\sigma=(k-1)/2+\delta$. The new integrals are evaluated in terms
of the function $J_k$ using \eqref{chap1:eq1.4.6}. Finally $J_k$
is approximated by \eqref{chap1:eq1.3.15} (which holds trivially with
the error term $o(x^{-1/2})$ even in the interval $(0, \delta_2)$) to
give the formula \eqref{chap1:eq1.5.4}. The proof of the theorem is
now complete.

Choosing $N=k^{2/3}x^{1/3}$ and estimating the sums on the right of
\eqref{chap1:eq1.5.3} and \eqref{chap1:eq1.5.4} by absolute values,
one obtains the following estimates for $\Delta(x,r)$ and $A(x,r)$, 
\begin{Coro*}
For $x\geq 1$ and $k\leq x$ we have 
\begin{align}
\Delta(x,h/k) & \ll k^{2/3}x^{1/3+\epsilon},\label{chap1:eq1.5.20}\\
A(x,h/k) & \ll k^{2/3}x^{k/2-1/6+\epsilon}.\label{chap1:eq1.5.21}
\end{align}
\end{Coro*}

As another application of Theorem \ref{chap1:thm1.1} we deduce mean
value results for $\Delta(x,r)$ and $A(x,r)$. 
\begin{THM}\label{chap1:thm1.2}
For $X\geq 1$ we have 
\begin{align}
& \int\limits_1^X|\Delta(x,h/k)|^2\,dx=c_1kX^{3/2}+o(k^2X^{1+\epsilon})
+o(k^{3/2}X^{5/4+\epsilon}),\label{chap1:eq1.5.22}\\
\text{and}
& \int\limits_1^X|A(x,h/k)|^2\,dx=c_2(k)kX^{k+1/2}+o(k^2
X^{k+\epsilon})+o(k^{3/2}X^{k+1/4+\epsilon}),\label{chap1:eq1.5.23}
\end{align}
where
\begin{equation*}
\begin{split}
c_1=(6\pi^2)^{-1}\sum\limits_{n=1}^\infty d^2(n)n^{-3/2}\\
\intertext{and}
c_2(k)=\left((4k+2)\pi^2\right)^{-1}
\sum\limits_{n=1}^\infty |a(n)|^2n^{-k-1/2}.
\end{split}
\end{equation*}
\end{THM}

\begin{proof}
The proofs of these assertions are very similar; so it suffices
to\pageoriginale consider the verification of \eqref{chap1:eq1.5.22}
as an example. We are actually going to prove the formula
\begin{align}
\int\limits_X^{2X}|\Delta(x,h/k)|^2 \,dx=c_1k\left((2X)^{3/2}
-X^{3/2}\right)\label{chap1:eq1.5.24} &\\
+o\left(k^2X^{1+\epsilon}\right)+0\left(k^{3/2}X^{5/4+\epsilon}\right)&\notag
\end{align}
for $k\leq X$, and for $X\ll k$ we estimate trivially
\begin{equation}\label{chap1:eq1.5.25}
\int\limits_1^X|\Delta(x,h/k)|^2\,dx\ll k^2X^{1+\epsilon}
\end{equation}
noting that $\Delta(x,h/k)\ll k\log 2k$ for $x\ll k$ by
\eqref{chap1:eq1.5.2} and \eqref{chap1:eq1.1.4}. Clearly
\eqref{chap1:eq1.5.22} follows from \eqref{chap1:eq1.5.24} and
\eqref{chap1:eq1.5.25}.

Turning to the proof of \eqref{chap1:eq1.5.24}, let $X\leq x\leq 2X$,
and choose $N=X$ in the formula \eqref{chap1:eq1.5.3}, which we write
as 
$$
\Delta(x,h/k)=S(x,h/k)+0(kx^\epsilon).
$$
We are going to prove that 
\begin{equation}\label{chap1:eq1.5.26}
\int\limits_X^{2X}|S(x,h/k)|^2 \,dx=c_1k\left((2X)^{3/2}-X
^{3/2}\right)+o\left(k^2X^{1+\epsilon}\right),
\end{equation}
which implies \eqref{chap1:eq1.5.24} by Cauchy's inequality.

Squaring out $|S(x,h/k)|^2$ and integrating term by term, we find that 
\begin{equation}\label{chap1:eq1.5.27}
\int\limits_X^{2X}|S(x,h/k)|^2\,dx=S_\circ+o(k(|S_1|+|S_2|)),
\end{equation}
where
\begin{align*}
S_\circ &= (4\pi^2)^{-1}k\sum\limits_{n\leq X} d^2(n)n^{-3/2}
\int\limits_X^{2X}X^{1/2}\,dx,\\
S_1 &= \sum\limits_{\underset{m\neq n}{m,n\leq X}}d(m)d(n)(mn)^{-3/4}
\int\limits_X^{2X}x^{1/2}e(2(\sqrt{m}-\sqrt{n})\sqrt{x}/k)\,dx,\\
S_2 &= \sum\limits_{m,n\leq X}d(m)d(n)(mn)^{-3/4} \int\limits_X^{2X}
x^{1/2}e\left(2\left(\sqrt{m}+\sqrt{n}\right)\sqrt{x}/k\right)\,dx.
\end{align*}\pageoriginale

The sum $S_\circ$ gives the leading term in \eqref{chap1:eq1.5.26},
for 
$$
S_\circ =c_1k\left((2X)^{3/2}-X^{3/2}\right)
+o\left(kX^{1+\epsilon}\right).
$$
Further, by Lemma \ref{chap1:lem1.6},
\begin{align*}
S_1 &\ll kX\sum\limits_{\underset{m<n}{m,n\leq X}}d(m)d(n)(mn)^{-3/4}
\left(\sqrt{n}-\sqrt{m}\right)^{-1}\\
&\ll kX\sum\limits_{\underset{m<n}{m,n\leq X}}d(m)d(n)m^{-3/4}
n^{-1/4}(n-m)^{-1}\\
&\ll kX^{1+\epsilon/2}\sum\limits_{m\leq X}m^{-1}\ll kX^{1+\epsilon},
\end{align*}
and similarly for $S_2$. Hence \eqref{chap1:eq1.5.26} follows from
\eqref{chap1:eq1.5.27}, and the proof of \eqref{chap1:eq1.5.22} is
complete. 
\end{proof}

\begin{Coro*}
For $k\ll X^{1/2-\epsilon}$ and $X\to\infty$ we have
\begin{align}
\int\limits_1^X|\Delta(x,h/k)|^2\,dx &\sim
c_1kX^{3/2},\label{chap1:eq1.5.28}\\
\int\limits_1^X|A(x,h/k)|^2\,dx &\sim c_2(k)
kX^{k+1/2}.\label{chap1:eq1.5.29} 
\end{align}

It is seen that for $k\ll x^{1/2-\epsilon}$ the typical order of
$|\Delta(x,h/k)|$ is $k^{1/2}x^{1/4}$, and that of $|A(x,h/k)|$ is
$k^{1/2}x^{k/2-1/4}$. This suggests the following 
\end{Coro*}

\begin{conjecture*}
For $x\geq 1$ and $k\ll x^{1/2}$
\begin{align}
|\Delta(x,h/k)| &\ll k^{1/2}x^{1/4+\epsilon}\label{chap1:eq1.5.30}\\
|A(x,h/k)| &\ll k^{1/2}x^{k/2-1/4+\epsilon}.\label{chap1:eq1.5.31}
\end{align}
\end{conjecture*}

Note\pageoriginale that \eqref{chap1:eq1.5.30} is a generalization of
the old conjecture 
$$
|\Delta(x)|\ll x^{1/4+\epsilon}
$$
in Dirichlet's divisor problem.

\section{Identities for $D_a(x,r)$ and $A_a(x,r)$}\label{chap1:sec1.6}
The Case $a\geq 1$.

As generalizations of the sum functions $D(x,r)$ and $A(x,r)$, define
the Riesz means
\begin{align}
D_a(x,r) &= \frac{1}{a!}\sideset{}{'}\sum\limits_{n\leq x} d(n)e(nr)
(x-n)^a\label{chap1:eq1.6.1}\\
\intertext{and}
A_a(x,r) &= \frac{1}{a!}\sideset{}{'}\sum\limits_{n\leq x}a(n)e(nr)
(x-n)^a,\label{chap1:eq1.6.2} 
\end{align}
where a is a nonnegative integer. Thus $D_\circ(x,r)=D(x,r)$ and
$A_\circ(x,r)=A(x,r)$. Actually, for our later purposes, only the case
$a=0$ will be of relevance, but just in order to be able to deal with
this somewhat delicate case by an induction from a to $a-1$, we shall
need identities for $D_a(x,r)$ and $A_a(x,r)$ as well. These are
contained in the following theorem.

\begin{THM}\label{chap1:thm1.3}
Let $a\geq 0$ be an integer. Then for $x>0$ we have
\begin{align}
D_a(x,h/k)= \frac{x^{1+a}}{(1+a)!k}\left(\log
x+2\gamma-2\log k-\sum\limits_{n=1}^{a+1}\frac{1}{n}
\right)\label{chap1:eq1.6.3}\\
+\sum\limits_{n=0}^a\frac{(-1)^n}{n!(a-n)!}E(-n,h/k)x^{a-n}+\Delta_a
(x,h/k),\notag 
\end{align}
where
\begin{align}
& \Delta_a(x,h/k)=-(k/2\pi)^ax^{(1+a)/2}\sum\limits_{n=1}^\infty d(n)
n^{-(1+a)/2}\times\label{chap1:eq1.6.4}\\
& \times \left\{e_k(-n\bar{h})Y_{1+a}(4\pi\sqrt{nx}/k)+(-1)^a(2/\pi)e_k
(n\bar{h})K_{1+a}(4\pi\sqrt{nx}/k)\right\}.\notag
\end{align}

Also,\pageoriginale
\begin{align}
A_a(x,h/k) &= (-1)^{k/2}(k/2\pi)^ax^{(k+a)/2}
\times\label{chap1:eq1.6.5}\\
&\times \sum\limits_{n=1}^\infty a(n)n^{-(k+a)/2}e_k(-n\bar{h})
J_{k+a}(4\pi\sqrt{nx}/k).\notag
\end{align}
\end{THM}

\begin{proof}
(the case $a\geq 1$). By a well-known summation formula (see
\cite{key13}, p.~487, equation (A.14)), we have for any $c>1$
$$
D_a(x,r)=\frac{1}{2\pi i}\int\limits_{(c)}E(s,r)x^{s+a}(s(s+1)\cdots
(s+a))^{-1} \,ds.
$$

First let $a\geq 2$, and move the integration to the broken line $C_a$
joining the points $-1/3-i\infty, -1/3-i, -(a+1/2)-i, -(a+1/2)+i,
-1/3+i$, and $-1/3+i\infty$. The residues at $1,0,-1,\ldots,-a$ give
the initial terms in \eqref{chap1:eq1.6.3}; the expansion
\eqref{chap1:eq1.1.3} is used in the calculation of the residue at
$s=1$. Note also that the integrand is $\ll |t|^{-2\sigma-a}$ for
$|t|\geq 1$ and $\sigma$ bounded (the implied constant depends on $k$
and $x$), so that the theorem of residues gives
$$
\Delta_a(x,r)=\frac{1}{2\pi i}\int\limits_{C_a}E(s,r)x^{s+a}(s(s+1)
\cdots(s+a))^{-1}\,ds.
$$

The function $E(s,h/k)$ is now expressed by the functional equation
\eqref{chap1:eq1.1.2}, and the resulting series can be integrated term
by term by the last mentioned estimate. The new integrals are of the
type $I_1$ and $I_2$ in the notation of \S \ref{chap1:sec1.4}, and
\eqref{chap1:eq1.6.4} follows, for $a\geq 2$, when these integrals are
evaluated by Lemma \ref{chap1:lem1.5}.

Next we differentiate both sides of \eqref{chap1:eq1.6.3} with respect
to $x$. By the definition \eqref{chap1:eq1.6.1} we have for $a\geq 2$
\begin{gather}
D_a'(x,r)=D_{a-1}(x,r),\label{chap1:eq1.6.6}\\
\intertext{and consequently by \eqref{chap1:eq1.6.3}}
\Delta_a'(x,r)=\Delta_{a-1}(x,r).\label{chap1:eq1.6.7}
\end{gather}\pageoriginale

The right hand side of \eqref{chap1:eq1.6.4} shares the same property,
for its derivative equals the same expression with a replaced by
$a-1$, Formally, this can be verified by differentiation term by term
using the relations 
\begin{gather}
(x^nK_n(x))'=-x^nK_{n-1}(x)\label{chap1:eq1.6.8}\\
\intertext{and}
(x^nY_n(x))'=x^nY_{n-1}(x).\label{chap1:eq1.6.9}
\end{gather}

But by \eqref{chap1:eq1.3.16} and \eqref{chap1:eq1.3.17} the series in
\eqref{chap1:eq1.6.4} converges absolutely for $a\geq 1$, and the
convergence is uniform in any interval $[x_1,x_2]\subset (0,\infty)$,
which justifies the differentiation term by term for $a\geq 2$. This
argument proves \eqref{chap1:eq1.6.4} for $a=1$ also. 

The identity \eqref{chap1:eq1.6.5} is proved in the same way, starting
from the formula
$$
A_a(x,r)=\frac{1}{2\pi i}\int\limits_{(c)}\varphi(s,r)x^{s+a}
(s(s+1)\cdots (s+a))^{-1}\,ds,
$$
where $c>(k+1)/2$. For $a\geq 2$ the integration can be shifted
to the line $\sigma = k/2-2/3$, where we use the functional
equation \eqref{chap1:eq1.2.6} to rewrite $\varphi(s,h/k)$. This leads
to integrals of the type $I_3$, which can be expressed in terms of the
Bessel function $J_{k+a}$ by \eqref{chap1:eq1.4.6}. As a result,
we obtain the assertion \eqref{chap1:eq1.6.5} for $a\geq 2$. The case
$a=1$ is deduced from this by differentiation as above, using the
relation 
\begin{equation}\label{chap1:eq1.6.10}
(x^nJ_n(x))'=x^nJ_{n-1}(x)
\end{equation}
and the asymptotic formula \eqref{chap1:eq1.3.15}. We have now proved
the theorem in the case $a\geq 1$, and the case $a=0$ is postponed to
\S~ \ref{chap1:sec1.8}. 

Estimating\pageoriginale the series in \eqref{chap1:eq1.6.4} and
\eqref{chap1:eq1.6.5} by absolute values, one obtains estimates for
$\Delta_a(x,r)$ and $A_a(x,r)$. In the case $a=1$, the result is as
follows. 
\end{proof}

\begin{Coro*}
For $x\gg k^2$ we have
\begin{gather}
|\Delta_1(x,h/k)|\ll k^{3/2}x^{3/4}\label{chap1:eq1.6.11}\\
\intertext{and}
|A_1(x,h/k)|\ll k^{3/2}x^{k/2+1/4}.\label{chap1:eq1.6.12}
\end{gather}
\end{Coro*}

\begin{REM*}
The error term $\Delta_\circ(x,r)$ coincides with $\Delta(x,r)$,
defined in \eqref{chap1:eq1.5.2}. The relations \eqref{chap1:eq1.6.6}
and \eqref{chap1:eq1.6.7} remain valid also for $a=1$ if $x$ is not an
integer. Thus, in particular,
\begin{equation}\label{chap1:eq1.6.13}
\Delta_1'(x,r)=\Delta(x,r)\quad\text{for}\quad x>0,x\notin\mathbb{Z}. 
\end{equation}

Together with \eqref{chap1:eq1.6.4} for $a=1$, this yields
\eqref{chap1:eq1.6.4} for $a=0, x\notin\mathbb{Z}$ as well, if the
differentiation term by term of \eqref{chap1:eq1.6.4} for $a=1$ can be
justified. This step is not obvious but requires an analysis which is
carried out in the next section. After that the remaining case
$a=0,x\in\mathbb{Z}$ is dealt with in \S~ \ref{chap1:sec1.8} by a
limiting argument.

In analogy with \eqref{chap1:eq1.6.13}, we have 
\begin{equation}\label{chap1:eq1.6.14}
A_1'(x,r)=A(x,r)\quad\text{for}\quad x>0, x\notin \mathbb{Z}.
\end{equation}

This relation, which follows immediately from the definition
\eqref{chap1:eq1.6.2}, is the starting point in the proof of
\eqref{chap1:eq1.6.5} for $a=0$.
\end{REM*}

\section{Analysis of the Convergence of the Voronoi
  Series}\label{chap1:sec1.7} 

In this section we are going to study the series \eqref{chap1:eq1.6.4}
and \eqref{chap1:eq1.6.5} for $a=0$ as a preliminary for the proof of
Theorem \ref{chap1:thm1.3} for this\pageoriginale remaining value of
$a$. In virtue of the analogy between $d(n)$ and $a(n)$, we may
restrict ourselves to the analysis of the first mentioned
series. Thus, let us consider the series
\begin{equation}\label{chap1:eq1.7.1}
x^{1/2}\sum\limits_{n=1}^\infty d(n)n^{-1/2}\left\{ e_k(-n\bar{h})
Y_1(4\pi\sqrt{nx}/k)+(2/\pi)e_k(n\bar{h})K_1(4\pi\sqrt{nx}/k)\right\}.
\end{equation}

For $k=1$ this is - up to sign - Voronoi's expression for $\Delta(x)$,
and the more general series \eqref{chap1:eq1.7.1} will also be called
a {\bf Voronoi series}.

From the point of view of convergence, the factor $x^{1/2}$ in front
of the Voronoi series is of course irrelevant, but because we are
going to consider $x$ as a variable in the next section, we prefer
keeping $x$ explicit all the time.

Denote by $\sum(a,b;x)$ that part of the Voronoi series in which the
summation is taken over the finite interval $[a,b]$. The following
theorem gives an approximate formula for $\sum(a,b;x)$. 

\begin{THM}\label{chap1:thm1.4}
Let $[x_1,x_2]\subset (0,\infty)$ be a fixed interval. Then uniformly
for $x\in[x_1,x_2]$ and $2\leq a<b<\infty$ we have 
\begin{gather}
\sum(a,b;x)=Ax^{5/4}d(m)m^{-5/4}e_k(mh)
\int\limits_{\sqrt{a}}^{\sqrt{b}}u^{-1}\sin(4\pi(\sqrt{m}-\sqrt{x})u/k)
\,du\label{chap1:eq1.7.2}\\
+o(a^{-14}\log a),\notag
\end{gather}
where $m$ is the positive integer nearest to $x$ (or any one of the
two possibilities if $x>1$ is half an odd integer), and $A$ is a
number depending only on $k$.

For the proof, we shall need the following elementary lemma. 
\end{THM}

\begin{lem}\label{chap1:lem1.7}
Let\pageoriginale $f\in C^2[a,b]$, where $0<a<b$. Then 
\begin{align}
& \sideset{}{'}\sum\limits_{a\leq n\leq b}f(n)d(n)e_k(nh)=
\int\limits_a^b(\Delta(t,h/k)f(t)-\Delta_1(t,h/k)
f'(t))\label{chap1:eq1.7.3}\\
& +\int\limits_a^b\Delta_1(t,h/k)f''(t)\,dt+k^{-1} \int\limits_a^b
(\log t+2\gamma-2\log k)f(t)\,dt.\notag
\end{align}
\end{lem}

\begin{proof}
According to \eqref{chap1:eq1.5.2}, the sum under consideration is 
$$
\int\limits_a^bf(t)dD(t,h/k)=k^{-1}\int\limits_a^b f(t)(\log
t+2\gamma-2\log k)\,dt+\int\limits_a^bf(t)d\Delta(t,h/k).
$$

By repeated integrations by parts and using \eqref{chap1:eq1.6.13}, we
obtain
\begin{align*}
\int\limits_a^bf(t)d\Delta(t,h/k) &= \int\limits_a^bf(t)\Delta(t,h/k)-
\int\limits_a^b f'(t)\Delta(t,h/k)\,dt\\
&= \int\limits_a^b(f(t)\Delta(t,h/k)-\Delta_1(t,h/k)f'(t))\\
&{} +\int\limits_a^b\Delta(t,h/k)f''(t)\,dt,
\end{align*}
and the formula \eqref{chap1:eq1.7.3} follows.
\end{proof}
\medskip

\noindent {\bf Proof of Theorem \ref{chap1:thm1.4}}. Because $h/k,
x_1$, and $x_2$ will be fixed during the following discussion, we may
ignore the dependence of constants on time.

First, by the asymptotic formulae \eqref{chap1:eq1.3.16} and
\eqref{chap1:eq1.3.17} for Bessel functions, we have 
\begin{gather}\label{chap1:eq1.7.4}
\begin{aligned}
\sum(a,b;x)=Ax^{1/4}\sum\limits_{a\leq n\leq b} d(n)n^{-3/4}& e_k
(-n\bar{h})\cos(4\pi\sqrt{nx}/k-\pi/4)\\
&+o(a^{-1/4}\log a).
\end{aligned}
\end{gather}

Lemma \ref{chap1:lem1.7} is now applied to the sum here, with
$-\bar{h}/k$ in place of $h/k$, and with
$$
f(t)=x^{1/4}t^{-3/4}\cos(4\pi\sqrt{tx}/k-\pi/4).
$$

The\pageoriginale integrated terms in \eqref{chap1:eq1.7.3} are $\ll
a^{-1/4}$, by \eqref{chap1:eq1.5.20} and \eqref{chap1:eq1.6.11}. Also, by Lemma
\ref{chap1:lem1.6}, the last term in \eqref{chap1:eq1.7.3} is $\ll
a^{-1/4}\log a$. Thus it remains to consider the integral
\begin{equation}\label{chap1:eq1.7.5}
\int\limits_a^b\Delta_1(t,-\bar{h}/k)f''(t)\,dt.
\end{equation}

In our case,
$$
f''(t)=Ax^{5/4}t^{-7/4}\cos(4\pi\sqrt{tx}/k-\pi/4)+o(t^{-9/4}).
$$
The contribution of the error term to \eqref{chap1:eq1.7.5} is $\ll
a^{-1/2}$. Hence, in place of \eqref{chap1:eq1.7.5}, it suffices to
deal with the integral 
\begin{equation}\label{chap1:eq1.7.6}
x^{5/4}\int\limits_a^bt^{-7/4}\Delta_1(t,-\bar{h}/k)\cos(4\pi\sqrt{tx}/k
-\pi/4)\,dt. 
\end{equation}

For $\Delta_1(t,\bar{h}/k)$ we have the formula \eqref{chap1:eq1.6.4},
which gives 
$$
\Delta_1(t,-\bar{h}/k)=At^{3/4}\sum\limits_{n=1}^\infty d(n)n^{-5/4}e_k
(nh)\cos(4\pi\sqrt{nt}/k+\pi/4)+o(t^{1/4}).
$$
The contribution of the error term to \eqref{chap1:eq1.7.6} is $\ll
a^{-1/2}$. Thus, the result of all the calculations so far is that 
\begin{multline*}
\sum(a,b;x)=Ax^{5/4}\sum\limits_{n=1}^\infty d(n)n^{-5/4}e_k(nh)\times\\
\times\int\limits_a^b t^{-1}\cos(4\pi\sqrt{tx}/k-\pi/4)\cos(4\pi\sqrt{nt}/k
+\pi/4)\,dt +o(a^{-1/4}\log a).
\end{multline*}

Further, when the product of the cosines is written as the sum of two
cosines, and the variable $u=\sqrt{t}$ is introduced, this equation
takes the shape
\begin{multline*}
 \sum(a,b;x)=Ax^{5/4}\sum\limits_{n=1}^\infty d(n)n^{-5/4}e_k(nh)\times\\
 \times\left\{\int\limits_{\sqrt{a}}^{\sqrt{b}}u^{-1}\cos(4\pi(\sqrt{n}+
\sqrt{x})u/k)du-\int\limits_{\sqrt{a}}^{\sqrt{b}}u^{-1}\sin(4\pi
(\sqrt{n}-\sqrt{x})u/k)du\right\}\\
 +o(a^{-1/4}\log a).
\end{multline*}

By\pageoriginale Lemma \ref{chap1:lem1.6}, the integrals here are $\ll
a^{-1/2}|\sqrt{n}\pm\sqrt{x}|^{-1}$. Hence, if the integral standing
on the right of \eqref{chap1:eq1.7.2} is singled out, the rest can be
estimated uniformly as $\ll a^{-1/2}$. This completes the proof.

The problem on the nature of convergence of the Voronoi series is now
reduced to the estimation of an elementary integral, and it is a
simple matter to deduce the following

\begin{THM}\label{chap1:thm1.5}
The series \eqref{chap1:eq1.7.1} is boundedly convergent in any
interval $[x_1,x_2]\subset(0,\infty)$, and uniformly convergent in any
such interval free from integers. The same assertions hold for the
series \eqref{chap1:eq1.6.5} for $a=0$. 
\end{THM}

\begin{proof}
The integral in \eqref{chap1:eq1.7.2} vanishes if $x=m$, and otherwise
it tends to zero as $a$ and $b$ tend to infinity. Thus, in any case,
the Voronoi series \eqref{chap1:eq1.7.1} converges. Moreover, if the
interval $[x_1,x_2]$ contains no integer, then the integral in
question is $\ll a^{-1/2}$ uniformly in this interval, where the
Voronoi series is therefore uniformly convergent.

Finally, to prove the boundedness of the convergence in $[x_1,x_2]$,
let $x$ and $m$ be as in Theorem \ref{chap1:thm1.4}, and put
$x=m+\delta, c=\min(\sqrt{b},\max(\sqrt{a}$, $1/|\delta|))$. Then 
\begin{gather*}
\int\limits_{\sqrt{a}}^{\sqrt{b}}u^{-1}\sin(4\pi(\sqrt{m}-\sqrt{x})u/k)
\,du=\int\limits_{\sqrt{a}}^c+\int\limits_c^{\sqrt{b}}\\
\ll \int\limits_{\sqrt{a}}^c|\sqrt{m}-\sqrt{x}|\,du+c^{-1}|\sqrt{m}-
\sqrt{x}|^{-1}\ll 1.
\end{gather*}

Hence $\sum(a,b;x)\ll 1$ uniformly for all $0<a<b$ and
$x\in[x_1,x_2]$. 
\end{proof}

\section{Identities for $D(x,r)$ and $A(x,r)$}\label{chap1:sec1.8} 

We are now in a position to prove Theorem \ref{chap1:thm1.3} for
$a=0$. For convenience of reference and because of the importance of
this result, we\pageoriginale state it separately as a theorem. 

\begin{THM}\label{chap1:thm1.6}
For $x>0$ we have 
\begin{gather}
D(x,h/k)=k^{-1}(\log x+2\gamma-1-2\log
k)x+E(0,h/k)\label{chap1:eq1.8.1}\\
-x^{1/2}\sum\limits_{n=1}^\infty d(n)n^{-1/2}\left\{e_k(-n\bar{h})Y_1
(4\pi\sqrt{nx}/k)+(2/\pi)e_k(n\bar{h})K_1(4\pi\sqrt{nx}/k)\right\}\notag\\
\intertext{and}
A(x,h/k)=(-1)^{k/2}x^{k/2}\sum\limits_{n=1}^\infty
a(n)n^{-k/2}e_k(-n\bar{h})J_k(4\pi\sqrt{nx}/k).\label{chap1:eq1.8.2}
\end{gather}
\end{THM}

\begin{proof}
Consider first the case when $x$ is not an integer. Let $[x_1,x_2]$ be
an interval containing $x$ but no integer. Then the series on the
right of \eqref{chap1:eq1.8.1} converges uniformly in this interval,
by Theorem \ref{chap1:thm1.5}. Therefore the differentiation term by
term of the identity \eqref{chap1:eq1.6.4} for $\Delta_1(x,h/k)$ is
justified, which gives the formula \eqref{chap1:eq1.6.4} for $a=0$,
and thus also the formula \eqref{chap1:eq1.8.1} (see the remark in the
end of \S ~ \ref{chap1:sec1.6}).

The case when $x=m$ is an integer will now be settled by Theorem
\ref{chap1:thm1.4} and the previous case. Let
\begin{multline*}
  S(x)= -x^{1/2}\sum\limits_{n=1}^\infty d(n)n^{-1/2}\\
  \left\{e_k(-n\bar{h})Y_1(4\pi\sqrt{nx}/k)+(2/\pi)e_k
  (n\bar{h})K_1(4\pi\sqrt{nx}/k)\right\}.
\end{multline*}

Then $S(x)=\Delta(x,h/k)$ if $x>0$ is not an integer, and $S(m)$ is
the value of $\Delta(m,h/k)$ asserted. We are going to show that 
\begin{align}
  & \frac{1}{2}\lim\limits_{\delta\to o +}(D(m+\delta,h/k)+D
  (m-\delta,h/k))\label{chap1:eq1.8.3}\\ 
  & =k^{-1}(\log m+2\gamma-1-2\log k)m+E(0,h/k)+S(m).\notag
\end{align}

Because $\frac{1}{2}(D(m+\delta,h/k)+D(m-\delta,h/k))$ equals
$D(m,h/k)$ for all $\delta\in(0,1)$, this implies
\eqref{chap1:eq1.8.1} for $x=m$. 

First,\pageoriginale the leading terms of the formula for
$D(m\pm\delta,h/k)$, just proved, give in the limit the leading terms
on the right of \eqref{chap1:eq1.8.3}. Therefore, it remains to prove
that 
\begin{equation}\label{chap1:eq1.8.4}
\lim\limits_{\delta\to 0+}(S(m+\delta)+S(m-\delta)-2S(m))=0.
\end{equation}

Where
$$
S(x)=S_1(x)+S_2(x)+S_3(x),
$$
where the range of summation in the sums $S_i(x)$ is, respectively,
$[1,\delta^{-1})$, $[\delta^{-1}, \delta^{-3}]$, and
$(\delta^{-3},\infty)$. We estimate separately the quantities 
$$
\Delta_i(\delta)=S_i(m+\delta)+S_i(m-\delta)-2S_i(m).
$$

Consider first $\Delta_1(\delta)$, writing
$$
\Delta_1(\delta)=\sum\limits_{n<\delta^{-1}}d(n)\alpha_n(\delta).
$$

By the formulae
\begin{align}
(x^{1/2}Y_1(4\pi\sqrt{nx}/k))' &= 2\pi(\sqrt{n}/k)Y_\circ(4\pi
\sqrt{nx}/k),\label{chap1:eq1.8.5}\\
(x^{1/2}K_1(4\pi\sqrt{nx}/k))' &= -2\pi(\sqrt{n}/k)K_\circ
(4\pi\sqrt{nx}/k),\label{chap1:eq1.8.6} 
\end{align}
which follow from \eqref{chap1:eq1.6.8} and \eqref{chap1:eq1.6.9}, we
find that $\alpha_n(\delta)\ll n^{-1/4}\delta$. Hence
\begin{equation}\label{chap1:eq1.8.7}
\Delta_1(\delta)\ll \delta^{1/4}\log(1/\delta).
\end{equation}

Next, by definition,
{\fontsize{10}{12}\selectfont
\begin{equation}\label{chap1:eq1.8.8}
\Delta_2(\delta)=-\sum\left(\delta^{-1},\delta^{-3};m+\delta\right)
-\sum\left(\delta^{-1},\delta^{-3};m-\delta\right)+2\sum
\left(\delta^{-1},\delta^{-3};m\right).
\end{equation}}

To facilitate comparisons between the sums on the right, we write the
factor $(m\pm\delta)^{5/4}$ in front of the formula
\eqref{chap1:eq1.7.2} for $\sum(\delta^{-1},\delta^{-3};m\pm\delta)$
as\pageoriginale $m^{5/4}+0(\delta)$. Then, by \eqref{chap1:eq1.8.8}
and \eqref{chap1:eq1.7.2},
\begin{multline*}
\Delta_2(\delta)\ll \Bigg|\, \int\limits_{\delta^{-1/2}}^{\delta^{-3/2}}
u^{-1}\Big\{\sin(4\pi(\sqrt{m}-\sqrt{m+\delta})u/k)\\
+\sin(4\pi (\sqrt{m}-\sqrt{m-\delta})u/k)\Big\}\,du\Bigg|
+\delta^{1/4}\log(1/\delta).
\end{multline*}

The expression in the curly brackets is estimated as follows:
\begin{align*}
|\{\cdots\}|&
=|2\sin \left(2\pi
\left(\sqrt{m+\delta}+\sqrt{m-\delta}-2\sqrt{m}\right)u/k\right) \\
& \hspace{4cm}\cos
\left(2\pi\left(\sqrt{m-\delta}-\sqrt{m-\delta}\right)u/k\right)|\\
& \ll \delta^2u.
\end{align*}

Hence 
\begin{equation}\label{chap1:eq1.8.9}
\Delta_2(\delta)\ll \delta^{1/4}\log (1/\delta).
\end{equation}

Finally, by Theorem \ref{chap1:thm1.4} and Lemma \ref{chap1:lem1.6},
we have for any $b>\delta^{-3}$
$$
\sum(\delta^{-3},b;m\pm\delta)\ll\delta^{3/2}\delta^{-1}+\delta^{3/4}
\log(1/\delta)\ll \delta^{1/2},
$$
and the same estimate holds also for $\sum(\delta^{-3},b;m)$. Hence
\begin{equation}\label{chap1:eq1.8.10}
\Delta_3(\delta)\ll \delta^{1/2},
\end{equation}

Now \eqref{chap1:eq1.8.7}, \eqref{chap1:eq1.8.9}, and
\eqref{chap1:eq1.8.10} give together
$$
S(m+\delta)+S(m-\delta)-2S(m)\ll \delta^{1/4}\log (1/\delta),
$$
and the assertion \eqref{chap1:eq1.8.4} follows. This completes the
proof of \eqref{chap1:eq1.8.1}, and \eqref{chap1:eq1.8.2} can be
proved likewise.
\end{proof}

\section{The Summation Formulae}\label{chap1:sec1.9} 

We are now in a position to deduce the main results of this chapter,
the summation formulae of the Voronoi type involving an exponential
factor. 

\begin{THM}\label{chap1:thm1.7}
Let\pageoriginale $0<a<b$ and $f\in C^1[a,b]$. Then 
{\fontsize{10}{12}\selectfont
\begin{gather}
\sideset{}{'}\sum\limits_{a\leq n\leq b}d(n)e_k(nh)f(n)=k^{-1}
\int\limits_a^b(\log x+2\gamma-2\log k)f(x)\,dx +k^{-1}\label{chap1:eq1.9.1}\\
\sum\limits_{n=1}^\infty d(n)\int\limits_a^b \{-2\pi
e_k(-n\bar{h})Y_\circ(4\pi\sqrt{nx}/k)+4e_k(n\bar{h})K_\circ
(4\pi\sqrt{nx}/k)\}f(x)\,dx\notag\\
\intertext{and}
\sideset{}{'}\sum\limits_{a\leq n\leq b}a(n)e_k(nh)
f(n)\label{chap1:eq1.9.2}\\
=2\pi k^{-1}(-1)^{k/2}\sum\limits_{n=1}^\infty
a(n)e_k(-n\bar{h})n^{(k-1)/2}\int\limits_a^b x^{(k-1)/2}
J_{k-1}(4\pi\sqrt{nx}/k)f(x)\,dx.\notag
\end{gather}}

The series in \eqref{chap1:eq1.9.1} and \eqref{chap1:eq1.9.2} are
boundedly convergent for $a$ and $b$ lying in any fixed interval
$[x_1,x_2]\subset (0,\infty)$.
\end{THM}

\begin{proof}
We may suppose that $0<a< 1$, for the general case then follows by
subtraction. Accordingly, the sum in \eqref{chap1:eq1.9.1} is 
$$
\sideset{}{'}\sum\limits_{n\leq b}d(n)e_k(nh)f(n)=\int\limits_a^b
f(x)dD(x,h/k).
$$

By an integration by parts, this becomes
\begin{equation}\label{chap1:eq1.9.3}
f(b)D(b,h/k)-\int\limits_a^bf'(x)D(x,h/k)\,dx.
\end{equation}

We substitute $D(x,h/k)$ from the identity \eqref{chap1:eq1.8.1},
noting that the resulting series can be integrated term by term
because of bounded convergence. Thus
\begin{multline*}
\int\limits_a^bf'(x)D(x,h/k)\,dx=\int\limits_a^bf'(x)
\Big\{k^{-1}(\log x+2\gamma-1\\
-2 \log k)x+E(0,h/k)\Big\}\,dx -\sum\limits_{n=1}^\infty
d(n)n^{-1/2}\int\limits_a^bf'(x)x^{1/2}\\ 
\left\{e_k(-h\bar{h})Y_1(4\pi\sqrt{nx}/k)+(2/\pi)e_k(n\bar{h})K_1
(4\pi\sqrt{nx}/k)\right\}\,dx. 
\end{multline*}

This is transformed by another integration by parts, using also\break
\eqref{chap1:eq1.8.5} and \eqref{chap1:eq1.8.6}. The integrated terms
then yield $f(b)D(b,h/k)$, again by
\eqref{chap1:eq1.8.1},\pageoriginale and the right hand side of the
preceding equation becomes
\begin{multline*}
f(b)D(b,h/k)-k^{-1}\int\limits_a^b(\log x+2\gamma-2 \log k)f(x)\,dx\\
+2\pi k^{-1}\sum\limits_{n=1}^\infty d(n)\int\limits_a^b
\left\{e_k(-n\bar{h})Y_\circ(4\pi\sqrt{nx}/k)-(2/\pi)e_k(n\bar{h})
K_\circ(4\pi\sqrt{nx}/k)\right\}\\
f(x)\,dx.
\end{multline*}

Substituting this into \eqref{chap1:eq1.9.3} we obtain the formula
\eqref{chap1:eq1.9.1}. It is also seen that the boundedness of the
convergence of the series \eqref{chap1:eq1.9.1}.
\end{proof}
The proof of \eqref{chap1:eq1.9.2} is analogously based on the
identity \eqref{chap1:eq1.8.2} and the formula
$$
(x^{k/2}J_k(4\pi\sqrt{nx}/k))'=2\pi(\sqrt{n}/k)x^{(k-1)/2}
J_{k-1}(4\pi\sqrt{nx}/k),
$$
which follows from \eqref{chap1:eq1.6.10}.

\bigskip

\section*{Notes} 

Our estimate \eqref{chap1:eq1.1.4} for $E(0,h/k)$ is stronger by a
logarithm than the bound $E(0,h/k)\ll k\log^22k$ of Estermann
\cite{key8}. 

The value $\zeta(0)=-1/2$ can also be deduced from
\eqref{chap1:eq1.1.9} by observing that for fixed $s\neq 1$ the
function $\zeta(s,a)$ is continuous in the interval $0<a\leq 1$ (this
follows \eg from the loop integral representation
(2.17.2) of $\zeta(s,a)$ in \cite{key27}).

The integrals $I_1, I_2$, and $I_3$ in \S~ \ref{chap1:sec1.4} can also
be evaluated by the inversion formula for the Mellin transformation,
using the Mellin transform pairs (see (7.9.11), (7.9.8), and (7.9.1)
in \cite{key26})
\begin{align*}
& x^{-\nu}K_\nu(x), \qquad 2^{s-\nu-2}\Gamma\left(\frac{1}{2}s\right)
\Gamma\left(\frac{1}{2}s-\nu\right),\\
& x^{-\nu}Y_\nu(x), \qquad -2^{s-\nu-1}\pi^{-1}\Gamma\left(\frac{1}{2}s
\right)\Gamma\left(\frac{1}{2}s-\nu\right)\cos\left(\left(\frac{1}{2}
s-\nu\right)\pi\right),\\
& x^{-\nu}J_\nu(x), \qquad 2^{s-\nu-1}\Gamma\left(\frac{1}{2}s\right)
\ \Gamma \left(\nu-\frac{1}{2}s+1\right).
\end{align*}\pageoriginale

Theorems \ref{chap1:thm1.1}, \ref{chap1:thm1.2}, \ref{chap1:thm1.6} 
and \ref{chap1:thm1.7} (for sums involving $d(n)$)
appeared in \cite{key18}. The error terms in Theorem
\ref{chap1:thm1.2} could be imporved. In fact, Tong \cite{key28}
proved that 
\begin{equation*}\label{chap1:eq*}
\int\limits_2^X\Delta^2(x)\,dx=C_1X^{3/2}+o(X\log^5X)\tag{*}
\end{equation*}
(for a simple proof, see Meurman \cite{key22}), and similarly it can
be shown that \eqref{chap1:eq1.5.22} and \eqref{chap1:eq1.5.23} hold with
error terms $o(k^2X\log^5X)$ and \break $o(k^2X^k\log^5X)$,
respectively. An analogue of \eqref{chap1:eq*} for the error term
$E(T)$ in \eqref{int:eq0.6} was obtained by Meurman in the above
mentioned paper. 

The general summation formulae of Berndt (see \cite{key3}, in
particular part V) cover \eqref{chap1:eq1.9.2} but not
\eqref{chap1:eq1.9.1}, because the functional equation
\eqref{chap1:eq1.1.2} for $E(s,r)$ is not of the form required in
Berndt's papers. 

The novelty of the proof of Theorem \ref{chap1:thm1.6} for integer
values of $x$ lies in the equation \eqref{chap1:eq1.8.3}. 

Analogues of the results in this and subsequent chapters can be proved
for sums and Dirichlet series involving Fourier coefficients of {\bf
Maass waves}. H. Maass \cite{key21} introduced non-holomorphic cusp
forms as auto-morphic functions in the upper half-plane $H$ for the
full modular group, which are eigenfunctions of the hyperbolic
Laplacian $-y^2(\partial_x^2+\partial_y^2)$ and square integrable over
the fundamental domain
$$
\left\{z=x+yi\left|-\frac{1}{2}\right.\leq x \leq\frac{1}{2},y>0,|z|
\geq 1\right\} 
$$\pageoriginale
with respect to the measure $y^{-2}\,dx\,dy$. Such functions, which
are moreover orthonormal with respect to the Petersson inner product,
eigen-functions of all Hecke operators $T_n$, and either even or odd
as functions of $x$, are called Maass waves. A Maass wave $f$, which
is associated with the eigenvalue $1/4+r^2(r\in\mathbb{R})$ of the
hyperbolic Laplacian and an even function of $x$, can be expanded to a
Fourier series of the form (see \cite{key20})
$$
f(z)=f(x+yi)=\sum\limits_{n=1}^\infty a(n)y^{1/2}K_{ir}(2\pi\,ny) \cos
(2\pi\,nx).
$$

It has been conjectured that $a(n)\ll n^\epsilon$, but this hypothesis-an
analogue of \eqref{chap1:eq1.2.5} - is still unsettled. The weaker
estimate $a(n)\ll n^{1/5+\epsilon}$ has been proved by J.-P. Serre.

As an analogue of the Dirichlet series $\varphi(s)$, one may define
the L-function
$$
L(s)=\sum\limits_{n=1}^\infty a(n)n^{-s}.
$$

This can be continued analytically to an entire function satisfying
the functional equation (see \cite{key7})
$$
\pi^{-s}L(s)\Gamma\left(\frac{s+ir}{2}\right)\Gamma\left(\frac{s-ir}{2}
\right)= \pi^{s-1}L(1-s)\Gamma\left(\frac{1-s+ir}{2}\right)\Gamma
\left(\frac{1-s-ir}{2}\right).
$$

More generally, it can be proved that the function
$$
L(s,h/k)=\sum\limits_{n=1}^\infty a(n)\cos(2\pi nh/k)n^{-s}
$$
has the functional equation
\begin{multline*}
(k/\pi)^sL(s,h/k)\Gamma\left(\frac{s+ir}{2}\right)\Gamma\left(
\frac{s-ir}{2}\right)\\
=(k/\pi)^{1-s}L(1-s,\bar{h}/k)\Gamma\left(
\frac{1-s+ir}{2}\right)\Gamma\left(\frac{1-s-ir}{2}\right),
\end{multline*}
which\pageoriginale is an analogue \eqref{chap1:eq1.2.6}. Results of
this kind can be proved for ``odd'' Maass waves as well, and having
the necessary functional equations at disposal, one may pursue the
analogy between holomorphic and non-holomorphic cusp forms further. 

