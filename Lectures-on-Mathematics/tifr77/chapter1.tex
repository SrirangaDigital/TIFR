
\chapter{Fourier Coefficients of Siegel Modular Forms}\label{c1}

\section*{Introduction}\label{c1-sec1}\pageoriginale

The problem of the representation of a natural number $t$ as the sum
of a given number $m$ of squares of integers is quite classical and
although its history goes back to Diophantus, it may be said to have
begun effectively with Fermat's theorem that every prime number
congruent to $1$ modulo $4$ is a sum of two squares of
integers. Practically, every mathematician of repute since Fermat has
made a contribution to problems of this type in the theory of
numbers. One has, thanks to Jacobi, a formula for the number
$r_{m}(t)$ of representations of $t$ as a sum of $m$ squares of
integers, with $m=2$, $4$, $6$ and $8$; for example,
\begin{align*}
r_{2}(t) &= 4\sum_{\substack{d|t\\ d\text{ odd}}} (-1)^{(d-1)/2},\\
r_{4}(t) &= 
\begin{cases}
 8\sum\limits_{d|t} d(t\text{ odd})\\
24 \sum\limits_{\substack{d|t\\ d\text{ odd}}} d(t \text{even}).
\end{cases}
\end{align*}
Analogously, one can ask for the determination of $(m,n)$ integral
matrices $G$ or of the {\em number} $r(A,B)$ of all {\em such} $G$,
for which
\begin{equation*}
(A[G]:=)^{t}GAG=B\tag{$\ast$}
\end{equation*}
where\pageoriginale $A$ and $B$ are given $(m,m)$ and $(n,n)$ integral positive
definite matrices. As a first step, one can seek suitable conditions
under which $(\ast)$ has a solution. A recent result in this direction
is given by

\begin{alphtheorem}[\cite{key8}]\label{c1:alphthmA}
If $m\geq 2n+3$ and if, for every prime number $p$, there exists a
matrix $G_{p}$ with entries in the ring $\mathbb{Z}_{p}$ of $p$-adic
integers with ${}^{t}G_{p}AG_{p}=B$, then we have an integral matrix
$G$ satisfying the equation ${}^{t}GAG=B$, provided that for the
minimum of $B$, viz.\@ $\min(B):=\inf\limits_{0\neq X\in
  \mathbb{Z}^{n}}{}^{t}XBX$, we have $\min(B)>\mathscr{X}(A)$ for a
suitable constant $\mathscr{X}(A)$.
\end{alphtheorem}

The proof of this theorem is arithmetical in nature and is given in
Chapter \ref{c2}. \S  \ref{c2:sec2.4}.

\begin{remarks}\label{c1:rems1}
If, on the other hand, $A$ is {\em indefinite} with $m\geq n+3$ and
if, for every prime $p$ including $\infty$, $(\ast)$ admits a solution
$G$ with entries in $\mathbb{Z}_{p}$, then it is known that $(\ast)$
has a solution $G$ with entries in $\mathbb{Z}$. The proof is given in
Chapter \ref{c2}, \S\ \ref{c2:sec2.4}.
\end{remarks}

\begin{enumerate}
\setcounter{enumi}{1}
\item In the case $n=1$ and $m\geq 5$, under the solvability of
  $(\ast)$ with $G$ over $\mathbb{Z}_{p}$ for every prime $p$
  (including $\infty$), Theorem \ref{c1:alphthmA}, in this case, is
  well-known 
  (\cite{key27}, \cite{key4}). For $n=1$ and $m=4$, however, if, in
  addition, to the solvability of $(\ast)$ in $G$ over
  $\mathbb{Z}_{p}$ for every prime $p$, one assumes further that for
  every prime $q$ dividing $2\det A$, the power of $q$ dividing $B$
  does not exceed a fixed integer $t$, then for all
  $B>\mathscr{X}=\mathscr{X}(t)$, the equation $(\ast)$ is solvable
  over $\mathbb{Z}$. The proof of a stronger form of this assertion
  viz.\@ $A$ is anisotropic over $q$ instead of ``$q$ dividing $2\det
  A$'', is purely arithmetic in nature and may be found in Kneser's
  Lectures \cite{key15}. An analytic proof using the\pageoriginale
  decomposition of theta series into Eisenstein series and a cusp form
  is also possible. If $m=3$ and $n=1$, assuming conditions as for
  $m=4$ above, $(\ast)$ is solvable over $\mathbb{Z}$ for all
  $B>\mathscr{X}=\mathscr{X}(t)$, provided that $B$ does not belong to
  a finite number of ``exceptional spinor classes'' and further that
  the Generalized Riemann Hypothesis holds; the proof is arithmetical
  in nature. The case $m=2$ and $n=1$ reduces to a problem of
  representation over quadratic fields.
\end{enumerate}

There is an analytic approach to Theorem \ref{c1:alphthmA}, based on the asymptotic
behaviour of $r(A,B)$ or, more precisely, on an asymptotic formula for
$r(A,B)$ as $B$ ``goes to infinity''. Clearly $r(A,B)>0$ if and only
if $(\ast)$ is solvable for $G$ over $\mathbb{Z}$. One first looks for
a generating function for $r(A,B)$. Let
$\mathscr{G}_{n}=\{Z\in\mathscr{M}_{n}(\mathbb{C})|Z={}^{t}Z, \;
i^{-1}(Z-\ob{Z}) > 0\}$, 
the Siegel upper half space of degree $n$ (or ``genus $n$''). For the
given $A>0$ and any $Z$ in $\mathscr{G}_{n}$, let 
$$
\vartheta(Z)=\vartheta(Z;A):=\sum_{B}e(\tr({}^{t}GAGZ))
$$
where $e(\alpha):=\exp(2\pi i\alpha)$, $\tr$ denotes the trace and $G$
runs over all $(m,n)$ integral matrices. Then it is clear that
$\vartheta(Z)=\sum\limits_{B\geq 0}r(A,B)e(\tr (BZ))$ where $B$ now
runs over all $(n,n)$ non-negative definite integral matrices. It
turns out that the theta series $\vartheta(Z)$ is a Siegel modular
form of degree $n$, weight $m/2$ and level $N$ (some $N$ depending on
$A$). Thus the problem now reduces to studying the asymptotic
behaviour of Fourier coefficients of Siegel modular forms which is in
the very centre of the analytic approach referred to. 


If\pageoriginale $A_{1}={}^{t}UAU$ for $U$ in $GL_{m}(\mathbb{Z})$,
then obviously $\vartheta(Z;A_{1})=\vartheta(Z;A)$ \ie $\vartheta(Z)$
is a class-invariant associated with $A$, depending only on the class
(of matrices $A_{1}$ ``equivalent'' to $A$ as above). The genus of $A$
consists of all positive-definite matrices $A^{\ast}$ such that for
every prime number $p$, $A^{\ast}={}^{t}U_{p}AU_{p}$ for $U_{p}$ in
$GL_{m}(\mathbb{Z}_{p})$; it is known from the reduction theory of
quadratic forms, that the genus of $A$ consists of finitely many
classes. Let $A_{1}$, $A_{2},\ldots,A_{h}$ be a complete set of
representatives of the classes in the genus of $A$ and let $o(A_{i})
$ be the order of the unit group of $A_{i}$, consisting of all $U$ in
$GL_{m}(\mathbb{Z})$ with ${}^{t}UA_{i}U=A_{i}$. Then we have the
genus - invariant
$E(Z):=\{\sum_{i}\vartheta(Z;A_{i})/o(A_{i})\}/\{\sum\limits_{i}1/o(A_{i})\}$
associated with $A$, having the Fourier expansion $\sum\limits_{B\geq
  0}a(B)e(\tr(BZ))$. From Siegel \cite{key23}, we know that, for $B>0$,
$$
a(B)=\pi^{n(2m-n+1)/4}\prod^{n-1}_{k=0}\{1/\Gamma\left(\frac{m-k}{2}\right)\}|\det
A|^{-n/2}|\det B|^{\frac{m-n-1}{2}}\prod_{p}\alpha_{p}(A,B)
$$
the product $\prod\limits_{p}$ being extended over all prime numbers
$p$ and $\alpha_{p}(A,B)$, the $p$-adic density of representation of
$B$ by $A$ is defined as
$$\lim\limits_{t\to\infty}p^{tn(n+1-2m)/2}\sharp
\{G\in\mathscr{M}_{m,n}(\mathbb{Z}/p^{t}\mathbb{Z})|{}^{t}GAG\equiv
B(\rm{mod} \; p^{t})\}.$$ 
We note that $a(B)\neq 0$ if and only if for every
prime $p$, ${}^{t}GAG=B$ is solvable for $G$ over
$\mathbb{Z}_{p}$. One then defines the modular form $g$ by
$g(Z)=\vartheta(Z)-E(Z)$ so that, denoting the Fourier coefficients of
$g$ by $b(B)$, we have
$$
r(A,B)=a(B)+b(B).
$$
One\pageoriginale expects this to be an asymptotic formula for
$r(A,B)$, with $a(B)$ as the ``main term'' and $b(B)$ as the ``error
term'', one needs to estimate $a(B)$ \ie essentially
$\prod\limits_{p}\alpha_{p}(A,B)$, from below, as indeed shown to be
possible by

\begin{alphtheorem}\label{c1:alphthmB}
If $m\geq 2n+3$ and if ${}^{t}GAG=B$ is solvable for $G$ over
$\mathbb{Z}_{p}$ for every prime $p$, then
$\prod\limits_{p}\alpha_{p}(A,B)>\mathscr{X}(A)>0$, for a constant
$\mathscr{X}(A)$. 
\end{alphtheorem}

\setcounter{remarks}{2}
\begin{remarks}\label{c1:rems3}
The condition $m\geq 2n+3$ in Theorem \ref{c1:alphthmB} is best
possible. (Likewise in Theorem 1 too, this condition seems best
possible; however, no counter examples are available to establish the same).
\end{remarks}

\begin{enumerate}
\setcounter{enumi}{3}
\item Let $m>n$ and $P:\{p \, | \,  p\nmid 2\det A\}$. Then if
  $B={}^{t}X_{p}AX_{p}$ for every prime $p$ with primitive $X_{p}$
  (\ie with $(X_{p}\ast)\in GL_{m}(\mathbb{Z}_{p})$), then
  $\prod\limits_{p\in
  P}\alpha_{p}(A,B)>\mathscr{X}(A)\prod\limits_{p\in
  P(B)}(1+\varepsilon_{p}p^{-1})$ for a constant
  $\mathscr{X}(A)>0$. Here 
  $P(B)$  is defined as the set of primes $p$ for which $m-2n+t_{p}=2$
  and $\varepsilon_{p}$ is the Legendre symbol
  $\left(\dfrac{(-1)^{m-n-1}dN_{0}\det A}{p}\right)$; if $B\equiv
  ((v_{i},v_{j}))(\rm{mod} \; p)$ for a basis $\{v_{i},\ldots,v_{n}\}$ of the
  associated quadratic space $N$ over $\mathbb{Z}/p\mathbb{Z}$ with
  the orthogonal decomposition $N=\Rad N\bot N_{0}$, $t_{p}=\dim
  N_{0}$ and $dN_{0}$ the discriminant of $N_{0}$. For almost all $p$,
  $B$ is unimodular and $t_{p}=n$.

If $m>n+2$, $P(B)$ is a finite set. If $m=2n+2$, $B={}^{t}X_{p}AX_{p}$
for a primitive $X_{p}$, whenever $p\in P$; in that case,
$\prod\limits_{p\in
  P(B)}(1+\varepsilon_{p}p^{-1})>\prod\limits_{p|e(B)}(1-p^{-1})\gg
e(B)^{-\varepsilon}$ for every $\varepsilon>0$, with $e(B)$ denoting the
first elementary divisor of $B$. For $m=2n+2$,
{\fontsize{9}{11}\selectfont
$$
p\in P(B)\Longleftrightarrow t_{p}=0\Longleftrightarrow N=\Rad
N\Longleftrightarrow B=0 (\rm{mod} \;  p)\Longleftrightarrow p|e(B).
$$}\relax

\item The\pageoriginale next step is naturally to get upper estimates
  for the Fourier coefficients $b(B)$ of $g(Z)$ which, by its very
  construction, has the property that for every modular substitution  
$Z\to M<Z>(:=(AZ+B)(CZ+D)^{-1})$, of degree $n$, the constant term in
  the Fourier expansion $g(M<Z>)\det
  (CZ+D)^{-m/2}=\sum\limits_{B}b(B,M)e(\tr (BZ)/N)$ vanishes. For
  $n=1$, this property characterises a cusp form; however, $g$ is not
  a cusp form for $n>1$, in general, preempting an appeal to the
  estimation of Fourier coefficients of cusp forms of degree $n$.
\end{enumerate}

Using Hecke's estimate $b(B)=O(B^{m/4})$ for the Fourier coefficients
of cusp forms (of degree $1$ and weight $m/2$), we have for $m\geq 5$
an asymptotic formula $r(A,B)=a(B)+O(B^{m/4})$, noting that $a(B)\gg
\break B^{(m/2-1)}$, whenever $a(B)\neq 0$. For $n=1$ and $m=4$, we can say
that $a(B)\gg B^{1-\varepsilon}\prod\limits_{p|2\det A}\alpha_{P}(A,B)\gg
B^{1-\varepsilon}$ for every $\varepsilon>0$, whenever $a(B)$ is non-zero,
provided that an additional restriction that the power of primes $p$
dividing $2\det A$ does not exceed $p^{t}$ for a fixed $t$; the
implied constants in $\gg$ depend on $t$. Using Kloosterman's method,
the ``error term'' $b(B)$ in this case has the estimate
$b(B)=O(B^{(m/4)-1/4+\varepsilon})$ for every $\varepsilon>0$ and thus we
have again a genuine asymptotic formula for $r(A,B)$. Very little is
known, in this respect, for $n=1$ and $m=3$.

Coming to the general case $n\geq 1$ and $m\geq 2n+3$, we shall prove
the following theorems, using Siegel's generalized circle method 

\begin{alphtheorem}[\cite{key10},\cite{key19}]\label{c1:alphthmC}
For\pageoriginale $n\geq 1$, $m\geq 2n+3$ and $B>0$ with $\det B\ll (\min
(B))^{n}b(B)=O(\min(B)^{(n+1-m/2)/2}(\det B)^{(m-n-1)/2})$. (For
$n=2$, the condition $\det B\ll (\min(B))^{2}$ is unnecessary).
\end{alphtheorem}

\begin{alphtheorem}[\cite{key10},\cite{key20}]\label{c1:alphthmD}
For $n\geq 1$ and even $m\geq 4n+4$, $b(B)=O(\min(B)^{1-m/4}(\det
B)^{(m-n-1)/2})$ 
\end{alphtheorem}

\setcounter{remarks}{5}
\begin{remarks}\label{c1:rems6}
Since $\prod\limits_{p}\alpha_{p}(A,B)\gg 1$, $1-m/4<0$ and
$1-m/2+n<0$, both Theorems \ref{c1:alphthmC} and \ref{c1:alphthmD} yield
asymptotic formulae for $r(A,B)$, as the `minimum' $\min(B)$ of $B$
goes to infinity. The condition $\det B\ll (\min B)^{n}$ for $n\geq 3$
in Theorem \ref{c1:alphthmC} is substantially the same as insisting that
$\min(B)^{-1}B$ lies in a compact set.
\end{remarks}

The case when $m\leq 2n+2$ and in particular $n=2$, $m=6$ is difficult
and a conditional result can be obtained in this special case, by
using a generalization of Kloosterman's method (involving the
estimation of exponential sums).

For $m=6$ and $n=2$, let $\mathfrak{g}=\{Z\in\mathscr{G}_{2}|\abs
\det(CZ+D)\geq 1$ for every modular matrix
$M=\left(\begin{smallmatrix} \ast & \ast\\ C & D
\end{smallmatrix}\right)$ of degree $2\}$. Let us {\em make the
  following}

\medskip
\noindent{\textbf{Assumption.}}
Let $c_{1}$, $c_{2}$ be natural numbers, $c_{1}|c_{2}$ and
$Z\in\mathscr{G}_{2}$. Then, for
\begin{gather*}
\sum\limits_{\substack{g_{1},g_{2}\rm{mod} \; c_{1}\\ g_{4}\rm{mod} \;
    c_{2}}}|\sum\limits_{\substack{u_{1},u_{2}\rm{mod} \; c_{1}\\ u_{4}\rm{mod} \;
    c_{2}}}e((u_{1}g_{1} + u_{2}g_{2})/c_{1} +
u_{4}g_{4}/c_{2})|=O(c^{2+a_{1}+\varepsilon}_{1}c_{2}^{1+a_{2}})\\ 
\begin{pmatrix}
u_{1}/c_{1} & u_{2}/c_{1}\\
u_{2}/c_{1} & u_{4}/c_{2}
\end{pmatrix}
+Z\in \mathfrak{g}
\end{gather*}
where $0\leq a_{1}\leq 3/2$, $0\leq a_{2}<1/2$ and the $O$-constant is
independent of $Z$. Then we can prove 

\begin{alphtheorem}\label{c1:alphthmE}
For\pageoriginale $m=6$, $n=2$ and Minkowski-reduced
$B=\left(\begin{smallmatrix}
\ast & \ast\\ \ast & b_{22}
\end{smallmatrix}\right)>0$, with $\min(B)\geq $ an absolute constant
$\mathscr{X}>0$, we have
$$
b(B)=O(((\min(B))^{(2a_{2}-1)/4+\varepsilon} +
(\min(B))^{-1}\log\frac{\sqrt{\det}B}{\min(B)})(\det 
B)^{3/2}) 
$$
under the assumption above, where $\omega(b_{22})$ is the number of
distinct prime divisors of $b_{22}$.
\end{alphtheorem}

\medskip
\noindent
{\bf Notation and Terminology.}
\smallskip

For any matrix $A$, the transpose is denoted by ${}^{t}A$. By
$\mathscr{M}_{r,s}(R)$ we mean the set of $(r,s)$ matrices with
entries in a commutative ring $R$ with identity. If $A\in
\mathscr{M}_{r,r}(R)=\mathscr{M}_{r}(R)$, then the determinant and the
trace of $A$ are denoted by $\det A$ and $\tr(A)$ respectively. For
given matrices $A$, $B$ we abbreviate ${}^{t}BAB$ (when defined) by
$A[B]$. Superscripts $r$, $s$ on a matrix $A^{(r,s)}$ indicate that it
has $r$ rows and $s$ columns; by $A^{(r)}$, we mean an $(r,r)$ matrix
$A$. By $GL_{n}(R)$ we mean the group of $(n,n)$ matrices with entries
in $R$ and $\det R$ invertible in $R$. For two matrices $A$, $B$ in
$\mathscr{M}_{r,s}(\mathbb{Z})$, we say $A\equiv B(\rm{mod} \; q)$ if all the
entries of $A-B$ are divisible by $q$. The $(n,n)$ identity matrix is
denoted by $E_{n}$ and $0$ represents a matrix, of the appropriate
size, with all entries equal to $0$. We write $A>B$ (respectively
$A\geq B$) to say that $A-B$ is a symmetric positive-definite
(respectively non-negative-definite) matrix; $A<B$ (respectively
$A\leq B$) if $B>A$ (respectively $B\geq A$). We use the $O$ and $o$
notation of Hardy-Littlewood as well as the notation $\ll$ or $\gg$
(due to Vinogradov). When $f\ll g$ as well as $f\gg g$, we simply
write $f\asymp g$; a similar notation applies to
matrices. By\pageoriginale $GL_{m}(\mathbb{Z};q)$, we mean the
congruence subgroup $\{U\in GL_{m}(\mathbb{Z})|U\equiv E_{m}(\rm{mod} \;
q)\}$ of level $q$. A matrix $F^{(n,r)}$ in
$\mathscr{M}_{n,r}(\mathbb{Z})$ with $r\leq n$ is called primitive, if
there exists $U=(F\ast)$ in $GL_{n}(\mathbb{Z})$. By
$[a_{1},\ldots,a_{n}]$ we mean a diagonal matrix with
$a_{1},\ldots,a_{n}$ as diagonal elements. By an integral matrix, we
mean a matrix with entries from $\mathbb{Z}$. For a complex matrix
$W=(w_{ij})$, the matrix $(\ob{w}_{ij})$ with the complex conjugates
$\ob{w}_{ij}$ as corresponding entries is denoted by $\ob{W}$.

Let $\Lambda_{n}=\{S={}^{t}S\in\mathscr{M}_{n}(\mathbb{Z})\}$ and
$\Lambda^{\ast}_{n}$, the dual of $\Lambda_{n}$, viz.\@
$\{S={}^{t}S=(s_{ij})\in
\mathscr{M}_{n}(\mathbb{Q})|s_{ii},2s_{ij}\in\mathbb{Z}\}=\{S={}^{t}S|\tr(ST)\in\mathbb{Z}$
for every $T$ in $\Lambda_{n}\}$. 


\section[Estimates for Fourier Coefficients...]{Estimates for Fourier Coefficients of Cusp\hfil\break Forms of Degree
1}\label{c1:sec-1.1}\pageoriginale

We first give an elaborate description of the case of modular forms of
one variable, which is quite typical, in a sense but not entirely so,
since the higher dimensional cases are fairly difficult. We have
already remarked that the modular form $g$ introduced earlier is a
cusp form for $n=1$ but not so, in general, for $n>1$.

Let $H=\{z\in\mathbb{C}|\Iim z>0\}$ and $k,N$ be natural numbers. The
principal congruence subgroup
$\Gamma(N)=\{\sigma=\left(\begin{smallmatrix} a & b\\ c & d
\end{smallmatrix}\right)\in SL_{2}(\mathbb{Z})|\sigma \equiv
\left(\begin{smallmatrix} 1 & 0\\ 0 & 1
\end{smallmatrix}\right)(\rm{mod} \; N)\}$ of the modular group
$\Gamma=\Gamma(1)$ acts on $H$ via the conformal mappings of $H$ given
by $Z\mapsto \sigma(z)=(az+b)(cz+d)^{-1}$ for
$\sigma=\left(\begin{smallmatrix} a & b\\ c & d
\end{smallmatrix}\right)$ in $\Gamma(N)$. We recall that
$e(\alpha)=\exp (2\pi i\alpha)$ for $\alpha\in \mathbb{C}$.

\begin{defi*}
A holomorphic function $f:H\to \mathbb{C}$ is called a cusp form
(respectively a modular form) of weight $k$ and level $N$ if, for
every 
$$
\sigma=
\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}\in\Gamma(N), f((az+b)(cz+d)^{-1})(cz+d)^{-k}=f(z)
$$
and further for every
$$
\sigma=
\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}
\in\Gamma, f(\sigma(z))(cz+d)^{-k}=\sum_{m>0}a_{m}e(mz/N)
$$
(respectively $=\sum\limits_{m\geq 0}a_{m}e(mz/N)$).
\end{defi*}

For $\sigma=\left(\begin{smallmatrix} a & b\\ c & d
\end{smallmatrix}\right)\in\Gamma$ and $f:H\to \mathbb{C}$, we
abbreviate $f(\sigma(z))(cz+d)^{-k}$ by $f|\sigma$. It is easy to
verify that, for $\sigma_{1}$, $\sigma_{2}$ in $\Gamma$, we have
$f|\sigma_{1}\sigma_{2}=(f|\sigma_{1})|\sigma_{2}$.

The following two theorems give estimates for the Fourier coefficients
of cusp forms.

\begin{subtheorem}[Hecke \cite{key7}]\label{c1:thm-1.1.1}
For\pageoriginale the Fourier coefficients $a_{m}$ of a cusp form
$f(z)=\sum\limits_{m>0}a_{m}e(mz)$ of weight $k(\geq 2)$ and level
$N$, we have $a_{m}=O(m^{k/2})$.
\end{subtheorem}

\begin{subtheorem}\label{c1:thm-1.1.2}
For $f$ as in Theorem \ref{c1:thm-1.1.1}, $a_{m}=O(m^{k/2-1/4+\varepsilon})$
for every $\varepsilon>0$.
\end{subtheorem}

We fix some notation and prove a few lemmas, before going on to the
proofs of these two theorems.

We know that $\mathfrak{F}=\{z=x+iy\in H| \; |x|\leq 1/2$, $|z|\geq 1\}$
is a fundamental domain for the modular group $\Gamma$ in $H$.

Let
{\fontsize{9}{11}\selectfont
$$
\sigma<\mathfrak{F}>=\{\sigma(z)|z\in\mathfrak{F}\},
\Gamma_{\infty}\left\{
\begin{pmatrix}
\pm 1 & n\\
0 & \pm 1
\end{pmatrix}
\in\Gamma|n\in \mathbb{Z}\right\}\text{ \  and \ }
\mathfrak{g}=\bigcup_{\sigma\in\Gamma_{\infty}}\sigma<\mathfrak{F}>. 
$$}
For any fixed $m$ as in the assertion of Theorem \ref{c1:thm-1.1.1} or
\ref{c1:thm-1.1.2} and for $\sigma=\left(\begin{smallmatrix} a & b\\ c &
  d
\end{smallmatrix}\right)$ in $\Gamma$, let $\beta(\sigma):=\{x\in
    [0,N]|\sigma(x+\text{im}^{-1})\in \mathfrak{g}\}$. We also write
    $\beta(c,d)$ for $\beta(\sigma)$, as may be seen to be
    appropriate.

Since $H=\bigcup\limits_{\sigma\in\Gamma_{\infty}\backslash
  \Gamma}\sigma<\mathfrak{g}>$, we have
$I_{N,m}=\{x+\text{im}^{-1}|0\leq x\leq
N\}=\bigcup\limits_{\sigma\in\Gamma_{\infty}\backslash
  \Gamma}\beta(\sigma)$, this being indeed a finite union, in view of
the compactness of $I_{N,m}$. Further $I_{N,m}\cap \mathfrak{g}=\emptyset$
for $m>2/\sqrt{3}$, since for $z=x+iy\in\mathfrak{g}$, $y\geq
\sqrt{3}/2$. Thus $I_{N,m}=\bigcup\limits_{\substack{\sigma\in
    \Gamma_{\infty}\backslash \Gamma\\ \sigma\not\in
    \Gamma}}\beta(\sigma)$. Further, measure $(\beta(\sigma_{1})\cap
\beta(\sigma_{2}))=0$, for $\sigma_{1}\not\in
\Gamma_{\infty}\sigma_{2}$.

Now 
\begin{align*}
Na_{m}
&=\int\limits^{N}_{0}f(x+\text{im}^{-1})e(-m(x+\text{im}^{-1}))dx\\
&= e^{2\pi}\sum\limits_{\substack{\sigma\in
    \Gamma_{\infty}\backslash\Gamma\\ \sigma\not\in\Gamma_{\infty}}}\int\limits_{\beta(\sigma)}f(z)e(-mx)dx\\
\text{\ie}\qquad a_{m} &=
\frac{e^{2}\pi}{N}\sum_{\substack{(c,d)=1\\ c\geq 1}}
\alpha(c,d),\tag{1}\label{c1:eq1} 
\end{align*}
writing\pageoriginale $\alpha(c,d)$ for the integral over
$\beta(\sigma)=\beta(c,d)$. If $\beta(\sigma)=\emptyset$, then the
corresponding $\alpha(c,d)$ is $0$. On the other hand, if
$\beta(\sigma)\neq \emptyset$, there exists $x$ in $\mathbb{R}$ with
$\sigma(x+\text{im}^{-1})\in\mathfrak{g}$ implying that 
$$
\Iim (\sigma(x+\text{im}^{-1}))=m^{-1}/((cx+d)^{2}+c^{2}/m^{2})\geq
\sqrt{3}/2.
$$
Hence, in this case $m/c^{2}=m^{1}/(c^{2}m^{-2})\geq
m^{-1}/((cx+d)^{2}+c^{2}/m^{2})\geq \sqrt{3}/2$ \ie $\beta(\sigma\neq
\emptyset$ implies that $c=O(\sqrt{m})$. Thus in the sum over $(c,d)$ in
(\ref{c1:eq1}) with $(c,d)=1$, we may restrict $c$ to satisfy the condition
$1\leq c\ll \sqrt{m}$.

\setcounter{sublemma}{2}
\begin{sublemma}\label{c1:lem-1.1.3}
If $f$ is a cusp form of weight $k$ and level $N$ and if
$(f|\sigma)\break (z)=\sum\limits_{n\geq 1}a'_{n}e(nz/N)$ for
$\sigma\in\Gamma$, then
$$
\sum_{n>0}|a'_{n}||e(nz/N)|= O(\exp(-\mathscr{X}_{1}\Iim z))\text{
  \ for \ } \Iim z\geq \mathscr{X}>0
$$
where $\mathscr{X}_{1}=\pi/N$ and the $O$-constant depends only on
$\mathscr{X}$ and on $f$ in general.
\end{sublemma}

\begin{proof}
Since $[\Gamma(1):\Gamma(N)]<\infty$, the set
$\{f|\sigma,\sigma\in\Gamma\}$ is finite, even for any modular form
which is not necessarily a cusp form. Since
$(f|\sigma)(i\mathscr{X}/2)=\Sigma a'_{n}\exp(-\pi n\mathscr{X}/N)$ is
convergent, we obtain $|a'_{n}|\break\exp(-\pi n\mathscr{X}/N)=O(1)$. Hence,
for any $\sigma$ in $\Gamma(1)$ and $\Iim z\geq \mathscr{X}$, we have
\begin{align*}
\sum_{n\geq 1}|a'_{n}||e(nz/N)| &= \sum_{n\geq 1}|a'_{n}|\exp(-\pi
n\Iim z/N)\exp(-\pi n\Iim z/N)\\
&< \sum_{n}|a'_{n}|\exp(-\pi n\mathscr{X}/N)\exp(-\pi n\Iim z/N)\\
&\ll \sum_{n}\exp(-\pi n\Iim z/N)\\
&\quad=\exp(-\pi \Iim z/N)/(1-\exp(-\pi
\Iim z/N))\\
&\ll \exp(-\pi z/N)/(1-\exp(-\pi\mathscr{X}/N)). 
\end{align*}
The finiteness of $\{f|\sigma;\sigma\in\Gamma\}$ now completes the proof.
\end{proof}

\begin{sublemma}\label{c1:lem-1.1.4}
If\pageoriginale $b>a>0$ and $r<-1/2$, then
$$
J(b,r):=\int\limits^{\infty}_{-\infty}(x^{2}+1)^{r}\exp(-b/(x^{2}+1))dx=O_{a,r}(b^{r+1/2})
$$
\end{sublemma}

\begin{proof}
Splitting up the integral as the sum of integrals over
$A=\{x\in\mathbb{R}|x^{2}+1>2b/a\}$ and $B=\{x\in
\mathbb{R}|x^{2}+1\leq 2b/a\}$, we have
\begin{gather*}
J(b,r)=\int\limits_{A}\ldots dx+\int\limits_{B}\ldots
dx=J_{1}+J_{2},\text{ \ say. Now}\\
J_{1}\leq \int\limits_{A}(x^{2}+1)^{r}dx<\int\limits_{A}x^{2r}dx\text{
  (since } r<0)
\end{gather*}
\ie
\begin{align*}
J_{1} &< \int\limits_{x^{2}>b/a}x^{2r}dx\text{ \ (since} b>a)\\
&= \frac{2}{2r+1}x^{2r+1}|^{\infty}_{\sqrt{b/a}}\\
&= O(b^{r+1/2})
\end{align*}
with the constants in $O$ involving $a$ and $r$.
\end{proof}

For $x$ in $B$, we use the estimate $\exp(-y)\ll y^{r}$ and obtain 
$$
J_{2}\leq
\int\limits_{B}(x^{2}+1)^{r}(b/(x^{2}+1))^{r}=2b^{r}\sqrt{2(b/a)-1}=O(b^{r+1/2}) 
$$
which proves the lemma.

For the proof of Theorem~\ref{c1:thm-1.1.1}, we use the well-known 
circle met\-hod. 

\begin{proofoftheorem}\label{c1:proofofthm-1.1.1}
For\pageoriginale given $(c,d)=1$ with $1\leq c\ll\sqrt{m}$,
{\fontsize{10}{12}\selectfont
$$
\sum_{d_{1}\in\mathbb{Z}}|\alpha(c,d+cd_{1})|\ll
\sum_{d_{1}\in\mathbb{Z}}\int\limits_{\beta(c,d+cd_{1})}|cz+d+cd_{1}|^{-k}\exp(-\mathscr{X}_{1}/(m|cz+d+cd_{1}|^{2}))dx  
$$}\relax
using Lemma \ref{c1:lem-1.1.3} with $\mathscr{X}=\sqrt{3}/2$ for
$\sigma=\left(\begin{smallmatrix} \ast &\ast\\ c & d+cd_{1}
\end{smallmatrix}\right)$, $x+\text{im}^{-1}c\beta(\sigma)$ and 
$$
f(x+\text{im}^{-1})=(c(x+\text{im}^{-1})+d+cd_{1})^{-k}\sum_{n}a'_{n}e(n\sigma(x+\text{im}^{-1})/N).
$$
Thus
\begin{align*}
\sum_{d_{1}\in\mathbb{Z}}|\alpha(c,d+cd_{1})| &\leq
\sum_{d_{1}\in\mathbb{Z}}\int\limits^{d_{1}+N}_{d_{1}}((cx+d)^{2}+c^{2}/m^{2})^{-k/2}\\
&\qquad\exp\left(-\frac{\mathscr{X}_{1}}{m((cx+d)^{2}+c^{2}/m^{2})}\right)\,dx\\
&\leq N\int\limits^{\infty}_{-\infty}(c^{2}x^{2}+c^{2}/m^{2})^{-k/2}
\exp\left(-\frac{\mathscr{X}_{1}/m}{c^{2}x^{2}+c^{2}/m^{2}}\right)\,dx\\
&= Nc^{-k}m^{k-1}\int\limits^{\infty}_{-\infty}(x^{2}+1)^{-k/2}\exp
\left(-\frac{\mathscr{X}_{1}m/c^{2}}{x^{2}+1}\right)\,dx\\
&\ll c^{-k}m^{k-1}(m/c^{2})^{-k/2}+1/2\\
&\qquad\text{(by Lemma 1.4, for  $k\geq 2$)}
\end{align*}
\ie 
$$
\sum\limits_{d_{1}\in\mathbb{Z}}(\alpha(c,d+cd_{1})|\leq
c^{-1}m^{k/2-1/2}.
$$
This leads us, for fixed $c$, to 
$$
|\sum_{\substack{(d,c)=1\\ c\text{ fixed}}}\alpha(c,d)|\ll
(\varphi(c)/c)m^{k/2-1/2}\ll m^{k/2-1/2}
$$
where $\varphi$ is Euler's function. The theorem now follows, since
$$
a_{m}\ll \sum_{1\leq c\ll \sqrt{m}}m^{k/2-1/2}\ll m^{k/2}.
$$
\end{proofoftheorem}

For the proof of Theorem \ref{c1:thm-1.1.2}, we use a variation of the
usual method of Kloosterman \cite{key14}, by rendering it suitable for a
generalization to the\pageoriginale case of modular forms of degree
$2$. First we need to fix some notation. Let $m$ and $f$ be as given
in Theorem 1.2. For $z=x+\text{im}^{-1}$ in
$\beta(\sigma)=\beta(c,d)$ with $\sigma=\left(\begin{smallmatrix} a &
  b\\ c & d\end{smallmatrix}\right)$ in $\Gamma$ and $c\geq 1$, we
  have
$$
\sigma(z)=\frac{az+b}{cz+d}=\frac{a}{c}-\frac{1}{c^{2}(z+d/c)}=\frac{a}{c}-\frac{1}{c^{2}(x+\text{im}^{-1}+d/c)}=\frac{a}{c}+\tau,
\text{ \ say}
$$
$\tau=\tau(\theta,c)=-1/\{c^{2}(\theta+i/m)\}$ with
$\theta:=x+d/c$. Now if
$(f|\sigma^{-1})(w)=\sum\limits_{n}a'_{n}e(nw/N)$ for $w\in H$, then
by the definition of $\alpha(\sigma)$, we have
{\fontsize{9}{11}\selectfont
\begin{align*}
\alpha(\sigma)=\alpha(c,d) &=
\int\limits_{\beta(\sigma)}(cz+d)^{-k}(f|\sigma^{-1})(\sigma(z))e(-mx)dx\\
&= c^{-k}\int\limits_{\substack{d/c\leq \theta \leq
    N+d/c\\ a/c+\tau\in\mathfrak{g}}}(\theta+i/m)^{-k}\sum_{n}a'_{n}e(n(a/c+\tau)/N)e(-m(\theta-d/c))d\theta\\
&= c^{-k}\int\limits_{\substack{d/c\leq \theta\leq N+d/c\\ a/c+\tau\in
  \mathfrak{g}}}(\theta+i/m)^{-k}\sum_{n\geq 1}a'_{n}e(n\tau/N)e(-m\theta)e
\left(\frac{na+mNd}{cN}\right)d\theta.\tag{2}\label{c1:eq2}
\end{align*}}\relax
For the proof of Theorem~\ref{c1:thm-1.1.2}, we need to estimate
$\sum\limits_{c,d}\alpha(c,d)$ afresh. But, as one may notice, in the
expression \eqref{c1:eq2} for $\alpha(\sigma)$, we have also the element
$a$ of $\sigma$ featuring along with $c$ and $d$. This calls for the
following variation of the usual Kloosterman sums and estimates for
the same.

For $a$, $c$ in $\mathbb{Z}$ with $c\geq 1$ and for $z\in H$, let
$$
g(a,c,z)=
\begin{cases}
1 & \text{if \ } a/c+z\in\mathfrak{g}\\
0 & \text{otherwise.} 
\end{cases}
$$
Then\pageoriginale $g(a+nc,c,z)=g(a,c,z)$ for every
$n\in\mathbb{Z}$. Thus we have a finite Fourier expansion
\begin{equation*}
g(a,c,z)={}_{t \, \rm{mod} \,  c} \; b_{t}(c,z) \;
e(ta/c).\tag{3}\label{c1:eq3} 
\end{equation*}

\setcounter{sublemma}{4}
\begin{sublemma}\label{c1:lem-1.1.5}
$\sum\limits_{t\rm{mod} \; c}|b_{t}(c,z)|=O(c^{\varepsilon})$ for every
  $\varepsilon>0$, with an $O$-constant independent of $z$.
\end{sublemma}

\begin{proof}
Clearly $b_{t}(c,z)=c^{-1}\sum\limits_{\ell\rm{mod} \;
  c}g(\ell,c,z)e(-t\ell/c)$. The boundary of $\mathfrak{g}$ in $H$
consists of the union of the translates $w\mapsto w+n$ of the arc
$\{(x,y)|x^{2}+y^{2}=1$, $-1/2\leq x\leq 1/2\}$. Hence for any $z$ in
$H$, the intersection of the line $\{u+z|0\leq u\leq 1\}$ with
$\mathfrak{g}$ has at most {\em two} connected components say
$[a_{1},b_{1}]$, $[a_{2},b_{2}]$. Using the definition of
$g(\ell,c,z)$ in the expansion for $b_{t}(c,z)$ above, we have the
estimate
$$
|b_{t}(c,z)|\leq c^{-1}|\sum\limits_{\frac{\ell}{c}\epsilon
  [a_{1},b_{1}]}e(-t\ell/c)|+c^{-1}|\sum_{\frac{\ell}{c}\epsilon[a_{2},b_{2}]}e(-t\ell/c)|
$$
But $\ell/c\in [a_{j},b_{j}]$ means that $0\leq u_{j}\leq \ell \leq
v_{j}\leq c$ for suitable integers $u_{1}$, $v_{1}$, $u_{2}$, $v_{2}$
with $0\leq u_{1}\leq v_{1}\leq u_{2}\leq v_{2}\leq c$. Now
\begin{gather*}
|\sum_{\ell/c\in[a_{j},b_{j}]} e(-t \; \ell/c)| =
|\sum^{v_{j}}_{\ell=u_{j}}e(-t\ell/c)| =
||\sum^{v_{j}-u_{j}}_{\ell=0}e(-t\ell/c)|\\  
=|(e(-t(v_{j}-u_{j}+1)/c)-1)/(e(-t/c)-1)|\\
\leq 1/(\sin \pi t/c)\text{
  \ for \ } 1\leq t<c\text{ \  and \ } j=1,2.
\end{gather*}
Hence,\pageoriginale for $1\leq t<c$, we have $|b_{t}(c,z)|\leq
2/(c\sin (\pi t/c))$.
\end{proof}

Clearly $|b_{c}(c,z)|\leq 1$. Combining these estimates
\begin{align*}
\sum_{t\rm{mod} \; c}|b_{t}(c,z)| &\leq 1+\frac{2}{c}\sum^{c-1}_{t=1}1/\sin
\frac{t\pi}{c}\\
&\leq 1+\frac{4}{c}\sum_{1\leq t\leq c/2}\cosec (t\pi /c)\\
&\leq 1+\frac{4}{c}\sum_{1\leq t\leq c/2}2c/(t\pi)\\
&= 1+\frac{8}{\pi}\sum^{[c/2]}_{t=1}1/t\\
&\leq 1+\frac{8}{\pi}\log c\\
&= O(c^{\varepsilon}), \text{ \  proving the lemma.}
\end{align*}

Our object now is to estimate first
$\sum\limits_{\substack{(c,d)=1\\ d\equiv d_{0}(\rm{mod} \; N)}}\alpha(c,d)$
where $c\geq 1$ is fixed and the summation is over all $(c,d)=1$ with
$d$ lying in a given residue class modulo $N$, say $d\equiv d_{0}(\rm{mod} \;
N)$. Let $cr$ = least common multiple of $c$ and $N$; so that $r\geq
1$ and $r|N$. We fix, once for all,
$\sigma_{0}=\left(\begin{smallmatrix} \alpha & \beta\\ c &\delta
\end{smallmatrix}\right)$ in $\Gamma$ with some $\delta\equiv
d_{0}(\rm{mod} \; N)$. If $X:= $ $\{x\text{ in } \mathbb{Z} \text{ modulo }
cr|(x,c)=1 \text{ and } x\equiv d_{0}(\rm{mod} \; N)\}$ then
$\{d\in\mathbb{Z}|(c,d)=1, d\equiv d_{0}(\rm{mod} \;
N)\}=\bigcup\limits_{x\in X}\{d\in\mathbb{Z}|d\equiv x(\rm{mod} \; cr)\}$, as
can be readily verified. Hence, for the fixed $c\geq 1$, $d_{0}$
modulo $N$ and $\sigma_{0}$,
\begin{align*}
\sum_{\substack{(c,d)=1\\ d\equiv d_{0}(\rm{mod} \;
    N)}}\alpha(c,d)&  =\sum_{x\in X} \; \sum_{d\equiv x(\rm{mod} \;
  cr)}\alpha(c,d)\\
& =\sum_{x\in X} \; \sum_{y\rm{mod}\; N/r} \; \sum_{d\equiv x+cry
  (\rm{mod}\, c N)}\alpha(c,d)\tag{4}\label{c1:eq4}
\end{align*}

\begin{sublemma}\label{c1:lem-1.1.6}
For\pageoriginale $x$ in $X$ and $y$, $t$ in $\mathbb{Z}$, there
exists $\sigma_{x}=\left(\begin{smallmatrix} a_{x} & b_{x}\\ c & x
\end{smallmatrix}\right)\equiv \sigma_{0}(\rm{mod} \; N)$ in
$\Gamma$. Moreover, we have an element
$\sigma=\left(\begin{smallmatrix}
a_{x}-a^{2}_{x}\cry & \ast\\ c & x+\cry+cNt
\end{smallmatrix}\right)$ in $\Gamma$, congruent to $\sigma_{0}$
modulo $N$.
\end{sublemma}

\begin{proof}
Since $x\in X$, we have $(c,x)=1$ and $x\equiv d_{0}\equiv \delta(\rm{mod} \;
N)$. Hence there exists $\sigma_{1}=\left(\begin{smallmatrix} a &
  b\\ c & x\end{smallmatrix}\right)$ in $\Gamma$. Now
  $\sigma_{1}\sigma^{-1}_{0}\equiv \left(\begin{smallmatrix} a' & h\\ 0
    & 1  \end{smallmatrix}\right)(\rm{mod} \; N)$ so that $a'\equiv 1(\rm{mod} \; N)$
  necessarily. Thus $\left(\begin{smallmatrix} 1 & -h\\ 0 & 1
  \end{smallmatrix}\right)\sigma_{1}\equiv \sigma_{0}(\rm{mod} \; N)$, so
  that we can take $a_{x}=a-ch$, $b_{x}=b-hx$. Next, $\Gamma$ clearly
  contains
$$
\begin{pmatrix}
1 & -a^{2}_{x}ry\\ 0 & 1
\end{pmatrix}
\begin{pmatrix}
a_{x} & b_{x}\\
c & x
\end{pmatrix}
\begin{pmatrix}
1 & ry+tN\\
0 & 1
\end{pmatrix}
=
\begin{pmatrix}
a_{x}-a^{2}_{x}\text{ryc} & b'\\
c & x+\cry+ctN
\end{pmatrix}
=\sigma
$$
and further $\sigma=\sigma_{0}(\rm{mod} \; N)$, since $N|cr$ and
$$
b'=(a_{x}-a^{2}_{x}\text{ryc})(ry+tN)+b_{x}-xa^{2}_{x}ry\equiv
a_{x}ry+b_{x}-a^{2}_{x}ry\ x(\rm{mod} \; N)
$$
\ie
$$
b'\equiv a_{x}ry (1-a_{x}x)+b_{x}\equiv b_{x}\rm{mod} \; N,
$$
on noting that $c|(1-a_{x}x)$ and $N|cr$.
\end{proof}

For the given cusp form $f$ (and indeed for any modular form) of level
$N$, we have $f|\sigma=f|\sigma_{0}$ since $\sigma\equiv
\sigma_{0}(\rm{mod} \; N)$. Let
$$
(f|\sigma^{-1})(w)=(f|\sigma^{-1}_{0})(w)=\sum_{n}a'_{n}e(nw/N)\text{
  \ for \ } w\in H.
$$
Then for any $x\in X$, we\pageoriginale have, with the notation as in
\eqref{c1:eq2} and \eqref{c1:eq4},
\begin{align*}
& \sum_{y{\rm{mod}} \; N/r}  \;\; \sum_{d\equiv x+\cry(\rm{mod}\;
    cN)}\alpha(c,d)\\ 
&= \sum_{\substack{y\rm{mod}\;
      N/r\\ t\in\mathbb{Z}}}\int\limits_{\substack{c^{-1}x+ry+Nt\leq
      \theta\leq c^{-1}x+ry+N(t+1)\\ c^{-1}(a_{x}-a^{2}_{x}\cry)+\tau
      \in\mathfrak{g}}}\sum_{n}a'_{n}e(n\tau/N)\\
&\hspace{2cm} e(-m\theta)e\left(\frac{n(a_{x}-a^{2}_{x}\cry)+mN(x+\cry+cNt)}{cN}\right)d\theta\\
&= c^{-k}\int\limits_{c^{-1}a_{x}+\tau
    \in\mathfrak{g}} (\theta+i/m)^{-k} \sum_{n} a'_{n} e(n\tau/N)
  e(-m\theta)\\
&\hspace{2cm}\sum_{y\rm{mod} \; 
    Nr^{-1}} e(y(-na^{2}_{x}r/N) e\left(\frac{na_{x}+mNx}{cN}\right)
  d\theta. \tag{5}\label{c1:eq5}  
\end{align*}

We claim now that $(a_{x},N/r)=1$. for, $\text{cr}$ is the least
Common multiple of $c$ and $N$ and if a prime $p$ divides $N/r$, then
$p$ necessarily divides $c$, and so $p$ cannot divide $a_{x}$, since
$\sigma_{x}\in\Gamma$. Hence
$$
\sum_{y\rm{mod} \; N/r}e(y(-na^{2}_{x}r/N))=
\begin{cases}
N/r & \text{if } N/r \text{ \ divides \ }n\\
0 & \text{otherwise.}
\end{cases}
$$
The expression in \eqref{c1:eq5} now reduces to
\begin{align*}
& \frac{N}{rc^{k}}\int\limits_{c^{-1}a_{x}+\tau
    \varepsilon \mathfrak{g}}(\theta+i/m)^{-k}\sum_{\substack{(N/r)|n\\ n>0}}a'_{n}e(n\tau/N)e(-m\theta)e\left(\frac{na_{x}+mNx}{cN}\right)d\theta\\
&= \frac{N}{rc^{k}}\int\limits_{\Iim
    \tau\geq\sqrt{3}/2}(\theta+i/m)^{-k}g(a_{x},c,\tau)\\
&\hspace{2cm}\sum_{\substack{(N/r)|n\\ n>0}}a'_{n}e(n\tau/N)e(-m\theta)e\left(\frac{na_{x}+mNx}{cN}\right)d\theta\\
&= \frac{N}{rc^{k}}\int\limits_{\Iim
    \tau\geq\sqrt{3}/2}(\theta+i/m)^{-k}\sum_{(N/r)|n}a'_{n}e(n\tau/N)e(-m\theta)\\
&\hspace{2cm}\sum_{t\rm{mod} \;
    c}b_{t}(c,\tau)e\left(\frac{(tN+n)a_{x}+mNx}{cN}\right)d\theta\text{
    \ (by \eqref{c1:eq3})}
\end{align*}
As\pageoriginale a consequence, for fixed $c\geq 1$ and $d_{0}$ in
$\mathbb{Z}$, we obtain, from \eqref{c1:eq4},
\begin{align*}
& \sum_{\substack{(c,d)=1\\ d\equiv d_{0}(\rm{mod} \;
      N)}}\alpha(c,d)=\frac{N}{rc^{k}}\int\limits_{\Iim \tau\geq
      \sqrt{3}/2}(\theta+i/m)^{-k}\sum_{\substack{n>0\\ (N/r)|n}}a'_{n}e(n\tau/N)e(-m\theta)\\
&\sum_{t\rm{mod} \; c}b_{t}(c,\tau)\sum_{x\in
        X}e\left(\frac{(tN+n)a_{x}+mxN}{cN}\right)d\theta 
\end{align*}
Let us assume for a moment, that the inner most exponential sum, for
every $n>0$ divisible by $N/r$, has the estimate
\begin{equation*}
\sum_{x\in
  X}e\left(\frac{(tN+n)a_{x}+mxN}{cN}\right) =
O(c^{\frac{1}{2}+\varepsilon}(c,m)^{\frac{1}{2}}).\tag{6}\label{c1:eq6}  
\end{equation*}
for every $\varepsilon > 0$, with an $O$-constant independent of $t$ and
$N$. Then using \eqref{c1:eq6} and Lemmas \ref{c1:lem-1.1.3}
 and \ref{c1:lem-1.1.5}, we may conclude form above, that
\begin{align*}
\left|\sum_{\substack{(c,d)=1\\ d\equiv d_{0}(\rm{mod} \; N)}}
\alpha(c,d)\right|&\ll
c^{-k}\int\limits_{\Iim \tau\geq
  \sqrt{3}/2} (\theta^{2}+m^{-2})^{-k/2}\\ 
&\hspace{1.4cm}\exp\left(-\mathscr{X}_{1}
\frac{m^{-1}}{c^{2} (\theta^{2}+m^{-2})}\right) c^{\varepsilon}
c^{\frac{1}{2}+\varepsilon}(c,m)^{\frac{1}{2}}d\theta\tag{7}\label{c1:eq7}   
\end{align*}
(recalling that $\tau:=-1/\{c^{2}(\theta+i/m)\}$. Making the change of
variable $\theta\mapsto \theta/m$ on the right hand side of \eqref{c1:eq7},
it is
$$
\ll
c^{-k+1/2+2\varepsilon} (c,m)^{\frac{1}{2}}m^{k-1}
\int\limits^{\infty}_{-\infty} (\theta^{2}+1)^{-k/2}
\exp(-\mathscr{X}_{1}m/(c^{2}(1+\theta^{2})))d\theta  
$$
with $c\ll \sqrt{m}$ and now by Lemma \ref{c1:lem-1.1.4}, we have a
majorant
$$
\ll
c^{-k+1/2+2\varepsilon}(c,m)^{\frac{1}{2}}m^{k-1}(m/c^{2})^{-k/2+1/2}\ll
c^{-\frac{1}{2}+2\varepsilon}m^{k/2-1/2}(c,m)^{\frac{1}{2}}.
$$
Thus we have finally, as in the proof of Theorem \ref{c1:thm-1.1.1},
$$
a_{m}=O\left(\sum_{c\ll \sqrt{m}}\alpha(c,d)\right)\ll \sum_{1\leq c\ll
  \sqrt{m}}c^{-1/2+2\varepsilon}(c,m)^{1/2}m^{k/2-1/2}
$$
But\pageoriginale now writing $(c,m)=u$, $c=uv\ll \sqrt{m}$, we have
$v\ll\sqrt{m}/u$ so that
\begin{align*}
\sum_{1\leq c\ll \sqrt{m}}c^{-1/2+2\varepsilon}(c,m)^{1/2} &\ll
\sum_{u|m}\sqrt{u}\sum_{v\ll\sqrt{m}/u}(uv)^{-1/2+2\varepsilon}\\
&= \sum_{u|m}u^{2\varepsilon}\sum_{v\ll \sqrt{m}/u}v^{-1/2+2\varepsilon}\\
&\ll \sum_{u|m}u^{2\varepsilon}(\sqrt{m}/u)^{\frac{1}{2}+2\varepsilon}\\
&= m^{1/4+\varepsilon}\sum_{u|m}1/\sqrt{u}\\
&= m^{1/4+\varepsilon}\sum_{u|m}1\\
&\ll m^{1/4+2\varepsilon}
\end{align*}
and hence
\begin{equation*}
a_{m}=O(m^{k/2-1/4+2\varepsilon})\tag{8}\label{c1:eq8}
\end{equation*}
proving Theorem \ref{c1:thm-1.1.2}, {\em under the assumption of the
  estimate} \eqref{c1:eq6}.

Before proceeding to the proof of \eqref{c1:eq6}, we make a few
remarks. Namely, any estimate $\sum\limits_{t\rm{mod} \;
  c}|b_{t}(c,z)|=O(c^{f})$ with $f\leq 1/2$, may be seen to imply
$a_{m}=O(m^{k/2-1/4+f/2+\varepsilon})$ in place of \eqref{c1:eq8}. Clearly
and $f<1/2$ represents an improvement over Hecke's estimate. A
straightforward application of Schwarz's inequality immediately yields
an estimate with $f=1/2$ but then we are in no better position than in
Theorem \ref{c1:thm-1.1.1}.

Let\pageoriginale us denote by $K$ the exponential sum in
\eqref{c1:eq6}. For any $x$ in $X$, $(x,c)=1$ and so let us fix an
integer $a$ with $ax\equiv 1(\rm{mod} \; c)$. Now since $a_{x}x\equiv 1(\rm{mod} \;
c)$, we have $a_{x}\equiv a(\rm{mod} \; c)$, so that $a_{x}=a+cs$ for some
$s$ in $\mathbb{Z}$, which is unique modulo $r$, since
$a+cs=a_{x}\equiv \alpha(\rm{mod} \; N)$ by Lemma \ref{c1:lem-1.1.6} and
$N|cr$. We observe that $N|cf$ if and only if $r|f$. Indeed, if $r|f$,
$cr|cf$ and so $N|cf$; on the other hand, if $N|cf$, then $cr|cf$
since $c|cf$ and so $r|f$. Since $a_{x}=a+cs\equiv\alpha(\rm{mod} \; N)$, we
may write $K$ as
$$
K=\sum_{x\in X} \; \sum_{s\rm{mod}\; r} e((tN+n)(a+cs)+m \, \times \,
N)/cN)\cdot \frac{1}{N}\sum_{u\rm{mod} \; N}e((a+cs-\alpha)u/N) 
$$
Now the coefficient of $s$ in the expression above is
$(tN+n)/N=t+(n/N(/r))/r$ and hence we are justified in taking $s$ only
modulo $r$. Thus
{\fontsize{10}{12}\selectfont
$$
K=N^{-1}\sum_{\substack{x\in X\\ u\rm{mod} \;
    N}}e((tN+n)a+mxN)/cN)e((a-\alpha)u/N)\sum\limits_{s\in\mathbb{Z}/(r)}e((tN+n+cu)s/N) 
$$}\relax
and the inner sum over $s$ modulo $r$ is $r$ or $0$ according as
$N|(n+cu)$ or not, if we note that $(n+cu)/N=(n/(N+r))/r+(cr/N)u/r$
has denominator dividing $r$. As a result,
\begin{align*}
K &= (r/N)\sum_{\substack{u\rm{mod} \; N\\ N|(n+cu)}}e(-\alpha u/N)\sum_{x\in
  X}e((a((tN+n)+cu)+mxN)/cN)\\
&= (r/N)\sum_{\substack{u\rm{mod} \; N\\ N|(n+cu)}}e(-\alpha u/N)\sum_{x\in X}e\left(a\left(t+\frac{n+cu}{N}\right)+mx\right)/c)\tag{9}\label{c1:eq9}
\end{align*}
wherein\pageoriginale the second sum may be recognised as nearly a
Kloosterman sum, since $ax\equiv 1(\rm{mod} \; c)$.

We remark now that there is a bijective correspondence $x\mapsto x$
between $X=\{x\in\mathbb{Z}/(cr)|(x,c)=1$, $x\equiv \delta(\rm{mod} \; N)\}$
and $X'=\{x\in\mathbb{Z}/(c)|(x,c)=1$, $x+cs\equiv \delta(\rm{mod} \; N)$ for
some $s$ in $\mathbb{Z}\}$. First, the map is one-one, since, for
$x_{1}$, $x_{2}\in X$ with $x_{1}\equiv x_{2}(\rm{mod} \; c)$, we have
$x_{1}=x_{2}+cf$ which, in view of $x_{1}\equiv \delta\equiv
x_{2}(\rm{mod} \; N)$, implies that $N|cf$ \ie $r|f$ (by the arguments in the
preceding paragraph) and so $x_{1}\equiv x_{2}(\rm{mod} \; cr)$. The mapping
is onto, since for any $x\in X'$, we need only remark that $x+cs$
modulo $cr$ (for the $s$ involved in the definition of $X'$) maps to
$x$ in $X'$. Suppose now $ad\equiv 1(\rm{mod} \; c)$. Then $d+cs_{1}\equiv
\delta(\rm{mod} \; N)$ for some $s_{1}$ in $\mathbb{Z}\Longleftrightarrow
a+cs_{2}\equiv \alpha(\rm{mod} \; N)$ for an $s_{2}$ in $\mathbb{Z}$. We
prove only the implication $\Longrightarrow$ (the proof for the
reverse implication being similar). For $ad\equiv 1(\rm{mod} \; c)$, there
exists $\sigma^{\ast}=\left(\begin{smallmatrix} a & b\\ c & d
\end{smallmatrix}\right)$ in $\Gamma$ and
$\sigma^{\ast}\left(\begin{smallmatrix} 1 & s_{1}\\ 0 & 1
\end{smallmatrix}\right)=\left(\begin{smallmatrix} a & as_{1}+b\\ c &
  cs_{1}+d
\end{smallmatrix}\right)\equiv \left(\begin{smallmatrix} a & \ast\\ c
  &\delta\end{smallmatrix}\right)(\rm{mod} \; N)$.

Hence
\begin{align*}
& \sigma^{\ast}
\begin{pmatrix}
1 & s_{1}\\
0 & 1
\end{pmatrix}
\sigma^{-1}_{0}\equiv 
\begin{pmatrix}
1 & -s_{2}\\
0 & 1
\end{pmatrix}
(\rm{mod} \; N)\text{ \  for some \ } s_{2} \text{ \ in \ } \mathbb{Z}.\\
\text{\ie \ } &
\begin{pmatrix}
1 & s_{2}\\
0 & 1
\end{pmatrix}
\sigma^{\ast}
\begin{pmatrix}
1 & s_{1}\\
0 & 1
\end{pmatrix}
\equiv \sigma_{0}(\rm{mod} \; N), \text{ \  implying that}
\end{align*}
$a+cs_{2}\equiv \alpha(\rm{mod} \; N)$, since
$\sigma_{0}=\left(\begin{smallmatrix} \alpha & \beta\\ c & \delta
\end{smallmatrix}\right)$. Writing $t+(n+cu)/N$ in \eqref{c1:eq9} as
$\ob{u}$ and using the bijection between $X$ and $X'$, the inner sum
over $x$ in \eqref{c1:eq9} becomes\pageoriginale now
\begin{align*}
& \sum_{x\in
    X}e\left(\frac{a\ob{u}+mx}{c}\right)=\sum_{\substack{a\rm{mod} \;
      c\\ (a,c)=1,a+cs_{2}\equiv \alpha(\rm{mod} \; N)\text{ for some }
      s_{2}\in \mathbb{Z}}}e\left(\frac{a\tilde{u}+md}{c}\right)\\
&= \sum_{\substack{a\rm{mod} \;
      c\\ (a,c)=1}}e\left(\frac{a\tilde{u}+md}{c}\right)\times
  \frac{1}{N}\sum_{s_{2}\rm{mod}\, r} \; \sum_{v\rm{mod}\,
    N}e((a+cs_{2}-\alpha)v/N),\\
&\hspace{3cm}  \text{ \ by arguments as before},\\
&= N^{-1}\sum_{\substack{a\rm{mod} \;
      c\\ (a,c)=1}}e\left(\frac{a\tilde{u}+md}{c}\right)\sum_{v\rm{mod} \;
    N}e((a-\alpha)v/N)\sum_{s_{2}\rm{mod} \; r}e(cs_{2}v/N)\\
&= rN^{-1}\sum_{\substack{v\rm{mod} \; N\\ N|cv}}e(-\alpha
  v/N)\sum_{\substack{a\rm{mod} \; c\\ (a,c)=1\\ ad\equiv 1(\rm{mod} \; c)}}e\left(\frac{a(\tilde{u}+cv/N)+md}{c}\right)\tag{10}\label{c1:eq10}
\end{align*}
since the inner sum over $s_{2}$ modulo $r$ is $r$ or $0$ according as
$N$ divides $cv$ or not. The inner sum over a modulo $c$ in
\eqref{c1:eq10} is a genuine Kloosterman sum (Note that
$\tilde{u}+cv/N\in \mathbb{Z}$) and is
$O(c^{1/2+\epsilon}(c,m)^{1/2})$, by Weil's well-known estimate
\cite{key28}). This finally proves \eqref{c1:eq6} and hence establishes
Theorem \ref{c1:thm-1.1.2} as well.



\section{Reduction Theory}\label{c1:sec-1.2}\pageoriginale
In this section, we give a quick survey of Minkowski's reduction
theory for positive definite quadratic forms, as carried over to the
general linear group $GL_{m}(\mathbb{R})$.

Let
$$
G=GL_{m}(\mathbb{R}), A=
\left\{
\begin{pmatrix}
a_{1} & \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots & a_{m}
\end{pmatrix}
\in G|\text{all} \ a_{i}>0\right\}
$$
and
$$
N=\left\{
\begin{pmatrix}
1 &\ldots & \ast\\
0 & \ddots & \\
0 & \ldots & 01
\end{pmatrix}
\in G\right\}.
$$
For any $g\in G$, the matrix ${}^{t}gg$ is positive definite and we
have the Babylonian decomposition ${}^{t}gg={}^{t}PP$ where
$P=\left(\begin{smallmatrix} p_{1} &\ldots & p_{ij}\\
\vdots & \ddots & \vdots\\
0 & \ldots & p_{m}\end{smallmatrix}\right)$ with all $p_{i}>0$ and
$p_{ij}=0$ for $i>j$. Thus if $\mathcal{O}(m)$ denotes the orthogonal
group of degree $m$, then $gp^{-1}\in \mathcal{O}(m)$ and further
$G=\mathcal{O}(m)AN$ \ie for every $g$ in $G$, $g=kan$ with
$k\in\mathcal{O}(m)$, $a\in A$ and $n\in N$. This decomposition
$G=KAN$ is known as the Iwasawa decomposition and is unique, for every
$g$ in $G$. For given $t$, $u>0$, let
{\fontsize{10}{12}\selectfont
\begin{align*}
A_{t} &= \left\{
\begin{pmatrix}
a_{1} &\ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots & a_{m}
\end{pmatrix}
\in A|\text{all} \ a_{i}>0, a_{i}\leq ta_{i+1}\text{ \ for \ } 1\leq
i\leq m-1\right\}\text{ \  and \ }\\
N_{u} &= \left\{
\begin{pmatrix}
1 & \ldots & n_{ij}\\
\vdots & \ddots & \vdots\\
0 & \ldots & 1
\end{pmatrix}
\in N| \; | n_{ij}|\leq u\text{ \  for all \ } n_{ij}\right\}. \text{ \ Then}
\end{align*}}
$\mathfrak{S}=\mathfrak{S}^{(m)}_{t,u}:=\mathcal{O}(m)A_{t}N_{u}$ is a
so-called Siegel domain; note that while $\mathcal{O}(m)$ and $N_{u}$
are compact, $A_{t}$ is not compact. The following theorem shows that
$\mathfrak{S}^{(m)}_{2/\sqrt{3},1/2}$ is almost a fundamental domain
$G/GL_{m}(\mathbb{Z})$ for\pageoriginale $GL_{m}(\mathbb{Z})$ in $G$;

\begin{subtheorem}\label{c1:thm-1.2.1}
$GL_{m}(\mathbb{R})=\mathfrak{G}_{2/\sqrt{3},1/2}GL_{m}(\mathbb{Z})$.

We prove first a few lemmas necessary for this theorem.
\end{subtheorem}

\setcounter{sublemma}{1}
\begin{sublemma}\label{c1:lem-1.2.2}
If $N_{\mathbb{Z}}:=N\cap GL_{m}(\mathbb{Z})$, then $N=N_{1/2}\cdot
N_{\mathbb{Z}}$. 
\end{sublemma}

\begin{proof}
If $x=\left(\begin{smallmatrix} 1 & \ldots & x_{ij}\\ \vdots & \ddots
  & \vdots\\
0 & \ldots & 1
\end{smallmatrix}\right)\in N$ and $y=\left(\begin{smallmatrix} 1 &
  \ldots & y_{ij}\\ \vdots & \ddots & \vdots\\ 0 & \ldots & 1
\end{smallmatrix}\right)\in N_{\mathbb{Z}}$, then
$z=xy=\left(\begin{smallmatrix} 1 & \ldots & z_{ij}\\ \vdots & \ddots
  & \vdots\\ 0 & \ldots & 1\end{smallmatrix}\right)$ with
  $z_{ij}=y_{ij}+\sum\limits_{i<k<j}x_{ik}y_{kj}+x_{ij}$. In the order
  $(m-1,m)$, $(m-2,m-1)$, $(m-2,m),\ldots,(i,i+1),\ldots(i,m)$, choose
  $y_{ij}\in\mathbb{Z}$ such that $|z_{ij}|\leq 1/2$ for $i<j$ (Note
  that for $i=m-1$, $j=m$, the sum over $i<k<j$ is empty). This proves
  the lemma.
\end{proof}

Let, for any column $x:={}^{t}(x_{1},\ldots,x_{m})$, its norm
$\sqrt{x^{2}_1 + \cdots + x^{2}_{m}}$ be denoted by $||x||$. For $g$ in $G$,
we now put $\varphi(g)=||ge_{1}||$ where $e_{1}$ is the unit vector
${}^{t}(1,0\ldots0)$. Using the Iwasawa decomposition $g=kan$ for $g$
in $G$, we have
$$
\varphi(g)=||kane_{1}||=||ane_{1}||=a_{1}=\varphi(a)
$$
where $a_{1}$ is the leading entry of the diagonal matrix $a$.

\begin{remark*}
For $g$ in $G$ and $\gamma$ in $GL_{m}(\mathbb{Z})$, clearly $g\gamma
e_{1}\in g\mathbb{Z}^{m}$ and $\inf\limits_{\gamma\in
  GL_{m}(\mathbb{Z})}\varphi\break(g\gamma)=\inf\limits_{0\neq x\in
  \mathbb{Z}^{m}}||gx||$ is attained at some $x$ in $\mathbb{Z}^{m}$. 
\end{remark*}

\begin{sublemma}\label{c1:lem-1.2.3}
Let\pageoriginale $g=kan$ be the Iwasawa decomposition of $g$ in $G$
and let further $\inf \underset{\gamma \in
  GL_m(\mathbb{Z})}{\varphi(g\gamma)} = \varphi(g)$. Then, for the
first two (diagonal) 
entries $a_{1}$, $a_{2}$ of $a$, we have $a_{1}/a_{2}\leq 2/\sqrt{3}$.
\end{sublemma}

\begin{proof}
By lemma \eqref{c1:lem-1.2.2}, we can find $n'$ in $N_{\mathbb{Z}}$ such
that $nn'\in N_{1/2}$. Our hypothesis tells us that
$\varphi(gn'\gamma)\geq \varphi(g)$ for every $\gamma$ in
$GL_{m}(\mathbb{Z})$. But, from the form of $n'$ and the definition of
$\varphi$, we have $\varphi(g)=\varphi(gn')$. Writing
$t=nn'=\left(\begin{smallmatrix} 1 & \ldots & t_{ij}\\ \vdots & \ddots
  & \vdots\\ 0 & \ldots & 1
\end{smallmatrix}\right)$, we have, by our choice of $n'$,
$|t_{ij}|\leq 1/2$. If $J_{0}=\left(\begin{smallmatrix} 0 & 1\\ 1 & 0
\end{smallmatrix}\right)$ and $E_{m-2}$ is the $(m-2)$-rowed identity
matrix, we take $\gamma_{0}=\left(\begin{smallmatrix} J_{0} & 0\\ 0 &
  E_{m-2}
\end{smallmatrix}\right)$. Then
$$
gn'\gamma_{0}e_{1}=gn'{}^{t}(010\ldots0)=ka^{t}(t_{12}10\ldots
0)=k{}^{t}(a_{1}t_{12}a_{2}0\ldots 0)
$$
Thus $\sqrt{a^{2}_{1}t^{2}_{12}+a^{2}_{2}}=\varphi(gn'\gamma_{0})\geq
\varphi(g)=a_{1}$, implying that $|t_{12}|^{2}\leq 1/4$ \ie
$a^{2}_{2}\geq a^{2}_{1}(1-t^{2}_{12})\geq (3/4)a^{2}_{1}$ and
establishing our lemma.
\end{proof}

Theorem \ref{c1:thm-1.2.1} is now seen to be immediate from

\begin{sublemma}\label{c1:lem-1.2.4}
For $g$ in $G$, there exists $\gamma_{0}$ in $GL_{m}(\mathbb{Z})$ such
that $\varphi(g\gamma_{0})=\int\limits_{\gamma\in
  GL_{m}(\mathbb{Z})}\varphi(g\gamma)$. Moreover $g\gamma_{0}\in
\mathfrak{S}_{2/\sqrt{3}, 1/2}$. 
\end{sublemma}

\begin{proof}
For $m=2$, we know that for some $\gamma_{1}$ in $GL_{2}(\mathbb{Z})$,
we have $\varphi(g\gamma_{1})=\inf\limits_{\gamma\in
  GL_{2}(\mathbb{Z})}\varphi(g\gamma)$. We can then evidently find
$n'$ in $N_{\mathbb{Z}}$ such that, with $\gamma_{0}=\gamma_{1}n'$, we
have $g\gamma_{0}\in\mathfrak{S}_{2/\sqrt{3},1/2}$. Hence the Lemma is
true for\pageoriginale $m=2$ and let us suppose that, for $m\geq 3$,
the Lemma has been upheld with  $m-1$ in place of $m$. Now
$\inf\limits_{0\neq x\in \mathbb{Z}^{m}}||gx||$ is attained at an
$x'\neq 0$ in $\mathbb{Z}^{m}$ and such an $x'$ is necessarily
(`primitive' and hence) of the form $\gamma_{1}e_{1}$ for some
$\gamma_{1}$ in $GL_{m}(\mathbb{Z})$. Thus we have (by the Remark
following Lemma \ref{c1:lem-1.2.2}),
\begin{equation*}
\varphi(g\gamma_{1})=\inf\limits_{\gamma\in
  GL_{m}(\mathbb{Z})}\varphi(g\gamma).\tag{11}\label{c1:eq11} 
\end{equation*}
Let $g\gamma_{1}=kan$ be the Iwasawa decomposition, so that
$$
k^{-1}g\gamma_{1}=an=
\begin{pmatrix}
a_{1} & \ast\\
0 & g'
\end{pmatrix}
\quad\text{with}\quad 
g'\text{ \ in \ } GL_{m-1}(\mathbb{R}).
$$
By the induction hypothesis, there exists $\gamma'_{0}$ in
$GL_{m-1}(\mathbb{Z})$ such that $g'\gamma'_{0}$ is in
$\mathfrak{S}^{(m-1)}_{2/\sqrt{3},1/2}$. Consider the Iwasawa
decomposition $g'\gamma'_{0}=k'a'n'$ with
$a'=\left(\begin{smallmatrix} a_{2} & & 0\\ & \vdots & \\ 0 & & a_{m}
\end{smallmatrix}\right)$. Then we have
$$
g_{1}:=k^{-1}g\gamma_{1}
\begin{pmatrix}
1 & 0\\
0 & \gamma'_{0}
\end{pmatrix}
=
\begin{pmatrix}
a_{1} & \ast\\
0 & k'a'n'
\end{pmatrix}
=
\begin{pmatrix}
1 & 0\\
0 & k'
\end{pmatrix}
\begin{pmatrix}
a_{1} & & 0\\
 & \vdots & \\
0 & & a_{m}
\end{pmatrix}
\begin{pmatrix}
1 & \cdot & \ast\\
\cdot & \cdot & \cdot \\
0 & \cdot & 1
\end{pmatrix}
$$
Now
\begin{align*}
& \left.\varphi(g_{1})=\varphi(k^{-1}g\gamma_{1}
\begin{pmatrix}
1 & 0\\ 
0 & \gamma'_{0}
\end{pmatrix}
=\varphi(g\gamma_{1}
\begin{pmatrix}
1 & 0\\
0 & \gamma'_{0}
\end{pmatrix}
\right)=\\
& ||g\gamma_{1}
\begin{pmatrix}
1 & 0\\
0 & \gamma'_{0}
\end{pmatrix}
e_{1}||=||g\gamma_{1}e_{1}||=\varphi(g\gamma_{1})=\inf\limits_{\gamma}\varphi(g\gamma)\text{
  \  by \eqref{c1:eq11}},\\
&=
\left.\inf\limits_{\gamma}\varphi(k^{-1}g\gamma)=\inf\limits_{\gamma}\varphi(k^{-1}g\gamma_{1}
\begin{pmatrix}
1 & 0\\
0 & \gamma'_{0}
\end{pmatrix}
\gamma\right)=\inf\limits_{\gamma}\varphi(g_{1}\gamma) 
\end{align*}
since $\gamma_{1}\left(\begin{smallmatrix} 1 & 0\\ 0 & \gamma'_{0}
\end{smallmatrix}\right)\gamma$ runs over $GL_{m}(\mathbb{Z})$ along
with $\gamma$. Lemma \ref{c1:lem-1.2.3} now applies to $g_{1}$ and so we
have $a_{1}/a_{2}\leq 2/\sqrt{3}$. Already, by induction, we know that
$a_{i}/a_{i+1}\leq 2/\sqrt{3}$ for $2\leq i\leq m$. Now for some
$$
n_{1}\in N_{\mathbb{Z}}, g_{1}n_{1}=k^{-1}g\gamma_{1}
\begin{pmatrix}
1 & 0\\
0 & \gamma'_{0}
\end{pmatrix}
n_{1}\text{ \ is in \ } \mathfrak{S}^{(m)}_{2/\sqrt{3},1/2}
$$ 
in view of Lemma\pageoriginale \ref{c1:lem-1.2.2} and so Lemma
\ref{c1:lem-1.2.4} is proved. 
\end{proof}

\begin{coro*}
For $g$ in $GL_{m}(\mathbb{R})$, $\inf\limits_{0\neq x\in
  \mathbb{Z}^{m}}||gx||\leq (2/\sqrt{3})^{(m-1)/2}(abs(\det g))^{1/m}$
\end{coro*}

\begin{proof}
In view of Theorem \ref{c1:thm-1.2.1}, we may assume that $g$ is in a
Siegel domain
$\mathfrak{S}^{(m)}_{2/\sqrt{3},1/2}=\mathcal{O}(m)A_{2/\sqrt{3}} N_{1/2}$
since both $\inf\limits_{0\neq x}||gx||$ and $abs(\det g)$ depend only
on the coset $gGL_{m}(\mathbb{Z})$. Let then $g=kan$ with
$$
k\in \mathcal{O}(m), a=
\begin{pmatrix}
a_{1} & \cdot &  0\\
\cdot & \vdots & \cdot\\
0 & \cdot & a_{n}
\end{pmatrix}
\in A_{2/\sqrt{3}}\text{ \ and \ } n\in N_{1/2}.
$$
Clearly $a_{1}/a_{i}=\prod_{1\leq j\leq i-1}(a_{j}/a_{j+1})\leq
(2/\sqrt{3})^{i-1}$ and so $a^{m}_{1}=\prod\limits_{1\leq j\leq
  m}\break(a_{1}/a_{j})\times \det a\leq \prod\limits_{1\leq j\leq
  m}(2/\sqrt{3})^{j-1}\det a=(2/\sqrt{3})^{m(m-1)/2}abs(\det g)$. This
gives us 
\begin{equation*}
\varphi(g)=a_{1}\leq (2/\sqrt{3})^{(m-1)/2}(abs\det
g)^{1/m}\tag{12}\label{c1:eq12} 
\end{equation*}
As we know, $\inf\limits_{0\neq x\in \mathbb{Z}^{m}}||gx||$ is
attained at a primitive vector $x'$ in $\mathbb{Z}^{m}$ and such an
$x'$ is of the form $\gamma'e_{1}$ with some $\gamma'$ in
$GL_{m}(\mathbb{Z})$. Thus\pageoriginale 
$$\inf\limits_{0\neq x\in
  \mathbb{Z}^{m}}||gx||=\inf\limits_{\gamma\in
  GL_{m}(\mathbb{Z})}||g\gamma
e_{1}||=\inf\limits_{\gamma}\varphi(g\gamma)\leq \varphi(g)$$ which
proves the Corollary, in view of \eqref{c1:eq12}.
\end{proof}

\begin{defi*}
For $P$ in the space $\mathscr{P}_{m}$ of real $m$-rowed symmetric
positive definite matrices, the minimum $\min(P_:=\inf\limits_{0\neq
  x\in \mathbb{Z}^{m}}P[x]$.
\end{defi*}

If we define in the space $\mathscr{P}_{m}$, the domain $S_{t,u}$
corresponding to the Siegel domain $\mathfrak{S}_{t,u}$, by
$S_{t,u}=\{a[n]|a\in A_{t}, n\in N_{u}\}$, then $S_{t^{2},u}$ is just
the image of $\mathfrak{S}_{t,u}$ under the mapping $g\mapsto
{}^{t}gg$ from $GL_{m}$ onto $\mathfrak{P}_{m}$. Theorem
\ref{c1:thm-1.2.1} and its corollary give us immediately.

\setcounter{subtheorem}{4}
\begin{subtheorem}\label{c1:thm-1.2.5}
$\mathscr{P}_{m}=\bigcup\limits_{\gamma\in
    GL_{m}(\mathbb{Z})}S_{4/3,1/2}[\gamma]$ and
  $\mu_{m}:=\sup\limits_{p\in\mathscr{P}_{m}}\dfrac{\min(P)}{(\det
    P)^{1/m}}\leq (4/3)^{\frac{m-1}{2}}$. 
\end{subtheorem}

\begin{proof}
Writing $P$ in $\mathscr{P}_{m}$ as ${}^{t}gg$ with $g$ in
$GL_{m}(\mathbb{R})$, we know from Theorem \ref{c1:thm-1.2.1} that, for
some $g\gamma$ in $GL_{m}(\mathbb{Z})$, $r=kan\in
\mathfrak{S}_{2/\sqrt{3},1/2}$. Then $P[\gamma]=a^{2}[n]$ which is
clearly in $S_{4/3,1/2}$, proving the first assertion of the
theorem. Now
$$
\min (P)=\inf\limits_{0\neq x\in
  \mathbb{Z}^{m}}p[x]=\inf\limits_{0\neq x\in
  \mathbb{Z}^{m}}||gx||^{2}\leq (4/3)^{(m-1)/2}(\det g)^{2/m} 
$$
by the Corollary. Hence $\min (p)\leq (4/3)^{(m-1)/2}(\det P)^{1/m}$
giving the required upper bound for $\mu_{m}$.
\end{proof}

\begin{remark*}
The constant $\mu_{m}$ known as Hermite's constant is known explicitly
for all $m\leq 8$ (\eg $\mu_{2}=2/\sqrt{3}$, being also the best
possible value). It is related to constants in the packing of spheres
and also to the first eigenvalue of the Laplacian on some spaces. 
\end{remark*}

For\pageoriginale two positive definite matrices $P_{1}$, $P_{2}$ we
use the notation $P_{1}\asymp P_{2}$ to indicate the existence of
constants $c_{1}$, $c_{2}$ for which $P_{1}-c_{1}P_{2}>0$ and
$c_{2}P_{2}-c_{2}P_{1}>0$, \ie to say that $P_{1}$ and $P_{2}$ are of
the same order of magnitude.

\begin{subtheorem}\label{c1:thm-1.2.6}
For $t$, $u>0$ and any $P(=(p_{ij}))=a[n]$ in $S_{t,u}$, we have
$P \asymp a \asymp \left(\begin{smallmatrix} p_{11} & \cdot & 0\\ \cdot
  & \cdot & \cdot\\
0 & \cdot & p_{mm}\end{smallmatrix}\right)$.
\end{subtheorem}

\begin{proof}
Writing $a=\left(\begin{smallmatrix} a_{1} & \cdot & 0\\ \cdot & \cdot
  & \cdot\\ 0 & \cdot & a_{m}\end{smallmatrix}\right)$,
$n=\left(\begin{smallmatrix} 1 & \cdot & n_{ij}\\ \cdot & \cdot &
  \cdot\\ 0 & \cdot & 1\end{smallmatrix}\right)$ and
  $x=\left(\begin{smallmatrix} x_{1}\\ \cdot \\ x_{m}
  \end{smallmatrix}\right)\neq 0$, we have for
  $y:=nx=\left(\begin{smallmatrix} y_{1}\\ \cdot \\ y_{m}
  \end{smallmatrix}\right)$.
$$
\frac{a_{i}y^{2}_{1}}{a[x]}=\frac{a_{i}}{a[x]}\left(x_{i}+\sum_{k>i}n_{ik}x_{k}\right)^{2}\leq
\left[\frac{\sqrt{a}_{i}|x_{i}|}{\sqrt{a[x]}}+\sum_{k>i}\frac{\sqrt{a}_{i}|x_{k}||n_{ik}|}{\sqrt{a[x]}}\right)^{2}. 
$$
Now $a[x]\geq a_{i}|x_{i}|^{2}$ gives
$(\sqrt{a}_{i}|x_{i}|/\sqrt{a[x]})\leq 1$ while, for $k>i$, 
$$
\sqrt{a_{i}}|x_{k}|=\sqrt{\frac{a_{i}}{a_{k}}}\sqrt{a_{k}|x_{k}|^{2}}\leq
\sqrt{\frac{a_{i}}{a_{k}}}\sqrt{a[x]}\leq t^{(k-i)/2}\sqrt{a[x]}. 
$$
Hence $\dfrac{a_{i}y^{2}_{i}}{a[x]}\leq
\left(1+\sum_{k>i}t^{(k-i)/2}u\right)^{2}\ll 1$ where $\ll$ involves
constants depending on $t$ and $u$. We see therefore that
$a[nx]=a[y]\ll a[x]$ for every $x$ \ie $P\leq c_{1}a$. On the other
hand, $n\in N_{u}$ implies at once $n^{-1}\in N_{u}$, for some $u'>0$
depending on $u(\text{and } m)$. Hence $a[n^{-1}]\in S_{t,u'}$. By the
arguments above, we have $a[n^{-1}]\leq c^{-1}_{2}a$ \ie $P\geq
c_{2}a$, so that $P\asymp a$. Taking $x=e_{i}$, the unit
vector\pageoriginale with $1$ at the $i^{\text{th}}$ place and $0$
elsewhere, we have now $c_{2}a_{i}=c_{2}a[e_{i}]\leq
P[e_{i}]=p_{ii}\leq c_{1}a[e_{i}]=c_{1}a_{i}$ so that $P_{ii}\asymp
a_{i}$ for every $i$. In other words, $a\asymp
\left(\begin{smallmatrix} p_{11} &\cdot & 0\\ \cdot & \cdot & \cdot\\
0 & \cdot & p_{mm}\end{smallmatrix}\right)$ and the theorem is proved.
\end{proof}

The following theorem shows that any Siegel domain $S_{t,u}$ in
$\mathscr{P}_{m}$ intersects at most finitely many $S_{t,u}[\gamma]$
for $\gamma\in GL_{m}(\mathbb{Z})$ and so a ``fundamental domain'' $F$
for $GL_{m}(\mathbb{Z})$ in $\mathscr{P}_{m}$ can have at most
finitely many ``neighbours'' $F[\gamma]$.

\begin{subtheorem}\label{c1:thm-1.2.7}
For any $d\geq 1$ and given $S=S_{t,u}\subset \mathscr{P}_{m}$, the
number of $X$ in $\mathscr{M}_{m}(\mathbb{Z})$ with $1\leq abs(\det
X)\leq d$ and $S[X]\cap S\neq \emptyset$ is finite.
\end{subtheorem}

\begin{proof}
We use induction on $m$, for the proof. Let first
$X=\left(\begin{smallmatrix} X_{1} & X_{12}\\ 0 & X_{2}
\end{smallmatrix}\right)$ with $X_{i}\in
\mathscr{M}_{m_{i}}(\mathbb{Z})$, $i=1$, $2$, $1\leq m_{1}$, $m_{2}$
and $m_{1}+n_{2}=m$. Then writing $||M||$ for $abs(\det M)$,
$||X||=||X_{1}|| \ ||X_{2}||\leq d$ implies that $1\leq ||X_{i}||\leq
d$ for $i=1,2$. Take $a[n]$ in $S$ with $(a[n])[X]=a'[n']\in S$. Using
the obvious decompositions 
$$
a=
\begin{pmatrix}
a^{(m_{1})}_{1} & 0\\
0 & a^{(m_{2})}_{2}
\end{pmatrix},
n=
\begin{pmatrix}
n_{1}^{(m_{1})} & n_{12}\\
0 & n^{(m_{2})}_{2}
\end{pmatrix},
a'=
\begin{pmatrix}
a'_{1} & 0\\
0 & a'_{2}
\end{pmatrix},
n'=
\begin{pmatrix}
n'_{1} & n'_{12}\\
0 & n'_{2}
\end{pmatrix}
$$
we have
\begin{align*}
a[n]&=
\begin{pmatrix}
a_{1}[n_{1}] & 0\\
0 & a_{2}[n_{2}]
\end{pmatrix}
\left[
\begin{pmatrix}
E & n^{-1}_{1}n_{12}\\
0 & E
\end{pmatrix}
\right],\\ 
a'[n']&=
\begin{pmatrix}
a_{1}[n'_{1}] & 0\\
a & a'_{2}[n'_{2}]
\end{pmatrix}
\left[
\begin{pmatrix}
E & {n'}^{-1}n'_{12}\\
0 & E
\end{pmatrix}
\right]
\end{align*}
where\pageoriginale $E$ now stands for the identity matrix of the
appropriate size. Then, for $X$ in the form given above, we see that
$$
a[nX]=
\begin{pmatrix}
a_{1}[n_{1}X_{1}] & 0\\
0 & a_{2}[n_{2}X_{2}]
\end{pmatrix}
\left[
\begin{pmatrix}
E & X^{-1}X_{12}+X^{-1}_{1}n^{-1}_{1}n_{12}X_{2}\\
0 & E
\end{pmatrix}
\right]
$$
By definition, $a_{i}$, $a'_{i}\in A^{(m_{i})}_{t}$ and $n_{i}$,
$n'_{i}\in N^{(m_{i})}_{u}$ for $i=1,2$.

Now
\begin{align*}
a'_{1}[n'_{1}]&=a_{1}[n_{1}X_{1}],\\
{n'}^{-1}_{1}n'_{12}&=X^{-1}_{1}X_{12}+X^{-1}_{1}n^{-1}_{1}n_{12}X_{2},\\
a_{2}[n'_{2}]&=(a_{2}[n_{2}])[X_{2}].
\end{align*}
Since $m_{1}$ and $m_{2}$ are both less than $m$, the induction
hypothesis yields the finiteness of the number of such $X_{1}$ and
$X_{2}$ and hence their boundedness as well. Further,
$X_{12}=X_{1}{n'}^{-1}_{1}n'_{12}-n^{-1}_{1}n_{12}X_{2}$ wherein
$n_{12}$, $n'_{12}$ are bounded by virtue of $n$, $n'$ being in
$N_{u}$ and moreover the inverses of the bounded unipotent matrices
$n_{1}$, $n'_{1}$ are again (unipotent and) bounded. Thus the integral
matrix $X_{12}$ is bounded and the number of such $X_{12}$ is
finite. Consequently, we have shown that the number of integral
$X=\left(\begin{smallmatrix} X_{1} & X_{12}\\ 0 & X_{2}
\end{smallmatrix}\right)$ with $1\leq ||X||\leq d$ and $S[X]\cap S\neq
\emptyset$ is finite. Let us now take the case of $X=(x_{ij})$ not
necessarily in any such simple form (for some $m_{1}$, $m_{2}$) but
with $S[X]\cap S\neq \emptyset$ and $1\leq ||X||\leq d$. In fact, for
$1\leq i\leq m-1$, there exist then integers $h_{i}$, $k_{i}$ with
$x_{k_{i},h_{i}}\neq 0$ and $h_{i}\leq i<k_{i}$. Denote the column
${}^{t}(x_{1i}\ldots x_{mi})$ of $X$ by $x^{(i)}$, for $1\leq i\leq
m$. Let, as before, $p=a[n]\in S$ with
$(p'_{ij})=p'=(a[n])[X]=a'[n]\in S$. From Theorem \ref{c1:thm-1.2.6}, we
have (for fixed $S_{t,u}$),
$$
a'_{i}\asymp p'_{ii}=a[n][x^{(i)}]\asymp
a[x^{(i)}]=\sum_{j}a_{j}x^{2}_{ji}.
$$
Hence\pageoriginale
\begin{gather*}
a'_{i}\gg a'_{h_{i}}\asymp \sum_{j}a_{j}x^{2}_{j,h_{i}}\geq
a_{k_{i}}x^{2}_{k_{i},h_{i}}\geq a_{k_{i}}\quad\text{since}\quad
x_{k_{i},h_{i}}\neq 0.\\
\text{\ie}\quad a'_{i}\gg a_{i}\quad \text{(some \ }
k_{i}>i).\tag{13}\label{c1:eq13} 
\end{gather*}
Writing $||X||=d_{1}$, we have
$d_{1}X^{-1}\in\mathscr{M}_{m}(\mathbb{Z})$. Some $P\in S_{t,u}$
implies that $\lambda p\in S_{t,u}$ for $\lambda>0$, we have
$p'[d_{1}X^{-1}]=d^{2}_{1}P$ belongs to $S_{t,u}$ along with
$P'$. Moreover, the integral matrix $d_{1}X^{-1}$ is not in any simple
(block) form as above, since, otherwise, $X$ itself would then take
such a simple (block) form. Applying now to $P'$, $P'[d_{1}X^{-1}]$ in
$S_{t,u}$ the same arguments as we used to derive \eqref{c1:eq13}, we
find that $d^{2}_{1}a_{i}\gg a'_{i}$. But since $d_{1}\leq d$, we may
conclude that $a_{i}\asymp a'_{i}$. Further $a_{i+1}\ll a_{k_{i}}$
(since $k_{i}>i$) and $a_{k_{i}}\ll a'_{i}$, as we have noted prior to
deriving \eqref{c1:eq13}. 

Hence $a_{i+1}\ll a_{k_{i}}\ll a'_{i}\asymp a_{i}\ll a_{i+1}$ \ie
$a_{i}\asymp a_{i+1}$ for every $i$.
\end{proof}

In other words, we have the chain of orders of magnitude:
$$
\begin{matrix}
a_{1} & \asymp & a_{2}  & \asymp\ldots\asymp & a_{m}\\
)( & & )( & & )(\\
a'_{1} & \asymp & a'_{2} & \asymp\ldots\asymp & a'_{m}
\end{matrix}
$$
But then $\sum\limits_{i}a'_{j}x^{2}_{ji}\ll
\sum\limits_{i}a_{j}x^{2}_{ji}\asymp (a[n])[x^{(i)}]=a'_{i}\asymp
a'_{j}$ yields immediately that $x_{ji}\ll 1$ for all $i$ and $j$ and
the theorem as well. 


\section{Minkowski Reduced Domain}\label{c1:sec-1.3}\pageoriginale

For any $P=(p_{ij})$ in $\mathscr{P}_{m}$, we can introduce in
$\mathbb{Z}^{m}$ an inner product ( , ) by defining $(x,y)={}^{t}xPy$
whenever $x$, $y$ are in $\mathbb{Z}^{m}$ and give it the structure of
a quadratic module over $\mathbb{Z}$. If
$e_{i}={}^{t}(0,\ldots,0,1,0,\ldots,0)$ is the standard unit vector
with $1$ at the $i^{\text{th}}$ place (and $0$ elsewhere), then
$\{e_{1},\ldots,e_{m}\}$ is a natural basis for this quadratic module,
with $(e_{i},e_{j})=p_{ij}$. We define, however, a new basis
$\{f_{1},\ldots,f_{m}\}$ as follows. Since $P$ is positive-definite,
the number of integral vectors with $(x,x)\leq \mu$ for any given
$\mu$, is necessarily finite. Hence we can find $f_{1}$ in
$\mathbb{Z}^{m}$ to satisfy the condition
$(f_{1},f_{1})=\inf\limits_{0\neq x\in \mathbb{Z}^{m}}(x,x)$; of
course, $f_{1}$ is not unique (since one can take, for example,
$-f_{1}$ instead of $f_{1}$). Assuming that $f_{1},\ldots,f_{i}$ have
been chosen already, we can proceed to find $f_{i+1}$ in
$\mathbb{Z}^{m}$ meeting the requirements: $f_{1},\ldots,f_{i+1}$ can
be extended to a basis of $\mathbb{Z}^{m}$ and moreover,
$(f_{i+1},f_{i+1})=\inf\limits_{x}(x,x)$ where the infimum is taken
over all $x$ in $\mathbb{Z}^{m}$ for which $f_{1},\ldots,f_{i}$, $x$
can be extended to a basis of $\mathbb{Z}^{m}$. By picking $-f_{i+1}$
instead of $f_{i+1}$, if necessary, we impose the additional
restriction that $f_{i,i+1}\geq 0$; still, $f_{i+1}$ is not unique but
certainly exists. In this manner, we can find a $\mathbb{Z}$-basis
$\{f_{1},\ldots,f_{m}\}$ for the above quadratic module. Writing
$f_{i}=\sum\limits_{1\leq j\leq m}u_{ji}e_{j}$ with $u_{ji}$ in
$\mathbb{Z}$ (for $1\leq i\leq m$), we find that $U:=(u_{ij})$ is in
$GL_{m}(\mathbb{Z})$; further, if $q_{ij}:=(f_{i},f_{j})$, then
$Q:=(q_{ij})={}^{t}UPU$ is in the same `class' as the given $P$ in
$\mathscr{P}_{m}$, besides being {\em ``Minkowski-reduced''} in the
following sense. Indeed, for any $x={}^{t}(x_{1},\ldots,x_{m})$ in
$\mathbb{Z}^{m}$ with the last $m-k+1$ elements $x_{k}$,
$x_{k+1},\ldots,x_{m}$ having $1$ as greatest common divisor, the
matrix $\left(\begin{smallmatrix} E_{k-1} & \\ & x\\ 0 & 
\end{smallmatrix}\right)$ is easily seen to be ``primitive'' \ie
capable of\pageoriginale being completed to an element of
$GL_{m}(\mathbb{Z})$. Thus $f_{1},\ldots,f_{k-1}$, $\sum\limits_{1\leq
  i\leq m}x_{i}f_{i}$ can be completed to a $\mathbb{Z}$-basis of
$\mathbb{Z}^{m}$. Therefore, by our definition of $f_{k}$, we have
$$
Q[x]=\left\{\sum_{1\leq i\leq m}x_{i}f_{i},\sum_{1\leq i\leq
  m}x_{i}f_{i}\right\}\geq (f_{k},f_{k})=q_{kk}(1\ub{\leq}k\ub{\leq} m) 
$$

Thus the matrix $Q$ in the same class as $P$ satisfies the ``reduction
conditions'': 
\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item $Q[x]\geq q_{kk}(1\leq k\leq m)$, for every
  $x={}^{t}(x_{1},\ldots,x_{m})$ with the g.c.d.\@
  $(x_{k},x_{k+1},\ldots,x_{m})$ equal to $1$ and

\item $q_{k,k+1}\geq 0$.
\end{enumerate}

\begin{defi*}
Any positive definite matrix in $\mathscr{P}_{m}$ satisfying the
``reduction conditions'' \eqref{c1:eq1} - \eqref{c1:eq2} above is called
Minkowski-reduced (or merely $M$-reduced).
\end{defi*}

Let us first note that $q_{11}=\min (Q)=\inf\limits_{0\neq
  x\in\mathbb{Z}^{m}}Q[x]$. For any $M$-reduced $Q$, taking $x$ in
\eqref{c1:eq1} to be $e_{\ell}$ with $\ell\ub{\geq}k$, we see that
\begin{equation*}
q_{k,k}\leq q_{\ell,\ell}(k\leq \ell)\tag{14}\label{c1:eq14}
\end{equation*}
If, on the other hand, we take $x$ in \eqref{c1:eq1} to be $e_{k}\pm
e_{\ell}$ for $\ell\neq k$, then condition \eqref{c1:eq1} reads
$$
q_{k,k}\pm 2q_{k\ell}+q_{\ell \; \ell}\geq q_{k,k}
$$\pageoriginale
\ie
\begin{equation*}
|q_{k\ell}|\leq 1/2 \cdot  q_{\ell \; \ell}\quad\text{for}\quad k\neq
\ell.\tag{15}\label{c1:eq15} 
\end{equation*}

Let $\mathscr{R}_{m}=\mathscr{R}$ denote the set of all $M$-reduced
matrices in $\mathscr{P}_{m}$. We have just shown that in every
$GL_{m}(\mathbb{Z})$-orbit $\{P[U]|U\in GL_{m}(\mathbb{Z})\}$ in
$\mathscr{P}_{m}$, there exists an element $Q$ in $\mathscr{R}$. We
may then state the following theorem presenting the reduction theory
due to Minkowski and Siegel for positive-definite matrices.

\begin{subtheorem}\label{c1:thm-1.3.1}
\begin{itemize}
\item[\rm(i)] $\mathscr{P}_{m}=\bigcup\limits_{U\in
  GL_{m}(\mathbb{Z})}\mathscr{R}[U]$; 

\item[\rm(ii)] $\mathscr{R}$ is contained in a Siegel domain $S_{t,u}$
  for some $t$, $u$ (depending only on $m$) and

\item[\rm(iii)] For any $U\neq \pm E_{m}$ in $GL_{m}(\mathbb{Z})$,
  $\mathscr{R}\cap \mathscr{R}[U]$ is contained in the boundary of
  $\mathscr{R}$ (relative to $\mathscr{P}_{m}$).
\end{itemize}
\end{subtheorem}

Actually, $\mathscr{R}$ is a fundamental domain for
$GL_{m}(\mathbb{Z})$ in $\mathscr{P}_{m}$ for the action $p\mapsto
P[U]$ with $P$ in $\mathscr{P}_{m}$ and $U$ in
$GL_{m}(\mathbb{Z})$. It is a convex cone with vertex at $0$ and its
boundary is contained in a finite union of hyperplanes. Moreover,
$\mathscr{R}$ has only finitely many neighbours (\ie $\mathscr{R}\cap
\mathscr{R}[U]\neq \emptyset$, only for finitely many $U$ in
$GL_{m}(\mathbb{Z})$). For all this, a detailed treatment may be
found, for example, in Maass (\cite{key17}, \S\ 9).

Only the assertions (ii) and (iii) in Theorem \ref{c1:thm-1.3.1}.\@ are
to be proved and we need some lemmas for that purpose. 

\setcounter{sublemma}{1}
\begin{sublemma}\label{c1:lem-1.3.2}
For\pageoriginale any $R=(r_{ij})$ in $\mathscr{R}$, $r_{11}\ldots
r_{mm}\mathop{\ll}\limits_{m}\det R$.
\end{sublemma}

\begin{proof}
The leading $(\ell,\ell)$ principal minor
$R_{\ell}=R[{}^{E_{\ell}}_{0}]$ in $R$ is also $M$-reduced (in
$\mathscr{P}_{\ell}$) for $1\leq \ell\leq m$. Let us assume the lemma
proved with $m-1$ in place of $m$; then writing $r_{j}$ for
$r_{jj}(1\leq j\leq m)$, we have
\begin{equation*}
r_{1}r_{2}\ldots r_{m-1}\ll \det R_{m-1}\tag{16}\label{c1:eq16}
\end{equation*}
where the constant in $\ll$ depends only on $m-1$. Defining
$\rho_{k\ell}$ by $(\rho_{k\ell})=(\det R_{m-1})R^{-1}_{m-1}$, we have
on using the inequalities \eqref{c1:eq14} corresponding to $R_{m-1}$,
$|\rho_{k\ell}|r_{\ell}\ll r_{1}r_{2}\ldots r_{m-1}$ and hence
$$
|\rho_{k\ell}|/(\det R_{m-1})\ll (r_{1}r_{2}\ldots
r_{m-1})/(r_{\ell}\det R_{m-1})
$$
\ie
\begin{equation*}
|\rho_{k\ell}|/(\det R_{m-1})\ll 1/r_{\ell}.\tag{17}\label{c1:eq17}
\end{equation*}
If, now, we write
\begin{equation*}
R=
\begin{pmatrix}
R_{m-1} & r\\
t_{r} & r_{m}
\end{pmatrix}
=
\begin{pmatrix}
R_{m-1} & 0\\
t_{0} & s
\end{pmatrix}
\left[
\begin{pmatrix}
E_{m-1} & R^{-1}_{m-1}r\\
t_{0} & t
\end{pmatrix}
\right]\tag{18}\label{c1:eq18}
\end{equation*}
with $s:=r_{m}-R^{-1}_{m-1}[r]$, we have, on applying \eqref{c1:eq14},
\eqref{c1:eq15} and \eqref{c1:eq17},
$$
R^{-1}_{m-1}[r]\ll \sum_{1\leq i,j\leq m-1}(1/r_{i})r_{i}r_{j}\ll r_{m-1}.
$$
Thus
\begin{equation*}
r_{m}=s+R^{-1}_{m-1}[r]\ll s+r_{m-1}.\tag{19}\label{c1:eq19}
\end{equation*}
Since\pageoriginale $\det R=(\det R_{m-1})\cdot s$ from \eqref{c1:eq18},
we obtain from \eqref{c1:eq16} and \eqref{c1:eq19} that $r_{1}r_{2}\ldots
r_{m}\ll (\det R_{m-1})\cdot r_{m}\ll ((\det R)/s)\cdot r_{m}\ll
(1+r_{m-1}/s)\cdot \det R$. Once we establish that
\begin{equation*}
r_{m-1}\ll s\tag{20}\label{c1:eq20}
\end{equation*}
the lemma will follow. In order to prove \eqref{c1:eq20}, let us assume
that for some integer $k\leq m-1$,
\begin{equation*}
r_{\ell+1}<4(m-1)^{2}r_{\ell} \quad (\text{for } \ell=m-2,
  m-3,\ldots,k+1,k\text{ \  but \underline{not} \ } k-1).\tag{21}\label{c1:eq21}
\end{equation*}
Here \eqref{c1:eq21} is to be properly interpreted whenever $k$ equals
$m-1$ or $1$. Writing $z$ for ${}^{t}(x_{1},\ldots,x_{m-1})$,
\eqref{c1:eq18} gives, for $x={}^{t}(x_{1},\ldots,x_{m})$,
\begin{equation*}
R[x]=R_{m-1}[x+x_{m}R^{-1}_{m-1}r]+sx^{2}_{m}.\tag{22}\label{c1:eq22}
\end{equation*}
Let $c=(2m-2)^{m-1}$ and let $x_{i}+a_{i}x_{m}(1\leq i\leq m-1)$
denote the entries of the column $z+x_{m}R^{-1}_{m-1}r$. Now, given an
integer $x'_{m}$ in the closed interval $[0,c^{m-k}]$, we can
certainly find integers $x'_{k}$, $x'_{k+1},\ldots,x'_{m-1}$ to
satisfy $0\leq x'_i + a_{i}x'_{m}<1$, for $k\leq i\leq m-1$. Dividing
the closed interval $[0,1]$ into $c$ closed subintervals of equal
length, we can get a decomposition of the $(m-k)$-dimensional unit
cube (in $\mathbb{R}^{m-k}$) into $c^{m-k}$ cubes of equal volume. By
Dirichlet's pigeonhole principle, at least two of the $1+c^{m-k}$
vectors, say, $(x'_{k}+a_{k}x'_{m},\ldots,x'_{m-1}+a_{m-1}x'_{m})$,
$(x''_{k}+a_{k}x''_{m},\ldots,x''_{m-1}+a_{m-1}x''_{m})$ must be
contained in one of these $c^{m-k}$ cubes; in other words,
$|x'_{i}-x''_{i}+a_{i}(x'_{m}-x''_{m})|\leq 1/c$ for $k\leq i\leq
m-1$. Hence there exist integers $x_{k}$, $x_{k+1},\ldots,x_{m}$,
which we may indeed even assume to have greatest common 
divisor\pageoriginale 1, such that
$$
|x_{i}+a_{i}x_{m}|\leq 1/c, 0<x_{m}\leq c^{m-k}(k\leq i\leq m-1).
$$
Trivially, there exist integers $x_{1},\ldots,x_{k-1}$ satisfying the
conditions 
$$
|x_{i}+a_{i}x_{m}|<1(i=1,2,\ldots,k-1).
$$
For the corresponding column $x={}^{t}(x_{1},\ldots,x_{m})$, we have
$R[x]\geq r_{k}$, since $R$ is $M$-reduced. But then \eqref{c1:eq22}
gives
\begin{align*}
r_{k}(\leq R[x])&\leq
(k-1)^{2}r_{k-1}+(k-1) (m-k)r_{k-1} / c\\
&\qquad+(m-k)^{2} (4(m-1)^{2})^{m-k-1}
r_{k} / c^{2}+c^{2(m-k)}s
\end{align*}
if we use the inequalities
\begin{align*}
& |r_{ij}|\leq r_{k-1}(1\leq i, j\leq k-1), |r_{pq}|\leq
\frac{1}{2}r_{k-1}(p\leq k-1<q-1)\\
& |r_{uv}|\leq (4(m-1)^{2})^{m-k-1}r_{k}(k+1\leq u, v\leq m-1)
\end{align*}
(the last one arising from \eqref{c1:eq21}). Again $r_{k}\geq
4(m-1)^{2}r_{k-1}$, by \eqref{c1:eq21} and therefore finally
$$
r_{k}\leq
\frac{1}{4}r_{k}+\frac{1}{4}r_{k}+\frac{1}{4}r_{k}+c^{2(m-k)}s=\frac{3}{4}r_{k}+c^{2(m-k)}s\quad\text{\ie}\quad
r_{k}\ll s.
$$
Since $r_{m-1}\leq (4(m-1)^{2})^{m-k-1}r_{k}$, \eqref{c1:eq20} is
immediate and so is our lemma.
\end{proof}

\begin{remark*}
The (reverse) inequality
\begin{equation*}
\det R\leq r_{1}\ldots r_{m}\tag{23}\label{c1:eq23}
\end{equation*}
for any $R$ in $\mathscr{P}_{m}$ follows at once from the relation
$\det R=(\det R_{m-1})s$ implied by \eqref{c1:eq18}, the obvious
inequality $s\leq r_{m}$ and the inequality, corresponding to
\eqref{c1:eq23}, for $R_{m-1}$ viz.\@ $\det R_{m-1}\leq r_{1}\ldots
r_{m-1}$ from an inductive hypothesis.
\end{remark*}

For\pageoriginale any $R=(r_{ij})$ in $\mathscr{P}_{m}$, we denote by
$R_{0}$, the diagonal matrix with (the same) diagonal elements
$r_{11}$, $r_{22},\ldots,r_{mm}$ (as $R$).

\begin{sublemma}\label{c1:lem-1.3.3}
For any $R$ in $\mathscr{R}$, we have $c_{1}R_{0}<R<c_{2}R_{0}$ with
constants $c_{1}$, $c_{2}$ depending only on $m$.
\end{sublemma}

\begin{proof}
Let $R^{1/2}_{0}$ denote the positive square root
$[\sqrt{r}_{11},\ldots,\sqrt{r}_{mm}]$ of the diagonal matrix
$R_{0}=[r_{11},\ldots,r_{mm}]$. For the eigenvalues
$\rho_{1},\ldots,\rho_{m}$ of $R[R^{-1/2}_{0}]$, we have
$\rho_{1}+\cdots+\rho_{m}=$ trace $(R[R^{-1/2}_{0}])=$ trace
$(RR^{-1}_{0})=m$. While $\rho_{1}\ldots \rho_{m}=\det R/\det
R_{0}\geq c'$ for some constant $c'=c'(m)$, by the preceding
lemma. Hence $c_{1}:= m^{-(m-1)} c'<\rho_{i}<c_{2}:=m$, for $1\leq i\leq
m$ which means that $c_{1}E_{m}<R[R^{-1/2}_{0}]<c_{2}E_{m}$ \ie
$c_{1}R_{0}<R<c_{2}R_{0}$, on transforming both sides of the
inequalities by $R^{1/2}_{0}$.
\end{proof}

The Iwasawa decomposition $g=\kan$ for $g$ in $GL_{m}$ implies at once
that every $R(={}^{t}gg)$ in $\mathscr{P}_{m}$ has the (unique) Jacobi
decomposition $R=D[B]$ where $D=[d_{1},\ldots,d_{m}]$ is diagonal with
positive diagonal entries $d_{i}$ and $B=(b_{ij})$ is upper triangular
with $b_{ii}=1$ for all $i$. The entries $d_{1},\ldots,d_{m}$ and
$b_{ij}(i<j)$ are called the {\em Jacobi coordinates} of $R=(r_{ij})$
in $\mathscr{P}_{m}$. Denoting $r_{ii}$ by $r_{i}$ as before, the
relation $R=D[B]$ gives $r_{i}=d_{i}+\sum\limits_{1\leq j\leq
  i-1}d_{j}b^{2}_{ji}$ for $1\leq i\leq m$ (so that $r_{i}\geq d_{i}$
always) and further $\det R=d_{1}\ldots d_{m}$. (Thus $\det R\leq
r_{1}\ldots r_{m}$, giving another proof for \eqref{c1:eq23}).

Suppose now that $R$ is $M$-reduced. Then
$\prod\limits^{m}_{j=1}(r_{j}/d_{j})=(r_{1}\ldots r_{m})/\det\break R\leq
c''=c''(m)$, by Lemma \ref{c1:lem-1.3.2}. On the other hand, for any
$R=D[B]$ in $\mathscr{P}_{m}$, we have $1\leq (r_{i}/d_{i})\leq
\prod\limits^{m}_{j=1}(r_{j}/d_{j})$. Hence for $R$ in $\mathscr{R}$,
$1\leq r_{i}/d_{i}\leq c''$ and so $(1\leq)r_{i}/d_{i}\ll r_{j}/d_{j}$
for\pageoriginale all $i$, $j$. Consequently for $R$ in $\mathscr{R}$,
we conclude, in view of \eqref{c1:eq14}, that
$$
0<\frac{d_{j}}{d_{i}}\ll \frac{r_{j}}{r_{i}}(\leq
1)\quad\text{for}\quad j\leq i.
$$
Now, to prove that all $b_{ij}$ are bounded (in absolute value) by a
constant depending only on $m$, we use induction on $m$. In fact, let
us assume that for $1\leq p < i$ and $\ell>p$, we have $|b_{p\ell}|\leq
c_{1}$. Then from the relation 
$$
r_{ij}=d_{i}b_{ij}+\sum\limits_{1\leq p\leq
  i-1}d_{P}b_{pi}b_{pj}(i<j).
$$
we obtain that
\begin{align*}
d_{i}|b_{ij}| &\leq |r_{ij}|+\sum_{p}d_{p}|b_{pi}||b_{pj}|\\
&\leq \frac{1}{2}r_{i}+\sum_{P}d_{p}c^{2}_{1}\\
&\qquad \text{(in view of \eqref{c1:eq15} and the bound for
  $|b_{p\ell}|$)}\\
\text{\ie}\qquad |b_{ij}| &\leq \frac{1}{2}(r_{i}/d_{i})+\sum\limits_{1\leq p\leq
  i}c^{2}_{1}d_{p}/d_{i}\ll 1 \quad (\text{for } i<j).
\end{align*}
We have thus proved assertion (ii) of Theorem \ref{c1:thm-1.3.1}. Along
with Theorem \ref{c1:thm-1.2.7}, this gives us the important.

\begin{coro*}
If $R$ and $R[U]$ are both in $\mathscr{R}$ for some $U$ in
$GL_{m}(\mathbb{Z})$, the number of such $U$ is finite.
\end{coro*}

Before we proceed to prove assertion (iii) of Theorem \ref{c1:thm-1.3.1},
we make a few remarks about the interior $\mathscr{R}^{0}$ and the
boundary $\partial(\mathscr{R})$ of $\mathscr{R}$. Among the
``reduction conditions'' (1) and (2), some are trivial; for example,
if $x=\pm e_{k}$, then $R[x]=r_{k}$ for {\em every} $R$ in
$\mathscr{P}_{m}$. We therefore omit those inequalities which impose
no condition on $\mathscr{R}$. Then $\mathscr{R}^{0}$ consists of
points of\pageoriginale $\mathscr{R}$ for which the ``nontrivial''
reduction conditions among (1) and (2) hold good with strict
inequality. Hence $\partial(\mathscr{R})$ consists precisely of those
points of $\mathscr{R}$ at which even one of these nontrivial
reduction conditions holds with an equality (in place of $\geq$).

Let now both $R_{1}$ and $R_{2}=R_{1}[U]$ for some $U$ in
$GL_{m}(\mathbb{Z})$ belong to $\mathscr{R}$. In view of the Corollary
above, the matrix $U$ belongs to a finite set of matrices (in
$GL_{m}(\mathbb{Z})$) depending only on $m$. First let us suppose
$U=(u_{1}u_{2}\ldots u_{m})$ with columns $u_{1},\ldots,u_{m}$ be no
diagonal matrix, so that we have a first column, say $u_{k}$, which is
different from $\pm e_{k}$. Then the column $v_{k}$ of
$U^{-1}=(v_{1}\ldots v_{m})$ is again $\neq \pm e_{k}$. Since
$U=\left(\begin{smallmatrix} W & \\ & u_{k}\ldots u_{m}\\ 0 & 
\end{smallmatrix}\right)$ with a diagonal $(k-1,k-1)$ matrix $W$
having $\pm 1$ as its diagonal entries, we find, on expanding $\det
U(=\pm 1)$ along the $k^{\text{th}}$ column, that the last $m-k+1$
elements of $u_{k}$ have necessarily $1$ as the greatest common
divisor. From the reduction conditions (1) for
$R_{1}=\left(r^{(1)}_{ij}\right)$, we have $R_{1}[u_{k}]\geq
r^{(1)}_{kk}$ \ie if $R_{2}=\left(r^{(2)}_{ij}\right)$, then
$r^{(2)}_{kk}\geq r^{(1)}_{kk}$. Similarly, from
$R_{1}=R_{2}[U^{-1}]$, it follows that $r^{(1)}_{kk}\geq
r^{(2)}_{kk}$. Thus we have
$$
R_{1}[u_{k}]=r^{(1)}_{kk}=r^{(2)}_{kk}=R_{2}[v_{k}]
$$
and so $R_{1}$, $R_{2}$ belong to the boundary $\partial
(\mathscr{R})$ with $u_{k}$, $v_{k}$ belonging to a finite set of
possible columns. We consider next the case when $U$ is a diagonal
matrix with $\pm 1$ as diagonal entries but $U\neq \pm E_{m}$. Suppose
the first change of sign among the diagonal entries occurs, as we pass
from the $k^{\text{th}}$ diagonal entry to the next one (on the
diagonal). Then
$r^{(2)}_{k,k+1}={}^{t}u_{k}R_{1}u_{k+1}=-r^{(1)}_{k,k+1}$. By (2),
$r^{(1)}_{k,k+1}$ and $r^{(2)}_{k,k+1}$ are non-negative and so
necessarily, $r^{(1)}_{k,k+1}=0=r^{(2)}_{k,k+1}$.

It\pageoriginale follows again that both $R_{1}$ and $R_{2}$ are on
the boundary of $\mathscr{R}$ (We have also proved incidentally that
the points of $\mathscr{P}_{m}\cap \partial (\mathscr{R})$ lie on a
finite set of hyperplanes. We remark, without proof that
$\mathscr{R}^{0}\neq \emptyset$). All the assertions in Theorem
\ref{c1:thm-1.3.1} have now been established.

\begin{example*}
In the special case when $m=2$, $P=\left(\begin{smallmatrix} a & b\\ b
  & c\end{smallmatrix}\right)$ is $M$-reduced, if and only if $0\leq
  b\leq a/2\leq c/2$ and $a>0$. These conditions imply that $ac\leq
  (4/3)\det P$. The reduced domain $\mathscr{R}$ is contained in
  $S_{4/3,1/2}$ and $\mu_{2}=4/3$.
\end{example*}

\section[Estimation for Fourier Coefficients of...]{Estimation for Fourier Coefficients of Modular Forms of
  Degree $n$}\label{c1:sec-1.4}

Let\pageoriginale $\mathscr{G}_{n}$ denote the Siegel half-space of degree $n$,
consisting of all $(n,n)$ complex symmetric matrices $Z=X+iY$ with
$\Iim (Z)=Y:=\dfrac{1}{2i}(Z-\ob{Z})>0$. The modular group
$\Gamma_{n}=\Sp(2n,\mathbb{Z})=\left\{M=\left(\begin{smallmatrix} A &
  B\\ C & D\end{smallmatrix}\right)\in
  \mathscr{M}_{2n}(\mathbb{Z})|M \right.$\break
  $\left. J^{t}_{n}M=J_{n}=\left(\begin{smallmatrix} 
    0 & E_{n}\\ E_{n} & 0  \end{smallmatrix}\right)\right\}$ acts on
  $\mathscr{G}_{n}$ as a discontinuous group of holomorphic
  automorphisms $Z\mapsto M<Z>:=(AZ+B)(CZ+D)^{-1}$ of
  $\mathscr{G}_{n}$, where $A$, $B$, $C$, $D$ are $(n,n)$ matrices
  constituting $M$ in $\Gamma$; observe that $M\{Z\}:=CZ+D$ is
  invertible. Also note that whenever $M=\left(\begin{smallmatrix} A &
    B\\ C & D  \end{smallmatrix}\right)$ is in $\Gamma$, ${}^{t}M$ is
  also in $\Gamma$ and further $M^{-1}=\left(\begin{smallmatrix} {}^{t}{D}
    & {}^{t}{B}\\ {}^{t}{C} & {}^{t}{A}  \end{smallmatrix}\right)$;
  $M=\left(\begin{smallmatrix} A & B\\ C & D
  \end{smallmatrix}\right)$ is in $\Gamma$ if and only if
  $A{}^{t}D-B{}^{t}C=E_{n}$, $A{}^{t}B=B{}^{t}A$ and $C{}^{t}D=
  D{}^{t}C$. The subgroup of $M=\left(\begin{smallmatrix} A & B\\ O &
    D
  \end{smallmatrix}\right)\in\Gamma$ with $A$, $D={}^{t}A^{-1}$ in
  $GL_{n}(\mathbb{Z})$ and symmetric integral $S=A^{-1}B$ is denoted
  by $\Gamma_{n,\infty}$; if $M=\left(\begin{smallmatrix} \ast\\ CD
  \end{smallmatrix}\right)$ and $N=\left(\begin{smallmatrix}\ast\\ CD
  \end{smallmatrix}\right)$ are both in $\Gamma$, then
  $M=\left(\begin{smallmatrix} E_{n} & S\\ 0 & E_{n}
  \end{smallmatrix}\right)N\Gamma_{n,\infty}N$. For
  $Z\in\mathscr{G}_{n}$ and $M=\left(\begin{smallmatrix} A & B\\ C & D
  \end{smallmatrix}\right)$ in $\Gamma$,
  $\Iim(M<Z>)={}^{t}(C\ob{Z}+D)^{-1}\Iim(Z)(CZ+D)^{-1}>0$. 

A fundamental domain $\Gamma\backslash \mathscr{G}_{n}$ for the
discontinuous action of $\Gamma$ on $\mathscr{G}_{n}$ is given by
$$
\mathfrak{g}_{n}:=
\left\{
Z\in \_{n}\left|
\begin{array}{r@{\;\,}p{7cm}}
(1) & \text{Abs $\det (CZ+D)\geq 1$ for every primitive} \text{ integral $(CD)$ with
  $C{}^{t}D=D{}^{t}C$}\\
(2) & \text{$\Iim(Z)$ is $M$-reduced}\\
(3) & \text{The elements of $X:=\frac{1}{2}(Z+\ob{Z})$}\break \text{are $\leq 1/2$ in
  absolute value.}
\end{array}
\right.\right.
$$
Introducing 
\begin{align*}
\mathfrak{g}_{n} &=
\bigcup_{M\in\Gamma_{n,\infty}}M <\mathcal{F}_{n}>=\bigcup_{\substack{U\in
GL_{n}(\mathbb{Z})\\ S={}^{t}S\in
\mathscr{M}_{n}(\mathbb{Z})}}(\mathcal{F}_{n}[U]+S), \text{ \  we
  remark}\\
Z &\in \mathfrak{g}_{n}\Longrightarrow \min(\Iim (Z))\geq
\sqrt{3}.2.\tag{24}\label{c1:eq24} 
\end{align*}
Indeed,\pageoriginale $\min(\Iim(Z[U]+S))=\min (\Iim (Z[U]))=\min
(\Iim(Z))$ for every $U$ in $GL_{n}(\mathbb{Z})$ and $S={}^{t}S$ in
$\mathscr{M}_{n}(\mathbb{Z})$. We may therefore assume, without loss
of generality, that $Z$ is already in $\mathfrak{F}_{n}$. Taking
$M=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)$ in $\Gamma_{n}$ 
\begin{align*}
\text{with } A&=
\begin{pmatrix}
0 & 0\\
{}^{t}0 & E_{n-1}
\end{pmatrix}
,\qquad B=
\begin{pmatrix}
-1 & 0\\
t_{0} & 0
\end{pmatrix}\\
C&=
\begin{pmatrix}
1 & 0\\
t_{0} & 0
\end{pmatrix}
\text{ \  and \ }\qquad D=
\begin{pmatrix}
0 & 0\\
t_{0} & E_{n-1}
\end{pmatrix},\text{ \  and}
\end{align*}
inequality $\abs(\det (CZ+D))>1$ for $Z=(z_{pq})=(x_{pq}+iy_{pq})$,
gives $|z_{11}|\geq 1$, $|x_{11}|\leq 1/2$ and so $y_{11}\geq
\sqrt{3}/2$. Since $\Iim(Z)$ is reduced, $\min(\Iim Z)=y_{11}\geq
\sqrt{3}/2$. Conversely, it can be shown that a constant
$\mathscr{X}_{n}$ exists such that $Z\in \mathfrak{g}_{n}$ whenever
$\min(\Iim(Z))>\mathscr{X}_{n}$. Let us fix a natural number $q$ and a
number $k$ with $2k$ integral once for all; $q$ will serve as the
``level'' and $k$ as the ``weight'' of the modular forms to be
considered in the sequel. Let $\Gamma_{n}(q)$ denote the principal
congruence subgroup of level $q$ in $\Gamma_{n}$, consisting of all
$M$ in $\Gamma_{n}$ with $M\equiv E_{2n}(\rm{mod} \; q)$.

\begin{defi*}
For any $f:\mathscr{G}_{n}\mapsto \mathbb{C}$ and $M\in\Gamma_{n}$,
we define $f|_{k}M=f|M$ by $(f|M)(Z)=f(M<Z>)\det (CZ+D)^{-k}$, with a
fixed determination of the branch.
\end{defi*}

For $M_{1}$, $M_{2}\in\Gamma_{n}$, we have $f|M_{i}M_{2}=\pm
(f|M_{1})|M_{2}$.

\begin{defi*}
By a Siegel modular form of degree $n$, weight $k$ and level $q$, we
mean a holomorphic function $f:\mathscr{G}_{n}\to \mathbb{C}$ such
that $f|M=u(M)f$ {\em for} every $M$ in $\Gamma_{n}(q)$ with a
constant $u(M)$ of absolute value $1$ and which, for $n=1$, satisfies
the condition $f|M$ is bounded in $\mathcal{F}_{1}$ for every $M$ in
$\Gamma_{1}$.
\end{defi*}

It\pageoriginale is known (see \cite{key16}) that for every $M$ in
$\Gamma_{n}$, $f|M$ has the Fourier expansion 
$$
\sum\limits_{0\leq T\in
  \Lambda^*_{n}} \;  a_{M}(T) e(\tr(TZ)/q).
$$

\begin{examples*}
are given by the theta series $\sum\limits_{G^{(m,n)}}e(\pi
i\tr(S[G]Z))$ for even integral $S^{(m)}>0$ and the Eisenstein series
of $\Gamma_{n}(q)$.
\end{examples*}

\begin{defi*}
A Siegel modular form of degree $n$, weight $k$ and level $q$ is said
to vanish at every cusp, if for every $M$ in $\Gamma_{n}$, the
constant term $a_{M}(0)$ {\em in} the Fourier expansion of $f|M$ is
zero. (Note that this definition is independent of the choice of the
branch $\det (CZ+D)^{-k}$).
\end{defi*}

\begin{defi*}
A Siegel modular form of degree $n$, weight $k$ and level $q$ is
called a cusp form, if for every $M$ in $\Gamma_{n}$, the Fourier
coefficients $a_{M}(T)$ of $f|M$ corresponding to all $T$ in
$\Gamma^{\ast}_{n}$ with $\det T=0$ vanish.
\end{defi*}
(This definition coincides for $n=1$ with the preceding
definition. For $n>1$, however, a modular form vanishing at every
cusp, is not a cusp form in general).

One of our main objective is to estimate the Fourier coefficients
$a(T)$ of a Siegel modular form of degree $n$, weight $k$ and level
$q$, vanishing at every cusp. Replacing $f(Z)$ by $f(qZ)$ (of level
$q^{2}$), if necessary, we {\em assume} that the Fourier expansion of
$f$ is given by $f(Z)=\sum\limits_{0\leq P\in
  \Lambda^{\ast}_{n}}a(P)e(\tr(PZ))$, {\em in the sequel}. Now, for
given $T>0$, we know that $T_{1}=T[U]$ is $M$-reduced for some $U$ in
$GL_{m}(\mathbb{Z})$. But, if $f(Z)=\sum\limits_{P}a(P)e(\tr(PZ))$,
then 
\begin{align*}
(\det U)^{-k}f(Z[{}^{t}U])&=(\det
U)^{-k}\sum\limits_{P}a(P)e(\tr(P[U]Z)\\
&=(\det
U)^{-k}\sum\limits_{P}a(P[U^{-1}])e(\tr(PZ)). 
\end{align*}
Denoting $(\det U)^{-k}a(P[U^{-1}])$ by $b(P)$, we see that $a(T)(\det
U)^{-k}$ occurs as the Fourier coefficient, corresponding to the
$M$-reduced matrix $T_{1}$, for $f|\left(\begin{smallmatrix} U & 0\\ 0
  & {}^{t}U^{-1}\end{smallmatrix}\right)$. Since\pageoriginale $f$ is
of level $q$ and since
$(GL_{m}(\mathbb{Z}):GL_{m}(\mathbb{Z};q))<\infty$, we have only
finitely many distinct functions of this form, as $U$ varies over
$GL_{m}(\mathbb{Z})$. {\em We shall therefore assume in the sequel
  that}, for the estimation of the Fourier coefficient $a(T)$, $T$ is
$M$-{\em reduced} and {\em further} $\min (T)\gg 0$ (\ie $\min(T)$ is
large enough).

The following lemma is essential for later applications.

\begin{sublemma}\label{c1:lem-1.4.1}
If the series $\sum\limits_{0\leq
  P={}^{t}P\in\mathscr{M}_{n}(\mathbb{Z})}a(P)e(\tr(PZ))$ converges
absolutely for every $Z$ in $\mathscr{G}_{n}$ and if $a(P)=0$ for all
$p$ with $\rank(P)<\ell(\leq n)$, then for $\gamma=\Iim (Z)$ in
$S_{t,u}$ with $\min(Y)\geq \varepsilon>0$, we have
$$
\mathscr{S} (Z): = \sum\limits_{P} |a(P)||
e(\tr(PZ)|=O_{\varepsilon}(\exp(-\mathscr{X}\tr(Y_{\ell}))   
$$
where $\mathscr{X}$ is a positive constant and $Y_{\ell}$ is the
leading $(\ell,\ell)$ minor of $Y$.
\end{sublemma}

\begin{proof}
Since $Y$ is in $S_{t,u}$, we see exactly as in Lemma \ref{c1:lem-1.3.3},
that $Y\asymp Y_{0} = \left(\begin{smallmatrix} y_{11} &\ldots & 0\\
\vdots &\ddots & \vdots\\ 0 & \ldots & y_{nn}
\end{smallmatrix}\right)$ and since $\min(Y)\geq \varepsilon$, we also
have $Y\geq \varepsilon'E_{n}$ for some $\varepsilon'>0$ depending on
$\varepsilon$, $t$ and $u$. The given series converges absolutely for
$Z=i(\varepsilon'/2)E_{n}$ and hence $a(P)\exp(-\pi
\varepsilon'\tr(P))=O(1)$. Thus
\begin{align*}
\mathscr{S} (Z) &= \sum\limits_{P}|a(P)|\exp (-\pi \tr (PY))\\
&\leq \sum|a(P)|\exp(-\pi\varepsilon'\tr(P))\exp(-\pi \tr(PY))\\
&\ll \sum_{\substack{P={}^{t}P\geq 0\\ \rank P\geq
    \ell}}\exp(-\pi\tr(PY))=\mathscr{T},\text{ \ say.} 
\end{align*}
Since $Y\asymp Y_{0}$ and $tr(Y_{\ell})=y_{11}+\cdots+y_{\ell\ell}$,
we may assume that $Y=Y_{0}$, without\pageoriginale loss of
generality. For any $h$ with $\ell\leq h\leq n$, we set
$$
\alpha_{0}(h)=\sum\limits_{\substack{0\leq p={}^{t}p\in
  \mathscr{M}_{n}(\mathbb{Z})\\ \rank (P)=h}}\exp(-\pi \tr(PY))
$$
so that $\mathscr{T}=\sum\limits_{\ell\leq h\leq n}\alpha_{0}(h)$. In
order to prove the lemma, it suffices clearly to show that
\begin{equation*}
\alpha_{0}(h)=O(\exp(-\mathscr{X}\tr(Y_{\ell}))\text{ \ for a constant
  \ }\mathscr{X}>0.\tag{25}\label{c1:eq25}
\end{equation*}
\end{proof}

Since $P\geq 0$ and $\rank(P)=h$, there exists $U$ in
$GL_{n}(\mathbb{Z})$ such that $P=\left(\begin{smallmatrix} P_{1} &
  0\\ 0 & 0\end{smallmatrix}\right)[U]$ for an integral
  $P^{(h)}_{1}>0$. Suppose now that for $U_{1}$, $U_{2}$ in
  $GL_{n}(\mathbb{Z})$ and $P^{(h)}_{2}>0$, we have
  $\left(\begin{smallmatrix} P_{1} & 0\\ 0 & 0
  \end{smallmatrix}\right)[U_{1}]=\left[\begin{smallmatrix}
      P^{(h)}_{2} & 0\\ 0 & 0
    \end{smallmatrix}\right][U_{2}]$. Then $\left[\begin{smallmatrix}
      P_{1} & 0\\ 0 &
      0    \end{smallmatrix}\right][U_{1}U^{-1}_{2}]=\left[\begin{smallmatrix}
      P_{2} & 0\\ 0 & 0    \end{smallmatrix}\right]$ implying that
  $U_{1}U^{-1}_{2}=\left[\begin{smallmatrix} W_{1} & 0\\ W_{3} & W_{4}
    \end{smallmatrix}\right]$ with $W_{1}$ in $GL_{h}(\mathbb{Z})$,
  $W_{4}$ in $GL_{n-h}(\mathbb{Z})$,
  $W_{3}\in\mathscr{M}_{h,h-h}(\mathbb{Z})$ and
  $P_{1}[W_{1}]=P_{2}$. Since the number of $W_{1}$ in
  $GL_{n}(\mathbb{Z})$ with $P_{1}[W_{1}]=P_{1}$ is at least $2$ (\eg
  $P_{1}[\pm E_{h}]=P_{1}$), we have the inequality
\begin{equation*}
\alpha_{0}(h)<\sum_{P_{1}} \; \sum_{U\in
  GL_{n}^{(h)} (\mathbb{Z})\backslash GL_{n}(\mathbb{Z})} \exp\left(-\pi
\tr
\begin{pmatrix} 
P_{1} & 0\\ 
0 & 0
\end{pmatrix}
[U]Y\right)\tag{26}\label{c1:eq26}
\end{equation*}
where now $P_{1}$ runs over $M$-reduced integral matrices in
$\mathscr{P}_{h}$ (representing the various
$GL_{h}(\mathbb{Z})$-orbits of positive-definite integral matrices in
$\mathscr{P}_{h}$) and $U$ runs over a complete set of representatives
of right cosets of 
$$
GL^{(h)}_{n}(\mathbb{Z}):=
\left\{
\begin{pmatrix}
E_{h} & 0\\
\ast & \ast
\end{pmatrix}
\in GL_{n}(\mathbb{Z})
\right\}\quad\text{in}\quad GL_{n}(\mathbb{Z}).
$$
Any such $U$ can be written\pageoriginale as
$U=\left(\begin{smallmatrix} t_{F}\\ \ast 
\end{smallmatrix}\right)$ with primitive $F^{(n,h)}$ in
$\mathscr{M}_{n,h}(\mathbb{Z})$. 

Further, $\tr\left(\left(\begin{smallmatrix} p_{1} & 0\\ 0 & 0
\end{smallmatrix}\right)[U]Y\right)=\tr(P_{1}Y[P])$. Thus \eqref{c1:eq26}
becomes 
\begin{equation*}
\alpha_{0}(h)<\sum_{P_{1}\in \mathscr{R}_{h}\cap \mathscr{M}_{h}}
\sum_{(\mathbb{Z})F^{(n,h)}\text{ primitive}}\exp(-\pi
\tr(P_{1}Y[F])).\tag{27}\label{c1:eq27} 
\end{equation*}
From the reduction conditions, the number of $M$-reduced integral
$P_{1}$ with given diagonal elements $p_{1}, p_{2},\ldots,p_{h}$ is
seen to be $\ll p^{h-1}_{1}p^{h-2}_{2}\break\ldots p_{h-1}$ (Actually, it is
not hard to verify that the number of
($GL_{h}(\mathbb{Z})$-equivalence) classes $\{P_{1}\}$ of integral
symmetric matrices $P_{1}$ with $\det\break P_{1}\leq d$ is $\ll
d^{(h-1)/2+\varepsilon}$ for any $\varepsilon>0$. We, however, do not need
to use this fact). From \eqref{c1:eq27}, we are led to the simple estimate
{\fontsize{10}{12}\selectfont
\begin{equation*}
\alpha_{0}(h)\ll \sum_{p_{1},\ldots,p_{h}\in
  \mathbb{N}} \; \sum_{F^{(n,h)}_{\text{ \ primitive \ }} (p_{1}p_{2}\ldots
p_{h})^{h-1}}\exp \left(-\mathscr{X}'\tr\left(
\begin{pmatrix}
p_{1} & \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots & p_{h}
\end{pmatrix}
Y[F]\right)\right)\tag{28}\label{c1:eq28}
\end{equation*}}\relax
with a constant $\mathscr{X}'=\mathscr{X}'(h)>0$. Writing
$P^{\ast}=\left(\begin{smallmatrix} p_{1} & \ldots & 0\\ \vdots &
  \ddots & \vdots\\ 0 & \ldots & p_{h}
\end{smallmatrix}\right)$ and $F=(f_{1}\ldots f_{h})$, we have
$P^{\ast}\geq E_{h}$, $\tr(P^{\ast}Y[F])\geq \tr(Y[F])$ and
$\tr(P^{\ast}Y[F])\geq
\varepsilon'\tr(P^{\ast}E_{h}[F])=\varepsilon'\sum\limits_{1\leq i\leq
  h}p_{i}{}^{t}f_{i}f_{i}\geq \varepsilon'\tr(P^{\ast})$ since
${}^{t}f_{i}f_{i}\geq 1(1\leq i\leq h)$ in view of $F$ being
primitive. Thus
$$
\exp(-\mathscr{X}'\tr(P^{\ast}Y[F]))\leq
\exp(-\frac{1}{2} \mathscr{X}' \tr(Y^{\ast}[F]))
\exp(-\frac{1}{2}\mathscr{X}'\varepsilon'\tr(P^{\ast})).  
$$
Now since 
$$
\sum\limits_{p_{1},\ldots,p_{h}\in
  \mathbb{N}}(p_{1}p_{2}\ldots
p_{h})^{h-1}\exp(-\frac{1}{2}\mathscr{X}'\varepsilon'\tr(p_{1}+\cdots+p_{h}))<\infty,  
$$
we obtain from \eqref{c1:eq28} that
\begin{equation*}
\alpha_{0}(h)\ll\sum_{F^{(n,h)}_{\text{primitive}}} \exp(-\frac{1}{2}\mathscr{X}'\tr(Y[F]))\tag{29}\label{c1:eq29} 
\end{equation*}
If,\pageoriginale for $1\leq i_{1}<i_{2}<\ldots<i_{p}\leq n$, the
non-zero rows of any $F$ in \eqref{c1:eq29} have indices
$i_{1},\ldots,i_{p}$, then $p\geq h$ and $i_{p}\geq h$.

Hence
\begin{align*}
\tr(Y[F])&=\sum_{\substack{1\leq i\leq h\\ 1\leq k\leq
    n}}f_{ki}y_{k}f_{ki}\geq \sum\limits_{1\leq r\leq
  p}y_{i_{r}}\left(\sum_{1\leq j= \leq h}f^{2}_{i_{r},j}\right)\geq
\sum_{1\leq r\leq p}y_{r}\\
&\qquad\geq \tr(Y_{h})\geq \tr(Y_{\ell}) 
\end{align*}
and further 
$$
\tr(Y[F])\geq \varepsilon'\tr({}^{t}FF) = \varepsilon'\sum_{\substack{1\leq
    i\leq n\\ 1\leq j\leq h}}f^{2}_{ij}.
$$
It is now immediate that (for some $\mathscr{X}>0$)
\begin{align*}
\exp(-\frac{1}{2}\mathscr{X}'\tr(Y[F] &=
\exp(-\frac{1}{4}\mathscr{X}'\tr(Y[F])\exp
(-\frac{1}{4}\mathscr{X}'\tr(Y[F]))\\
&\leq
\exp(-\mathscr{X}\tr(Y_{\ell}))\exp\left(-\mathscr{X}\varepsilon'\sum_{\substack{1\leq i\leq n\\ 1\leq j\leq h}}f^{2}_{ij}\right)
\end{align*}
and as a result,
\begin{align*}
\alpha_{0}(h) &\ll \exp(-\mathscr{X}\tr(Y_{\ell}))\left(\sum_{g\in
  \mathbb{Z}}\exp(-\varepsilon'\mathscr{X}g^{2})\right)^{hn}\\
&\ll \exp(-\mathscr{X}\tr(Y_{\ell}))
\end{align*}
proving \eqref{c1:eq25} and the lemma.

Let
$$
\mathfrak{t}=\mathfrak{t}_{n}(q):=\{X=(x_{ij})\in\mathscr{M}_{n}(\mathbb{R})|X={}^{t}X,
0\leq x_{ij}<q\}
$$
and, for any given $M$ in $\Gamma_{n}$ and $M$-reduced $T\to 0$ in
$\Lambda^{\ast}_{n}$ with $\min T\gg 0$ (as we have assumed prior to
the statement of Lemma \ref{c1:lem-1.4.1}),
$$
\tilde{\beta}(M):=\{X\in\mathfrak{t}|M<X+iT^{-1}>\varepsilon\mathfrak{g}_{n}\} 
$$
so\pageoriginale that $\tilde{\beta}(M)=\tilde{\beta}(NM)$ for every $N$ in
$\Gamma_{n,\infty}$. Let $M_{1}=E_{2n}$, $M_{2},\ldots,\break M_{r},\ldots$
be a complete set of representatives of the right cosets of
$\Gamma_{n,\infty}$ in $\Gamma_{n}$. Now since $T=(t_{ij})$ is
$M$-reduced, $T\asymp \left(\begin{smallmatrix} t_{11} & \ldots & 0\\ \vdots
  \ddots & \vdots\\ 0 & \ldots & t_{nn}
\end{smallmatrix}\right)$ and so the assumption $\min T\gg 0$ yields
that $t_{ii}\gg 0$ for every $i\geq 1$. Thus $t^{-1}_{ii}$ is
sufficiently small; for $T^{-1}\asymp \left(\begin{smallmatrix}
  t^{-1}_{11} & \ldots & 0\\ \vdots & \ddots & \vdots\\ 0 & \ldots &
  t^{-1}_{11} \end{smallmatrix}\right)$, $\min(T^{-1})$ is also
sufficiently small.

Hence if $\tilde{\beta}(E_{2n})\neq \emptyset$, then
$X+iT^{-1}\in\mathfrak{g}_{n}$ for some $X$ and as a consequence,
$\min(T^{-1})\geq \sqrt{3}/2$, which gives a contradiction. For all
but finitely many $i$, $\tilde{\beta}(M_{i})=\emptyset$. Defining
$\beta(M_{2})=\tilde{\beta}(M_{2})$ and
$\beta(M_{i})=\tilde{\beta}(M_{i})\cap
\{\tilde{\beta}(M_{2})\cup\ldots\cup\tilde{\beta}(M_{i-1})\}^{c}$
inductively for $i\geq 3$, where $\{\quad\}^{c}$ denotes set
complementation, the following lemma is immediate.

\begin{sublemma}\label{c1:lem-1.4.2}
$\mathfrak{t}=\coprod\limits_{i\geq 2}\beta(M_{i})$
\end{sublemma}

For $n=2$, the measure of the intersection of two distinct
$\tilde{\beta}(M_{i})$'s is $0$ (and presumably this is true for
$n=3$ as well).

For $M=\left(\begin{smallmatrix} \ast & \ast\\ C & D
\end{smallmatrix}\right)\in \Gamma_{n}$ and a modular form $f$ of
degree $n$, weight $k$ and level $q$ and $T$ as above, let 
$$
\alpha(C,D)=\alpha(M)=\alpha(\Gamma_{n,\infty}M)=\int\limits_{\beta(M)}f(X+iT^{-1})e(-\tr(TX))dX
$$
where $dX:=\prod\limits_{1\leq i\leq j\leq n}dx_{ij}$ denotes the
volume element in $\mathfrak{t}$. Then\pageoriginale if
\begin{align*}
f(Z) &= \sum_{0\leq T\in\Lambda^*_{n}} \;  a(T)e(\tr(TZ)),\\
a(T) &= q^{-n(n+1)/2}e^{2\pi
  n}\int\limits_{\mathfrak{t}}f(X+iT^{-1})e(-\tr(TX))dX\\
&= q^{-n(n+1)/2}e^{2\pi n}\sum_{i\geq 2}\alpha(M_{i}), \text{ \  by
  Lemma \ref{c1:lem-1.4.2}.}\\
&= O\left(\sum_{i\geq 2}\alpha(M_{i})\right).\tag{30}\label{c1:eq30}
\end{align*}

\begin{sublemma}\label{c1:lem-1.4.3}
For $f$ as above vanishing at all cusps and
$M=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in \Gamma_{n}$ with
$M<Z>\in\mathfrak{g}_{n}$,
$$
f(Z)=\abs (\det (CZ+D)^{-k})O(\exp(-\mathscr{X}\min(\Iim(M<Z>)),
$$
for a constant $\mathscr{X}$.
\end{sublemma}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item Since $[\Gamma_{n}:\Gamma_{n}(q)]<\infty$ and further
  $f|M_{1}M_{2}=(f|M_{1})|M_{2}$ along with $f|M=v(M)f$, for all $M$
  in $\Gamma_{n}(q)$, where $|v(M)|=1$, the number of functions
  $\abs(f|N)$ for $N$ in $\Gamma_{n}$ is finite.

\item If $N=\left(\begin{smallmatrix} \ast & \ast\\ C' & D'
\end{smallmatrix}\right)\in\Gamma_{n}$, then
  $|f|=|f|(N^{-1}N)|=(\abs\det(C'Z+D'))^{-k}|(f|N^{-1})(N<Z>)|$.

\item If $Z$ is in the fundamental domain $\mathcal{F}_{n}$ for
  $\Gamma_{n}$ in $\mathscr{G}_{n}$, $Y=(y_{ij}):=\Iim(Z)$ is
  $M$-reduced and hence belongs to $S_{t,u}$ for some $t$, $u$
  depending only on $n$, by Theorem \ref{c1:thm-1.3.1} (ii). We also know
  from \eqref{c1:eq24} that $\min Y\geq \sqrt{3}/2$. Since $f$ vanishes
  at all cusps,
$$
(f|N)(Z)=\sum\limits_{\substack{0\leq p\in \Lambda_{n}^{\ast}\\ \rank
      P \geq 1}}a(p;N)e(\tr(PZ)/q) 
$$
for\pageoriginale every $N$ in $\Gamma_{n}$. Applying Lemma
\ref{c1:lem-1.4.1}.\@ we have then, for every $N$ in $\Gamma_{n}$,
$|(f|N)(Z)|=O(\exp(-\mathscr{X}y_{11}))=O(\exp(-\mathscr{X}\break
\min(\Iim(Z))))$.

\item Let $M<Z>\in\mathfrak{g}_{n}$; then there exist $U$ in
  $GL_{n}(\mathbb{Z})$ and integral symmetric $S$ such that
  ${}^{t}UM<Z>U+S\in\mathscr{F}_{n}$. For $N=\left(\begin{smallmatrix}
  {}^{t}U & SU^{-1}\\ 0 & U^{-1}
\end{smallmatrix}\right)M$, we have $\min(\Iim N<Z>)=\min (\Iim
  (M<Z>))\geq \sqrt{3}2$ and further $N<Z>$ is $M$-reduced. From b),
  c) and a), it is immediate that
\begin{align*}
|f(Z)| &= \abs(\det(CZ+D)^{-k})|(f|N^{-1})(N<Z>)|\\
&= \abs(\det(CZ+D)^{-k})O(\exp(-\mathscr{X}\min (\Iim(M<Z>))).
\end{align*}
Lemma \ref{c1:lem-1.4.3} implies at once
\end{enumerate}
\end{proof}

\begin{sublemma}\label{c1:lem-1.4.4}
For $f$, $T$ and $M$ as above,
\begin{align*}
|\alpha(M)|&\ll
\int\limits_{\beta(M)}\abs(\det(C(X+iT^{-1})+D))^{-k}\\
&\qquad\exp(-\mathscr{X}\min(\Iim(M<X+iT^{-1}>)))dX. 
\end{align*}
\end{sublemma}

\begin{defi*}
A pair of $(n,n)$ matrices $C$, $D$ is called a symmetric pair if
$C{}^{t}D=D{}^{t}C$ and is said to be {\em coprime}, if, whenever $GC$
and $GD$ are both integral, $G$ is necessarily integral.
\end{defi*}

If $M=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in \Gamma_{n}$, then $(CD)$ is a coprime
symmetric pair. Conversely, it is not hard to prove that given any
coprime symmetric pair $C$, $D$ of $(n,n)$ integral matrices, there
exists $M=\left(\begin{smallmatrix} \ast\\ CD
\end{smallmatrix}\right)$ in $\Gamma_{n}$.

\begin{defi*}
Two\pageoriginale coprime symmetric pairs $C$, $D$ and $C_{1}$,
$D_{1}$ are called {\em associated} if there exists $U$ in
$GL_{n}(\mathbb{Z})$ such that $(CD)=U(C_{1}D_{1})$.
\end{defi*}

Let $\{C,D\}$ denote the equivalence class of all ($n$-rowed) coprime
symmetric pairs associated with a given pair $C$, $D$. We wish to
determine a special representative in each class $\{C,D\}$, where
$r=\rank C$. If $r=0$, then $C=0$; then $D$ is necessarily in
$GL_{n}(\mathbb{Z})$ and we choose $O$, $E$ as a representative. Let
then $0<r\leq n$. There exist $U_{1}$, $U_{2}$ in
$GL_{n}(\mathbb{Z})$, such that
$$
U_{1}C=
\begin{pmatrix}
C_{1} & 0\\
0 & 0
\end{pmatrix}
{}^{t}U_{2}\text{ \  where \ } C_{1}=C^{(r,r)}_{1}\text{ \ with \ }
\det C_{1}\neq 0.
$$
If we write analogously
$$
U_{1}D=
\begin{pmatrix}
D_{1} & D_{2}\\
D_{3} & D_{4}
\end{pmatrix}
U^{-1}_{2}\text{ \ with \ }D_{1}=D_{1}^{(r,r)},
$$
then $C{}^{t}D=D{}^{t}C$ implies $U_{1}C$, $U_{1}D$ is symmetric and
so
$$
\begin{pmatrix}
C_{1} & 0\\
0 & 0
\end{pmatrix}
\begin{pmatrix}
{}^{t}D_{1} & {}^{t}D_{3}\\
{}^{t}D_{2} & {}^{t}D_{4}
\end{pmatrix}
=
\begin{pmatrix}
D_{1} & D_{2}\\
D_{3} & D_{4}
\end{pmatrix}
\begin{pmatrix}
{}^{t}C_{1} & 0\\
0 & 0
\end{pmatrix}
$$
Thus $C_{1}{}^{t}D_{1}=D_{1}{}^{t}C_{1}$ and $D_{3}=0$, so that
$C_{1}$, $D_{1}$ is symmetric.

Since $\left(\begin{smallmatrix} C_{1} & D_{1} & D_{2}\\ 0 & 0 & D_{4}
\end{smallmatrix}\right)$ is primitive, $D_{4}\in
GL_{n-r}(\mathbb{Z})$ and further $(C_{1}D_{1})$ is primitive. Thus
the symmetric pair $C_{1}$, $D_{1}$ is also coprime.

If $Q_{1}$, $Q_{2}$ are primitive $(n,r)$ matrices (\ie capable of
being completed to elements of $GL_{n}(\mathbb{Z})$), we say $Q_{1}$,
$Q_{2}$ are {\em associated}, whenever
$Q_{1}=Q_{2}U_{3}$\pageoriginale for some $U_{3}\in
GL_{r}(\mathbb{Z})$. We denote the class of matrices associated with
$Q_{1}$ by $\{Q_{1}\}$. Hence replacing $U_{2}=(Q^{\ast})$ by
$U_{2}\left(\begin{smallmatrix} U_{3} & 0\\ 0 & E_{n-r}
\end{smallmatrix}\right)$ with $U_{3}\in GL_{r}(\mathbb{Z})$, we can
ensure that the primitive matrix $Q^{(n,r)}$ is a chosen
representative in its class. Under $U_{2}\mapsto
U_{2}\left(\begin{smallmatrix} U_{3} & 0\\ 0 & E_{n-r}
\end{smallmatrix}\right)$ with $U_{3}\in GL_{r}(\mathbb{Z})$, the form
of $U_{1}C$, $U_{1}D$ is unchanged, except for the replacement of
$C_{1}$, $D_{1}$, $Q$ by $C_{1}{}^{t}U_{3}$, $D_{1}U^{-1}_{3}$,
$QU_{3}$ respectively. Replacing now $U_{1}$ by
$\left(\begin{smallmatrix} U_{4} & 0\\ 0 & E_{n-r}
\end{smallmatrix}\right)U_{1}$ with $U_{4}$ in $GL_{r}(\mathbb{Z})$,
we can replace $C_{1}$, $D_{1}$ by any representative in its class
$\{C_{1},D_{1}\}$. Let us fix, for $1\leq r\leq n$, from the classes
of $r$-rowed coprime symmetric pairs a complete set of representatives
as well as a complete system of representatives $F$ from the classes
$\{F\}$ of primitive $(n,r)$ matrices and to each $F$, let us assign a
matrix $U=(F\ast)$ in $GL(n,\mathbb{Z})$, once for all. Thus we have
established already a part of

\begin{sublemma}\label{c1:lem-1.4.5}
Let $F=F^{(n,r)}$ run over a complete set of representatives of the
classes $\{F\}$ of primitive matrices and $C_{1}$, $D_{1}$ over a
complete set of representatives of classes $\{C_{1},D_{1}\}$ with
$C_{1}$, $D_{1}$ coprime and $\det C_{1}\neq 0$. To each such $F$, let
$U=(F\ast)\in GL_{n}(\mathbb{Z})$ be assigned once for all. Then the
pairs
$$
C=
\begin{pmatrix}
C_{1} & 0\\
0 & 0
\end{pmatrix}
{}^{t}U, D=
\begin{pmatrix}
D_{1} & 0\\
0 & E_{n-r}
\end{pmatrix}
U^{-1}
$$
form a complete set of representatives of the classes $\{C,D\}$ with
$C$, $D$ coprime symmetric and $\rank C=r$.
\end{sublemma}

\begin{proof}
What\pageoriginale remains to be proved is only that the different
pairs $C$, $D$ obtained in this manner belong to different classes. If
possible, let
$$
C^{\ast}=
\begin{pmatrix}
C^{\ast}_{1} & 0\\
0 & 0
\end{pmatrix}
{}^{t}U^{\ast}, D^{\ast}=
\begin{pmatrix}
D^{\ast}_{1} & 0\\
0 & E_{n-r}
\end{pmatrix}
U^{\ast-1}
$$
satisfy $C^{\ast}=U_{1}C$, $D^{\ast}=U_{1}D$ for some $U_{1}$ in
$GL_{n}(\mathbb{Z})$. From this, we get
$C^{\ast}{}^{t}D=D^{\ast}{}^{t}C$ and so 
$$
\begin{pmatrix}
C^{\ast}_{1} & 0\\
0 & 0
\end{pmatrix}
{}^{t}U^{\ast} {}^{t}U^{-1}
\begin{pmatrix}
{}^{t}D_{1} & 0\\
0 & E_{n-r}
\end{pmatrix}
=
\begin{pmatrix}
D^{\ast}_{1} & 0\\
0 & E_{n-r}
\end{pmatrix}
U^{\ast^{-1}}U
\begin{pmatrix}
{}^{t}C_{1} & 0\\
0 & 0
\end{pmatrix}
$$
Writing
$$
{}^{t}U^{\ast}{}^{t}U^{-1}=
\begin{pmatrix}
V_{1} & V_{2}\\
V_{3} & V_{4}
\end{pmatrix},
U^{\ast^{-1}}U=
\begin{pmatrix}
W_{1} & W_{2}\\
W_{3} & W_{4}
\end{pmatrix}
$$
we obtain
\begin{equation*}
\begin{pmatrix}
C^{\ast}_{1}V_{1}{}^{t}D_{1} & C^{\ast}_{1}V_{2}\\
0 & 0
\end{pmatrix}
=
\begin{pmatrix}
D^{\ast}_{1}W_{1}{}^{t}C_{1} & 0\\
W_{3}{}^{t}C_{1} & 0
\end{pmatrix}.\tag{30}
\end{equation*}
Hence $V_{2}=0$, $W_{3}=0$ and from
$U=U^{\ast}\left(\begin{smallmatrix} W_{1} & W_{2}\\ 0 & W_{4}
\end{smallmatrix}\right)$, it follows then that $W_{1}\in
GL_{r}(\mathbb{Z})$. If $U=(F \; G)$ and $U^{\ast}=(F^{\ast}G^{\ast})$,
then $F=F^{\ast}W$, \ie $\{F\}=\{F^{\ast}\}$ and so $F=F^{\ast}$
giving $U=U^{\ast}$, since, corresponding to $F$, we have assigned $U$
once for all. Hence
$$
U_{1}
\begin{pmatrix}
C_{1} & D_{1} & 0\\
0 & 0 & E_{n-r}
\end{pmatrix}
=
\begin{pmatrix}
C^{\ast}_{1} & D^{\ast}_{1} & 0\\
0 & 0 & E_{n-r}
\end{pmatrix}
\text{  \ and so \ } U_{1}=
\begin{pmatrix}
U'_{1} & 0\\
\ast & \ast
\end{pmatrix}.
$$
But since $U_{1}$ is in $GL_{n}(\mathbb{Z})$, $U'_{1}$ is in
$GL_{r}(\mathbb{Z})$ and so
$\{C_{1},D_{1}\}=\{C^{\ast}_{1}D^{\ast}_{1}\}$ \ie
$C_{1}=C^{\ast}_{1}$, $D_{1}=D^{\ast}_{1}$ and the lemma is proved.
\end{proof}

\begin{sublemma}\label{c1:lem-1.4.6}
Between\pageoriginale the family of classes $\{C,D\}$ of $n$-rowed
coprime symmetric paris $C$, $D$ with $\det C\neq 0$ and the set of
all $(n,n)$ rational symmetric matrices $P$, there exists a one - one
correspondence given by $\{C,D\}\leftrightarrow p(=C^{-1}D)$.
\end{sublemma}

\begin{proof}
Clearly $\{C,D\}$ uniquely determines $P=C^{-1}D$. Suppose now
$\{C_{1},D_{1}\}$ and $\{C,D\}$ are mapped into the same $P$ \ie
$C^{-1}_{1}D_{1}=C^{-1}D={}^{t}D{}^{t}C^{-1}$ so that
$C_{1}{}^{t}D=D_{1}{}^{t}C$. This in turn means at once that for
$M=\left(\begin{smallmatrix} \ast & \ast\\ C & D
\end{smallmatrix}\right)$, $M_{1}=\left(\begin{smallmatrix} \ast &
  \ast\\ C_{1} & D_{1}\end{smallmatrix}\right)$ in $\Gamma_{n}$,
$M_{1}M^{-1}=\left(\begin{smallmatrix} \ast & \ast\\ 0 & U
\end{smallmatrix}\right)$ with $U\in GL_{n}(\mathbb{Z})$ and therefore
$\{C,D\}=\{C_{1},D_{1}\}$. We have thus shown that $\{C,D\}\mapsto
P=C^{-1}D$ is well-defined and one-one and we need only to show that
it is onto. For any given rational symmetric $(n,n)$ matrix $P$, there
exist $U_{3}$, $U_{4}$ in $GL_{n}(\mathbb{Z})$ such that $U_{3}PU_{4}$
is a diagonal matrix with diagonal elements $a_{i}/b_{i}(1\leq i\leq
n)$, for $a_{i}$, $b_{i}$ in $\mathbb{Z}$ with $(a_{i},b_{i})=1$ and
$b_{i}>0$. If we now take $C_{1}=B_{0}U_{3}$, $D_{1}=A_{0}U^{-1}_{4}$
with diagonal matrices $A_{0}=[a_{1},\ldots,a_{n}]$ and
$B_{0}=[b_{1},\ldots,b_{n}]$, then clearly $P=C^{-1}_{1}D_{1}$. Since
$P={}^{t}P$, we have $C_{1}{}^{t}D_{1}=D_{1}{}^{t}C_{1}$. Since
$(C_{1}D_{1})\left(\begin{smallmatrix} U^{-1}_{3} & 0\\ 0 & U_{4}
\end{smallmatrix}\right)=(B_{0} \; A_{0})$ is clearly primitive, it
follows that $C_{1}$, $D_{1}$ is a coprime symmetric pair
corresponding to $P(=C^{-1}D)={}^{t}P$.
\end{proof}

As an immediate corollary of Lemma \ref{c1:lem-1.4.6}, we see that
$\Gamma_{n,\infty}\backslash \{M=\left(\begin{smallmatrix} A & B\\ C &
  D
\end{smallmatrix}\right)\in \Gamma_{n}|\det C\neq 0\}$ is in one - one
correspondence with $\{P={}^{t}p\in \mathscr{M}_{n}(Q)\}$ via $C$,
$D\mapsto (C^{-1}D=)P$.

\begin{defi*}
For $P=C^{-1}D={}^{t}P\in \mathscr{M}_{n}(Q)$, define
$\house{P}=\abs(\det C)$. (It is clear that if
$C^{-1}D=P=C^{-1}_{1}D_{1}$, then $abs\det C=\abs\det C_{1}$ from
above and so $\house{P}$ is well-defined).
\end{defi*}


The\pageoriginale following three lemmas have been reproduced from
Siegel \cite{key25}, for the sake of completeness.

\begin{sublemma}\label{c1:lem-1.4.7}
Let $K$ be an $n$-rowed diagonal matrix $[c_{1},c_{2},\ldots,c_{n}]$
with integers $c_{1},\ldots,c_{n}$, $c_{i}|c_{i+1}(1\leq i\leq n-1)$
as diagonal entries and $\mathscr{K}=\{U\in
GL_{n}(\mathbb{Z})|KUK^{-1}$ {\em integral}$\}$. Then
$$
[GL_{n}(\mathbb{Z}):\mathscr{K}]\leq
\prod\limits_{p|c_{n}}(1-p^{-1})^{1-n}\prod_{1\leq k\leq
  n}c^{2k-n-1}_{k} 
$$
where $p$ runs over the distinct primes dividing $c_{n}$.
\end{sublemma}

\begin{proof}
Since $\mathcal{Q}:=GL_{n}(\mathbb{Z};q)$ is a subgroup of
$\mathscr{K}$ for every positive multiple $q$ of $c_{n}$, we have
$[GL_{n}(\mathbb{Z}):\mathscr{K}]=[GL_{n}(\mathbb{Z})/\mathcal{Q}:\mathscr{K}/\mathcal{Q}]$. Now
$GL_{n}(\mathbb{Z})/\mathcal{Q}$ is isomorphic to the group of all
$n$-rowed integral matrices $V$ modulo $q$ with $\det V\equiv \pm
1(\rm{mod} \; q)$. In view of the Chinese Remainder Theorem, it suffices to
show that
$$
[\mathscr{U}^{\ast}:\mathscr{K}^{\ast}]\leq
(1-p^{-1})^{1-n}\prod\limits_{1\leq k\leq n}c^{2k-n-1}_{k}
$$
under the conditions that $q=c_{n}$ is a power of a fixed prime number
$p$. $\mathscr{U}^{\ast}$ consists of all $n$-rowed integral matrices
$V$ modulo $q$ with $\det V\equiv \pm 1(\rm{mod} \; q)$ and
$\mathscr{K}^{\ast}$ is the subgroup of all such $V$ with integral
$KVK^{-1}$.

Let $\mathscr{V}_{n}$ be the group of $(n,n)$ integral $V$ modulo $q$
with $(\det V,q)=1$ and $\mathscr{K}_{n}$ the subgroup of all $V$ in
$\mathscr{V}_{n}$ with integral $KVK^{-1}$. Then it is clear that
$[\mathscr{V}_{n}:\mathscr{U}^{\ast}]=[\mathscr{K}_{n}:\mathscr{K}^{\ast}]$
and so
$[\mathscr{V}_{n}:\mathscr{K}_{n}]=[\mathscr{U}^{\ast}:\mathscr{K}^{\ast}]$. If
$\sharp \mathscr{V}_{n}$ and $\sharp \mathscr{K}_{n}$ denote the
orders of $\mathscr{V}_{n}$ and $\mathscr{K}_{n}$ respectively, it
suffices then to\pageoriginale show that
\begin{equation*}
\sharp \mathscr{K}_{n}\geq (\sharp
\mathscr{V}_{n})(1-p^{-1})^{n-1}\prod_{1\leq k\leq
  n}c^{n-2k+1}_{k}.\tag{31}\label{c1:eq31} 
\end{equation*}
It is well-known that
\begin{equation*}
\sharp\mathscr{V}_{n}=q^{n^{2}}\prod\limits_{1\leq k\leq
  n}(1-p^{-k}).\tag{32}\label{c1:eq32} 
\end{equation*}

When $c_{1}=c_{n}$, we have $K=c_{1}E_{n}$,
$\mathscr{K}_{n}=\mathscr{V}_{n}$ and \eqref{c1:eq31} is true, since
$\sum\limits_{1\leq k\leq n}(n+1-2k)=0$; in particular, this holds for
$n=1$. Let us apply induction on $n$ and suppose that
$c_{1}<c_{n}$. Define $h$ by the condition that $c_{h}<c_{h+1}=c_{n}$,
then $1\leq h\leq n-1$. Let $V=(v_{k\ell})=\left(\begin{smallmatrix}
  V_{1} & V_{2}\\ V_{3} & V_{4}\end{smallmatrix}\right)$ with
$V_{1}=V_{1}^{(h,h)}$. The matrices $V$ and $KVK^{-1}$ are both
integral if and only if $v_{k\ell}$ and $c_{k}v_{k\ell}c^{-1}_{\ell}$
are in $\mathbb{Z}$ for $k$, $\ell=1,2,\ldots,n$. Then $V_{3}$ and
$V_{4}$ are arbitrary integral matrices, while $V_{1}$ and $V_{2}$ are
integral matrices subject to the conditions $v_{k\ell}\equiv 0(\rm{mod} \;
c_{\ell}/c_{k})$ for $k\leq h$, $k<\ell$. Since $p|(c_{\ell}/c_{k})$
for $k\leq h<\ell$, we have $V_{2}\equiv 0(\rm{mod} \; p)$ and so $\det
V\equiv(\det V_{1})(\det V_{4})(\rm{mod} \; p)$. Consequently, we get the
elements $V$ of $\mathscr{K}_{n}$ as follows: $V_{4}$ is any element
of $\mathscr{V}_{n-h}$, $V_{3}$ is an arbitrary integral matrix modulo
$q$, $V_{2}$ is any matrix modulo $q$ satisfying the conditions
$c^{-1}_{k}c_{\ell}|v_{k\ell}$ for $k\leq h<\ell$ and $V_{1}$ is any
element of $\mathscr{K}_{h}$. It follows that $\sharp
\mathscr{K}_{n}=aq^{h(n-h)}\cdot \sharp\mathscr{V}_{n-h}\cdot \sharp
\mathscr{K}_{h}$, where $a$ is the number of matrices $V_{2}$, namely
$a=q^{h(n-h)}\prod\limits_{k\leq h<\ell}(c_{k}/c_{\ell})$. Applying
\eqref{c1:eq31} with $h$ instead of $n$ and \eqref{c1:eq32} with $h$, $n-h$
in place of $n$, we obtain
\begin{align*}
\sharp \mathscr{K}_{n}&\geq
q^{n^{2}}(1-p^{-1})^{h-1}\prod\limits_{1\leq k\leq
  h}c^{h-2k+1}_{h}\prod\limits_{k\leq
  h<\ell}(c_{k}/c_{\ell})\\
&\qquad\prod\limits_{1\leq k\leq
  h}(1-p^{-k})\prod\limits_{1\leq k\leq n-h}(1-p^{-k})
\end{align*}
Since\pageoriginale
$$
q^{n^{2}}\prod_{1\leq k\leq h}(1-p^{-k})>\sharp
\mathscr{V}_{n},\prod_{1\leq k\leq n-h} (1-p^{-k})\geq
(1-p^{-1})^{n-h}
$$
and
$$
\prod_{1\leq k\leq h}c^{h-2k+1}_{k}\prod_{k\leq
  h<\ell}(c_{k}/c_{\ell})=c^{-h(n-h)}_{n}\prod_{1\leq k\leq
  h}c^{n-2k+1}_{j}=\prod_{1\leq k\leq n}c^{n-2k+1}_{k}
$$
the assertion \eqref{c1:eq31} follows and the lemma is proved.
\end{proof}

The exact value of $[GL_{n}(\mathbb{Z}):\mathscr{K}]$ can be obtained
from the paper of A.N.\@ Andrianov on `Spherical functions for
$GL_{n}$ over local fields and summation of Hecke series, Math.\@
Sbornik 12 (1970), 429-452.

\begin{sublemma}\label{c1:lem-1.4.8}
Let $A(c_{1},\ldots,c_{n})$ denote the number of modulo $1$
incongruent rational $(n,n)$ symmetric matrices $P=C^{-1}D$ whose
`denominators' $C$ have $c_{1},\ldots,c_{n}$ as elementary
divisors. Then
$$
A(c_{1},\ldots,c_{n})\leq \prod_{p|c_{n}}(1-p^{-1})^{1-n}\prod_{1\leq
  k\leq n}c^{k}_{k}.
$$
\end{sublemma}


\begin{proof}
Let $C^{\ast}$ be any $(n,n)$ integral matrix with
$c_{1},\ldots,c_{n}$ as elementary divisors and let $C^{\ast}=U_{0}KU$
with $U_{0}$, $U\in GL_{n}(\mathbb{Z})$ and diagonal
$K=[c_{1},\ldots,c_{n}]$. If $A(C^{\ast})$ is the number of modulo $1$
incongruent symmetric $R$ with integral $C^{\ast}R$ and if
$R[{}^{t}U]=R_{1}=(r_{k\ell})$ say, then
$C^{\ast}R{}^{t}U=U_{0}KR_{1}$ and so $A(C^{\ast})=A(K)$. The matrix
$KR_{1}$ is integral if and only if $c_{k}r_{k\ell}$ is in
$\mathbb{Z}$ for $1\leq k$, $\ell\leq n$. Since $r_{k\ell}=r_{\ell k}$
and $c_{1}|c_{2}|\ldots|c_{n}$, we obtain
\begin{equation*}
A(K)=\prod_{1\leq k\leq n}c^{n-k+1}_{k}\tag{33}\label{c1:eq33}
\end{equation*}
Now,\pageoriginale the number of modulo $1$ incongruent symmetric $R$
with the same denominator $C^{\ast}$ is at most $A(C^{\ast})$. On the
other hand, $C^{\ast}=U_{0}KU$ and $C_{1}=U_{1}KU_{2}$ with $U_{1}$,
$U_{2}\in GL_{n}(\mathbb{Z})$ are denominators of the same rational
symmetric matrix $R$, if and only if $C^{\ast}C^{-1}_{1}\in
GL_{n}(\mathbb{Z})$; the latter implies that $KU_{2}U^{-1}K^{-1}$ is
integral, $U_{2}U^{-1}$ is in $\mathscr{K}:=\{V\in
GL_{n}(\mathbb{Z})|KVK^{-1}$ is integral$\}$ and so $U$, $U_{2}$ are
in the same right coset of $\mathscr{K}$ in $GL_{n}(\mathbb{Z})$. Thus
$A(c_{1},\ldots,c_{n})\leq [GL_{n}(\mathbb{Z}):\mathscr{K}]A(K)$ and
the lemma is immediate from \eqref{c1:eq33} and Lemma \ref{c1:lem-1.4.7}.
\end{proof}

We need one more lemma, for our later purposes.

\begin{sublemma}\label{c1:lem-1.4.9}
Let $R$ run over a complete set of modulo $1$ incongruent $(n,n)$
rational symmetric matrices. Then the Dirichlet series
$$
\psi(s):=\sum_{R\rm{mod} \; 1}\house{R}^{-s-n}
$$
converges for $s>1$. If $u>0$ and $s>1$, then 
$$
u^{-s}\sum_{\house{R}<u}{\house{R}}^{-n}+\sum_{\house{R}\geq
  u}{\house{R}}^{-n-s}<a\left(2+\frac{1}{s-1}\right)u^{1-s} 
$$
where a depends only on $n$.
\end{sublemma}

\begin{proof}
For two Dirichlet series
$\alpha(s)=\sum\limits_{n}a_{n}\lambda^{-s}_{n}$ and
$\beta(s)=\sum\limits_{n}b_{n}\lambda^{-s}_{n}$, we write
$\alpha(s)< \beta(s)$ if $|a_{n}|\leq |b_{n}|$ for every $n$. From
the definition of $A(c_{1},\ldots,c_{n})$ above, we have
$\psi(s)=\sum\limits_{c_{1}|c_{2}|\ldots|c_{n}}A(c_{1},\ldots,c_{n})(c_{1}\ldots
c_{n})^{-n-s}$ where\pageoriginale $c_{1},\ldots,c_{n}$ run over all
systems of natural numbers with $c_{1}|c_{2}|\ldots\break |c_{n}$. From
Lemma \ref{c1:lem-1.4.8}, we obtain, on letting $c_{1},\ldots,c_{n}$ run
over all natural numbers, that
\begin{align*}
\psi(s)&<
\sum_{c_{1},\ldots,c_{n}}\prod_{p|c_{n}}(1-p^{-1})^{1-n}\prod_{1\leq
  k\leq n}c^{k-n-s}_{k}\\
&=\prod_{p}\left(1+(1-p^{-1})^{1-n}\sum_{1\leq
  \ell <\infty}p^{-\ell s}\right)\prod_{1\leq k\leq n-1}\zeta(s+n-k) 
\end{align*}
Let 
$$
\nu=2^{n}+n-3, \gamma(s)=\zeta^{\nu}(s+1)\quad\text{and}\quad
b_{p}:=p((1-p^{-1})^{1-n}-1).
$$
Then $0\leq b_{p}\leq 2^{n}-2=\nu -n+1$ for all $p\geq 2$ and
{\fontsize{10}{12}\selectfont
$$
1+(1-p^{-1})^{1-n}\sum_{1\leq \ell <\infty} p^{-\ell
  s}=(1+b_{p}p^{-1-s})/(1-p^{-s})(1-p^{-1-a})^{n-\nu-1}/(1-p^{-s}) 
$$}\relax
whence
\begin{equation*}
\psi(s)<\gamma(s)\zeta(s)\tag{34}\label{c1:eq34}
\end{equation*}
proving the first assertion of the lemma.
\end{proof}

Let $\psi(s)=\sum_{1\leq n<\infty}a_{n}n^{-s}$ and
$\gamma(s)=\sum\limits_{1\leq n<\infty}d_{n}n^{-s}$. Further, let
$\sigma_{k}=\sum\limits_{1\leq \ell\leq k}a_{\ell}$,
$\gamma(1)=\zeta^{\gamma}(2)=a$. Then, from \eqref{c1:eq34}, we have
$$
\sigma_{k}\leq \sum_{1\leq \ell\leq k}d_{\ell}[\frac{k}{\ell}]\leq
k\sum_{1\leq \ell \leq k}d_{\ell}/\ell <k\sum_{1\leq \ell
  <\infty}d_{\ell}/\ell=ak(k=1,2,\ldots) 
$$
Hence, for all $u>0$,
\begin{equation*}
\sum_{\house{R}<u}{\house{R}}^{-n}=\sum_{\ell<u}a_{\ell}<au\tag{35}\label{c1:eq35} 
\end{equation*}
Moreover,\pageoriginale for
\begin{align*}
 s>1, \sum_{\house{R}\geq
    u}{\house{R}}^{-n-s}&=\sum_{k\geq u}a_{k}k^{-s}\\
&=\sum_{k\geq
    u}(\sigma_{k}-\sigma_{k-1})^{k^{-s}}\leq \sum_{k\geq
    u}\sigma_{k}(k^{-s}-(k+1)^{-s})\\
& =s\sum_{k\geq
    u}\sigma_{k}\int\limits^{k+1}_{k}x^{-s-1}dx<as\\
&\qquad\sum\limits_{k\geq
    u}\int\limits^{k+1}_{k}x^{-s}dx\leq as  \int\limits^{\infty}_{u}x^{-s}dx\\
&=as u^{1-s}/(s-1).\tag{36}\label{c1:eq36}
\end{align*}
The second assertion of the lemma follows from \eqref{c1:eq35} and
\eqref{c1:eq36}.


It is known that
$\psi(s)=\dfrac{\zeta(s)}{\zeta(s+n)}\prod\limits^{n}_{r=1}\dfrac{\zeta(2s+n-r)}{\zeta(2s+2n-2r)}$
where $\zeta$ is Riemann's zeta function. This assertion may be found
in $H$. Maass \cite{key17}. For a proof, see Kitaoka's paper `Dirichlet
series in the theory of Siegel modular forms, Nagoya Math.J.\@ 35
(1984), 73-84 (\cf G.\@ Shimura: On Eisenstein series, Duke
Math.J.50(1983), 417-476).

Returning to the problem of estimating $\sum\limits_{M}\alpha(M)$, we
first state the following Propositions, which essentially go back to
Siegel \cite{key25}.

\setcounter{subprop}{9}
\begin{subprop}\label{c1:prop-1.4.10}
For $f$ and $T$ as above and half-integral $k>n+1/2$, we have
\begin{equation*}
\sum_{\substack{\left(\begin{smallmatrix} A & B\\ C & D
    \end{smallmatrix}\right)=M\in \Gamma_{n,\infty}\backslash
    \Gamma_{n}\\ \det C\neq 0}}\alpha(M)\ll (\min T)^{(n+1-k)/2}(\det
T)^{k-(n+1)/2}\tag{37}\label{c1:eq37} 
\end{equation*}
if $\min T>\mathscr{X}>0$ for $\mathscr{X}$ depending only on $n$.
\end{subprop}

\begin{subprop}\label{c1:prop-1.4.11}
For $f$ and $T$ as above and half-integral $k\geq n+1/2$, we have
\begin{equation*}
\sum_{\substack{\left(\begin{smallmatrix} A & B\\ C & D
    \end{smallmatrix}\right)=M\in \Gamma_{n,\infty}\backslash
    \Gamma\\ \det C=0}}\alpha(M)\ll (\min T)^{n-k}(\det
T)^{k-(n+1)/2}\tag{38}\label{c1:eq38} 
\end{equation*}
provided that $\det T\ll (\min T)^{n}$ and $\min T>\mathscr{X}>0$ as
in Proposition \ref{c1:prop-1.4.10}.
\end{subprop}

\begin{remarks*}
Since $(n+1-k)/2<0$ for $k\geq n+3/2$, the right hand side of
\eqref{c1:eq37} is of a strictly lower order than the term $(\det
T)^{k-(n+1)/2}$ occurring in the corresponding Fourier coefficient of
Siegel's genus invariant for $S>0$; therefore, for $\min T\gg 0$, we
have a truly asymptotic formula $r(S,T)$. We also note that the
condition $\det T\ll (\min T)^{n}$\pageoriginale in Proposition
\ref{c1:prop-1.4.11} is {\em not} necessary for the proof of \eqref{c1:eq37}. 
\end{remarks*}

\setcounter{sublemma}{11}
\begin{sublemma}\label{c1:lem-1.4.12}
For $M=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in \Gamma_{n}$ with $\det C\neq 0$ and real
$X={}^{t}X$, we have
$\Iim(M<X+iT^{-1}>)=(T[X+C^{-1}D]+T^{-1})^{-1}[C^{-1}]\leq
T[C^{-1}]$. Further $\beta(M)\neq \emptyset$ implies that $\min
(T[C^{-1}])\geq \sqrt{3}/2$.
\end{sublemma}

\begin{proof}
Indeed for 
$$
Z=X+iY\in\mathscr{G}_{n},
\Iim(M<Z>)={}^{t}(C\ob{Z}+D)^{-1}Y(CZ+D)^{-1}
$$
so that
\begin{align*}
(\Iim
(M<Z>))^{-1}&=(CX+D+iCY)Y^{-1}({}^{t}(CX+D)-iY{}^{t}C)\\
&=Y^{-1}[X{}^{t}C+{}^{t}D]+Y[{}^{t}C].
\end{align*}
Hence
\begin{align*}
\Iim(M<X+iT^{-1}>)&=(T[X+C^{-1}D]+T^{-1})^{-1}[C^{-1}]\leq
(T^{-1})^{-1}[C^{-1}]\\
&=T[C^{-1}]. 
\end{align*}

If $X_{1}\in \beta(M)$, then $M<X_{1}+iT^{-1}>\epsilon
\mathfrak{g}_{n}$ and hence $\min T[C^{-1}]\geq \min (\Iim
(M<X_{1}+iT^{-1}>))\geq \sqrt{3}/2$.

For given $M=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{n}$ with $\det C\neq 0$, we now
proceed to estimate the series
$\sum\limits_{S\in\Lambda}\alpha\left(M\left(\begin{smallmatrix} E_{n}
  & S\\ 0 & E_{n}
\end{smallmatrix}\right)\right)=\sum\limits_{S\in
  \Lambda}\alpha(C,D+CS)$. Applying Lemmas \ref{c1:lem-1.4.4} and
\ref{c1:lem-1.4.12}, we have $\sum\limits_{S\in\Lambda}\alpha(C,D+CS)$
\begin{align*}
&\ll
  \sum_{S\in\Lambda}\int\limits_{\beta\left(M\left(\begin{smallmatrix}
      E_{n} & S\\ 0 & E_{n} \end{smallmatrix}\right)\right)} (\abs
  (\det
  (C(X+iT^{-1})+D+CS))^{-k}\\
&\quad\exp(-\mathscr{X}\min((T[X+C^{-1}D+S]+T^{-1,-1}[C^{-1}]))dX\\
&\ll q^{n(n+1)/2}\int\limits_{\mathscr{S}_{n}}(\abs(\det
  (C(X+iT^{-1})))^{-k}\\
&\quad\exp(-\mathscr{X}\min((T[X]+T^{-1})^{-1}[C^{-1}]))dX 
\end{align*}
where the integration is now over the $n(n+1)/2$-dimensional space
$\mathscr{S}_{n}$ of all real\pageoriginale $X={}^{t}X$. For
$X\in\mathscr{S}_{n}$, we define $\Theta$ by $\Theta=T^{1/2}XT^{1/2}$
where $T^{1/2}$ is the unique positive definite square root of
$T$. Then $dX=(\det T)^{-(n+1)/2}d\Theta$ and 
\begin{align*}
& \sum_{S\in\Lambda}\alpha(C,D+CS)\ll(\det T)^{k-(n+1)/2}(\abs \det
C)^{-k}\int\limits_{\varphi_{n}}\\
& \qquad \qquad \det
(\Theta^{2}+E)^{-k/2}\exp(-\mathscr{X}\min(\Theta^{2}+E_{n})^{-1}|T^{\frac{1}{2}}C^{-1}])d\Theta 
\end{align*}
Writing 
\begin{align*}
\Theta&=
\begin{pmatrix}
w_{1} &\ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots & w_{n}
\end{pmatrix}
[V]\text{ \ with orthogonal \ } V^{(n,n)}\\
&=(v_{ij})\text{ \  and \ }
|w_{1}|\geq\ldots\geq|w_{n}|, 
\end{align*}
we have
\begin{gather*}
\Theta^{2}+E_{n}=
\begin{pmatrix}
w^{2}_{1}+1 & \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots & w^{2}_{n}+1
\end{pmatrix}
[V]\leq (1+w^{2}_{1})E_{n},\\
\det(\Theta^{2}+E_{n}) = \prod_{1\leq j\leq n}(1+w^{2}_{j}),
d\Theta=\prod\limits_{k<\ell}|w_{k}-w_{\ell}|dw_{1}\ldots dw_{n}d\mu
\end{gather*}
where $d\mu$ is the Haar measure on the orthogonal group
$\mathcal{O}(n)$ and $|w_{k}-w_{\ell}|\leq
(1+w^{2}_{k})^{1/2}(1+w^{2}_{\ell})^{1/2}$. Since the volume of
$\mathcal{O}(n)$ is finite, we see that the integral over
$\varphi_{n}$ above is 
\begin{align*}
&\ll \int\limits_{\mathbb{R}^{n}}\prod\limits_{1\leq j\leq
    n}(1+w^{2}_{j})^{-k/2}\exp(-\mathscr{X}(1+w^{2}_{1})^{-1}\min
  (T[C^{-1}]))\\
&\qquad\prod\limits_{1\leq j\leq
    n}(1+w^{2}_{j})^{(n-1)/2}dw_{1},\ldots dw_{n}\\
&\ll
  \int\limits^{\infty}_{\infty}(1+w^{2}_{1})^{-k/2+(n-1)/2}\exp(-\mathscr{X}(1+w^{2}_{1})^{-1}\min
  T[C^{-1}])dw_{1}\\
&\qquad\qquad\text{ \ since \ } k//-(n-1)/2>-1/2\\
&\ll (\min T[C^{-1}])^{(n-k)/2}\text{ \  noting that \ } \min
  T[C^{-1}]\geq \sqrt{3}/2,\\
&\qquad\qquad \text{ \ by Lemma \ref{c1:lem-1.4.12}.}
\end{align*}
Now
$$
\min(T[C^{-1}])=|\det C|^{-2}\min (T[(\det C)C^{-1}])\geq (\min
T)/|\det C|^{2}
$$
Hence we have
\begin{equation*}
\sum_{S\in \Lambda}|\alpha(C,D+CS)|\ll (\det T)^{k-(n+1)/2}
\begin{cases}
|\det C|^{-k}\\
|\det C|^{-n}(\min T)^{(n-k)/2}
\end{cases}\tag{39}
\end{equation*}
\end{proof}

\setcounter{proofofprop}{9}
\begin{proofofprop}\label{c1:proofofprop-1.4.10}
From\pageoriginale Lemma \ref{c1:lem-1.4.6} and \eqref{c1:eq39}, we have
{\fontsize{10}{12}\selectfont
\begin{align*}
&\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)=\sum_{\substack{M\in
    \Gamma_{n,\infty}\backslash \Gamma_{n}\\ \det C\neq
    0}}\alpha(M)=\sum_{P=C^{1}D\rm{mod} \;
  1} \; \sum_{S\in\Lambda}\alpha(C,D+CS)\\
&\ll (\det
T)^{k-(n+1)/2}\left\{\sum_{\substack{P={}^{t}P\in\mathscr{M}_{n}(Q)\rm{mod} \;
1\\ \house{P}<(\min T)^{1/2}}}{\house{P}}^{n}(\min
T)^{(n-k)/2}+\sum_{\substack{P={}^{t}P\in\mathscr{M}_{n}(Q)\rm{mod} \;
    1\\ \house{P}\geq (\min
    T)^{1/2}}}{\house{P}}^{k}\right\}\\
&\ll (\det T)^{k(n+1)/2}(\min T)^{(n+1-k)/2},
\end{align*}}\relax
applying Lemma \ref{c1:lem-1.4.9} with $u=(\min T)^{1/2}$ and $s=k-n(\geq
3/2)$, which proves \eqref{c1:eq37} and Proposition \ref{c1:prop-1.4.10}.
\end{proofofprop}

We proceed now to the proof of Proposition \ref{c1:prop-1.4.11}. By Lemma
\ref{c1:lem-1.4.4}, we have
\begin{equation*}
|\alpha(M)|=|\alpha(C,D)|\ll\int\limits_{\beta(M)}(\abs \det
((C(X+iT^{-1})+D))^{-k}dX\tag{39}\label{c1:eq39}
\end{equation*}
since for $X\in\beta(M)$, $\min(\Iim(M<X+iT^{-1}>))\geq
\sqrt{3}/2$. We should remark, however, that estimate \eqref{c1:eq39} is
rather crude and deserves to be improved with a better knowledge of
the geometry of $\mathcal{F}_{n}$, in order to obtain sharper
estimates for $a(T)$. Using the form of $C$, $D$ in Lemma
\ref{c1:lem-1.4.5} with $1\leq r<n$, we have
\begin{align*}
\abs(\det (C(X+iT^{-1})+D)) &= \abs
\det(C_{1}(X[F]+iT^{-1}[F])+D_{1})\\
&= |\det C_{1}||\det ((X[F]+iT^{-1}[F]+C^{-1}_{1}D_{1})|.
\end{align*}
Thus\pageoriginale
\begin{align*}
& \sum_{S={}^{t}S\in\Lambda_{r}}|\alpha\left(
\begin{pmatrix}
C_{1} & 0\\ 0 & 0
\end{pmatrix}
{}^tU, 
\begin{pmatrix}
D_{1}+C_{1}S & 0\\
0 & E_{n-r}
\end{pmatrix}
U^{-1}\right)|\ll\\
&\qquad (\det C_{1})^{-k}\sum_{S\in\Lambda_{r}}
\int\limits_{X\in \beta
\begin{pmatrix}
A_{1} & 0 & B_{1} & 0\\
A & E_{n-r} & 0 & 0\\
C_{1} & 0 & D_{1} & 0\\
0 & 0 & 0 & E_{n-r}
\end{pmatrix} \; 
\begin{pmatrix}
E_{n} & S & 0\\
 & 0 & 0\\
0 & & E_{n}
\end{pmatrix}
\left.
\begin{pmatrix}
{}^{t}U & 0\\
0 & U^{-1}
\end{pmatrix}
\right)
}\\
& \qquad |\det(X[F]+C^{-1}_{1}D_{1}+S+iT^{-1}[F])|^{-k}dX\\
&\qquad |\det
C_{1}|^{-k}\int\limits_{Q\in\bigcup\limits_{S\in\Lambda_{r}}(\mathfrak{t}[U]+
\left.
\begin{pmatrix}
S & 0\\
0 & 0
\end{pmatrix}
\right)}|\det|(Q_{1}+C^{-1}_{1}D_{1}+iT^{-1}[F])|^{-k}dQ\tag{40}\label{c1:eq40}
\end{align*}
under the change of variables $X\mapsto
Q:=X[U]=\left(\begin{smallmatrix} Q_{1}^{(r,r)} & Q_{2}\\ \ast & Q_{4}
\end{smallmatrix}\right)$, noting that
$dX=dQ$ and $Q_{1}=X[F]$. For a real symmetric $(r,r)$ matrix $S'$,
$\bigcup\limits_{S\in\Lambda_{r}}\left(\mathfrak{t}[U]+\left(\begin{smallmatrix}
  qS+S' & 0\\ 0 & 0
\end{smallmatrix}\right)\right)$ is a complete set of representations
of $\mathscr{S}_{n}$ modulo $\{q\left(\begin{smallmatrix} 0 &
  S_{2}\\ {}^{t}S_{2} & S_{4}\end{smallmatrix}\right)|S_{2}\in
\mathscr{M}_{r,n-r}(\mathbb{Z})$,
$S_{4}={}^{t}S_{4}\in\mathscr{M}_{n-r}(\mathbb{Z})\}$ and
$$
\{
\begin{pmatrix}
S_{1} & S_{2}\\
{}^{t}S_{2} & S_{4}
\end{pmatrix}
|S_{1}\in\varphi_{r},
S_{2}\in\mathscr{M}_{r,n-r}(\mathbb{R}/q\mathbb{Z}),
S_{4}={}^{t}S_{4}\in\mathscr{M}_{n-r}(\mathbb{R}/q\mathbb{Z})\}
$$
is another system of representatives. Suppose now that
\begin{align*}
Q&=
\begin{pmatrix}
Q^{(r,r)}_{1} & Q_{2}\\
{}^{t}Q_{2} & Q_{4}
\end{pmatrix}
\in \mathfrak{t}[U]+
\begin{pmatrix}
S & 0\\
0 & 0
\end{pmatrix}
\quad\text{and}\\
Q'&=
\begin{pmatrix}
Q_{1} & Q'_{2}\\
{}^{t}Q'_{2} & Q_{4}
\end{pmatrix}
\in \mathfrak{t}[U]+
\begin{pmatrix}
S' & 0\\
0 & 0
\end{pmatrix}
\end{align*}
with $S\equiv S'(\rm{mod} \; q)$, $Q_{2}\equiv Q'_{2}(\rm{mod} \; q)$ and
$Q_{4}\equiv Q'_{4}(\rm{mod} \; q)$. Then $Q-\left(\begin{smallmatrix} S &
  0\\ 0 & 0\end{smallmatrix}\right)$ and $Q'-\left(\begin{smallmatrix}
    S' & 0\\ 0 & 0  \end{smallmatrix}\right)$ are both in
  $\mathfrak{t}[U]$ and further are congruent modulo $q$. Since $U$ is
  in $GL_{n}(\mathbb{Z})$ and $\mathfrak{t}$ is the
  standard\pageoriginale cube with sides of length $q$, we have then
  necessarily $Q-\left(\begin{smallmatrix} S & 0\\ 0 & 0
  \end{smallmatrix}\right)=Q'-\left(\begin{smallmatrix} S' & 0\\ 0 & 0
  \end{smallmatrix}\right)$ implying that $Q_{2}=Q'_{2}$,
  $Q_{4}=Q'_{4}$ and $S=S'$. For any given $Q_{1}=X[F]$ and
  equivalence class in $\Lambda_{r}$ modulo $q$, $Q_{2}$, $Q_{4}$ run
  at most modulo $q$. Hence, after absorbing constants, we see that
  the expression in \eqref{c1:eq40} is 
\begin{align*}
&\ll [\Lambda_{r}:q\Lambda_{r}]q^{r(n-r)}q^{(n-r)(n-r+1)/2}|\det
C_{1}|^{-k}\\
&\qquad\int\limits_{Q_{1}={}^{t}Q_{1}\in\mathscr{M}_{r}(\mathbb{R})}|\det
(Q_{1}+iT^{-1}[F])^{-k}dQ_{1}\tag{41}\label{c1:eq41} 
\end{align*}
the integrand being now independent of $Q_{2}$ and $Q_{4}$. It is easy
to see again that the expression in \eqref{c1:eq41} is
\begin{align*}
&\ll |\det C_{1}|^{-k}(\det
  T^{-1}[F])^{(r+1)/2-k}\int\limits_{X_{1}={}^{t}X_{1}\in
    \mathscr{M}_{r}(\mathbb{R})}|\det (X_{1}+iE_{r})|^{-k}dX_{1}\\
&\ll |\det C_{1}|^{-k}(\det T^{-1}[F])^{(r+1)/2-k}.\tag{42}\label{c1:eq42}
\end{align*}

For fixed $r$ with $1\leq r<n$, we know (by Lemmas \ref{c1:lem-1.4.5} and
\ref{c1:lem-1.4.6}) that there exists a one - one correspondence
\begin{align*}
\Gamma_{\eta,\infty}\backslash \{M &=
\begin{pmatrix}
\ast & \ast\\
C & D
\end{pmatrix}
\in\Gamma_{n}|\rank C\\&
=r\}/\{
\left.
\begin{pmatrix}
E_{n} & \begin{pmatrix}
       S & 0\\
       0 & 0
\end{pmatrix}\\
0 & E_{n}
\end{pmatrix}
\right)
\in\Gamma_{n}\}\longleftrightarrow \{C^{-1}_{1}D_{1}\rm{mod} \; 1\},\{F\}
\end{align*}
where $C^{-1}_{1}D_{1}$ runs over a complete set of modulo $1$
incongruent $(r,r)$ rational symmetric matrices and $F^{(n,r)}$ over a
complete set of $(n,r)$ primitive matrices described in Lemma
\ref{c1:lem-1.4.5}. By assumption, $T\asymp \left(\begin{smallmatrix}
  t_{1} & \ldots & 0\\ \vdots & \ddots & \vdots\\ 0 & \ldots & t_{n}
\end{smallmatrix}\right)$ with $t_{i}:=t_{ii}$ and it is not hard to
see that 
$$(\det T^{-1}[F])\gg t^{-1}_{n}\ldots t^{-1}_{n-r+1}\det
E_{n}[F].$$

In fact, if $\left(\begin{smallmatrix} i_{1} & i_{2} & \ldots
  i_{r}\\ 1 & 2 & \ldots r\end{smallmatrix}\right)F$ is the
  determinant of the $(r,r)$ submatrix of $F$
formed\pageoriginale by the rows with indices $i_{1}$,
$i_{2},\ldots,i_{r}$, then
\begin{align*}
& \det T^{-1}F\gg \det
\begin{pmatrix}
t^{-1}_{1} & \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots & t^{-1}_{n}
\end{pmatrix}
[F]
=\sum_{1\leq i_{1}<i_{2}<\ldots i_{r}\leq n}\\
&=
\begin{pmatrix}
i_{1} & i_{2} & \ldots & i_{r}\\
1 & 2 & \ldots & r
\end{pmatrix}^{2}_{F}t^{-1}_{i_{1}}\ldots t^{-1}_{i_{r}}\\
&\gg t^{-1}_{n}\ldots t^{-1}_{n-r+1}\sum 
\begin{pmatrix}
i_{1} & i_{2} & \ldots & i_{r}\\
1 & 2 & \ldots & r
\end{pmatrix}^{2}_{F}=t^{-1}_{n}\ldots t^{-1}_{n-r+1}\det E_{n}[F].
\end{align*}
Using now the estimate \eqref{c1:eq42}, we conclude that
\begin{align*}
\sum_{S\in\Lambda_{r}}&|\alpha\left(
\begin{pmatrix}
C^{(r)}_{1} & 0\\
0 & 0
\end{pmatrix}
t_{U},
\begin{pmatrix}
D_{1}+C_{1}S & 0\\
0 & E_{n-r}
\end{pmatrix}
U^{-1}
\right)\\
&|\ll |\det C_{1} |^{-k}(t^{-1}_{n}\ldots t^{-1}_{n-r+1})^{\frac{r+1}{2}-k}\det (E_{n}[F])^{\frac{r+1}{2}-k}
\end{align*}

From the last estimate and the one - one correspondence referred to in
the preceding paragraph, it follows at once that 
\begin{align*}
M &= \sum_{
\left(
\begin{smallmatrix}
\ast & \ast\\
C & D
\end{smallmatrix}\right)\in \Gamma_{n,\infty}\backslash \Gamma_{n},\rank C=r}\alpha(M)
\ll \sum_{R={}^{t}R\in\mathscr{M}_{r}(\mathbb{Q})\rm{mod} \;
  1}{\house{R}}^{-k}\\
&\quad \sum_{\substack{F\in\mathscr{M}_{n,r}(\mathbb{Z})/GL_{r}\mathbb{Z}\\ F
    \text{ primitive}}}(\det E_{n}[F])^{\frac{r+1}{2}-k}(t_{n}\ldots
  t_{n-r+1})^{k-\frac{r+1}{2}}.\tag{43}\label{c1:eq43}
\end{align*}
The representatives $F$ in the summation in \eqref{c1:eq43} can be
assumed to have been chosen already to satisfy the condition that
$E_{n}[F]$ is $M$-reduced. If $F=(f_{1}\ldots f_{r})$, then, by Lemma
\ref{c1:lem-1.3.2}, $\det E_{n}[F]\gg \prod\limits_{1\leq i\leq
  r}E_{n}|f_{i}|$. Since $(r+1)/2-k\leq n/2-k<0$, we have
\begin{equation*}
\sum_{F}(\det E_{n}[F])^{(r+1)/2-k}\ll \left\{\sum_{0\neq
  x\in\mathbb{Z}^{n}}(E_{n}[x])^{(r+1)/2-k}\right\}^{r}.\tag{44}\label{c1:eq44} 
\end{equation*}
If $x_{1}\ldots x_{n}\neq 0$, then $\sum\limits_{1\leq i\leq
  n}x^{2}_{i}\geq n|x_{1}\ldots x_{n}|^{2/n}$. Therefore the series
over $x$ on the right hand side of \eqref{c1:eq44} is $\ll \sum_{1\leq
  s\leq n}\sum\limits_{y_{i}\in\mathbb{Z}\backslash
  \{0\}}(y^{2}_{1}+\cdots+y^{2}_{s})^{-(k-\frac{r+1}{2})}\ll
\sum\limits_{1\leq s\leq n}\zeta(2\{k-(r+1)/2)/s)\ll 1$\pageoriginale
since $k-(r+1)/2>n/2$ for $r<n$, in view of the hypothesis $k\geq
n+1/2$. Thus the series over $F$ in \eqref{c1:eq44} is $\ll 1$. On the
other hand, since $k-r>1$ for $r\leq n$, we can apply Lemma
\ref{c1:lem-1.4.9} to conclude that $\sum\limits_{R={}^{t}R\in
  \mathscr{M}_{r}(\mathbb{Q})\rm{mod} \; 1}{\house{R}}^{-k}\ll 1$. So
we finally see that the left hand side of \eqref{c1:eq43} is
{\fontsize{10}{12}\selectfont
$$
\ll (t_{n}\ldots t_{n-r+1})^{k-(r+1)/2}=(\det
T)^{k-(n+1)/2}(t_{1}\ldots t_{n-r})^{\frac{n+1}{2}-k}(t_{n-r+1}\ldots t_{n})^{\frac{n-r}{2}}.
$$}\relax  
We now use the assumption that $(\det T)\ll (\min T)^{n}$ for the
$M$-reduced $T\asymp \left(\begin{smallmatrix} t_{1} & \ldots &
  0\\ \vdots & \ddots & \vdots\\ 0 & \ldots & t_{n}
\end{smallmatrix}\right)$. Then $t_{1}\asymp t_{2}\asymp \ldots \asymp
t_{n}\asymp t$, say. 

For $1\leq r\leq n-1$, $(\dfrac{n+1}{2}-k)(n-r)+\dfrac{n-r}{2}\cdot
r\leq n-k$ with equality taking place when $r=n-1$. Thus
$$
(t_{1}\ldots t_{n-r})^{\frac{n+1}{2}-k}(t_{n-r+1}\ldots
t_{n})^{\frac{n-r}{2}}\asymp
t^{(\frac{n+1}{2}-k)(n-r)+\frac{n-r}{2}\cdot r}\leq t^{n-k}
$$
and the left hand side of \eqref{c1:eq43} is $\ll (\det
T)^{k-\frac{n+1}{2}}(\min T)^{n-k}$ for $1\leq r\leq n-1$. Summing
over \eqref{c1:eq43} for $1\leq r\leq n-1$, Proposition \ref{c1:prop-1.4.11}
is immediate. In view of the remarks preceding Lemma \ref{c1:lem-1.4.1}.,
we have the following theorem (and Theorem C in the Introduction) as
an immediate consequence of Proposition \ref{c1:prop-1.4.10} and
\ref{c1:prop-1.4.11}.

\setcounter{subtheorem}{12}
\begin{subtheorem}[\cite{key10},\cite{key19}]\label{c1:thm-1.4.13}
If $k=n+3/2$ and $f(Z)=\sum\limits_{0\leq
  T\in\Lambda^{\ast}}a(T)\break e(\tr(TZ)/q)$ is a Siegel modular form of
  degree $n$, weight $k(\in 1/2\mathbb{Z})$, level $q$ and with
  constant term vanishing at all cusps, then
$$
a(T)=O((\min T)^{(n+1-k)/2}(\det T)^{k-(n+1)/2})
$$
provided\pageoriginale that $\min T\geq \mathscr{X}_{1}(\det T)^{1/n}$
and $\min T\geq \mathscr{X}_{2}>0$ for constants $\mathscr{X}_{1}$,
$\mathscr{X}_{2}$ independent of $f$ (but depending only on $n$).
\end{subtheorem}

\begin{remarks*}
The condition $\min T\geq (\det T)^{1/n}$ seems unavoidable for
general $n$. The next theorem giving an estimate for coefficients of
modular forms of degree $2$, weight $k\geq 7/2$ and level $q$
vanishing at all cusps imposes no such condition. Sunder Lal (Math.\@
Zeit.\@ 88 (1965), 207-243) has considered an analogue of Theorem
\ref{c1:thm-1.4.13} for the Hilbert-Siegel modular forms.
\end{remarks*}

For any $m$-rowed integral $S>0$, the associated theta series
$f(Z)=\sum\limits_{G}e(\tr(S[G]Z))$ is a modular form of degree $n$,
weight $m/2$ and level 4 $\det S$ and $f(Z)-\varphi(Z)$ vanished at
every cusp, if we take $\varphi(Z)$ to be the analytic genus invariant
associated with $S$. The Fourier coefficients $b(T)$ of $\varphi(Z)$
are of the form $\ast(\det T)^{\frac{m-n-1}{2}}\times
\prod\limits_{p}\alpha_{p}(S,T)$ where
$\prod\limits_{p}\alpha_{p}(S,T)$ is the product of the $p$-adic
densities of representation of $T$ by $S$. Thus, for $m\geq 2n+3$ and
$\min T\gg (\det T)^{1/n}$, we have from Theorem \ref{c1:thm-1.4.13} an
asymptotic formula for $r(S,T)$:
$$
r(S,T)=\ast(\det T)^{\frac{m-n-1}{2}}\prod_{p}\alpha_{p}(S,T)+O\left((\det
T)^{\frac{m-n-1}{2}}(\min (T))^{\frac{2n+2-m}{4}}\right)
$$

For the case $n=2$, we have an improved version of Proposition
\ref{c1:prop-1.4.11}, and even {\em not} involving the unsatisfactory
condition $(\det T)\ll (\min (T))^{2}$ namely 

\setcounter{subprop}{13}
\begin{subprop}\label{c1:prop-1.4.14}
For $f$, $T$ as above with $\min T>\mathscr{X}$ (an absolute constant
independent of $f$) and $n=2$,
\begin{align*}
\sum_{\substack{M=\left(\begin{smallmatrix} A & B\\ C & D
    \end{smallmatrix}
\right)\in\Gamma_{2,\infty}\backslash \Gamma_{2}\\ \rank
C=1}}\alpha(M) &\ll (\min(T))^{2-k}(\det T)^{k-3/2}\\
&
\begin{cases}
1 \text{ if } k\geq 7/2\\
\log (\sqrt{\det T}/\min (T))\text{ if } k=3\\
((\det T)^{1/2}/\min (T))^{1/2}\text{ if } k=\frac{5}{2}
\end{cases}
\end{align*}
\end{subprop}

As\pageoriginale immediate consequences of the foregoing, we have

\setcounter{subtheorem}{14}
\begin{subtheorem}[\cite{key10}]\label{c1:thm-1.4.15}
Let $f(Z)=\sum\limits_{0\leq T\in\Lambda}\ast a(T)e(\tr(TZ)/q)$ be a
Siegel modular form of degree $2$, weight $k\geq 7/2$ (with $2k\in
\mathbb{Z}$), level $q$ and with constant term vanishing at all
cusps. Then for $T>0$ and $\min T>\mathscr{X}$ (an absolute constant
independent of $f$), we have,
$$
a(T)=O((\min T)^{(3-k)/2}(\det T)^{k-3/2})
$$
\end{subtheorem}

\begin{coro*}
If $A^{(m)}>0$, $B^{(2)}>0$ and if $A[X]=B$ is solvable with $X$
having entries in $\mathbb{Z}_{p}$ for every prime $p$, then for large
$\min (B)$ and $m\geq 7$, $A[X]=B$ has a solution $X$ with entries in
$\mathbb{Z}$. 
\end{coro*}

The proof of Proposition \ref{c1:prop-1.4.14} has to be preceded by
several lemmas.

\begin{defi*}
For given $T>0$ and $C=\left(\begin{smallmatrix} c & 0\\ 0 & 0
\end{smallmatrix}\right)^{t_{U}}$ with $U=\left(\begin{smallmatrix}
  f_{1} & \\ & \ast\\ f_{2} & \end{smallmatrix}\right)\in
GL_{2}(\mathbb{Z})$ and $c\neq 0$ in $\mathbb{Z}$, let
$a_{1}:=T^{-1}\left[\left(\begin{smallmatrix} f_{1}\\ f_{2}
  \end{smallmatrix}\right)\right]$ and
{\fontsize{10}{12}\selectfont
$$
P = P(x_{1},x_{2})=P_{T,U,c} (x_{1},x_{2}):=
\begin{pmatrix}
(a_{1}+x^{2}_{1}/a_{1})^{-1} & \\
0 & 1/(a_{1}\det T)
\end{pmatrix}
\left[
\begin{pmatrix}
1/c & x_{2}\\
0 & 1
\end{pmatrix}
\right]
$$}
\end{defi*}

\setcounter{sublemma}{15}
\begin{sublemma}\label{c1:lem-1.4.16}
For $M=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in \Gamma_{2}$ with
$C=\left(\begin{smallmatrix} c & 0\\ 0 & 0
\end{smallmatrix}\right){}^{t},D=\left(\begin{smallmatrix} d & 0\\ 0 &
  1 \end{smallmatrix}\right)U^{-1}$, $c\neq 0$ in $\mathbb{Z}$, $U$ in
$GL_{2}(\mathbb{Z})$, and $T\in\mathscr{P}_{2}$, we have
$$
\Iim(M<X+iT^{-1}>) = P(q_{1}+d/c,a_{2}(q_{1}+d/c)/a_{1}-q_{2})
$$
where $a_{1}$, $a_{2}$, $q_{1}$, $q_{2}$ are given by
$T^{-1}[U]=\left(\begin{smallmatrix} a_{1} & a_{2}\\ a_{2} & a_{4}
\end{smallmatrix}\right)$ and $X[U]=\left(\begin{smallmatrix}
q_{1} & q_{2}\\ q_{2} & q_{4}\end{smallmatrix}\right)$. 
\end{sublemma}

\begin{proof}
We\pageoriginale know that $(\Iim
(M<X+iT^{-1}>))^{-1}=T\{{}^{t}(CX+D+iCT^{-1})\}$ (using the
abbreviation $A\{B\}$ for ${}^{t}\ob{B}AB)
=T[{}^{t}(CX+D)]+T^{-1}[{}^{t}C]$
$$
=(\det T)
\begin{pmatrix}
a_{4}(cq_{1}+d)^{2}-2a_{2}cq_{2}(cq_{1}+d)+a_{1}c^{2}q^{2}_{2} &
\ast\\
-a_{2}(cq_{1}+d)+a_{1}cq_{2} & a_{1}
\end{pmatrix}
+
\begin{pmatrix}
a_{1}c^{2} & 0\\
0 & 0
\end{pmatrix}
$$
This is, on the other hand, the same as
{\fontsize{10}{12}\selectfont
\begin{align*}
& p^{-1}(q_{1}+c^{-1}d, a^{-1}_{1}a_{2}(q_{1}+c^{-1}d)-q_{2})\\
&\qquad 
\begin{pmatrix}
a_{1}+(q_{1}+d/c)^{2}/a_{1} & 0\\
0 & a_{1}\det T
\end{pmatrix}
\left[
\begin{pmatrix}
c & 0\\
cq_{2}-a^{-1}_{1}a_{2}(cq_{1}+d) & 1
\end{pmatrix}
\right]\\
&\qquad 
\begin{pmatrix}
(a_{1}+a^{-1}_{1}(q_{1}+d/c)^{2})c^{2}+a_{1}(\det
  T)(cq_{2}-a^{-1}_{1}a_{2}(cq_{1}+d))^{2} & \ast\\
a_{1}\det T(cq_{2}-a^{-1}_{1}a_{2}(cq_{1}+d)) & a_{1}\det T
\end{pmatrix}
\end{align*}}
\end{proof}

\begin{sublemma}\label{c1:lem-1.4.17}
With notation as in Lemma \ref{c1:lem-1.4.16},
$U=\left(\begin{smallmatrix} f_{1} & \ast\\ f_{2} & \ast
\end{smallmatrix}\right)$ and $M$-reduced $T=\left(\begin{smallmatrix}
t_{1} & 0\\ 0 &
t_{2} \end{smallmatrix}\right)\left[\left(\begin{smallmatrix} 
1 & u\\ 0 & 1  \end{smallmatrix}\right)\right]$, we have
$|c|f^{2}_{2}\leq (8/3)\sqrt{t_{2}/t_{1}}$ and $|f_{1}f_{2}c|\leq
4/3$, whenever $\min (P)\geq \sqrt{3}/2$; moreover, under this
condition, $|f_{1}|=0$ or $1$ and if $|f_{1}|=1$, then $|c|\leq 3$. 
\end{sublemma}

\begin{proof}
Since\pageoriginale
\begin{align*}
T &= 
\begin{pmatrix}
t_{1} & t_{1}u\\
t_{1}u & t_{1}u^{2}+t_{2}
\end{pmatrix}
, T^{-1}-\frac{1}{2}
\begin{pmatrix}
t^{-1}_{1} & 0\\
0 &  t^{-1}_{2}
\end{pmatrix}\\
& =\frac{1}{t_{1}t_{2}}
\begin{pmatrix}
t_{1}u^{2}+t_{2} & -t_{1}u\\
-t_{1}u & t_{1}
\end{pmatrix}
-\frac{1}{2}
\begin{pmatrix}
t^{-1}_{1} & 0\\
0 & t^{-1}_{2}
\end{pmatrix}\\
&= 
\begin{pmatrix}
u^{2}/t_{2}+1/(2t_{1}) & -u/t_{2}\\
-u/t_{2} & 1/(2t_{2})
\end{pmatrix}
\end{align*}
has determinant
$$
\frac{1}{4t^{2}_{2}}\left[\frac{t_{2}}{t_{1}}-2u^{2}\right]\geq
\frac{1}{4t^{2}_{2}}\left[\frac{3}{4}-\frac{1}{2}\right]>0 
$$
since $t_{1}\leq \dfrac{4}{3}t_{2}$ and $|u|\leq 1/2$. Hence
$T^{-1}>\dfrac{1}{2}
\left(\begin{pmatrix}
1/t_{1} & 0\\
0 &  1/t_{2}
\end{pmatrix}\right)$. On the other hand, since $P\in
\mathscr{P}_{2}$, $(\min (P))^{2}\leq (4/3)\det P$. If then
$\min(P)\geq \sqrt{3}/2$, we have
\begin{align*}
(3/4)^{2} &= (3/4)(3/4)\leq (3/4)\cdot (\min (P))^{2}\leq \det P\\
 &= \det P=1/\{c^{2}\det T(a^{2}_{1}+x^{2}_{1})\}\leq
  1/(a^{2}_{1}c^{2}\det T).
\end{align*}
But 
$$
a_{1}=T^{-1}
\left[
\begin{pmatrix}
f_{1}\\
f_{2}
\end{pmatrix}
\right]>1/2\left(\frac{1}{t_{1}}f^{2}_{1}+\frac{1}{t_{2}}f^{2}_{2}\right)
$$
and as a result, we have
\begin{align*}
\frac{1}{4}\left(2\sqrt{\dfrac{f^{2}_{1}}{t_{1}}\dfrac{f^{2}_{2}}{t_{2}}}\right)^{2}c^{2}t_{1}t_{2}
&\leq
\frac{1}{4}\left(\frac{1}{t_{1}}f^{2}_{1}+\frac{1}{t_{2}}f^{2}_{2}\right)^{2}c^{2}t_{1}t_{2}\tag{45}\label{c1:eq45}\\
&\leq (4/3)^{2} 
\end{align*}
\ie $c^{2}f^{2}_{1}f^{2}_{2}\leq (4/3)^{2}$ and so
$$
|f_{1}f_{2}|\leq |cf_{1}f_{2}|\leq 4/3.
$$
Hence if $f_{1}f_{2}\neq 0$, $|f_{1}|=|f_{2}|=1$. If $f_{1}f_{2}=0$,
then (since $U$ is in $GL_{2}(\mathbb{Z})$), either $f_{1}=0$,
$f_{2}=1$ or $f_{1}=1$, $f_{2}=0$ (taking only one primitive column
from each class). From \eqref{c1:eq45}, we have
$$
\frac{1}{4}\left(\frac{1}{t_{2}}f^{2}_{2}\right)^{2}c^{2}t_{1}t_{2}\leq
\frac{1}{4}\left(\frac{1}{t_{1}}f^{2}_{1}+\frac{1}{t_{2}}f^{2}_{2}\right)^{2}c^{2}t_{1}t_{2}\leq (4/3)^{2}
$$
which\pageoriginale gives us $c^{2}f^{4}_{2}\leq
4(4/3)^{2}(t_{2}/t_{1})$ \ie $|cf^{2}_{2}|\leq
(8/3)\sqrt{t_{2}/t_{1}}$. If $|f_{1}|=1=|f_{2}|$, then
$(1\leq)c^{2}=c^{2}f^{2}_{1}f^{2}_{2}\leq (4/3)^{2}$ implies
$|c|=1$. If $|f_{1}|=1$ and $f_{2}=0$, then from \eqref{c1:eq45}, we get
$c^{2}f^{4}_{1}t_{2}/t_{1}\leq 4(4/3)^{2}$ \ie $c^{2}\leq
4(4/3)^{2}(t_{1}/t_{2})\leq 4(4/3)^{3}<2^{4}$ \ie $|c|\leq 3$. This
proves all the assertions of our lemma.
\end{proof}

\begin{remarks*}
\begin{enumerate}
\renewcommand{\labelenumi}{\theenumi)}
\item Under the conditions of Lemma \ref{c1:lem-1.4.17}, the number of
  $U$ coming into play is at most $4$, namely
  $U=\left(\begin{smallmatrix} 0 & -1\\ 1 & 0
\end{smallmatrix}\right)$ if $f_{1}=0$, $U=\left(\begin{smallmatrix} 1
  & 0\\ n & 1\end{smallmatrix}\right)$ with $n\in\mathbb{Z}$ and
  $|n|\leq 1$ if $f_{1}\neq 0$ (\ie $f_{1}=1$). Whenever
  $|f_{1}f_{2}|=1$, we have $c=1$.

\item For $P$ as in Lemma \ref{c1:lem-1.4.17}, if $\sqrt{3}/2\leq
  \min(P)=P\left[\left(\begin{smallmatrix} b_{1}
      \\ b_{2}\end{smallmatrix}\right)\right]$ for some integral column
  ${}^{t}(b_{1}b_{2})$, we claim that $b_{2}\neq 0$. Otherwise, we can
  take $b_{1}=1$ and then
  $\min(P)=P[\binom{1}{0}]=1/(c^{2}(a_{1}+a^{-1}_{1}x^{2}_{1}))\leq
  (2/\sqrt{3})(\det
  P)^{1/2}=(2/\sqrt{3})/((a_{1}+a^{-1}_{1}x^{2}_{1})c^{2}a_{1}\det
  T)^{1/2}\leq \dfrac{2}{\sqrt{3}}\break(\min P/(a_{1}\det T))^{1/2}$ \ie
  $\sqrt{3}/2)^{1/2}\leq (\min (P))^{1/2}\leq (2/\sqrt{3})/(a_{1}\break\det
  T)^{1/2}$ so that $a_{1}\det T\leq 8/(3\sqrt{3})$. Together with the
  inequality
  $a_{1}>\dfrac{1}{2}\left(\frac{1}{t_{1}}f^{2}_{1}+\frac{1}{t_{2}}f^{2}_{2}\right)$
  derived in the course of the proof of Lemma \ref{c1:lem-1.4.17}, this
  leads us to
  $1/2(t_{2}f^{2}_{1}+t_{1}f^{2}_{2})<8/(3\sqrt{3})$. Since either
  $f_{1}$ or $f_{2}$ is different from $0$, we have
  $\min\left(\frac{1}{2} t_{1},\dfrac{1}{2}t_{2}\right)<8/(3\sqrt{3})$
  which contradicts $t_{1}$ and $t_{2}$ being sufficiently large (in
  view of $\min T\gg 0$, by assumption). This contradiction shows that
  when $\sqrt{3}/2\leq \min P=P\left[\left(\begin{smallmatrix}
      b_{1}\\ b_{2}\end{smallmatrix}
    \right)\right]$, $b_{2}\neq 0$. 


To any $\sigma=\left(\begin{smallmatrix} a & b\\ c & d
\end{smallmatrix}\right)$ in $SL_{2}(\mathbb{Z})$, let us associate
$\tilde{\sigma}=\left(\begin{smallmatrix} a & 0 & b & 0\\
0 & 1 & 0 & 0\\ c & 0 & d & 0\\ 0 & 0 & 0 & 1
\end{smallmatrix}\right)$ in\pageoriginale
$\Sp(2,\mathbb{Z})=\Gamma_{2}$.
Then $\sigma\mapsto \tilde{\sigma}$ is an injective
homomorphism. If $c\neq 0$, then $\sigma=\left(\begin{smallmatrix}  1
  & a/c \\ 0 & 1\end{smallmatrix}\right)\left(\begin{smallmatrix} 0 &
    -c^{-1}\\ c & d  \end{smallmatrix}\right)$. In this case, we have
  for $Z=\left(\begin{smallmatrix} z_{1} & z_{2}\\ z_{2} & z_{4}
  \end{smallmatrix}\right)$,
\begin{equation*}
\tilde{\sigma}<Z>=
\begin{pmatrix}
a/c & 0\\
0 & 0
\end{pmatrix}+
\begin{pmatrix}
-1/(c(cz_{1}+d)) & z_{2}/(cz_{1}+d)\\
z_{2}/(cz_{1}+d) & z_{4}-cz^{2}_{2}/(cz_{1}+d)
\end{pmatrix}\tag{46}\label{c1:eq46}
\end{equation*}
by straightforward verification.
\end{enumerate}
\end{remarks*}

Let $\sigma=\left(\begin{smallmatrix} a & b\\ c & d
\end{smallmatrix}\right)\in SL_{2}(\mathbb{Z})$ with $c\geq 1$ and
$U\in SL_{2}(\mathbb{Z})$. The following lemma gives an estimate for
$\alpha\left(\left(\begin{smallmatrix} c & 0\\ 0 & 0
\end{smallmatrix}\right)\right){}^{t}U,\left.\left(\begin{smallmatrix} d &
  0\\ 0 & 1
\end{smallmatrix}\right)U^{-1}\right)$ needed in connection with
Proposition \ref{c1:prop-1.4.14}.

\begin{sublemma}\label{c1:lem-1.4.18}
Let $\sigma$, $U$ be as above and let
$A=T^{-1}[U]=\left(\begin{smallmatrix} a_{1} & a_{2}\\ a_{2} & a_{4}
\end{smallmatrix}\right)$. For given
$\Theta:=\left(\begin{smallmatrix} \theta_{1} &
  \theta_{2}\\ \theta_{2} & \theta_{4}
\end{smallmatrix}\right)$, $A=T^{-1}[U]$ and
$C=\left(\begin{smallmatrix} c & 0\\ 0 & 0
\end{smallmatrix}\right){}^{t}U$, let
{\fontsize{10}{12}\selectfont
$$
\tau=\tau(\Theta,A,C):=
\begin{pmatrix}
-c^{-2}/(\theta_{1}+ia_{1}) &
c^{-1}(\theta_{2}+ia_{2})/(\theta_{1}+ia_{1})\\
c^{-1}(\theta_{2}+ia_{2})/(\theta_{1}+ia_{1}) &
\theta_{4}+ia_{4}-(\theta_{2}+ia_{2})^{2}/(\theta_{1}+ia_{1}) 
\end{pmatrix}
$$}
Then
\begin{align*}
& |\alpha\left(
\begin{pmatrix}
c & 0\\
0 & 0
\end{pmatrix}
{}^{t}U,
\begin{pmatrix}
d & 0\\
0 & 1
\end{pmatrix}
U^{-1}\right)|
\ll c^{-k}\int\limits_{\substack{\left(\begin{smallmatrix} a/c & 0\\ 0
      & 0    \end{smallmatrix}\right)+\tau
    \in\mathfrak{g}_{2}\\ \Theta\in\mathfrak{t}[U]+\left(\begin{smallmatrix}
      d/c & 0\\ 0 &
      0    \end{smallmatrix}\right)}}(\theta^{2}_{1}+a^{2}_{1})^{-k/2}\\
&\exp(-\mathscr{X}\min(P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2}))d\theta_{1}d\theta_{2}d\theta_{4}. 
\end{align*}
\end{sublemma}

\begin{proof}
In\pageoriginale view of Lemma \ref{c1:lem-1.4.4} for
$M=\tilde{\sigma}\left(\begin{smallmatrix} {}^{t}U & 0\\ 0 &
  U^{-1}
\end{smallmatrix}\right)=\left(\begin{smallmatrix} \ast & \ast \\ C &
  D
\end{smallmatrix}\right)$ in $\Gamma_{2}$, we see, on taking
$\Theta=X[U]+\left(\begin{smallmatrix} c^{-1}d & 0\\ 0 & 0
\end{smallmatrix}\right)=\left(\begin{smallmatrix} q_{1} +c^{-1}d &
  q_{2}\\ q_{2} & q_{4}\end{smallmatrix}\right)$ and noting
$dX=d\Theta(:=d\theta_{1}d\theta_{2}d\theta_{4})$,
$\Iim(M<X+iT^{-1}>)=
P_{T,U,c}(q_{1}+c^{-1}d,a^{-1}_{1}a_{2}(q_{1}+c^{-1}d)-q_{2})=P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2})$
(by Lemma \ref{c1:lem-1.4.16}) and $\abs \det (C(X+iT^{-1})+D)=\abs
(c\theta_{1}+cia_{1})$, that
\begin{align*}
|\alpha(M)|&=|\alpha(C,D)|\ll c^{-k}\int
(\theta^{2}_{1}+a^{2}_{1})^{-k/2}\exp(-\mathscr{X}\\
&\qquad\min(P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2})))d\Theta
\end{align*}
the domain of integration for $\Theta$ corresponding to $\beta(M)$ for
$X$ under $X\mapsto \Theta$. But
$M<X+iT^{-1}>=\tilde{\sigma}<\Theta-\left(\begin{smallmatrix}
  c^{-1}d & 0\\ 0 &
  0\end{smallmatrix}\right)+iT^{-1}[U]>=\left(\begin{smallmatrix} a/c
    & 0\\ 0 & 0  \end{smallmatrix}\right)+\tau$, by \eqref{c1:eq46} and
  so the lemma is proved.
\end{proof}

For $\sigma=\left(\begin{smallmatrix} a & b\\ c & d
\end{smallmatrix}\right)$ in $SL_{2}(\mathbb{Z})$ with $c\geq 1$ and
$U$ equal to one of the four matrices
$$
\begin{pmatrix}
0 & -1\\
1 & 0
\end{pmatrix}
\quad\text{or}\quad
\begin{pmatrix}
1 & 0\\
n & 1
\end{pmatrix}
\quad\text{with}\quad
n=0, 1\text{ \ or \ }-1,
$$
let
$$
\mathscr{R}^{\ast}(U):=\bigcup_{m\in\mathbb{Z}}\left\{\mathfrak{t}(U)+
\begin{pmatrix}
c^{-1}d+mq & 0\\ 
0 & 0
\end{pmatrix}\right\}=\mathscr{S}_{2}/
\left\{
\begin{pmatrix}
0 & s_{2}\\
s_{2} & s_{4}
\end{pmatrix}|s_{2},s_{4}\in q\mathbb{Z}\right\}
$$

An\pageoriginale application of Lemma \ref{c1:lem-1.4.18} with $d_{1}$
($\equiv d$ modulo $q$, for a fixed $d$) in place of $d$, leads to

\begin{sublemma}\label{c1:lem-1.4.19}
For $\sigma$, $U$ as above and $f$, $T$ as in Proposition
\ref{c1:prop-1.4.14}, we have 
\begin{align*}
& \sum\limits_{d_{1}\equiv d(\rm{mod} \; cq)}|\alpha
\left(\left(\begin{smallmatrix} c & 0\\ 0 & 0
\end{smallmatrix}\right){}^{t}U,\left(\begin{smallmatrix} d_{1} &
  0\\ 0 & 1
\end{smallmatrix}\right)U^{-1}\right)|\\
& \ll c^{-k}\int\limits_{\substack{\left(\begin{smallmatrix} a_{c} & 0\\ 0 &
      0
    \end{smallmatrix}\right)+\tau
    \in\mathfrak{g}_{2}\\ \theta_{1}\in\mathbb{R}, 0\leq
    \theta_{2},\theta_{4}<q}}
(\theta^{2}_{1}+a^{2}_{1})^{-k/2}\exp(-\mathscr{X}\min
(P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2})))d\Theta 
\end{align*}
\end{sublemma}

\begin{proof}
We need only to note that $A:=T^{-1}[U]=\left(\begin{smallmatrix}
  a_{1} & a_{2}\\ a_{2} & a_{4}
\end{smallmatrix}\right)$, $\tau=\tau(\Theta, A,c)$ and
$P=P_{T,U,c}(x_{1},x_{2})$ are all independent of $d$, taking an
extension $\left(\begin{smallmatrix} a & \ast\\ c & d_{1}
\end{smallmatrix}\right)$ of $(c \; d_{1})$ to $SL_{2}(\mathbb{Z})$ and
that
$\min(P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-(\theta_{2}+qn)))=\min
(P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2})\left[\left(\begin{smallmatrix}
    1 & cn\\ 0 & 1
  \end{smallmatrix}\right)\right])=\min(P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2}))$
for every $n\in\mathbb{Z}$.
\end{proof}

Before we begin the proof of Proposition \ref{c1:prop-1.4.14}, we note
that, for $\Theta=\left(\begin{smallmatrix} \theta_{1} &
  \theta_{2}\\ \theta_{2} & \theta_{4}
\end{smallmatrix}\right)$ in the domain of integration referred to in
Lemma \ref{c1:lem-1.4.19}, we have
$P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2})=\Iim
\tau(=\Iim(M<X+iT^{-1}>)$, by Lemma \ref{c1:lem-1.4.16}) $\geq
\sqrt{3/2}$. Hence, by Remark 2 following Lemma \ref{c1:lem-1.4.17}, we
have $\min
(P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2}))=P[\binom{b_{1}}{b_{2}}]$
for an integral column ${}^{t}(b_{1}b_{2})$ with $b_{2}\neq 0$. Thus,
we can remove the condition $\left(\begin{smallmatrix} a/c & 0\\ 0 & 0
\end{smallmatrix}\right)+\tau \in\mathfrak{g}_{2}$ on the domain of
integration for $\Theta$ in Lemma \ref{c1:lem-1.4.19}, if we majorize
$\exp(-\mathscr{X}\min
(P(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2})))$ by the series
$$
\sum_{0\neq b_{2},
  b_{1}\in\mathbb{Z}}\exp(-\mathscr{X}p(\theta_{1},a^{-1}_{1}a_{2}\theta_{1}-\theta_{2})\left[\begin{smallmatrix}
    b_{1}\\ b_{2}  \end{smallmatrix}\right]).
$$

\setcounter{proofofprop}{13}
\begin{proofofprop}\label{c1:proofofprop-1.4.14}
In\pageoriginale the light of the preceding paragraph, we see that
\begin{align*}
& \sum_{d_{1}\equiv d(\rm{mod} \; cq)}|\alpha\left(
\begin{pmatrix}
c & 0\\
 0 & 0
\end{pmatrix}{}^{t}U,
\begin{pmatrix}
d_{1} & 0\\
0 & 0
\end{pmatrix}
U^{-1}\right)|\ll c^{-k}\\
&\sum_{0\neq
  b_{2},b_{1}\in\mathbb{Z}}\exp(-\mathscr{X}a^{-1}_{1}b^{2}_{2}/\det
T)\int\limits_{\theta_{1}\in\mathbb{R},0\leq
  \theta_{2},\theta_{4}<q}(\theta^{2}_{1}+a^{2}_{1})^{-k/2}\times\\
&\times \exp(-\mathscr{X}\frac{b^{2}_{2}(c^{-1}b^{-1}_{2}b_{1}+a^{-1}_{1}a_{2}\theta_{1}-\theta_{2})^{2}}{a_{1}+\theta^{2}_{1}/a_{1}}d\Theta.
\end{align*}
If $b_{1}\in b'_{1}+cb_{2}q\mathbb{Z}$, then
$c^{-1}b^{-1}_{2}b_{1}+a^{-1}_{1}a_{2}\theta_{1}-\theta_{2}$ for any
fixed $\theta_{1}$, $\theta_{4}$, $b_{2}\neq 0$ (and {\em fixed}
$b'_{1}$ modulo $cb_{2}q$) covers $\mathbb{R}$ as $\theta_{3}$ runs
over an interval of length $q$. Thus the right hand side of the
preceding inequality is
\begin{align*}
&\ll c^{-k}\sum_{0\neq
    b_{2}\in\mathbb{Z}}c|b_{2}|\exp(-\mathscr{X}a^{-1}_{1}b^{2}_{2}/\det
  T)\\
&\qquad \int\limits^{\infty}_{-\infty}\int\limits^{\infty}_{-\infty}(\theta^{2}_{1}+a^{2}_{1})^{-k/2}\exp(-\mathscr{X}b^{2}_{2}\theta^{2}_{2}/(a_{1}+\theta^{2}_{1}/a_{1}))d\theta_{1}d\theta_{2}\\
&\ll c^{1-k}\sum_{0\neq
    b_{2}\in\mathbb{Z}}|b_{2}|\exp(-\mathscr{X}a^{-1}_{1}b^{2}_{2}/\det
  T)\\
&\qquad \int\limits^{\infty}_{-\infty}(\theta^{2}_{1}+a^{2}_{1})^{-k/2}(a_{1}+\theta^{2}_{1}/a_{1})^{1/2}|b_{2}|^{-1}d\theta_{1}\\
&\ll c^{1-k}\sum_{0\neq
    m\in\mathbb{Z}}\exp(-\mathscr{X}a^{-1}_{1}m^{2}/\det
  T)a^{-k+3/2}_{1}\int\limits^{\infty}_{-\infty}(1+x^{2})^{(1-k)/2}dx\\
&\ll
  c^{1-k}a^{-k+3/2}_{1}\sum_{m\in\mathbb{Z}}\exp(-\mathscr{X}a^{-1}_{1}m^{2}/\det
  T),\\
&\quad \text{by the convergence of the last integral for } k\geq
  5/2,\\
&\ll c^{1-k}a_{1}^{(3/2)-k}(a_{1}\det
  T)^{1/2}\sum_{m\in\mathbb{Z}}\exp(-\mathscr{X}^{-1}\pi^{2}a_{1}\det
  T\cdot m^{2})\\
&\quad \text{(in view of the Poisson summation formula}\\
&\quad \sum_{m\in\mathbb{Z}}e^{-\pi \lambda
    m^{2}}=\frac{1}{\sqrt{\lambda}}\sum_{m\in\mathbb{Z}}e^{-\frac{\pi}{\lambda}m^{2}},
  \text{ \ for \ } \lambda>0)\\
&= c^{1-k}a^{2-k}_{1}(\det
  T)^{1/2}\sum_{m\in\mathbb{Z}}\exp(-\pi^{2}\mathscr{X}^{-1}a_{1}\det Tm^{2})
\end{align*}
$\ll c^{1-k}a^{2-k}_{1}(\det T)^{1/2}$,\pageoriginale on noting that
the last series 
over $m$ is $\leq 1$, since $a_{1}\det T=T^{-1}[u_{1}]\det T$ with
$u_{1}={}^{t}(0 \ 1)$ or ${}^{t}(1 \ n)$ with $n=0$, $1$, $-1$ 
and, in view of $T^{-1}\asymp \left(\begin{smallmatrix} t^{-1}_{1} &
  0\\ 0 & t^{-1}_{2}\end{smallmatrix}\right)>t^{-1}_{2}E_{2}$,
$a_{1}\det T\gg t_{1}\gg 0$. If now, for $c\geq 1$, we define
$$
\mathscr{X}(c,U):=\sum_{(d,c)=1}\alpha\left(
\begin{pmatrix}
c & 0\\
0 & 0
\end{pmatrix}{}^{t}U,
\begin{pmatrix}
d & 0\\
0 & 0
\end{pmatrix}U^{-1}\right),
$$
then the above estimate for the sub-series over $d_{1}\equiv
d(\rm{mod} \;
cq)$ and summation over $d$ modulo $cq$ together yield the estimate
$$
\mathscr{X}(c,U)\ll c^{2-k}a^{2-k}_{1}(\det T)^{1/2}.
$$
Let us note here that $a_{1}=T^{-1}[\begin{smallmatrix} 0 \\ 1 
  \end{smallmatrix}]\asymp t^{-1}_{2}$ and
$a_{1}=T^{-1}[\begin{smallmatrix} 1 \\ n 
  \end{smallmatrix}]\asymp t^{-1}_{1}+n^{2}t^{-1}_{2}$ with
$|n|\leq 1$ corresponding to the respective possibilities for $U$; in
the former case $c\ll \sqrt{t_{2}/t_{1}}$ and in the latter case
$0<c\leq 3$. Hence
\begin{align*}
& \sum_{1\leq c<\infty}\mathscr{X}\left(c,
\begin{pmatrix}
0 & -1\\
1 & 0
\end{pmatrix}\right)\ll
\sum_{c\ll\sqrt{t_{2}/t_{1}}}c^{2-k}t^{k-2}_{2}(t_{1}t_{2})^{1/2}\\
&= (\min (T))^{2-k}(\det T)^{k-3/2}\sum_{1\leq c\ll
    \sqrt{t_{2}/t_{1}}}c^{2-k}\\
&\ll (\min(T))^{2-k}(\det T)^{k-3/2}=
\begin{cases}
1 & \text{for } k\geq 7/2\\
\log(t_{2}/t_{1}) & \text{for } k=3\\
\sqrt[4]{t_{2}/t_{1}} & \text{for } k=5/2
\end{cases}\\
&\ll 
\begin{cases}
(\min(T))^{2-k}(\det T)^{k-3/2}, \text{ for } k\geq 7/2\\
(\min(T))^{2-k}(\det T)^{k-3/2}\log (\sqrt{\det T}/\min(T)), \text{
    for } k=3\\
(\min(T))^{2-k}(\det T)^{k-3/2}(\det T)^{1/4}/(\min (T))^{1/2},\text{
    for }k=5/2,
\end{cases}
\end{align*}
while\pageoriginale
\begin{align*}
\sum_{\substack{1\leq c\leq 3\\ |n|\leq 1}}\mathscr{X}(c,U) &\ll
\sum_{\substack{1\leq c\leq
    3\\ n=0,1,-1}}c^{2-k}(t^{-1}_{1}+n^{2}t^{-1}_{2})^{2-k}(\det
T)^{1/2}\\
&\ll t^{k-2}_{1}(t_{1}t_{2})^{1/2}=(\det T)^{k-3/2}t^{2-k}_{2}\\
&\ll (\det T)^{k-3/2}(\min (T))^{2-k},\text{ since } t_{2}/t_{1}\gg
1\text{ and } k\geq 5/2.
\end{align*}
These estimates prove Proposition \ref{c1:prop-1.4.14} immediately.
\end{proofofprop}

\begin{remark*}
The case of $U=\left(\begin{smallmatrix} 0 & -1 \\ 1 & 0
\end{smallmatrix}\right)$ is troublesome. If $f_{1}\neq 0$, then
$f_{1}=1\leq c\leq 3$ and $a_{1}\gg 1/t_{1}$,
\begin{align*}
\mathscr{X}\left(c,
\begin{pmatrix}
1 & 0\\
n & 1
\end{pmatrix}
\right) &\ll \int\limits_{\theta^{2}_{1}+a^{2}_{1}\ll 1/\det
  T}(\theta^{2}_{1}+a^{2}_{1})^{-k/2}d\theta_{1}\\
&\qquad \text{(from Lemma \ref{c1:lem-1.4.19} and since}\\
&\qquad \det \Iim\tau=\det \Iim(M<X+iT^{-1}>)\gg 1)\\
&= a^{1-k}_{1}\int\limits_{x^{2}+1\ll 1/(a^{2}_{1}\det T)(\ll
  (t_{1}/t_{2})\ll 1)} (x^2 + 1)^{-k/2} dx \\
&\ll t^{k-1}_{1}.
\end{align*}
\end{remark*}

\section[Generalization of Kloosterman's Method...]{Generalization of Kloosterman's Method to the Case of
  Degree 2}\label{c1:sec-1.5}\pageoriginale

In this section, we generalize Theorem \ref{c1:thm-1.1.2} to the case of
modular forms of degree $2$ whose constant term vanishes at every
cusp. But our result is conditional because we do not have a good
estimate for a generalized Wey1 sum.

Let $k$, $q$ be natural numbers with $k\geq 3$ and
$f(z)=\sum\limits_{0\leq p\in\Lambda^{\ast}_{2}}a(P)\break e(\tr PZ)$ be a
Siegel modular form of degree $2$, weight $k$ and level $q$ whose
constant term vanishes at every cusp, and in addition we require
$f|M=f$ for every $M\in\Gamma_{2}(q)$. As before, we fix an
$M$-reduced positive definite matrix $T$ whose minimum is larger than
an absolute constant $\mathscr{X}$ fixed later.

Let $\mathscr{F}=\mathscr{F}_{2}$ be a fundamental domain as in
\S\ \ref{c1:sec-1.4} and $\mathscr{F}_{0}$ be a subset of
$\mathscr{F}$ such that for every point in $\mathscr{G}_{2}$ there is
a unique point in $\mathscr{F}_{0}$ which is mapped by
$\Gamma_{2}$. Put
$\mathfrak{g}=\bigcup\limits_{M\in\Gamma_{2,\infty}}M<\mathscr{F}_{0}>$
and for $\mathfrak{t}:=\{X\in\mathscr{M}_{2}(\mathbb{R})|0\leq
x_{ij}=x_{ji}<q(1\leq i,j\leq 2)\}$ and $M\in
\Gamma_{2}\beta(M):=\{X\in\mathfrak{t}|M<X+iT^{-1}>\in\mathfrak{g}\}$.

\begin{sublemma}\label{c1:lem-1.5.1}
$\mathfrak{t}=\bigcup\limits_{\substack{M\in\Gamma_{2,\infty\backslash
        \Gamma_{2}}\\ M\not\in \Gamma_{2,\infty}}}\beta(M)$ and the
  measure of 
$$
\beta(M_{1})\cap \beta(M_{2})\text{ \ equals \ } 0 \text{ \  if \ }
\Gamma_{2,\infty}M_{1}\neq \Gamma_{2,\infty}M_{2}.
$$
\end{sublemma}

\begin{proof}
The\pageoriginale first assertions is clear. Suppose
$X\in\beta(M_{1})\cap \beta(M_{2})$. Then we have $N_{1}$,
$N_{2}\in\Gamma_{2,\infty}$ such that
$N_{j}M_{j}<X+iT^{-1}>\varepsilon\mathscr{F}_{0}$. By definition of
$\mathscr{F}_{0}$, we obtain
$N_{1}M_{1}<X+iT^{-1}>=N_{2}M_{2}<X+iT^{-1}>$ and hence
$(N_{2}M_{2})^{-1}N_{1}M_{1}<X+iT^{-1}>=X+iT^{-1}$. Thus
$\beta(M_{1})\cap \beta(M_{2})$ is covered by a countable union of
fixed points of the above type. If the measure of $\beta(M_{1})\cap
\beta(M_{2})$ is not zero, then the above equation for some $N_{1}$,
$N_{2}\in\Gamma_{2,\infty}$ is trivial in $X$ and hence
$(N_{2}M_{2})^{-1}N_{1}M_{1}=\pm B_{2}$. This implies
$\Gamma_{2,\infty}M_{1}=\Gamma_{2,\infty}M_{2}$. 
\end{proof}

\begin{remark*}
As noted after Lemma \ref{c1:lem-1.4.2}, this lemma holds without the
replacement of $\mathscr{F}$ by $\mathscr{F}_{0}$. But the proof is lengthy.
\end{remark*}

\begin{sublemma}\label{c1:lem-1.5.2}
Let $C$, $D\in\mathscr{M}_{2}(\mathbb{Z})$ be a symmetric coprime pair
with $\det C\neq 0$. Then there exists
$A\in\mathscr{M}_{2}(\mathbb{Z})$ such that $\left(\begin{smallmatrix}
  A &\ast\\ C & D\end{smallmatrix}\right)\in\Gamma_{2}$ with $(\det A,q)=1$.
\end{sublemma}

\begin{proof}
Since $C$, $D$ is a coprime symmetric pair, there exists
$A\in\mathscr{M}_{2}(\mathbb{Z})$ with $\left(\begin{smallmatrix} A &
  \ast\\ C & D \end{smallmatrix}\right)\in\Gamma_{2}$. Since
$\left(\begin{smallmatrix} E_{2} & S\\ 0 & E_{2}
\end{smallmatrix}\right)\left(\begin{smallmatrix} A & \ast\\ C & D
\end{smallmatrix}\right)=\left(\begin{smallmatrix} A+SC &\ast\\ C & D
\end{smallmatrix}\right)$, we have only to prove, for any prime $p$,
$\det (A+SC)\not\equiv 0\rm{mod} \; p$ for some integral symmetric matrix $S$
by using the Chinese remainder theorem. Let $c_{1}|c_{2}$ be
elementary divisors of $C$ and $UCV=[c_{1},c_{2}]=\tilde{C}$ for
$U$, $V\in GL_{2}(\mathbb{Z})$. Put
${}^{t}U^{-1}AV=\left(\begin{smallmatrix} a_{1} & a_{2}\\ a_{3} &
  a_{4}
\end{smallmatrix}\right)=\tilde{A}$ and
$S[U^{-1}]=\left(\begin{smallmatrix} s_{1} & s_{2}\\ s_{2} & s_{4}
\end{smallmatrix}\right)$; then we have $\det (A+SC)=\det
(UV)\left|\begin{smallmatrix} a_{1}+s_{1}c_{1} & a_{2}+s_{2}c_{2}\\
a_{3}+s_{2}c_{1} & a_{4}+s_{4}c_{2}
\end{smallmatrix}\right|=$
$$
\pm
(a_{1}a_{4}-a_{2}a_{3}+s_{4}c_{2}(a_{1}+s_{1}c_{1})+a_{4}c_{1}s_{1}-a_{3}c_{2}s_{2}-c_{1}a_{2}s_{2}-c_{1}c_{2}s^{2}_{2}). 
$$
We suppose that this is congruent to $0\rm{mod} \; p$ for every $s_{1}$,
$s_{2}$, $s_{4}\in \mathbb{Z}$. Then\pageoriginale
$a_{1}a_{4}-a_{2}a_{3}$, $c_{2}a_{1}$, $c_{2}c_{1}$, $a_{4}c_{1}$,
$a_{3}c_{2}+c_{1}a_{2}$ are obviously congruent to $0\rm{mod} \; p$. Since
${}^{t}AC$ is symmetric, $a_{3}c_{2}+c_{1}a_{2}=2c_{1}a_{2}$
follows. If $p$ does not divide $c_{1}a_{2}=a_{3}c_{2}$, then $p\nmid
c_{1}c_{2}$. Thus $c_{1}a_{2}$ and so the determinant of every $(2,2)$
submatrix of $({}^{t}\tilde{A},{}^{t}\tilde{C})$ is divisible
by $p$. This 
contradicts $({}^{t}\tilde{A}, {}^{t}\tilde{C})$ being primitive.
\end{proof}

\begin{sublemma}\label{c1:lem-1.5.3}
Let $C$, $D'$ be a symmetric coprime pair with $\det C\neq 0$. Then
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\rm(\theenumi)}
\item there exists $\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)\in\Gamma_{2}$ with $(\det A',q)=1$,

\item for $D\in\mathscr{M}_{2}(\mathbb{Z})$ such that $C$, $D$ form a
  symmetric coprime pair and $D\equiv D'\rm{mod} \; q$, there exist $A$,
  $B\in\mathscr{M}_{2}(\mathbb{Z})$ such that $\Gamma_{2}\ni
  \left(\begin{smallmatrix} A & B\\ C & D
  \end{smallmatrix}\right)\equiv \left(\begin{smallmatrix} A' & B'\\ C
    & D' 
  \end{smallmatrix}\right)\rm{mod} \; q$ and

\item for $S\in \Lambda_{2}$ with $CS\equiv 0\rm{mod} \; q$, and for $A$,
  $B$, $D$ in {\em (ii)},
$$
\Gamma_{2}\ni 
\begin{pmatrix}
A-AS{}^{t}CA & \ast\\
C & CS+D
\end{pmatrix}
\equiv 
\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}
\rm{mod} \; q.
$$
\end{enumerate}
\end{sublemma}

\begin{proof}
First, (i) is nothing but the previous lemma.

Suppose $\left(\begin{smallmatrix} \tilde{A} & \tilde{B}\\ C &
  D
\end{smallmatrix}\right)\in\Gamma_{2}$ for $D$ in (ii); then
$\left(\begin{smallmatrix} \tilde{A} & \tilde{B}\\ C & D
\end{smallmatrix}\right)\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)^{-1}$
$$
=
\begin{pmatrix}
\tilde{A}{}^{t}D'-\tilde{B}{}^{t}C &
-\tilde{A}{}^{t}B'+\tilde{B}{}^{t}A'\\
C{}^{t}D'-D{}^{t}C & -C{}^{t}B'+D{}^{t}A'
\end{pmatrix}
\equiv
\begin{pmatrix}
E_{2} & \ast\\
0 & E_{2}
\end{pmatrix}
\rm{mod} \; q.
$$
Thus
$$
\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}
\equiv 
\begin{pmatrix}
E_{2} & G\\
0 & E_{2}
\end{pmatrix}
\begin{pmatrix}
\tilde{A} & \tilde{B}\\
C & D
\end{pmatrix}
\rm{mod} \; q\text{ \ for some \ } G\in \Lambda.
$$
Now\pageoriginale $A=\tilde{A}+GC$, $B=\tilde{B}+GD$ satisfy
the conditions in (ii).

Let $S$ be as in (iii). Put
$$
M=
\begin{pmatrix}
E_{2} & -AS{}^{t}A\\
0 & E_{2}
\end{pmatrix}
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\begin{pmatrix}
E_{2} & S\\
0 & E_{2}
\end{pmatrix}
;
$$
then it is easy to see
$$
M=
\begin{pmatrix}
A-AS{}^{t}AC & AS(E_{2}-{}^{t}AD)-AS^{t}ACS+B\\
C & CS+D
\end{pmatrix}
\in \Gamma_{2}.
$$
Further $S{}^{t}AC=S{}^{t}CA={}^{t}(CS)A\equiv 0\rm{mod} \; q$ and
$S(E_{2}-{}^{t}AD)=-S{}^{t}CB=-{}^{t}(CS)B\equiv 0\rm{mod} \; q$ imply
(iii). 
\end{proof}

\begin{sublemma}\label{c1:lem-1.5.4}
Let $\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)\in\Gamma_{2}$ with $(\det A',q)=1$ and $\det
C\neq 0$. Then we have
$$
\bigcup_{\tilde{D}}\Gamma_{2,\infty}
\begin{pmatrix}
\ast & \ast\\
C & \tilde{D}
\end{pmatrix}
=\bigcup_{D\in\mathscr{D}} \; 
\bigcup_{S\in\Lambda(C,q)}\Gamma_{2,\infty}
\begin{pmatrix}
A-AS{}^{t}CA & \ast\\
C & CS+D
\end{pmatrix}.
$$
where $\tilde{D}$ runs over $\tilde{D}\in
\mathscr{M}_{2}(\mathbb{Z})$ such that $\tilde{D}\equiv D'\rm{mod} \; q$
and $(C,\tilde{D})$ is a symmetric coprime pair,
$S\in\Lambda(C,q):=\{S={}^{t}S\in \Lambda_{2}|CS\equiv 0\rm{mod} \; q\}$ and
$\mathscr{D}:=\{D\in \mathscr{M}_{2}(\mathbb{Z})\rm{mod} \;
C\Lambda(C,q)|(C,D)$ is a symmetric coprime pair and $D\equiv D'\rm{mod} \;
q\}$, and coset representatives on the right are congruent to
$\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)\rm{mod} \; q$ for some
$A\in\mathscr{M}_{2}(\mathbb{Z})$ with $\Gamma_{2}\ni
\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\equiv \left(\begin{smallmatrix} A' & B'\\ C &
  D'
\end{smallmatrix}\right)\rm{mod} \; q$.
\end{sublemma}

\begin{proof}
By the previous lemma, for $\tilde{D}$ above, there exists
$\left(\begin{smallmatrix} \tilde{A} & \tilde{B}\\ C
  &\tilde{D}
\end{smallmatrix}\right)\in\Gamma_{2}$, which is congruent to
$\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)\rm{mod} \; q\cdot \mathscr{D}$ is a set of
representatives of such $\tilde{D}$ modulo $C\Lambda(C,q)$, and so
the rest follows from the previous lemma.
\end{proof}

\begin{sublemma}\label{c1:lem-1.5.5}
Let $A$, $C\in\mathscr{M}_{2}(\mathbb{Z})$ satisfying
${}^{t}AC={}^{t}CA$, det $C\neq 0$, $(\det
A,q)=1$. Then,\pageoriginale for $P\in\Lambda^{\ast}_{2}$, we have
\begin{align*}
& \sum_{S\in\Lambda(c,q)\rm{mod} \; q\Lambda}e(tr\text{ PAS }{}^{t}A/q)\\
&=
\begin{cases}
[\Lambda(c,q):q\Lambda] & \text{if \ } \house{\ast}\\
0 & \text{otherwise,}
\end{cases}
\end{align*}
where the condition $\house{\ast}$ on $P$ is as follows:
\begin{gather*}
\house{\ast}:\tr(PS)\equiv 0\rm{mod} \; q\text{ \ for every \ }
S\in\Lambda({}^{t}C,q),\\
\text{\ie \ } S\in\Lambda \text{ \ with \ } {}^{t}CS\equiv 0\rm{mod} \; q.
\end{gather*}
\end{sublemma}

\begin{proof}
It is clear that we have only to prove that the condition
$\house{\ast}$ is equal to $\tr(PAS^{t}A)\equiv 0\rm{mod} \; q$ for
every $S\in\Lambda(C,q)$. Since $(\det A,q)=1$, for $S\in\Lambda$ we
have $S\in\Lambda(C,q)\Longleftrightarrow CS\equiv 0(\rm{mod} \;
q)\Longleftrightarrow$ 
$$
{}^{t}ACS{}^{t}A\equiv 0\rm{mod} \; q\Longleftrightarrow
{}^{t}CAS{}^{t}A\equiv 0\rm{mod} \; q\Longleftrightarrow AS{}^{t}A\in
\Lambda({}^{t}C,q). 
$$
Since $S\equiv A(A_{1}S{}^{t}A_{1}){}^{t}A\rm{mod} \; q$ for
$A_{1}\in\mathscr{M}_{2}(\mathbb{Z})$ with $AA_{1}\equiv A_{1}A\equiv
E_{2}\rm{mod} \; q$, $AS{}^{t}A$ runs over $\Lambda({}^{t}C,q)\rm{mod} \; q\Lambda$
along with $S\in\Lambda(C,q)$. Thus we have proved the equality of two
conditions. 
\end{proof}

The following two propositions are proved at the end, in this section.

\setcounter{subprop}{5}
\begin{subprop}\label{c1:prop-1.5.6}
Let $\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)\in\Gamma_{2}$ with $\det C\neq 0$, $(\det
A',q)=1$, and $P$, $G$, $T\in\Lambda^{\ast}$. Suppose that $p$
satisfies the condition $\house{\ast}$ in Lemma
\ref{c1:lem-1.5.5}. We denote by $S(G,P,T,C,\left(\begin{smallmatrix} A'
  & B'\\ C & D'\end{smallmatrix}\right))$ the exponential sum
$$
\sum_{D\in\mathscr{D}}e(\tr(AC^{-1}(G+Pq^{-1})+TC^{-1}D).
$$
where\pageoriginale $A$, $D$, $\mathscr{D}$ are the same as in Lemma
\ref{c1:lem-1.5.4}. Then we have
$$
S(G,P,T,C,
\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}
)
=O(c^{2}_{1}c^{1/2+\varepsilon}_{2}(c_{2},t)^{1/2})\text{ \ for any
  \ } \varepsilon>0, 
$$
where
$$
C=U^{-1}
\begin{pmatrix}
c_{1} & 0\\
0 & c_{2}
\end{pmatrix}
V^{-1}, U, V\in GL_{2}(\mathbb{Z}), 0<c_{1}|c_{2}, T[V]=
\begin{pmatrix}
\ast &\ast\\
\ast & t
\end{pmatrix}.
$$
The implied constant depends only on $q$.
\end{subprop}

\begin{subprop}\label{c1:prop-1.5.7}
Let $n$ be a natural number and $S=\{{}^{t}(bd)|b,d\epsilon
\mathbb{Z},\break(b,d)=1\}$. We introduce the equivalence relation
${}^{t}(b,d)\sim {}^{t}(b',d')$ by ${}^{t}(b \; d)\equiv w{}^{t}(b'd')\rm{mod} \;
n$ for some $w\in\mathbb{Z}$, with $(w,n)=1$ and put
$S(n)=S/\sim$. Then, for $T=(t_{ij})\in\Lambda^{\ast}_{2}$, we have
$$
\sum_{S(n)\ni x} (T[x],n)^{1/2}=O(n^{1+\varepsilon}(e(T),n)^{1/2})\text{
  \ for  any  \ }\varepsilon >0,
$$
where $e(T)=(t_{11},t_{22},2t_{12})$.
\end{subprop}

As before we {\em put, for} $M=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$,
$$
\alpha(M)=\alpha(C,D):=\int\limits_{\beta(M)}f(X+iT^{-1})e(-\tr(TX)dX.
$$
Then we have
$$
a(T)=e^{4\pi}a^{-3}\left\{\sum_{\substack{M\in\Gamma_{2,\infty}\backslash
    \Gamma_{2}\\ \rank
    C=2}}\alpha(C,D)+\sum_{\substack{M\in\Gamma_{2,\infty}\backslash
      \Gamma_{2}\\ \rank C=1}}\alpha(C,D)\right\}
$$

Let $C\in\mathscr{M}_{2}(\mathbb{Z})$ with $\det C\neq 0$. For
$S={}^{t}S\in\mathscr{M}_{2}(\mathbb{Q})$ with
$SC\in\mathscr{M}_{2}(\mathbb{Z})$ and for $W\in\mathscr{G}_{2}$, we
put
$$
g(S,C;W)=
\begin{cases}
1 & \text{if \ } S+W\in\mathfrak{g},\\
0 & \text{otherwise.}
\end{cases}
$$
Then\pageoriginale $g(S,C;W)$ has the Fourier expansion
$$
\sum_{G\in\Lambda^{\ast}/\gamma(C)}b(G,C;W)e(\tr(SG)),
$$
where $\gamma(C):=\{G\in\Lambda^{\ast}|\tr(SG)\in\mathbb{Z}$ for every
$S={}^{t}S\in\mathscr{M}_{2}(\mathbb{Q})$ with
$SC\in\mathscr{M}_{2}(\mathbb{Z})\}$ and
$b(G,C;W)=[\Lambda^{\ast}:\gamma(C)]^{-1}\sum\limits_{S}e(-\tr(SG))g(S,C;W)$
where $S$ runs over $\{S={}^{t}S\in\mathscr{M}_{2}(\mathbb{Q})\rm{mod} \;
\Lambda|SC\in\mathscr{M}_{2}(\mathbb{Z})\}$. Now we have

\setcounter{sublemma}{7}
\begin{sublemma}\label{c1:lem-1.5.8}
Let $(C,D')$ be a symmetric coprime pair with $\det C\neq 0$. Then we
have
\begin{align*}
\sum_{D}\alpha(C,D)
&=[\Lambda(c,q):q\Lambda]\det c^{-k}\\
&\quad\int\limits_{\min(\Iim \tau)\geq
  \sqrt{3}/2}\det(\theta+iT^{-1})^{-k}\sum_{0\leq
  P\in\Lambda^{\ast}}a'(P)e(\tr(P\tau)/q)\times\\
&\times
e(-\tr(T\theta))\sum_{G\in\Lambda^{\ast}/\gamma(C)}b(G,C;\tau)S(G,P,T,C,
\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}
)d\theta
\end{align*}
where $D$ runs over $\{D\equiv D'\rm{mod} \; q|C$, $D$ are symmetric and
coprime$\}$,
$\tau=\tau(\theta,C)=-{}^{t}C^{-1}(\theta+iT^{-1})^{-1}C^{-1}$, and
$\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)\in\Gamma_{2}$ with $(\det A',q)=1$, and
$a'(P)$ are Fourier coefficients of 
\begin{align*}
f|\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}
(Z)&=\det(CZ+D')^{-k}f(A'Z+B')(CZ+D')^{-1})\\
&=\sum a'(P)e(\tr (PZ/q)).
\end{align*}
\end{sublemma}

\begin{proof}
By Lemma \ref{c1:lem-1.5.2}, there exists $\left(\begin{smallmatrix} A' &
  B'\\ C & D'
\end{smallmatrix}\right)\in\Gamma_{2}$ with $(\det A',q)=1$ and we put
$f|\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)^{-1}(Z)=\sum a'(P)e(\tr PZ/q)$. For
$D\in\mathscr{M}_{2}(\mathbb{Z})$ such that $(C,D)$ is a symmetric
coprime pair and $D\equiv D'\rm{mod} \; q$, there exists
$\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$ with $\left(\begin{smallmatrix}
  A & B\\ C & D\end{smallmatrix}\right)=\left(\begin{smallmatrix} A' &
    B'\\ C & D'  \end{smallmatrix}\right)\rm{mod} \; q$. Hence we have
  $f|\left(\begin{smallmatrix} A & B\\ C & D
  \end{smallmatrix}\right)^{-1}=f|\left(\begin{smallmatrix} A' &
    B'\\ C & D'  \end{smallmatrix}\right)^{-1}=F$ (say).
Then\pageoriginale we have $\alpha(C,D)=\int\limits_{\beta(M)}\det
(C(X+iT^{-1})+D)^{-k}F(M<X+iT^{-1}>)e(-\tr(TX))dX$, where
$M=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)$.

Since $\det C\neq 0$, we have
$M<Z>=AC^{-1}-{}^{t}C^{-1}(Z+C^{-1}D)^{-1}C^{-1}$.

Putting $X=\theta-C^{-1}D$,
$\tau=-{}^{t}C^{-1}(\theta+iT^{-1})^{-1}C^{-1}$,
\begin{align*}
\alpha(C,D) &= (\det
C)^{-k}\int\limits_{\substack{\theta\in\mathfrak{t}+C^{-1}D\\ 
AC^{-1}+\tau\in\mathfrak{g}}}(\theta+iT^{-1})^{-k}F(AC^{-1}+\tau)\\
&\qquad\qquad e(-\tr(T(\theta-C^{-1}D)))d\theta.\\
&= (\det C)^{-k}\int\limits_{\substack{\theta\in\mathfrak{t}+C^{-1}D\\ AC^{-1}+\tau
    \in\mathfrak{g}}}(\theta+iT^{-1})^{-k}\sum_{p}a'(P)e(\tr(P\tau)/q)\\
&\qquad\qquad e(-\tr(T\theta))
\times e(\tr(PAC^{-1}/q+TC^{-1}D))d\theta\\
&= (\det
C)^{-k}\int\limits_{\substack{\theta\in\mathfrak{t}+C^{-1}D\\ \min(\Iim(\tau))\geq
    \sqrt{3}/2}}(\theta+iT^{-1})^{-k}\sum_{P}a'(P)\\
&\qquad\qquad e(\tr(P\tau)/q)e(-\tr(T\theta))\\
&\quad \times
\sum_{G\in\Lambda^{\ast}/\gamma(C)}b(G,C;\tau)e(\tr(AC^{-1}G+PAC^{-1}/q+TC^{-1}D))d\theta 
\end{align*}
since $AC^{-1}+\tau\in\mathfrak{g}$ implies $\min(\Iim \tau)\geq \sqrt{3}/2$.
\end{proof}

Applying Lemma \ref{c1:lem-1.5.4}, the sum $\sum\limits_{D}\alpha(C,D)$
referred to is equal to 
{\fontsize{10}{12}\selectfont
\begin{gather*}
|\det
C|^{-k}\sum_{\substack{D\in\mathscr{D}\\ S\in\Lambda(C,q)}}
\int\limits_{\substack{\theta\in\mathfrak{t}+C^{-1}D+S\\ \min
(\Iim\tau)\geq\sqrt{3}/2}}(\theta+iT^{-1})^{-k}\sum_{P}a'(P)
e(\tr(P\tau)/q)e(-\tr T\theta)\times\\
\times \sum_{G\in\Lambda^{\ast}/\gamma(C)}b(G,C;\tau)e(\tau
r((A-AS{}^{t}CA)C^{-1}G+P(A-AS^{t}CA)C^{-1}/q+\\
+TC^{-1}(CS+D))d\theta,
\end{gather*}}\relax
(noting\pageoriginale that
$\tr((A-AS{}^{t}CA)C^{-1}G+P(A-AS{}^{t}CA)C^{-1}/q+TC^{-1}(CS+D))$
\begin{align*}
&\equiv \tr(AC^{-1}G+PAC^{-1}/q-PAS{}^{t}A/q+TC^{-1}D)\rm{mod} \; 1\\ 
&\qquad\qquad\text{since } {}^{t}CAC^{-1}={}^{t}A,)\\
&= (\det
  C)^{-k}\sum_{\substack{D\in\mathscr{D}\\ S\in\Lambda(C,q)/q\Lambda}}
  \;\; 
  \int\limits_{\min (\Iim\tau)\geq \sqrt{3}/2}(\theta+iT^{-1})^{-k}\sum
  a'(P)\\
&\qquad\qquad e(\tr(P\tau/q))e(-\tr(T\theta))
\times\sum_{G\in\Lambda^{\ast}/\gamma(C)}b(G,C;\tau)e(\tr(AC^{-1}G\\
&\qquad\qquad +PAC^{-1}/q-PAS{}^{t}A/q+TC^{-1}D))d\theta\\
&= [\Lambda(C,q):q\Lambda](\det C)^{-k}\int\limits_{\min (\Iim
    \tau)\geq\sqrt{3}/2}(\theta+iT^{-1})^{-k}\\
&\qquad\qquad\sum_{\house{\ast}}a'(P)e(\tr(P\tau
  /q))e(-\tr(T\theta)\times\\
&\quad \sum_{G\in\Lambda^{\ast}/\gamma(C)}b(G,C;\tau)S(G,P,T,C,
\left.
\begin{pmatrix} 
A' & B'\\
C & D'\\
\end{pmatrix}
\right)d\theta,
\end{align*}
which proves our lemma.

\begin{sublemma}\label{c1:lem-1.5.9}
Let $M=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$ and $f|M(Z)=\sum
a'(P)e(\tr(PZ/q))$. If $\ub{\min}(\Iim Z)\geq \sqrt{3}/2$, then $\sum
|a'(P)e(\tr(PZ/q))|=O(\exp(-\mathscr{X}\break\ub{\min} (\Iim Z))$ for some
$\mathscr{X}>0$. 
\end{sublemma}

\begin{proof}
Let $\Gamma_{2}=\bigcup\limits_{i}M_{i}\Gamma_{2}(q)$ and
$f|M_{i}(Z)=\sum a_{i}(P)e(\tr(PZ/q))$. Suppose $\Iim(Z[U^{-1}])$ is
$M$-reduced for $U\in GL_{2}(\mathbb{Z})$. Since
$f|M\left(\begin{smallmatrix} {}^{t}U & 0\\ 0 &  U^{-1}
\end{smallmatrix}\right)=f|M_{i}$ for some $i$, $(\det
U)^{k}a'(P[{}^{t}U^{-1}])=a_{i}(P)$ for every $0\leq
P\in\Lambda^{\ast}$, and then we have
\begin{align*}
&\quad\sum |a'(P)e(\tr(PZ/q)|\\
&= \sum|a'(P[{}^{t}U^{-1}])e(\tr(PZ[U^{-1}]/q))|\\
&= \sum |a_{i}(P)e(\tr(PZ[U^{-1}]/q))|\\
&=O(\exp(-\mathscr{X}\min(\Iim (Z[U^{-1}])))\quad\text{(Lemma
    \ref{c1:lem-1.4.1})}\\
&= O(\exp(-\mathscr{X}\min(\Iim Z))).
\end{align*}
This\pageoriginale completes the proof, since $[\Gamma:\Gamma(q)]<\infty$.
\end{proof}

Here we make an assumption, namely


\medskip
\noindent
{\bf Assumption (*):}
{\fontsize{10}{12}\selectfont
$$
\sum_{G\in\Lambda^{\ast}/\gamma(C)}|b(G,C;\tau)|
=O(c^{a_{1}+\varepsilon}_{1}c^{a_{2}}_{2})\text{
  \ for \ } 
\begin{cases}
0\leq a_{1}\leq 3/2 & \\
 &\text{and any } \varepsilon>0,\\
0\leq a_{2}<1/2
\end{cases}
$$}\relax
where $0<c_{1}|c_{2}$ are elementary divisors of $C$ and the implied
constant is independent of $\tau$.

This is discussed later.

Let $C$, $D\in\mathscr{M}_{2}(\mathbb{Z})$ form a symmetric coprime
pair with $\det C\neq 0$. Under Assumption $(\ast)$, we have, by
virtue of Lemma \ref{c1:lem-1.5.8}, Proposition \ref{c1:prop-1.5.6} and
Lemma \ref{c1:lem-1.5.9}, 
\begin{align*}
&\quad |\sum_{D'\equiv D\rm{mod} \; q}\alpha(C,D')|\\
&\ll |\det C^{-k}|\int\limits_{\min(\Iim \tau)\geq
    \sqrt{3}/2}|\det(\theta+iT^{-1})|^{-k}\exp(-\mathscr{X}\min(\Iim
  \tau))c^{a_{1}+\varepsilon a_{2}}_{c_{2}}\times\\
&\quad c^{2}_{1}c^{1/2+\varepsilon}_{2}(c_{2},t)^{1/2}d\theta
\end{align*}
where
$$ 
C=U^{-1}
\begin{pmatrix}
c_{1} & 0\\
0 & c_{2}
\end{pmatrix}
V^{-1}, U, V \in GL_{2}(\mathbb{Z}), 0 <c_{1}|c_{2}, T[V]=
\begin{pmatrix}
\ast & \ast\\
\ast & t
\end{pmatrix},
$$
since
$$
\tau=-{}^{t}C^{-1}(\theta + iT^{-1})^{-1}C^{-1},\Iim\tau  =
(T[\theta]+T^{-1})^{-1}[C^{-1}],
$$
and for $X=\sqrt{T}\theta\sqrt{T}$, we have $d\theta=\det
T^{-3/2}dX$. Hence
\begin{align*}
&\quad |\sum_{D'\equiv D\rm{mod} \; q}\alpha(C,D')|\\
&\ll
  c_{1}^{a_{1}+2-k+\varepsilon}c_{2}^{a_{2}+1/2-k/\varepsilon}(c_{2},t)^{1/2}(\det
  T)^{k-3/2}\int \det (X^{2}+1)^{-k/2}\times\\
&\quad \exp(-\mathscr{X}\min ((X^{2}+1)^{-1}[\sqrt{T}C^{-1}]))dX,\\
&\ll
  c_{1}^{a_{1}+2-k+\varepsilon}c_{2}^{a_{2}+1/2-k+\varepsilon}(c_{2},t)^{1/2}(\det
  T)^{k-3/2}(\min (T[c^{-1}]))^{1-k/2}\\
&\qquad \text{(as for the proof of Proposition \ref{c1:prop-1.4.10}).}
\end{align*}\pageoriginale
Thus we have proved

\begin{sublemma}\label{c1:lem-1.5.10}
Let $C\in\mathscr{M}_{2}(\mathbb{Z})$ with $\det C\neq 0$. Then we
have
\begin{align*}
\left|\sum_{D}\alpha(C,D)\right|&\ll(\det
T)^{k-3/2}c_{1}^{a_{1}+2-k+\varepsilon}c_{2}^{a_{2}+1/2-k+\varepsilon}\\
&\qquad\qquad (c_{2},t)^{1/2}(\min (T[C^{-1}]))^{1-k/2} 
\end{align*}
under Assumption $(\ast)$, where 
$$
C=U^{-1}
\begin{pmatrix}
c_{1} & 0\\ 
0 & c_{2}
\end{pmatrix}
V^{-1}, U, V\in GL_{2}(\mathbb{Z})0< c_{1} / c_{2}
$$
and $T[V]=\left(\begin{smallmatrix} \ast & \ast\\ \ast & t
\end{smallmatrix}\right)$.
\end{sublemma}

For the above $C$, $\min(T[C^{-1}])=\min(T[V\left(\begin{smallmatrix}
    c^{-1}_{1} & 0\\ 0 & c^{-1}_{2}
  \end{smallmatrix}\right)])=c^{-2}_{2}\break\min
T[V\left(\begin{smallmatrix} c_{2}/c_{1} & 0\\ 0 & 1
  \end{smallmatrix}\right)]>c^{-2}_{2}\min(T)$ holds. In the
decomposition $C=U^{-1}\break\left(\begin{smallmatrix} c_{1} & 0\\ 0 & c_{2}
\end{smallmatrix}\right)V^{-1}$, $V$ is uniquely determined in
$$GL_{2}(\mathbb{Z})/\left\{\left(\begin{smallmatrix} a & b\\ c & d
\end{smallmatrix}\right)\in GL_{2}(\mathbb{Z})|b\equiv 0\rm{mod} \;
c_{2}/c_{1}\right\}$$ 
so we have a bijection
$V=\left(\begin{smallmatrix} a & b\\ c & d
\end{smallmatrix}\right)\mapsto^{t}(b\ d)\in S(c_{2}/c_{1})$ defined
in Proposition \ref{c1:prop-1.5.7} and $(c_{2},t)\leq
c_{1}(c_{2}/c_{1},t)$.

Thus we have, by Proposition \ref{c1:prop-1.5.7}.


\begin{sublemma}\label{c1:lem-1.5.11}
Let $0<c_{1}|c_{2}$. Then, under Assumption $(\ast)$,
\begin{align*}
|\sum \alpha (C,D)|&\ll (\det
T)^{k-3/2}c_{1}^{a_{1}+5/2-k + \varepsilon}
c_{2}^{a_{2}+1/2-k+\varepsilon}\\
&\qquad(c_{2}/c_{1})^{1+2\varepsilon}(e(T),c_{2}/c_{1})^{1/2}\times\\ 
&\qquad\quad\times
\begin{cases}
1,\\
(c^{-2}_{2}\min T)^{1-k/2}\text{ \ for any \ }\varepsilon>0
\end{cases}
\end{align*}
where\pageoriginale $C$ runs over representatives of left cosets by
$GL_{2}(\mathbb{Z})$ of integral matrices with elementary divisors
$c_{1}$, $c_{2}$, and $D$ runs over all possible $D$ with
$\left(\begin{smallmatrix} \ast & \ast\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$.
\end{sublemma}

Now we can prove


\setcounter{subprop}{11}
\begin{subprop}\label{c1:prop-1.5.12}
Under Assumption $(\ast)$ we have, for any $\varepsilon>0$,
$$
|\sum_{\rank C=2}\alpha(C,D)|\ll (\min T)^{a|2+k/4-k/2+\varepsilon}(\det
T)^{k-3/2}
$$
if $\min(T)>\mathscr{X}$ (= an absolute constant $>0$) and $k\geq 3$.
\end{subprop}

\begin{remark*}
Since $a_{2}<1/2$, $a_{2}/2+5/4-k/2<0$ and so
$a_{2}/2+5/4-k/2+\varepsilon<0$ for a sufficiently small positive
$\varepsilon$. 
\end{remark*}

\begin{proof}
Decompose the sum $|\sum \alpha(C,D)|$ as
$$
|\sum_{c_{2}<(\min(T))^{1/2}}\alpha(C,D)|+|\sum_{c_{2}\geq(\min(T))^{1/2}}\alpha(C,D)|=\sum_{1}+\sum_{2}\quad\text{(say)}.  
$$
By virtue of Lemma \ref{c1:lem-1.5.11}, we have
\begin{align*}
(\det T)^{3/2-k}\sum_{1} &\ll
  \sum_{c_{1}|c_{2}<(\min(T))^{1/2}}c_{1}^{a_{1}+5/2-k+\varepsilon}
  c_{2}^{a_{2}+1/2-k+\varepsilon}(c_{2}/c_{1})^{1+2\varepsilon}\\
&\qquad\qquad(e(T),c_{2}/c_{1})^{1/2}(c^{-2}_{2}\min
  T)^{1-k/2}\\
&= (\min (T))^{1-k/2}\sum_{c_{1}|c_{2}<(\min
    (T))^{1/2}}c^{a_{1}+a_{2}+1-k+2\varepsilon}_{1}\\
&\qquad\qquad(c_{2}/c_{1})^{a_{2}-1/2+3\varepsilon}(e(T),c_{2}/c_{1})^{1/2}\\
&\leq (\min(T))^{1-k/2}\sum_{\substack{n,m\geq 1\\ nm<(\min(T))^{1/2}}}n^{a_{1}+a_{2}+1-k+2\varepsilon}\\
&\qquad\qquad m^{a_{2}-1/2+3\varepsilon}(e(T),m)^{1/2}.
\end{align*}
The\pageoriginale sum over $m$ does not exceed
\begin{align*}
&\quad
  \sum_{r|e(T)}r^{1/2}\sum_{s<(\min(T))^{1/2}/nr}(sr)^{a_{2}-1/2+3\varepsilon}\\
&<
  \sum_{r|e(T)}r^{a_{2}+3\varepsilon} \sum_{s<(\min(T))^{1/2}/nr}s^{a_{2}-1/2+3\varepsilon}\\ 
&\ll \sum_{r|e(T)}r^{a_{2}+3\varepsilon}((\min
  (T))^{1/2}/nr)^{a_{2}+1/2+3\varepsilon} \quad(\text{since \ }
    a_{2}+1/2+3\varepsilon>0)\\
&=
    (\min(T))^{a_{2}/2+1/4+3\varepsilon/2}
    n^{-a_{2}-1/2-3\varepsilon}\sum_{r|e(T)}r^{-1/2}\\ 
&\ub{\leq}
    (\min(T))^{a_{2}/2+1/4+3\varepsilon/2}n^{-a_{2}-1/2-3\varepsilon}\sum_{r|e(T)^{1}}\\
&\ll (\min
    (T))^{a_{2}/2+1/4+2\varepsilon}n^{-a_{2}-1/2-3\varepsilon}\quad(\text{since
      \ } e(T)\leq \min(T)).
\end{align*}
Thus we have
\begin{align*}
(\det T)^{3/2-k}\sum_{1} &\ll (\min
  T)^{a_{2}/2+5/4-k/2+2\varepsilon}\sum_{n\geq
    1}n^{a_{1}+1/2-k-\varepsilon}\\
&\ll (\min T)^{a_{2}/2+5/4-k/2+2\varepsilon}\quad(\text{since \ }
  a_{1}+1/2-k\leq -1).
\end{align*}
Similarly, we have
\begin{align*}
& (\det T)^{3/2-k}\sum_{2}\ll \sum_{\substack{c_{1}|c_{2}\\ c_{2}\geq
      (\min
      (T))^{1/2}}}c_{1}^{a_{1}+5/2-k+\varepsilon}c_{2}^{a_{2}+1/2-k+\varepsilon}\\
&\qquad\qquad(c_{2}/c_{1})^{1+2\varepsilon}(e(T),c_{2}/c_{1})^{1/2}\\
&= \sum_{\substack{c_{1}|c_{2}\\ c_{2}\geq (\min
      (T))^{1/2}}}c_{1}^{a_{1}+a_{2}+3-2k+2\varepsilon}
(c_{2}/c_{1})^{a_{2}+3/2-k+3\varepsilon}(e(T),c_{2}/c_{1})^{1/2}\\
&= \sum_{\substack{n,m\geq 1 \\ nm\geq(\min
      (T))^{1/2}}}n^{a_{1}+a_{2}+3-2k+2\varepsilon}m^{a_{2}+3/2-k+3\varepsilon}(e(T),m)^{1/2}. 
\end{align*}
The\pageoriginale sum over $m$ is less than
\begin{align*}
&\quad \sum_{r|e(T)}\sum_{s\geq
    (\min(T))^{1/2}/(nr)}(sr)^{a_{2}+3/2-k+3\varepsilon}r^{1/2}\\
&= \sum_{r|e(T)}r^{a_{2}+2-k+3\varepsilon}\sum_{s\geq
    (\min(T))^{1/2}/(nr)}s^{a_{2}+3/2-k+3\varepsilon}\\
&\ll
  \sum_{r|e(T)}r^{a_{2}+2-k+3\varepsilon}((\min(T))^{1/2}/nr)^{a_{2}+5/2-k+3\varepsilon}\\ 
&\qquad (\text{since \ } a_{2}+5/2-k+3\varepsilon<0\text{ \ for small \ }
  \varepsilon>0)\\
&= (\min(T))^{a_{2}/2+5/4-k/2+(3/2)\varepsilon}
  n^{-a_{2}-5/2+k-3\varepsilon}\sum_{r|e(T)}r^{-1/2}\\
&\ll (\min(T))^{a_{2}/2+5/4-k/2+2\varepsilon}n^{-a_{2}-5/2+k-3\varepsilon}.
\end{align*}
Thus we have
\begin{align*}
(\det T)^{3/2-k}\sum_{2} &\ll (\min
  (T))^{a_{2}/2+5/4-k/2+2\varepsilon}\sum_{n\geq
    1}n^{a_{1}+1/2-k-\varepsilon}\\
&\ll (\min(T))^{a_{2}/2+5/4-k/2+2\varepsilon}
\end{align*}
\end{proof}

The proof of Proposition \ref{c1:prop-1.5.12} is complete, but for the proof
of Proposition \ref{c1:prop-1.5.6} and \ref{c1:prop-1.5.7}.


\bigskip
\noindent
{\bf Remark on Assumption (*).}
Let $C=U^{-1}\left(\begin{smallmatrix} c_{1} & 0\\ 0 & c_{2}
\end{smallmatrix}\right)V^{-1}$ with $U$, $V\in GL_{2}(\mathbb{Z})$,
$0\leq c_{1}|c_{2}$, and put $C'=\left(\begin{smallmatrix} c_{1} &
  0\\ 0 & c_{2}\end{smallmatrix}\right)$. Then
\begin{align*}
\gamma(C') &= \{G\in\Lambda^{\ast}|\tr(SG)\in\mathbb{Z} \text{ \ for
  every \ } S\\
&={}^{t}S\in\mathscr{M}_{2}(\mathbb{Q})\text{ \ with \ }
SC'\in\mathscr{M}_{2}(\mathbb{Z})\}\\
&= \{G\in\Lambda^{\ast}|\tr(SG)\in\mathbb{Z}\text{  for every  }\\
&\qquad S={}^{t}S\in\mathscr{M}_{2}(\mathbb{Q})\text{ \ with \ }
S[U]C\in\mathscr{M}_{2}(\mathbb{Z})\}\\
&= \gamma(C)[{}^{t}U].
\end{align*}
Hence\pageoriginale
\begin{align*}
b(G,C;W) &=
[\Lambda^{\ast}:\gamma(C')]^{-1}\sum_{\substack{{}^{t}S=S\rm{mod} \;\Lambda\\ SC\in
\mathscr{M}_{2}(\mathbb{Z})}}e(-\tr(SG))g(S,C;W)\\
&=
[\Lambda^{\ast}\gamma(C')]^{-1}\sum_{\substack{S\rm{mod} \;\Lambda\\ SC'\in\mathscr{M}_{2}(\mathbb{Z})}}e(-\tr(S[U]G))g(S[U],C;W)\\
&=
[\Lambda^{\ast}:\gamma(C')]^{-1}\sum_{S}e(-\tr(SG[{}^{t}U]))g(S,C';W[U^{-1}])\\
&= b(G[{}^{t}U], C';W[U^{-1}]).
\end{align*}
Thus we obtain
$$
\sum_{G\in\Lambda^{\ast}/\gamma(C)}|b(G,C;\tau)|=\sum_{G\in\Lambda^{\ast}/\gamma(C')}|b(G,C':\tau[U^{-1}])|.
$$
For $S=\left(\begin{smallmatrix} s_{1} & s_{2}\\ s_{2} & s_{4}
\end{smallmatrix}\right)$, it is clear that
$SC'\in\mathscr{M}_{2}(\mathbb{Z})$ if and only if
$s_{1}=u_{1}/c_{1}$, $s_{2}=u_{2}/c_{1}$, $s_{4}=u_{4}/c_{2}$ for
$u_{1}$, $u_{2}$, $u_{4}\in \mathbb{Z}$.

For $G=\left(\begin{smallmatrix} g_{1} & g_{2}/2\\ g_{2}/2 & g_{4}
\end{smallmatrix}\right)$, $G\in\gamma(C')$ if and only if
$c_{1}|g_{1}$, $c_{1}|g_{1}$, $c_{1}|g_{2}$, $c_{2}|g_{4}$. Hence we
have
{\fontsize{10}{12}\selectfont
\begin{gather*}
 \sum_{G\in\Lambda^{\ast}/\gamma(C)}|b(G,C;\tau)|\\
=\sum_{\substack{g_{1},g_{2}\rm{mod} \; c_{1}\\ g_{4}\rm{mod} \;
    c_{2}}}c^{-2}_{1}c^{-1}_{2} \left| \sum_{\substack{u_{1},u_{2}\rm{mod} \;
    c_{1}\\ u_{4}\rm{mod} \; c_{2}\\ \left(\begin{smallmatrix} u_{1}/c_{1} &
      u_{2}/c_{1}\\ u_{2}/c_{1} & u_{4}/c_{2}
    \end{smallmatrix}\right)+\tau[U^{-1}]\in\mathfrak{g}}}e(u_{1}g_{1}/c_{1}+u_{2}g_{2}/c_{1}+u_{4}g_{4}/c_{2})
\right|. 
\end{gather*}}
Thus\pageoriginale Assumption $(\ast)$ is the same as
{\fontsize{10}{12}\selectfont
\begin{align*}
& (\sharp)\sum_{\substack{g_{1},g_{2}\rm{mod} \; c_{1}\\ g_{4}\rm{mod} \;
    c_{2}}}\left| \sum_{\substack{u_{1},u_{2}\rm{mod} \; c_{1}\\ u_{4}\rm{mod} \;
    c_{2}}} e((u_{1}g_{1}+u_{2}g_{2})/ c_{1}+u_{4} g_{4} / c_{2})\right| =
O(c^{2+a_{1}+\epsilon}_{1}c^{1+a_{2}}_{2})\\ 
& \qquad \qquad \text{for \ } 0<c_{1}|c_{2}\text{ \ and any \ }
\varepsilon>0 \qquad  
 \begin{pmatrix} u_{1}/c_{1} &
      u_{2}/c_{1}\\ u_{2}/c_{1} & u_{4}/c_{2}
    \end{pmatrix} + W \in \mathfrak{g}\qquad 
\end{align*}}
where $0\leq a_{1}\leq 3/2$, $0\leq a_{2}<1/2$ and the implied
constant is independent of $c_{1}$, $c_{2}$, $W$.

Using Schwarz's inequality, the left hand side does not exceed
{\fontsize{10}{12}\selectfont
\begin{align*}
\sqrt{c^{2}_{1}c_{2}}\sqrt{\sum_{g_{1},g_{2},g_{4}}|\sum\ldots|^{2}}=\sqrt{c^{2}_{1}c_{2}}\sqrt{c^{2}_{1}c_{2}\sum
  1}_{\left(\begin{smallmatrix} u_{1}/c_{1} &
    u_{2}/c_{1}\\ u_{2}/c_{1} & u_{4}/c_{2}
  \end{smallmatrix}\right)+W\in\mathfrak{g}}\leq c^{3}_{1}c^{3/2}_{2}.
\end{align*}}\relax
Hence Assumption $(\ast)$ is true once we get a sharper estimate than
the estimate via Schwarz's inequality. (\cf Remarks before the proof
of \eqref{c1:eq6} on page 21).

The left hand side of $(\sharp)$ does not exceed
$$
\sum_{u_{1},u_{2},g_{1},g_{2}\rm{mod} \; c_{1}}\left\{\sum_{g_{4}\rm{mod} \;
  c_{2}}|\sum_{\substack{u_{4}\rm{mod} \; c_{4}\\ \left(\begin{smallmatrix}
      u_{1}/c_{1} & u_{2}/c_{1}\\ u_{2}/c_{1} & u_{4}/c_{2}
    \end{smallmatrix}\right)+W\in\mathfrak{g}}}e(u_{4}g_{4}/c_{2})|\right\}.
$$
Suppose\pageoriginale the sum inside the curly brackets is
$O(c^{3/2-\delta}_{2})$ 
for some $\delta>0$ (Actually it is $O(c^{3/2}_{2})$, from Schwarz's
inequality); then Assumption $(\ast)$ holds for $a_{1}=3/2$,
$a_{2}=1/2-\delta/2$. 

({\bf Proof.} If $c^{\delta}_{2}=O(c_{1})$, then
$c^{3}_{1}c^{3/2}_{2}/(c^{7/2}_{1}c_{2}^{3/2-\delta/2})=c^{-1/2}_{1}c^{\delta/2}_{2}=O(1)$. If
$c_{1}=O(c^{\delta}_{2})$, then $c^{4}_{1}c^{3/2-\delta}_{2}/(c^{7/2}_{1}c^{3/2-\delta/2}_{2})=c^{1/2}_{1}c^{-\delta/2}_{2}=O(1)$.)

Combining Proposition \ref{c1:prop-1.5.12} with Proposition
\ref{c1:prop-1.4.14}, we have

\setcounter{subtheorem}{12}
\begin{subtheorem}\label{c1:thm-1.5.13}
Let $f(z)=\Sigma a(T)e(\tr(TZ/q))$ be a Siegel modular form fo (degree
$2$), level $q$, weight $k=3$ and with zero as the constant term at
all cusps. Then for $T>0$ and $\min T>\mathscr{X}$ {\em (= absolute
  constant)} 
$$
a(T)=O(((\min
(T))^{a_{2}/2-1/4+\varepsilon}+(\min(T))^{-1}\log\frac{\sqrt{\det
    T}{\min(T)}})\det T^{3/2}) 
$$
under Assumption $(\ast)$.
\end{subtheorem}

\begin{remark*}
If $\sqrt{\det T}=O(\min(T))$, then the above implies 
$$
a(T)/\det T^{3/2}\to 0\text{ \ as \ } \min(T)\to \infty.
$$
It remains to prove Proposition \ref{c1:prop-1.5.6} and
\ref{c1:prop-1.5.7}. 
\end{remark*}

For $P$, $T\in\Lambda^{\ast}$ and $C\in\mathscr{M}_{2}(\mathbb{Z})$
with $\det C\neq 0$, we put
$$
K(P,T;C)=\sum_{D}e(\tr(AC^{-1}P+C^{-1}DT)),
$$
where $D$ runs over the set $\{D\rm{mod} \; C\Lambda|(C,D)$ a symmetric
coprime pair$\}$ and $A$ is an integral matrix such that
$\left(\begin{smallmatrix} A & \ast\\ C & D
\end{smallmatrix}\right)\in \Gamma_{2}$. Another possible $A$ is of
the form $A+SC$, $S\in\Lambda_{2}$. Thus the generalized Kloosterman
sum $K(P,T;C)$ is\pageoriginale well defined. To prove Proposition
\ref{c1:prop-1.5.6}, we show that
\begin{itemize}
\item[i)] $S(G,P,T,C,\left.\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)\right)$ is reduced to the sum of $K(P,T;C)$

\item[ii)] the same estimate for $K(P,T;C)$ holds as well as for
  $S(\cdot\cdot)$. 
\end{itemize}

\medskip
\noindent
{\bf Reduction from $S(\cdot\cdot)$ to $K(\cdot\cdot)$}
\begin{itemize}
\item[R1)] The exponential sum
  $S(G,P,T,C,\left.\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right)\right)$ is well-defined.
\end{itemize}

\begin{proof}
Suppose that
$$
\begin{pmatrix}
A_{1} & B_{1}\\
C & D_{1}
\end{pmatrix}
\equiv
\begin{pmatrix}
A_{2} & B_{2}\\
C & D_{2}
\end{pmatrix}
\rm{mod} \; q\text{ \  and \ } D_{1}\equiv D_{2}\rm{mod} \; C\Lambda(C,q).
$$
There exists $S\in\Lambda$ such that $D_{1}=D_{2}+CS$ and $CS\equiv
0\rm{mod} \; q$, and then there exists $S_{1}\in\Lambda$ such that
$$
\begin{pmatrix}
A_{1} & B_{1}\\
C & D_{1}
\end{pmatrix}
=
\begin{pmatrix}
E_{2} & S_{1}\\
0 & E_{2}
\end{pmatrix}
\begin{pmatrix}
A_{2} & B_{2}\\
C & D_{2}
\end{pmatrix}
\begin{pmatrix}
E_{2} & S\\
0 & E_{2}
\end{pmatrix}
$$
and we have $A_{1}=A_{2}+S_{1}C$. Hence
$\tr(A_{1}C^{-1}(G+Pq^{-1})+TC^{-1}D_{1})-
\tr(A_{2}C^{-1}(G+Pq^{-1})+TC^{-1}D_{2})=\tr(S_{1}(G+Pq^{-1})+TS)\equiv
\tr(S_{1}Pq^{-1})\break\rm{mod} \; 1$. Since $A_{1}=A_{2}+S_{1}C\equiv A_{2}\rm{mod} \; q$
implies $S_{1}C\equiv 0\rm{mod} \; q$ and $P$ satisfies the condition
$\house{\ast}$, we have $\tr S_{1}P=0\rm{mod} \; q$. Thus the
exponential sum $S(\cdot\cdot)$ is well-defined.
\begin{itemize}
\item[R2)] For $\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$, $D\equiv D'\rm{mod} \; q$, there
  exists a unique $S\in \Lambda\break\rm{mod} \; \Lambda ({}^{t}C,q)$ such that
$$
\begin{pmatrix}
E_{2} & S\\
0 & E_{2}
\end{pmatrix}
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\equiv 
\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}
\rm{mod} \; q.
$$
\end{itemize}
\end{proof}

\begin{proof}
Since\pageoriginale
$$
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}^{-1}
\equiv 
\begin{pmatrix}
\ast & \ast\\
0 & E_{2}
\end{pmatrix}
\rm{mod} \; q,
$$
there exists $S\in\Lambda$ such that
$$
\begin{pmatrix}
E_{2} & S\\
0 & E_{2}
\end{pmatrix}
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\equiv
\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}
\rm{mod} \; q.
$$
If
$$
\begin{pmatrix}
E_{2} & S_{1}\\
0 & E_{2}
\end{pmatrix}
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\equiv 
\begin{pmatrix}
E_{2} & S_{2}\\
0 & E_{2}
\end{pmatrix}
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\rm{mod} \; q,
$$
then $A+S_{1}C\equiv A+S_{2}C\rm{mod} \; q$ and so
$S_{1}-S_{2}\in\Lambda({}^{t}C,q)$.
\end{proof}

Therefore 
\begin{align*}
S(G,P,T,C,\left(\begin{smallmatrix} A' & B'\\ C & D'
\end{smallmatrix}\right))&=\sum_{D\in\mathscr{D}}\sum_{S\in\Lambda/\Lambda({}^{t}C,q)}e(\tr(A+SC)C^{-1}(G+Pq^{-1})\\
&\quad+TC^{-1}D)q^{-4}\sum_{M\rm{mod} \; 
  q}e((A+SC - A')M/q)), 
\end{align*}
where $\left(\begin{smallmatrix} A & B\\ C & D 
\end{smallmatrix}\right)\in \Gamma_{2}$ is any extension of $(C,D)$,
\begin{align*}
& = q^{-4}\sum_{M\rm{mod}\;
    q} \; \sum_{D\in\mathscr{D}}e(\tr(AC^{-1}(G+Pq^{-1})+TC^{-1}D+(A-A')M/q))\times\\
&\times \sum_{S\in\Lambda/\Lambda({}^{t}C,q)}e(\tr(S(P+CM)q^{-1})).
\end{align*}
The last exponential sum is $[\Lambda:\Lambda({}^{t}C,q)]$ or $O$
according as $(P+\frac{1}{2}(CM+{}^{t}M{}^{t}C))/q\in\Lambda^{\ast}$
or not. Thus it is equal to
\begin{align*}
q^{-4}[\Lambda:\Lambda({}^{t}C,q)]&\sum_{\substack{M\rm{mod} \;
    q\\ (P+\frac{1}{2}(CM+{}^{t}M{}^{t}C)/q\in\Lambda^{\ast}}}\sum_{D\in\mathscr{D}}
e(\tr(AC^{-1}(G+(P+\frac{1}{2}(CM\\
&\qquad+{}^{t}M{}^{t}C))/q)+TC^{-1}D))e(-\tr(A'M/q)).
\end{align*}
Putting
$N_{M}:=G+(P+\frac{1}{2}(CM+{}^{t}M{}^{t}C))/q\in\Lambda^{\ast},
S(N_{M})=\sum\limits_{D\in\mathscr{D}}\break e(\tr(AC^{-1}N_{M}+TC^{-1}D))$,
we have
$$
S(G,P,T,C,
\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}
)
=q^{-4}[\Lambda:\Lambda({}^{t}C,q)]\sum_{\substack{M\rm{mod} \;
    q\\ N_{M}\in\Lambda^{\ast}}}S(N_{M})e(-\tr A'M/q).
$$
Note\pageoriginale that $[\Lambda:\Lambda({}^{t}C,q)]\leq
[\Lambda:q\Lambda]$ and the number of $M$ does not exceed $q^{4}$.
\begin{itemize}
\item[R3)] The mapping $D\mapsto D$ from $\mathscr{D}$ to
  $\mathscr{D}':=\{D\in\mathscr{M}_{2}(\mathbb{Z})/C\Lambda|C$, $D$
  are a symmetric coprime pair such that $D+CS\equiv D'\rm{mod} \; q$ for
  some $S\in\Lambda\}$ is bijective.
\end{itemize}

\begin{proof}
Suppose that $D_{1}\equiv D_{2}\rm{mod} \; C\Lambda$ for $D_{1}$,
$D_{2}\in\mathscr{D}$. Since $D_{1}$, $D_{2}\in\mathscr{D}$,
$D_{1}\equiv D_{2}\equiv D'\rm{mod} \; q$. Hence for $S\in\Lambda$ with
$D_{1}-D_{2}=CS$ we have $CS\equiv 0\rm{mod} \; q$ and then
$S\in\Lambda(C,q)$. This means $D_{1}\equiv D_{2}\rm{mod} \; C\Lambda(C,q)$
and the mapping is injective. Since, for $D\in\mathscr{D}'$,
$D+CS(\equiv D\rm{mod} \; C\Lambda)$ for the $S$ involved in the definition
of $\mathscr{D}'$ is contained in $\mathscr{D}$, the mapping is
surjective.
\end{proof}

\begin{itemize}
\item[R4)] If $\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$, $D+CS\equiv D'\rm{mod} \; q$ for
  $S\in\Lambda\Longleftrightarrow A+S_{1}C\equiv A\rm{mod} \; q$ for
  $S_{1}\in \Lambda$
\end{itemize}

\begin{proof}
$D+CS\equiv D'\rm{mod} \; q$ for $S\in\Lambda$
\begin{align*}
&\Longleftrightarrow 
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\begin{pmatrix}
E_{2} & S\\
0 & E_{2}
\end{pmatrix}
\equiv 
\begin{pmatrix}
\ast & \ast\\
C & D'
\end{pmatrix}
\rm{mod} \; q\\
&\Longleftrightarrow 
\begin{pmatrix}
E_{2} & S_{1}\\
0 & E_{2}
\end{pmatrix}
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\begin{pmatrix}
E_{2} & S\\
0 & E_{2}
\end{pmatrix}
\equiv 
\begin{pmatrix}
A' & B'\\
C & D'
\end{pmatrix}
\rm{mod} \; q.\\
&\Longleftrightarrow 
\begin{pmatrix}
E_{2} & S_{1}\\
0 & E_{2}
\end{pmatrix}
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
\equiv
\begin{pmatrix}
A' & \ast\\
C & \ast
\end{pmatrix}
\rm{mod} \; q\\
&\Longleftrightarrow A+S_{1}C\equiv A'\rm{mod} \; q.
\end{align*}
For\pageoriginale $N=N_{M}$ we have, with $S(N_{M})$ as in (R2)
{\fontsize{10}{12}\selectfont
\begin{align*}
S(N_{M}) &=
\sum_{D\in\mathscr{D}'}e(\tr(AC^{-1}N+TC^{-1}D))\quad\text{(by
  (R3))}\\
&= \sum_{\substack{D:A+S_{1}C\equiv A'\rm{mod} \; q\\ \text{for } S_{1}\in
    \Lambda^{1},\left(\begin{smallmatrix} A & \ast\\ C & D
    \end{smallmatrix}\right)\in\Gamma_{2}}}e(\tr(AC^{-1}N+TC^{-1}D))\quad\text{(by
  (R4))}\\
&= \sum_{\substack{D\rm{mod} \; C\Lambda\\ :\left(\begin{smallmatrix} A
      &\ast\\ C & D
    \end{smallmatrix}\right)\in\Gamma_{2}}} \; \sum_{S\in\Lambda\rm{mod}\;
  \Lambda({}^{t}C,q)}e(\tr(A+SC)C^{-1}N+TC^{-1}D))\times\\
&\quad q^{-4}\sum_{M\rm{mod} \; q}e(\tr((A+SC-A')M)/q)\quad\text{(by
  (R2)).}\\
&= q^{-4}\sum_{\substack{D\rm{mod} \; C\Lambda\\ :\left(\begin{smallmatrix} A
&\ast\\ C & D
    \end{smallmatrix}\right)\in\Gamma_{2}}}e(\tr(AC^{-1}N+TC^{-1}D))\sum_{M\rm{mod} \;
  q}e(\tr(A-A')M/q)\times\\
&\quad \times \sum_{S\in\Lambda\rm{mod} \; \Lambda({}^{t}C,q)}e(\tr(SCM/q))\\
&= q^{-4}[\Lambda:\Lambda({}^{t}C,q)]
\sum\limits_{\substack{\rm{mod}\; q\\(CM +{}^t (CM))/ 2 \varepsilon q\Lambda^*}}
e(-\tr (A' M/q))
\sum_{\substack{D\rm{mod} \;
    C\Lambda\\ :\left(\begin{smallmatrix} A & \ast\\ C & D
    \end{smallmatrix}
\right)\in \Gamma_{2}}}\\
& \hspace{1cm} e(\tr(AC^{-1}(N+(1/(2q))(CM+{}^{t}M{}^{t}C))+TC^{-1}D))\\
&= q^{-4}[\Lambda:\Lambda({}^{t}C,q)]\sum_{\substack{M\rm{mod} \;
    q\\ (CM+{}^{t}(CM))/2\in q\Lambda^{\ast}}}\\
& \hspace{1cm} e(-\tr
A'M/q))K(N+1/(2q)(CM+{}^{t}(CM)), T;C).
\end{align*}}
Hence Proposition \ref{c1:prop-1.5.6} would follow immediately from 
\end{proof}

\setcounter{subprop}{13}
\begin{subprop}\label{c1:prop-1.5.14}
Let\pageoriginale $C=U^{-1}\left(\begin{smallmatrix} c_{1} & 0\\ 0 & c_{2}
\end{smallmatrix}\right)V^{-1}$, for $U$, $V\in GL_{2}(\mathbb{Z})$,
$0<c_{1}|c_{2}$. For $P$, $T\in\Lambda^{\ast}$, we have, for any
$\varepsilon>0$.
$$
K(P,T:C)=O(c^{2}_{1}c^{1/2+\varepsilon}_{2}(c_{2},t)^{1/2}),
$$
where $t$ is the $(2,2)$ entry of $T[V]$.
\end{subprop}

To prove this, we need several lemmas.

\setcounter{sublemma}{14}
\begin{sublemma}\label{c1:lem-1.5.15}
We have $K(P,T;U^{-1}CV^{-1})=K(P[{}^{t}U],T[V];C)$ for $P$,
$T\in\Lambda^{\ast}$, $U$, $V\in GL_{2}(\mathbb{Z})$ and $C\in
M_{2}(\mathbb{Z})$ with $\det C\neq 0$.
\end{sublemma}

\begin{proof}
Since
\begin{gather*}
\begin{pmatrix}
{}^{t}U & 0\\
0 & U^{-1}
\end{pmatrix}
\begin{pmatrix}
A & \ast\\
C & D
\end{pmatrix}
\begin{pmatrix}
V^{-1} & 0\\
0 & {}^{t}V
\end{pmatrix}
=
\begin{pmatrix}
{}^{t}U & AV^{-1} & \ast\\
U^{-1} & CV^{-1} & U^{-1}D{}^{t}V
\end{pmatrix},\\
D\rm{mod} \; C\Lambda\Longleftrightarrow U^{-1}D{}^{t}V\rm{mod} \;
U^{-1}C\Lambda{}^{t}V\Longleftrightarrow U^{-1}D{}^{t}V\rm{mod} \; U^{-1}CV^{-1}\Lambda.
\end{gather*}
Hence we have 
\begin{align*}
K(P,T;U^{-1}CV^{-1}) & =\sum\limits_{D\rm{mod} \;
  C\Lambda}e(\tr({}^{t}UAV^{-1}(U^{-1}CV^{-1})^{-1}P\\
&\qquad\qquad+(U^{-1}CV^{-1})^{-1}U^{-1}D{}^{t}VT))\\
&=\sum\limits_{D\rm{mod} \;
  C\Lambda}e(\tr(AC^{-1}P[{}^{t}U]+C^{-1}DT[V]))\\
&=K(P[{}^{t}U],T[V]:C).
\end{align*}
\end{proof}

\begin{sublemma}\label{c1:lem-1.5.16}
For the diagonal matrices $C=[c_{1},c_{2}]$, $F=[f_{1},f_{2}]$,
$H=[h_{1},h_{2}]$, suppose that $f_{1}|f_{2}$, $h_{1}|h_{2}$,
$c_{i}=f_{i}h_{i}$, $f_{i}$, $h_{i}>0(i=1,2)$ and that $f_{2}$,
$h_{2}$ are relatively prime. Put $X_{1}=sf^{2}_{2}F^{-1}$,
$X_{2}=th^{2}_{2}H^{-1}$ for integers $s$, $t$ with
$sf^{2}_{2}+th^{2}_{2}=1$. If then
{\fontsize{10}{12}\selectfont
$$
\begin{pmatrix}
A_{1} & B_{1}\\
F & D_{1}
\end{pmatrix},
\begin{pmatrix}
A_{2} & B_{2}\\
H & D_{2}
\end{pmatrix}
\in \Gamma_{2},
\Gamma_{2}\ni 
\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}
=
\begin{pmatrix}
X_{2}A_{1}+X_{1}A_{2} & \ast\\
HF & HD_{1}+FD_{2}
\end{pmatrix}
$$}
and the mapping $\varphi : (D_{1},D_{2})\mapsto D$ induces a bijection
from
\begin{gather*}
 \{D_{1}\rm{mod} \; F\Lambda|
\begin{pmatrix}
\ast & \ast\\
 F & D_{1}
\end{pmatrix}
\in \Gamma_{2}\}\times \{D_{2}\rm{mod} \; H\Lambda|
\begin{pmatrix}
\ast & \ast\\
H & D_{2}
\end{pmatrix}
\in \Gamma_{2}\}\\
 \text{to \ } \{|D\rm{mod} \; C\Lambda|
\begin{pmatrix}
\ast & \ast\\
C & D
\end{pmatrix}
\in\Gamma_{2}\}.
\end{gather*}
\end{sublemma}

\begin{proof}
Let\pageoriginale $\left(\begin{smallmatrix} A_{1} & B_{1}\\ F & D_{1}
\end{smallmatrix}\right)$, $\left(\begin{smallmatrix} A_{2} &
  B_{2}\\ H & D_{2}\end{smallmatrix}\right)\in \Gamma_{2}$ and put 
$$
A=X_{2}A_{1}+X_{1}A_{2}, D=HD_{1}+FD_{2}, B=(HF)^{-1}({}^{t}AD-E_{2}).
$$
Recall that, for $\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in \mathscr{M}_{4}(\mathbb{Z})$,
$\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$ if and only if
${}^{t}AD-{}^{t}CB=E_{2}$ and ${}^{t}AC$, ${}^{t}BD$ are symmetric.
\end{proof}

Now
$$
B=X_{2}B_{1}+X_{1}B_{2}+th^{2}_{2}H^{-1}A_{1}H^{-1}D_{2}+sf^{2}_{2}F^{-1}A_{2}F^{-1}D_{1} 
$$
is integral and both 
$$
{}^{t}AC=({}^{t}A_{1}X_{2}+{}^{t}A_{2}X_{1})FH=th^{2}_{2}{}^{t}A_{1}F+sf^{2}_{2}{}^{t}A_{2}H
$$
and 
\begin{align*}
{}^{t}BD&=({}^{t}DA-E_{2})C^{-1}D={}^{t}DAC^{-1}D-C^{-1}D\\
&={}^{t}DAC^{-1}D-F^{-1}D_{1}-H^{-1}D_{2}
\end{align*}
are symmetric. Moreover, ${}^{t}AD-{}^{t}CB=E_{2}$ and so
$\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$. If $HD_{1}+FD_{2}\in
C\Lambda$, then $F^{-1}D_{1}+H^{-1}D_{2}\in\Lambda$ and so
$F^{-1}D_{1}$, $H^{-1}D_{2}\in\Lambda$ since $(f_{2},h_{2})=1$. Hence
$\varphi$ is injective. It is easy to see that for the above
$\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$, $\left(\begin{smallmatrix} HA
  & HB-X{}^{t}AD\\ F & X_{2}D\end{smallmatrix}\right)$ and
  $\left(\begin{smallmatrix} FA & FB-X_{2}{}^{t}AD\\ H & X_{1}D
  \end{smallmatrix}\right)$ are also in $\Gamma_{2}$. From
  $H(X_{2}D)+F(X_{1}D)=D$ follows the surjectivity of $\varphi$.

\begin{sublemma}\label{c1:lem-1.5.17}
Let $C$, $F$, $H$, $X_{1}$, $X_{2}$ be as in the previous lemma. Then
for $P$, $T\in \Lambda^{\ast}$, we have
$$
K(P,T;C)=K(tP[h_{2}H^{-1}]),T;F)K(sP[f_{2}F^{-1}],T;H).
$$
\end{sublemma}

\begin{proof}
By the previous lemma, we have
\begin{align*}
K(P,T;C)&= \sum_{D}e(\tr(AC^{-1}P+C^{-1}DT))\\
&=\sum_{D_{1},D_{2}}e(\tr((X_{2}A_{1}+X_{1}A_{2})F^{-1}H^{-1}P\\
&\qquad\qquad +F^{-1}H^{-1}(HD_{1}+FD_{2})T))\\
&=\sum_{D_{1}}e(\tr(X_{2}A_{1}F^{-1}H^{-1}P+F^{-1}D_{1}T))\\
&\qquad\qquad\sum_{D_{2}}e(\tr(X_{1}A_{2}F^{-1}H^{-1}P+H^{-1}D_{2}T))\\
&= K(tP[h_{2}H^{-1}], T;F)K(sP[f_{2}F^{-1}], T;H).
\end{align*}
By\pageoriginale virtue of Lemmas \ref{c1:lem-1.5.15} and
\ref{c1:lem-1.5.17}, in order to prove Proposition \ref{c1:prop-1.5.14}, we
have only to show
$$
K(P,T;
\begin{pmatrix}
p^{e_{1}} & 0\\
0 & P^{e_{2}}
\end{pmatrix}
)=O(P^{2e_{1}+e_{2}/2}(P^{e_{2}},t)^{1/2}),
$$
where $P$ is a prime number $0\leq e_{1}\leq e_{2}$,
$T=\left(\begin{smallmatrix} \ast & \ast\\ \ast & t
\end{smallmatrix}\right)$ and the implied constant is independent of
$p$, $e_{1}$, $e_{2}$, $P$, $T$. Put $C=\left(\begin{smallmatrix}
  p^{e_{1}} & 0\\ 0 & p^{e_{2}}
\end{smallmatrix}\right)$, $D=\left(\begin{smallmatrix} d_{1} &
  d_{2}\\ d_{3} & d_{4}\end{smallmatrix}\right)$. $C^{-1}D$ is
symmetric if and only if $d_{3}=p^{e_{2}-e_{1}}d_{2}$. Hence $C$, $D$
are symmetric and coprime if and only if $d_{3}=p^{e_{2}-e_{1}}d_{2}$
and one of (i) - (iv) holds:
\begin{center}
\begin{tabular}{r@{\;\,}lr@{\;\,}l}
(i) & $e_{1}=e_{2}=0$, & (ii) & $e_{1}=0$, $e_{2}>0$, $p\nmid
  d_{4}$,\\[4pt]
(iii) & $0<e_{1}<e_{2}$, $p\nmid d_{1}d_{4}$, & (iv) &
  $0<e_{1}=e_{2}$, $d_{1}d_{4}-d^{2}_{2}\not\equiv 0\rm{mod} \; p$.
\end{tabular}
\end{center}
$D$ runs over classes $\rm{mod} \; C$ if and only if $d_{1}$, $d_{2}$,
$d_{4}$ runs over classes $\rm{mod} \; p^{e_{1}}$, $\rm{mod} \; p^{e_{1}}$, $\rm{mod} \;
p^{e_{2}}$ respectively. For a symmetric coprime pair $C$, $D$, we can
take $A$ satisfying the conditions ${}^{t}AC$ is symmetric and
$B=C^{1}({}^{t}AD-E_{2})\in\mathscr{M}_{2}(\mathbb{Z})$, so that
$\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)\in\Gamma_{2}$.

Put
$$
P=
\begin{pmatrix}
p_{1} & p_{2}/2\\
p_{2}/2 & p_{4}
\end{pmatrix},\quad 
T=
\begin{pmatrix}
t_{1} & t_{2}/2\\
t_{2}/2 & t_{4}
\end{pmatrix}
$$
\begin{itemize}
\item[(i)] In case $e_{1}=e_{2}=0$, we can take $A=D=0$ and
  $K(P,T;C)=1$.

\item[(ii)] In case $e_{1}=0$, $e_{2}>0$, we can take
  $\left(\begin{smallmatrix} 0 & 0\\ 0 & d
\end{smallmatrix}\right)$, with $d\rm{mod} \; p^{e_{2}}$ and $p\nmid d$ as
  $D$ and then we may take $A=\left(\begin{smallmatrix} 0 & 0\\ 0 & a
\end{smallmatrix}\right)$ with $ad\equiv 1\rm{mod} \; p^{e_{2}}$. 
Now\pageoriginale $K(P,T;C)$
\begin{align*}
&= \sum\limits_{\substack{d\rm{mod} \; p^{e_{2}}\\ p\nmid d}}e(\tr(
\begin{pmatrix}
0 & 0\\
0 & a/p^{e_{2}}
\end{pmatrix}
p+
\begin{pmatrix}
0 & 0\\
0 & d/p^{e_{2}}
\end{pmatrix}
T))\\
&= \sum_{\substack{d\rm{mod} \; p^{e_{2}}\\ p\nmid
    d}}e((ap_{4}+dt_{4})/p^{e_{2}})\text{ \ is a genuine Kloosterman}
\end{align*}
sum and we are through.

\item[(iii)] In case $0<e_{1}<e_{2}$, put
  $\delta=d_{1}d_{4}-p^{e_{2}-e_{1}}d^{2}_{2}(\not\equiv 0\rm{mod} \; p)$ and
  for an integer $d$ with $d\delta\equiv 1\rm{mod} \; p^{e_{2}}$, we can take
  $A=d\left(\begin{smallmatrix} d_{4} & -p^{e_{2}-e_{1}}d_{2}\\ -d_{2}
      & d_{1}  \end{smallmatrix}\right)$. Then we have 
\begin{gather*}
K(P,T;C) =\sum_{\substack{d_{1},d_{2}\rm{mod} \; p^{e_{1}}\\ d_{4}\rm{mod} \;
    p^{e_{2}}\\ p\nmid
    d_{1}d_{4}}}e(d(d_{4}p_{1}
p^{-e_{1}}-d_{2}p_{2}p^{-e_{1}}+d_{1}p_{4}p^{-e_{2}})+\\ 
+d_{1}t_{1}p^{-e_{1}}+d_{2}t_{2}p^{-e_{1}}+d_{4}t_{4}p^{-e_{2}}), 
\end{gather*}
taking $a$ in $\mathbb{Z}$ with $ad_{1}\equiv 1\rm{mod} \;
p^{e_{2}}(\Longleftrightarrow d_{4}\equiv a\delta
+p^{e_{2}-e_{1}}ad^{2}_{2}\break\rm{mod} \; p^{e_{2}})$.

Hence
\begin{align*}
K(P,T;C) &= \sum_{d_{1},d_{2}\rm{mod} \; p^{e_{1}},p\nmid d_{1}}
e(d_{1}t_{1}p^{-e_{1}}+d_{2}t_{2}p^{-e_{1}}\\
&\qquad\qquad +ap_{1}p^{-e_{1}}+ad^{2}_{2}t_{4}p^{-e_{1}})\\
&\quad \times \sum_{\delta \rm{mod} \; p^{e_2},p\nmid \delta} 
e(\{d(ad^{2}_{2}p_{1}p^{2(e_{2}-e_{1})}-d_{2}p_{2}
p^{e_{2}-e_{1}}\\
&\qquad\qquad +d_{1}p_{4})+\delta(at_{4})\}/p^{e_{2}})
\end{align*}
where the last sum on $\delta$ is the ordinary Kloosterman sum, and
since $p\nmid a$, we have
$$
K(P,T;C)=p^{2e_{1}}O(p^{e_{2}/2}(t_{4},p^{e_{2}})^{1/2}).
$$

\item[(iv)] In case $0<e_{1}=e_{2}=e$, $d_{1}$, $d_{2}$, $d_{4}$ runs
  over $\mathbb{Z}/p^{e}$ with $\delta=d_{1}d_{4}-d^{2}_{2}\not\equiv
  0\rm{mod} \; p$. Taking $A=d\left(\begin{smallmatrix} d_{4} &
    -d_{2}\\ -d_{2} & d_{1}  \end{smallmatrix}\right)$ for an integer
  $d$ with\pageoriginale $d\delta\equiv 1\rm{mod} \; p^{e}$, we have
\begin{align*}
K(P,T;C) &= \sum_{\substack{d_{1},d_{2},d_{4}\rm{mod} \;
    p^{e}\\ \delta\not\equiv 0 \rm{mod} \;
    p}}e(\{d\{d_{4}p_{1}-d_{2}p_{2}+d_{1}p_{4})\\
&\qquad\qquad+(d_{1}t_{1}+d_{2}t_{2}+d_{4}t_{4})\}/p^{e})\\
&= \sum_{p|d_{2}}+\sum_{p\nmid d_{2}}=\sum_{1}+\sum_{2}\quad\text{(say).}
\end{align*}
We have $\sum\limits_{1}=O(p^{2e+e/2}(t_{4},p^{e})^{1/2})$ quite
similarly to the previous case. For dealing with $\sum_{2}$, we define
integers $\delta_{1}$, $\delta_{2}$ by $d_{1}\equiv
d_{2}\delta_{1}\rm{mod} \; p^{e}$ and $d_{4}\equiv d_{2}\delta_{4}\rm{mod} \;
p^{e}$; then $\delta:=d_{1}d_{4}-d^{2}_{2}\equiv
d^{2}_{2}(\delta_{1}\delta_{4}-1)\rm{mod} \; p^{e}$ and $1\equiv
dd^{2}_{2}(\delta_{1}\delta_{4}-1)\rm{mod} \; p^{e}$. Then $\sum_{2}$ is
transformed to 
$$
\sum_{2} =\sum_{\substack{d_{2},\delta_{1},\delta_{4}\rm{mod} \;
    p^{e}\\ p\nmid d_{2}\\ \delta_{1}\delta_{4}\not\equiv 1\rm{mod} \;
    p}}e(d_{2}\{d(\delta_{4}p_{1}-p_{2}+\delta_{1}p_{4})+\delta_{1}t_{1}+t_{2}+\delta_{4}t_{4}\}/p^{e}); 
$$
noting that $dd_{2}\cdot d_{2}(\delta_{1}\delta_{4}-1)\equiv 1\rm{mod} \;
p^{e}$ and denoting by $x'$ the inverse class of $x\rm{mod} \; p^{e}$,
\begin{align*}
\sum_{2} &= \sum_{\substack{\delta_{1},\delta_{4}\rm{mod} \;
    p^{e}\\ \delta_{1}\delta_{4}\not\equiv 1\rm{mod} \;
    p}} \;\; \sum_{\substack{d_{2}\rm{mod} \; p^{e}\\ p\nmid
    d_{2}}}e(\{d'_{2}((\delta_{1}\delta_{4}-1)'(\delta_{4}p_{1}-p_{2}+\delta_{1}p_{4}))+\\
&\qquad +d_{2}(\delta_{1}t_{1}+t_{2}+\delta_{4}t_{4})/p^{e})\\
&= \sum\limits_{\substack{\delta_{1},\delta_{4}\rm{mod} \;
    p^{e}\\ \delta_{1}\delta_{4}\not\equiv 1\rm{mod} \;
    p}}O(p^{e/2}(\delta_{1}t_{1}+\delta_{4}t_{4},p^{e})^{1/2}).\\
&= O(p^{e/2}\sum_{x\rm{mod} \;
  p^{e}}(x,p^{e})^{1/2})\sharp\left\{\delta_{1},\delta_{4}\rm{mod} \;
p^{e}\Big|^{\delta_{1}\delta_{4}\not\equiv 1\rm{mod} \; p}_{x\equiv
  \delta_{1}t_{1}+t_{2}+\delta_{4}t_{4}\rm{mod} \; p^{e}}\right\}
\end{align*}
\end{itemize}
\end{proof}

Put\pageoriginale $(t_{4},p^{e})=p^{s}$; then $0\leq s\leq e$. If
$s=e$, then $\sum\limits_{2}=O(p^{3e})$ (by the trivial estimation)
$=O(p^{2e+e/2}(p^{e},t_{4})^{1/2})$ is what we want. Suppose $s<e$ and
$t_{4}=up^{s}$ with $(u,p)=1$; then
\begin{align*}
\sum_{2} &= O(p^{e/2}\sum_{x\rm{mod} \;
  p^{e}}(x,p^{e})^{1/2}\sharp\left\{\delta_{1},\delta_{4}\rm{mod} \;
p^{e}\Big|^{x\equiv\delta_{1}t_{1}+t_{2}\rm{mod} \; p^{s}}_{u\delta_{4}\equiv
  (x-\delta_{1}t_{1}-t_{2})/p^{s}\rm{mod} \; p^{e-s}}\right\}\\
&= O(p^{e/2}\sum_{0\leq i\leq e}p^{(e-i)/2}\sum_{\substack{v\rm{mod} \;
    p^{i}\\ p\nmid v}}p^{s}\sharp\\
&\qquad\qquad 
\left\{
\begin{matrix}
\delta_{1}\rm{mod} \;
p^{e}|vp^{e-i}\equiv \delta_{1}t_{1}+t_{2}\rm{mod} \; p^{s}\\
(\text{taking \ } x=vp^{e-i})
\end{matrix}
\right\}\\
&= p^{e+s}\sum_{0\leq i\leq e}p^{-i/2}O(\sharp\left\{\delta_{1}\rm{mod} \;
p^{e},v\rm{mod} \; p^{i}|vp^{e-i}\right.\\
&\qquad\qquad\left.\equiv \delta_{1} t_{1}+t_{2}\rm{mod} \; p^{s}\quad
p\nmid v\right\}.
\end{align*}
In case $p^{s}|t_{1}$ and $p^{s}|t_{2}$, we have
$$
\sum_{2}=p^{e+s}\sum_{\substack{0\leq i\leq e\\ e-i\geq
    s}}p^{i/2+e}O(1)=O(1)p^{5e/2+s/2}. 
$$
In case $p^{s}|t_{1}$ but $p^{s}\nmid t_{2}$, $vp^{e-i}\equiv
\delta_{1}t_{1}+t_{2}\rm{mod} \; p^{s}$ if and only if $vp^{e-i}\equiv
t_{2}\rm{mod} \; p^{s}$. Putting $a_{2}=\ord_{p}t_{2}<s$, we have $e-i=a_{2}$
and then
\begin{align*}
\sum_{2} &= p^{e+s-(e-a_{2})/2}O\left(\sharp\left\{\delta_{1}\rm{mod} \;
p^{e},v\rm{mod} \; p^{e-a_{2}}\Big|^{vp^{a_{2}}\equiv t_{2}\rm{mod} \;
  p^{s}}_{p\nmid v}\right\}\right)\\
&= p^{e+s-(e-a_{2})/2+e+(e-a_{2}-(s-a_{2}))}O(1).\\
&= p^{5e/2+a_{2}/2}O(1)=p^{5e/2+s/2}O(1).
\end{align*}
Thus, we are through in case $p^{s}|t_{1}$. In\pageoriginale case
$a_{1}=\ord_{p}t_{1}<s$ and $a_{2}=\ord_{p}t_{2}<a_{1}$,
$vp^{c-i}\equiv \delta_{1}t_{1}+t_{2}\rm{mod} \; p^{s}(p\nmid v)$ implies
$\ord(\delta_{1}t_{1}+t_{2})=a_{2}<s$ and so $e-i=a_{2}$, moreover,
$v\equiv \delta_{1}t_{1}p^{-a_{2}}+t_{2}p^{-a_{2}}\rm{mod} \;
p^{s-a_{2}}$. Hence $v\equiv t_{2}p^{-a_{2}}\rm{mod} \; p^{a_{1}-a_{2}}$ and
the number of possible $v$ is at most $p^{e-a_{1}}$ and for each $v$,
$\delta_{1}$ satisfies $\delta_{1}\equiv
(v-t_{2}p^{-a_{2}})(t_{1}p^{-a_{2}})^{-1}\rm{mod} \; p^{s-a_{1}}$ and so the
number of possible $\delta_{1}$ is not larger than
$p^{e-s+a_{1}}$. Thus we have 
$$
\sum_{2} = p^{e+s+(a_{2}-e)/2+e-s+a_{1} + e-a_{1}} O(1) =
O(1)p^{5e/2+a_{2}/2}=O(1)p^{5e/2+s/2}. 
$$
Finally in case $a_{1}=\ord_{p}t_{1}<s$, $a_{2}=\ord_{p}t_{2}\geq
a_{1}$, $\delta_{1}t_{1}+t_{2}\equiv 0\rm{mod} \; p^{a_{1}}$ and $a_{1}<s$
imply $e-i\geq a_{1}$, and from
$\delta_{1}(t_{1}p^{-a_{1}})=vp^{e-i-a_{1}}-t_{2}p^{-a_{1}}\rm{mod} \;
p^{s-a_{1}}$, it follows that the number of possible $\delta_{1}$ is
at most $p^{e-(s-a_{1})}$ for each $v$. Hence we have
\begin{align*}
\sum_{2} &= O(1)p^{e+s}\sum_{0\leq i\leq
  e-a_{1}}p^{-i/2+i+e-s+a_{1}}\\
&= O(1)p^{2e+a_{1}+\frac{1}{2}(e-a_{1})}=O(p^{5e/2+s/2}).
\end{align*}
Thus we have completed the proof of Proposition \ref{c1:prop-1.5.14} and
so of\break Proposition~\ref{c1:prop-1.5.6}. 

Now it remains to prove Proposition \ref{c1:prop-1.5.7}.

Let $T=\left(\begin{smallmatrix} t_{1} & t_{2}/2\\ t_{2}/2 & t_{4}
\end{smallmatrix}\right)\in\Lambda^{\ast}$. Since
$T[x]=t_{1}x^{2}_{1}+t_{2}x_{1}x_{2}+t_{4}x^{2}_{2}$, the sum
$\gamma(T,n)=\sum\limits_{x\in S(n)}(T[x],n)^{1/2}$ is well-defined.

\begin{sublemma}\label{c1:lem-1.5.18}
For integers $m$, $n$ with $(m,n)=1$ we have
$\gamma(T,mn)=\gamma(T,m)\gamma(T,n)$. 
\end{sublemma}

\begin{proof}
For $x={}^{t}(bd)$, $y={}^{t}(b'd')$ with $(b,d)=(b',d')=1$ we take
$z={}^{t}(ac)$ with $(a,c)=1$ so that $z\equiv x\rm{mod} \; m$, $z\equiv
y\rm{mod} \; n$. It\pageoriginale is easy to see that this induces a
bijective mapping from $S(m)\times S(n)$ to $S(m,n)$.
\end{proof}

Hence the left hand side is equal to
\begin{align*}
&\quad \sum_{S(mn)\ni x}(T[x],m)^{1/2}(T[x],n)^{1/2}\\
&= \sum_{\substack{S(m)\ni x\\ S(n)\ni x}}(T[z],m)^{1/2}(T[z],n)^{1/2}
\begin{pmatrix}
z\equiv x & \rm{mod} \; m\\
z\equiv y & \rm{mod} \; n
\end{pmatrix}\\
&=\text{The right hand side.}
\end{align*}

Thus we have only to give the proof for the case $n=p^{e}$ where $p$
is a prime number and $e\geq 1$. Put $S'=\{({}^{t}(bd)|(b,d,p)=1\}$
and define the equivalence ${}^{t}(bd)\approx {}^{t}(b'd')$ by
${}^{t}(bd)\equiv n \; {}^{t}(b'd')\rm{mod} \; p^{e}$ for some integer $n$; then
we have
$$
\gamma(T,p^{e})=\sum_{S'/\approx \ni x}(T[x],p^{e})^{1/2},
$$
since $x\mapsto x$ induces a bijective mapping from $S(p^{e})$ to
$S'/\approx$. Since $V\in\mathscr{M}_{2}(\mathbb{Z})$ with $\det
V\not\equiv 0\rm{mod} \; p$ operates on $S'/\approx$, we have
$\gamma(T,p^{e})=\sum\limits_{S'/=\ni x}(T[V_{x}],p^{e})^{1/2}$.

Hence we may suppose, without loss of generality that $T$ has a
canonical form $\rm{mod} \; p^{e}$ and more explicitly (i) $T$ is the
diagonal matrix $=[up^{a_{1}},uvp^{a_{2}}] \; O\leq a_{1}\leq a_{2}$,
$p\nmid uv$~ (ii) $2^{a}\left(\begin{smallmatrix} 1 & 1/2\\ 1/2 & 1
\end{smallmatrix}\right)$, $a\geq 0$ if $p=2$ 
or\pageoriginale (iii) $2^{a}\left(\begin{smallmatrix} 0 & 1/2\\ 1/2 & 0
\end{smallmatrix}\right)a\geq 0$ if $p=2$. Our aim is to prove
$\gamma(T,p^{e})=O(p^{e(1+\epsilon)}(e(T),p^{e})^{1/2})$ where
$e(T)=p^{a_{1}}$, $2^{a}$, $2^{a}$ according to (i), (ii), (iii)
respectively. 

\begin{sublemma}\label{c1:lem-1.5.19}
\begin{itemize}
\item[{\rm (i)}] $\sum\limits_{\substack{n\rm{mod} \; p^{e}\\ p\nmid
  n}}(n^{2}+v,p^{e-a_{1}})^{1/2}=O(p^{e})$ if $p\nmid v$, $0\leq
  a_{1}<e$, and

\item[{\rm (ii)}] $\sum\limits_{n\rm{mod} \; p^{e-1}}(p^{2}n^{2}+vp^{a_{2}-a_{1}},
  p^{e-a_{1}})^{1/2}=O(p^{e(1+\varepsilon)})$ for any $\varepsilon>0$, if
  $p\nmid v$, $0\leq a_{1}<a_{2}$ and $e\geq 1$.
\end{itemize}
\end{sublemma}

\begin{proof}
First we prove (i). If $p\neq 2$ and $\left(\dfrac{-v}{p}\right)=-1$, then
(i) is trivial since $p\nmid (n^{2}+v)$. Suppose $p\neq 2$ and
$\left(\dfrac{-v}{p}\right)=1$. Take a $p$-adic integer $g$ so that
$g^{2}+v=0$. If $n^{2}+v\equiv 0\rm{mod} \; p$, then $n=\pm g+mp^{s}$ with
$m\in\mathbb{Z}^{\ast}_{p}$, $s\geq 1$ and so $n^{2}+v=p^{s}(\pm
2gm+m^{2}p^{s})$ is exactly divisible by $p^{s}$. Thus we have
\begin{align*}
&\quad \sum_{\substack{n\rm{mod} \; p^{e}\\ p\nmid n}}(n^{2}+v,
  p^{e-a_{1}})^{1/2}\\
&= \sum_{\substack{n\rm{mod} \; p^{e}\\ p\nmid n, n^{2}+v\equiv 0\rm{mod} \; p}}
  (n^{2}+v, p^{e-a_{1}})^{1/2}+\sum_{\substack{n\rm{mod} \; p^{e}\\ p\nmid
      n,n^{2}+v\not\equiv 0(p)}}1\\
&\ub{\leq} 2\sum_{1\leq s\leq e} \sum_{\substack{m\rm{mod} \;
      p^{e-s}\\ p\nmid m}}(p^{s}, p^{e-a_{1}})^{1/2}+p^{e}\\
&= 2\sum_{1\leq s\leq
    e-a_{1}}p^{s/2}\varphi(p^{e-s})+2\sum_{e-a_{1}<s\leq
    e}p^{(e-a_{1})/2}\varphi(p^{e-s})+p^{e} 
\end{align*}
where $\varphi$ is the Euler function.
\end{proof}

The\pageoriginale first partial sum is equal to
\begin{align*}
& \sum_{1\leq s\leq
  e-a_{1}-1}p^{s/2}p^{e-s}(1-p^{-1})+p^{(e-a_{1})/2}\varphi(p^{a_{1}})\\
&=
  p^{e-\frac{1}{2}}(1-p^{-(e-a_{1}-1)/2})(1+p^{-\frac{1}{2}})+p^{(e-a_{1})/2}\varphi(p^{a_{1}})=O(p^{e}). 
\end{align*}
The second is
$p^{(e-a_{1})/2}(\varphi(p^{a_{1}-1})+\cdots+\varphi(1))=p^{(e+a_{1})/2-1}=O(p^{e})$. Thus,
in this case, we are through.

Suppose $p=2$ and $v\not\equiv 7\rm{mod} \; 8$; then $n^{2}+V\not\equiv 0\rm{mod} \;
8$ for old $n$. Hence we have
$$
\sum_{\substack{n\rm{mod} \; 2^{e}\\ 2\nmid
    n}}(n^{2}+v,2^{e-a_{1}})^{\frac{1}{2}}\leq \sum(4,
2^{e-a_{1}})^{\frac{1}{2}}=O(2^{e}). 
$$
Lastly, we suppose $p=2$, $v\equiv 7\rm{mod} \; 8$, and take
$g\in\mathbb{Z}^{\ast}_{2}$ so that $g^{2}+v=0$. Since, for
$n=g+2^{r}m$ with $r\geq 1$, $2\nmid m$,
$n^{2}+v=2^{r+1}m(g+2^{r-1}m)$, we have
\begin{align*}
&\quad  \sum_{\substack{n\rm{mod} \; 2^{e}\\ 2\nmid n}}(n^{2}+v,
  2^{e-a_{1}})^{\frac{1}{2}}\\
&=\sum_{\substack{m\rm{mod} \; 2^{e-1}\\ 2\nmid m}}(2^{2}m(g+m),
  2^{e-a_{1}})^{\frac{1}{2}}+\sum_{2\leq r\leq e}\sum_{\substack{m\rm{mod} \;
  2^{e-r}\\ 2\nmid m}}(2^{r+1},2^{e-a_{1}})^{\frac{1}{2}}\\
&= \sum_{\substack{n\rm{mod} \; 2^{e-1}\\ 2|n}}(2^{2}n,
  2^{e-a_{1}})^{\frac{1}{2}} +\sum_{2\leq r\leq e}2^{e-r-1}(2^{r+1},
  2^{e-a_{1}})^{\frac{1}{2}}\\
&= \sum_{1\leq r\leq
    e-1}2^{e-2-r}(2^{2+r},2^{e-a_{1}})^{\frac{1}{2}}+\sum_{2\leq r\leq
    e}2^{e-r-1}(2^{r+1}, 2^{e-a_{1}})^{\frac{1}{2}}\\
&= 2^{e}\sum_{2\leq r\leq e}2^{-r}(2^{r+1},
  2^{e-a_{1}})^{\frac{1}{2}}=2^{e}\sum_{2\leq r\leq
    e-a_{1}-1}2^{\frac{1}{2}(1-r)}\\
&\qquad\qquad +2^{e}\sum_{e-a_{1}\leq r\leq e}2^{-r+\frac{1}{2}(e-a_{1})}=O(2^{e}).
\end{align*}
Thus\pageoriginale (i) has been proved. Let us prove (ii). If
$a_{2}\geq e$, then we have
\begin{align*}
&\quad \sum_{n\rm{mod} \; p^{e-1}}(p^{2}n^{2}+vp^{a_{2}-a_{1}},
  p^{e-a_{1}})^{1/2}\\
&= \sum_{n\rm{mod} \; p^{e-1}}(p^{2}n^{2},p^{e-a_{1}})^{1/2}=\sum_{0\leq
    r\leq e-1}\varphi(p^{e-1-r})(p^{2+2r}, p^{e-a_{1}})^{1/2}\\
&= \sum_{0\leq r\leq
    (e-a_{1})/2-1}\varphi(p^{e-1-r})p^{1+r}+\sum_{(e-a_{1})/2\leq
    r\leq e-1}\varphi(p^{e-1-r})p^{(e-a_{1})/2}\\
&=O(ep^{e})=O(p^{e(1+\varepsilon)}).
\end{align*}
Suppose $a_{2}<e$; then we have
\begin{align*}
& \sum_{n\rm{mod} \; p^{e-1}}(p^{2}n^{2}+vp^{a_{2}-a_{1}}, p^{e-a_{1}})^{1/2}\\
& \sum_{0\leq
    r<(a_{2}-a_{1}-2)/2}\varphi(p^{e-1-r})p^{1+r}+\sum_{\substack{r=(a_{2}-a_{1}-2)/2\\ m\rm{mod} \;
  p^{e-1-r}\\ p\nmid m}}p^{(a_{2}-a_{1})/2}(m^{2}+v,
  p^{e-a_{2}})^{1/2}+\\
&\quad \sum_{(a_{2}-a_{1})/2\leq r\leq
    e-1}\varphi(p^{e-1-r})p^{(a_{2}-a_{1})/2}(n=mp^{r},p\nmid m)\\
&=
  O(ep^{e})+p^{(a_{2}-a_{1})/2}\sum_{\substack{r=(a_{2}-a_{1}-2)/2\\ m\rm{mod} \;
  p^{e-1-r}\\ p\nmid m}}(m^{2}+v, p^{e-a_{2}})^{1/2}.
\end{align*}
The last partial sum vanishes if $a_{2}\not\equiv a_{1}\rm{mod} \;
2$. Suppose $a_{2}\equiv a_{1}\rm{mod} \; 2$ and put $E:=e-1-r$,
$A_{1}=0$. Then $E=e-(a_{2}-a_{1})/2>(a_{2}+a_{1})/2>0=A_{1}$
and\pageoriginale $E\geq e-a_{2}$. Hence the last partial sum is not
larger than
\begin{gather*}
p^{(a_{2}-a_{1})/2}\sum_{\substack{m\rm{mod} \; p^{E}\\ p\nmid
    m}}(m^{2}+v,p^{E})^{\frac{1}{2}}\\
=p^{(a_{2}-a_{1})/2}O(p^{E}), \quad\text{(by (i))} = O(p^{e}).
\end{gather*}
Thus we have completed the proof of Lemma \ref{c1:lem-1.5.19}.

To prove
$\gamma(T,p^{e})=O(p^{e(1+\varepsilon)}(e(T),p^{e})^{\frac{1}{2}})$, note
that ${}^{t}(n,1)(n\rm{mod} \; p^{e})$, ${}^{t}(m,p^{t})$ $(p\nmid m,m \rm{mod} \;
p^{e-t}, 1\leq t\leq e)$ give a complete set of representatives of
$S'/\approx$. Suppose $T$ to be in diagonal form $[up^{a_{1}},
  uvp^{a_{2}}]$, $0\leq a_{1}\leq a_{2}$, $p\nmid uv$; then
\begin{align*}
\gamma(T,p^{e})&=\sum_{n\rm{mod} \;
  p^{e}}(n^{2}up^{a_{1}}+uvp^{a_{2}},p^{e})^{\frac{1}{2}}\\
&\qquad+\sum_{1\leq
  t\leq e}\sum_{\substack{m\rm{mod} \; p^{e-t}\\ p\nmid
    m}}(m^{2}up^{a_{1}}+uvp^{a_{2}+2t}, p^{e})^{\frac{1}{2}}.
\end{align*}
We want to show that
$\gamma(T,p^{e})=O(p^{e(1+\varepsilon)+\min(a_{1},e)/2})$.

If $a_{1}\geq e$, then 
\begin{align*}
\gamma(T,p^{e}) &=
p^{e/2}\{p^{e}+\varphi(p^{e-1})+\cdots+\varphi(1)\}\\
&= p^{e/2}\{p^{e}+p^{e-1}\}=O(p^{3e/2}).
\end{align*}
In case $a_{1}<e$, we have 
\begin{align*}
\gamma(T,p^{e}) & =p^{\frac{1}{2}a_{1}}\sum_{\substack{n\rm{mod} \;
    p^{e}\\ p\nmid n}}(n^{2}+vp^{a_{2}-a_{1}},
  p^{e-a_{1}})^{\frac{1}{2}}+\\
&\quad +p^{\frac{1}{2}a_{1}}\sum_{n\rm{mod} \;
    p^{e-1}}(p^{2}n^{2}+vp^{a_{2}-a_{1}},
  p^{e-a_{1}})^{\frac{1}{2}}+\\
&\quad +p^{\frac{1}{2}a_{1}}\sum_{1\leq t\leq e}\sum_{\substack{m\rm{mod} \;
      p^{e-t}\\ p\nmid m}}(m^{2}+vp^{a_{2}-a_{1}+2t},
  p^{e-a_{1}})^{\frac{1}{2}} 
\end{align*}

Hence\pageoriginale if $a_{1}=a_{2}<e$, then
$$
\gamma(T,P^{e})=p^{a_{1}/2}\mathcal{O}(p^{e})+p^{\frac{1}{2}a_{1}+e-1}+p^{a_{1}/2}\sum_{1\leq
  t\leq e}\varphi(p^{e-t})=O(p^{a_{1}/2+e}).
$$
If $a_{1}<e$ and $a_{1}<a_{2}$, then
\begin{align*}
\gamma(T,P^{e}) &=
p^{a_{1}/2}\varphi(p^{e})+p^{a_{1}/2}O(p^{e(1+\varepsilon)})+p^{a_{1}/2}\sum_{1\leq
  t\leq e}\varphi(p^{e-t})\\
&= O(p^{e(1+\varepsilon)+a_{1}/2}). 
\end{align*}
Suppose $T=2^{a}\left(\begin{smallmatrix} 1 &
  \frac{1}{2}\\ \frac{1}{2} & 1\end{smallmatrix}\right)$, $a\geq 0$
  and $p=2$. Since
$$
T\begin{pmatrix}
x_{1}\\
x_{2}
\end{pmatrix}
=2^{a}(x^{2}_{1}+x_{1}x_{2}+x^{2}_{2}), \ord
T[x]=2^{a}\quad\text{if}\quad (x_{1},x_{2},2)=1.
$$
Hence 
$$
\gamma(T,2^{e})=\sum_{x\in S'/\approx}
(2^{a},2^{e})^{\frac{1}{2}}=(2^{e}+2^{e-1})2^{\mid
  (a,e)/2}=O(2^{e+\min(a,e)/2}).
$$
Suppose $T=2^{a}\left(\begin{smallmatrix} 0 &
  \frac{1}{2}\\ \frac{1}{2} & 0
\end{smallmatrix}\right)$; then 
\begin{align*}
\gamma(T,2^{e}) &=\sum\limits_{n\rm{mod} \;
  2^{e}}(2^{a}n, 2^{e})^{\frac{1}{2}}+\sum_{1\leq t\leq
  e} \; \sum_{\substack{m\rm{mod} \; 2^{e-t}\\ 2\nmid
    m}}(2^{a+t}m,2^{e})^{\frac{1}{2}}\\
&= \sum_{0\leq t\leq e}\varphi(2^{e-t})(2^{a+t},
2^{e})^{\frac{1}{2}}+\sum_{1\leq t\leq
  e}\varphi(2^{e-t})(2^{a+t},2^{e})^{\frac{1}{2}}\\
&= 2^{e-1+\min(a,e)/2}+2\sum_{1\leq t\leq
  e-1}2^{e-t-1+\min(a+t,e)/2}+2^{e/2}\\
&\leq
2^{e+\min(a,e)/2}+2(e-1)2^{e+\min(a,e)/2}=O(2^{e(1+\epsilon)+\min(a,e)/2})
\end{align*}
since $e-t-1+\min(a+t,e)/2\leq e+\min(a,e)/2$.


\section{Estimation of Fourier Coefficients of Modular
  Forms}\label{c1:sec-1.6}

Let\pageoriginale  $\{n,k,s\}$ denote the space of modular forms of
degree $n$, 
weight $k$ and level $s$. In this section, we first obtain a
Representation Theorem for $\{n,k,s\}$ with even $k\geq 2n+2$ in terms
of the Eisenstein series $E^{k}_{n,j}(Z;f)$ in the sense of Klingen
\cite{key13} arising as `lifts' of cusp forms $f$ in $\{j,k,s\}$ for
$j\leq n$. Then we shall derive an estimate for the Fourier
coefficients of modular forms in $\{n,k,s\}$ for even (integral)
$k\geq 2n+2$, following Kitaoka \cite{key10}. We first prove a few
preparatory lemmas for the Representation Theorem, following H.\@
Braun \cite{key3} and Christian \cite{key6}.

\begin{sublemma}\label{c1:lem-1.6.1}
For any $R\in \Sp(n,\mathbb{Q})$, there exist an upper triangular $Q$
in $GL(n,\mathbb{Q})$ and an $(n,n)$ rational symmetric $S$ such that
$M=R\left(\begin{smallmatrix} {}^{t}Q & {}^{t}QS\\ 0 & Q^{-1}
\end{smallmatrix}\right)$ is in $\Gamma_{n}$.
\end{sublemma}

\begin{proof}
Let $R=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)$ with $(n,n)$ matrices $A$, $B$, $C$,
$D$. For some $d\neq 0$ in $\mathbb{Z}$, $(-d{}^{t}C, d{}^{t}A)$ is an
integral symmetric pair and further $(-{}^{t}C{}^{t}A)$ has rank
$n$. Hence, for some $U$ in $GL(2n,\mathbb{Z})$,
$(-d{}^{t}Cd{}^{t}A)U=(G \; 0)$ with $(n,n)$ invertible integral
$G$. Clearly then $C':=-dG^{-1t}C$, $D':=dG^{-1t}A$ form a coprime
symmetric pair and constitute therefore the last $n$ rows of
$N=(M')^{-1}$ for some $M'$ in $\Gamma_{n}$, so that we have
$NR=\left(\begin{smallmatrix} \ast &
  \ast\\ dG^{-1}(-{}^{t}CA+{}^{t}AC) & \ast
\end{smallmatrix}\right)=\left(\begin{smallmatrix} \ast & \ast\\ 0 &
  Q_{1}
\end{smallmatrix}\right)$ with $Q_{1}$ in $GL(n,\mathbb{Q})$. By easy
induction, there exists $V$ in $GL(n,\mathbb{Z})$ with $Q:=VQ_{1}$ in
upper triangular form. The lemma is now immediate with
$M=M'\left(\begin{smallmatrix} t_{V} & 0\\ 0 & V^{-1}
\end{smallmatrix}\right)$. 
\end{proof}

Let us fix, in the sequel, $M_{1},\ldots,M_{t}$ in $\Gamma_{n}$ so
that $\Gamma_{n}=\coprod\limits_{1\leq i\leq
  t}\Gamma_{n}(s)M_{i}$. Then, by Lemma 1, any $R\in
\Sp(n,\mathbb{Q})$ can be written in the form\pageoriginale\break
$N'M_{i}\left(\begin{smallmatrix} {}^{t}Q & {}^{t}QS\\ 0 & Q^{-1}
\end{smallmatrix}\right)$ for some $N'$ in $\Gamma_{n}(s)$ and $M_{i}$
with $Q$, $S$ as in\break Lemma~\ref{c1:lem-1.6.1}. For $f$ in $\{n,k,s\}$, we
have therefore
\begin{align*}
(f|_{k}R)(Z) &= (((f|_{k}N')|_{k}M_{i}\left.
\begin{pmatrix}
{}^{t}Q & {}^{t}QS\\
0 & Q^{-1}
\end{pmatrix}
\right)(Z)\\
&= (f|_{k}M_{i})({}^{t}Q(Z+S)Q)\quad(\det Q)^{k}.
\end{align*}

Now, for any $j$ with $1\leq j\leq n$ and $Z_{1}\in\mathscr{G}_{n-j}$,
the $j^{\text{th}}$-iterate $\Phi^{j}$ of the Siegel operator on any
$f:\mathscr{G}_{n}\to \mathbb{C}$ is defined by
$$
(\Phi^{j} f)(Z_{1})=\lim\limits_{\lambda\to \infty}f
\left(
\begin{pmatrix}
Z_{1}(n-j) & 0\\
0 & i\lambda E_{j}
\end{pmatrix}
\right);
$$
it is known that for $f$ in $\{n,k,s\}$, $\Phi_{j}f$ exists and is in
$\{n-j,k,\ast\}$. 

\begin{defi*}
We call $f$ in $\{n,k,s\}$ a $j$-{\em cusp form}, if $\Phi^{j}(f|_{k}R)=0$
for every $R$ in $\Sp(n,\mathbb{Q})$. For $j=1$, we call $f$ just a
{\em cusp form}.
\end{defi*}

\begin{sublemma}\label{c1:lem-1.6.2}
Any $f$ in $\{n,k,s\}$ is a $j$-cusp form if any only if\break
$\Phi^{j}(f|_{k}M_{i})=0$ for $1\leq i\leq t$.
\end{sublemma}

\begin{proof}
To prove the lemma, it is enough to show that $f$ is a $j$-cusp form
if $\Phi^{j}(f|_{k}M)=0$ for every $M$ in $\Gamma_{n}$ (or
equivalently if $\Phi^{j}(f|_{k}M_{i})=0$ for $1\leq i\leq t$). The
limit as $\lambda$ tends to $\infty$ in the definition of
$\Phi^{j}(f|_{k}M_{i})(Z_{1})$ can be applied termwise to the Fourier
expansion
$$
(f|{k}M_{i})
\begin{pmatrix}
Z_{1} & 0\\
0 & i\lambda E_{j}
\end{pmatrix}
=\sum_{T=\left(\begin{smallmatrix} T_{1}^{(n-j)} & T_{2}\\ \ast &
    T_{3}
  \end{smallmatrix}\right)\geq 0}a(T;f;M_{i})e^{2\pi
  i\tr(T\left(\begin{smallmatrix} Z_{1} & 0\\ 0 & i\lambda E_{j}
  \end{smallmatrix}\right))/s}
$$
and hence
$$
\Phi^{j}(f|_{k}M_{i})(Z_{1})=\sum_{T_{1}^{(n-j)}\geq
  0}a(\left(\begin{smallmatrix} T_{1} & 0\\ 0 & 0
\end{smallmatrix}\right);f;M_{i})e^{2\pi i\tr((T_{1}Z_{1}))/s}
$$
The\pageoriginale assumption $\Phi^{j}(f|_{k}M_{i})=0$ for $1\leq
i\leq t$ is equivalent then to $a(\left(\begin{smallmatrix} T_{1} &
  0\\ 0 & 0\end{smallmatrix}\right);f;M_{i})=0$ for all $T_{1}\geq 0$
  and $1\leq i\leq t$. On the other hand, we know from Lemma
  \ref{c1:lem-1.6.1} that for $R$ in $\Sp(n;\mathbb{Q})$,
\begin{align*}
\Phi^{j}(f|_{k}R)(Z_{1}) &= \Phi^{j}(f|M_{i}
\begin{pmatrix}
{}^{t}Q & {}^{t}QS\\ 
0 & Q^{-1}
\end{pmatrix})
(Z_{1})\\
&\qquad\text{ \ for suitable $M_{i}$ and $Q$, $S$ as above}\\
&= \lim\limits_{\lambda\to \infty}(f|M_{i})({}^{t}Q(
\begin{pmatrix}
Z_{1} & 0\\
0 & i\lambda E_{j}
\end{pmatrix}+S)Q)\\
&= \sum a(T;f;M_{i})e^{\frac{2\pi
    i}{s}(\tr(T_{1}Z_{1}[Q_{1}]+\tr(T_{3}Z_{1}[Q_{2}])+2\tr(T_{2}{}^{t}Q_{2}Z_{1}Q_{1})))}\times\\
&\quad T=
\begin{pmatrix}
T_{1}^{(n-j)} & T_{2}\\
\ast & T_{3}
\end{pmatrix}
\geq 0\times e^{\frac{2\pi i}{s}\tr(TS')}\lim\limits_{\lambda\to
  \infty}e^{\frac{-2\pi \lambda}{s}}\tr(T_{3}[{}^{t}Q_{3}]), 
\end{align*}
writing $Q=\left[\begin{smallmatrix} Q_{1}^{(n-j)} & Q_{2}\\ 0 & Q_{3}
  \end{smallmatrix}\right)$ and $S'={}^{t}QSQ$. Now since $\det
  Q_{3}\neq 0$, $\tr(Q_{3}T_{3}{}^{t}Q_{3})\neq 0$ unless $T_{3}=0$
  and therefore for every $T$ with $T_{3}=0$ and therefore for every
  $T$ with $T_{3}\neq 0$, the limit of the corresponding term as
  $\lambda$ tends to $\infty$, is zero. If $T_{3}=0$, then $T_{2}=0$
  as well, in view of ``$T\geq 0$''. Thus in the limit as $\lambda$
  tends to $\infty$, at most the terms corresponding to
  $T=\left(\begin{smallmatrix} T_{1}^{(n-j)} & 0\\ 0 & 0
  \end{smallmatrix}\right)$ can survive. Our assumption
  ``$\Phi^{j}(f|_{k}M_{i})=0$'' above implies a $(T;f;M_{i})=0$ for
  these latter type of $T$ are $0$, leading to $\Phi^{j}(f|_{k}R)=0$
  for every $R$ in $\Sp(n,\mathbb{Q})$ and also proving the lemma.
\end{proof}

For $0< j\leq n$, let $\Delta_{n,n-j}(s)=\{M\in\Gamma_{n}(s)|$ the
entries of the first $2n-j$ columns of the last $j$ rows of $M$ are
$0\}$.

Then 
$$
\Delta_{n,n-j}(s)=
\left\{
M=
\begin{pmatrix}
A & 0 & B & \ast\\
\ast & Q^{-1} & \ast & \ast\\
C & 0 & D & \ast\\
0 & 0 & 0 & Q
\end{pmatrix}
\in\Gamma_{n}(s)
\right\}
$$
and is indeed a subgroup of $\Gamma_{n}(s)$; any $M$ in
$\Delta_{n,n-j}(s)$, $Q\equiv E_{j}(\rm{mod} \; s)$ in $GL(j,\mathbb{Z})$ and
further $\ub{M}:=\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)$\pageoriginale is in $\Gamma_{n-j}(s)$. The mapping
$M\mapsto \ub{M}$ is a homomorphism of $\Delta_{n,n-j}$ onto
$\Gamma_{n-j}(s)$, with kernel
$$
\left\{
\begin{pmatrix}
E_{n-j} & 0 & 0 & \ast\\
\ast & E_{j} & \ast & \ast\\
 & & E_{n-j} & \ast\\
0 & & 0 & E_{j}
\end{pmatrix}
\in \Delta_{n,n-j}(s)
\right\}
$$
We denote $\Delta_{n,n-j}(1)$ simply as $\Delta_{n,n-j}$.

\begin{defi*}
For $N_{1}$, $N_{2}$ in $\Gamma_{n}$, we say $N_{1}\tilde{j,s}$
$N_{2}$ if, for some $M$ in $\Gamma_{n}(s)$, we have
$N=N^{-1}_{1}MN_{2}\in\Delta_{n,n-j}$. 
\end{defi*}

\begin{sublemma}\label{c1:lem-1.6.3}
For $N_{1}$, $N_{2}$ in $\Gamma_{n}$ with $N_{1}\tilde{j,s}N_{2}$
and $f$ in $\{n,k,s\}$, we have
$\Phi^{j}(f|N_{1})=0\Longleftrightarrow \Phi^{j}(f|N_{2})=0$ for
$0\leq j\leq n$.
\end{sublemma}

\begin{proof}
Writing $Z=\left(\begin{smallmatrix} Z_{1} & Z_{2}\\ {}^{t}Z_{2} &
  Z_{3}
\end{smallmatrix}\right)$ with $Z_{1}\in\mathscr{G}_{n-j}$, we have,
for $j<n$, $N<Z>=\left(\begin{smallmatrix} \ub{N}<Z_{1}> & \ast\\ \ast
  & \ast\end{smallmatrix}\right)$ and $\det
  N\{Z\}=\det\ub{N}\{Z_{1}\}\det Q$ for some $Q$ in
  $GL(j,\mathbb{Z})$. Thus
  $\Phi^{j}(f|N_{2})=\Phi^{j}(f|MN_{2})=\Phi^{j}(f|N_{1}N)=(\det
  Q)^{k}(\Phi^{j}(f|N_{1}))|\ub{N}$, for $0\leq j<n$.
\end{proof}

The lemma follows on noting that for $j=n$, $\Phi^{n}(f|N_{i})=$ the
constant term in the Fourier expansion of $f|N_{i}$ and
$|a(0,N_{1})|=|a(0,N_{2})|$. 

For\pageoriginale $T\geq 0$, let 
$$
\Gamma_{n}(s;T)=
\left\{
\begin{pmatrix}
{}^{t}U & \ast\\
0 & U^{-1}
\end{pmatrix}
\in\Gamma_{n}(S)|T[{}^{t}U]=T\right\}.
$$
Then, for even (integral) $k>n+1+\rank T$, we define the Poincar\'e
series $g_{k}$ and $p_{k}$ by
{\fontsize{10}{12}\selectfont
\begin{align*}
& g_{k}(Z,T;\Gamma_{n}(s)): = \sum_{M\in\Gamma_{n}(s,T)\backslash \Gamma_{n}(s)}e^{\frac{2\pi
    i}{s}\tr(TM<Z>)}(\det M\{Z\})^{-k},\\
&
  p_{k}(Z,T;N;\Gamma_{n}(s)):=\sum_{N^{-1}M\in\Gamma_{n}(s,T)\backslash
    N^{-1}\Gamma_{n}(s)}e^{\frac{2\pi i}{s}\tr(TN^{-1}M<Z>)}(\det
  N^{-1}M\{Z\})^{-k} 
\end{align*}}
for $N$ in $\Gamma_{n}$. These series converge absolutely, uniformly
on compact subsets of $\mathscr{G}_{n}$ and belong to $\{n,k,s\}$ for
$k>n+1+\rank (T)$. For $T=0$, they are just Eisenstein series. Clearly
$p_{k}(Z,T;E_{2j};\Gamma_{n}(s))=g_{k}(Z,T;\Gamma_{n}(s))$. 

\begin{sublemma}\label{c1:lem-1.6.4}
For $k>n+1+\rank T$ and $N$ in $\Gamma_{n}$, we have
$$
p_{k}(Z,T;N;\Gamma_{n}(s))=g_{k}(Z,T;\Gamma_{n}(s))|N^{-1}.
$$
\end{sublemma}

\begin{proof}
Suppose $M'$ runs over a complete set of representatives of the right
cosets of $\Gamma_{n}(s)$ modulo $\Gamma_{n}(s^{\ast},T)$. Then
$M:NM'N^{-1}$ is in $\Gamma_{n}(s)$ and $M'N^{-1}=N^{-1}M$. Further
$M'N^{-1}$ runs over a complete set of representatives of elements in
$N^{-1}\Gamma_{n}(s)$ such that, for {\em no} two such distinct
elements, say $N^{-1}M_{1}$, $N^{-1}M_{2}$ we have
$N^{-1}M_{1}\in\Gamma_{n}(s,T)N^{-1}M_{2}$; otherwise, we will have
for $M'_{1}\neq M'_{2}$ with $M'_{i}N^{-1}:=N^{-1}M_{i}$, $i=1$, $2$,
$M'_{1}\in\Gamma_{n}(s,T)M'_{2}$, a contradiction. 
\end{proof}

Now\pageoriginale
\begin{align*}
g_{k}(Z,T;\Gamma_{n}(s)|N^{-1} &= \sum_{M'\in\Gamma_{n}(s,T)\backslash
  \Gamma_{n}(s)}e^{\frac{2\pi i}{s}\tr(TM'<N^{-1}<Z>>)}\\
&\qquad\det M'\{N^{-1}<Z>\}^{-k}\det N^{-1}\{Z\}^{-k}\\
&= \sum_{M'}e^{\frac{2\pi i}{s}\tr(T(M'N^{-1})<Z>)}(\det
(M'N^{-1})\{Z\})^{-k}\\
&= \sum_{N^{-1}M\in\Gamma_{n}(s,T)\backslash
  N^{-1}\Gamma_{n}(s)}e^{\frac{2\pi
    i}{s}\tr(T(N^{-1}M)<Z>)}\\
&\qquad(\det(N^{-1}M)\{Z\})^{-k}\\
&= p_{k}(Z,T;N;\Gamma_{n}(s))
\end{align*}

\begin{sublemma}\label{c1:lem-1.6.5}
For $T=\left(\begin{smallmatrix} T_{0}^{(n-j)} & 0\\ 0 & 0
\end{smallmatrix}\right)$ with $T_{0}^{(n-j)}>0$ and
$Z=\left(\begin{smallmatrix} Z_{0}^{(n-j)} & \ast\\ \ast & \ast
\end{smallmatrix}\right)\in\mathscr{G}_{n}$ we have
$\Phi^{j}(g_{k}(Z,T;\Gamma_{n}(s))=\ast
g_{k}(Z_{0},T_{0};\Gamma_{n-j}(s))$ if $0<j<n$ and
$\Phi^{n}(g_{k}(Z,0;\Gamma_{n}(s))=1$. 
\end{sublemma}

\begin{proof}
The involved limit with $Z=\left(\begin{smallmatrix} Z_{0} & 0\\ 0 &
  i\lambda E_{g}\end{smallmatrix}\right)$ (as $\lambda\to \infty$) in
$\Phi^{j}$ can be applied termwise to the series defining $g_{k}$,
namely to each term
$$
e^{\frac{2\pi i}{s}\tr(TM<Z>)}(\det M\{Z\})^{-k}=e^{\frac{2\pi
    i}{s}\tr(T_{0}(M<Z>)_{0})}(\det M\{Z\})^{-k} 
$$
where $(M<Z>)_{0}$ denote the top (leftmost) $(n-j,n-j)$ submatrix of
$M<Z>$. Let $\left(\begin{smallmatrix} C_{1} & C_{2} & D_{1} &
  D_{2}\\ C_{3} & C_{4} & D_{3} & D_{4}
\end{smallmatrix}\right)$ with $(n-j,n-j)$ submatrices $C_{1}$,
$D_{1}$ be the matrix formed by the last $n$ rows of $M(\text{in }
\Gamma_{n}(s))$ and $Y_{0}=\Iim(Z_{0})$. As in Klingen
(Math.\@\pageoriginale Zeit.\@ 102 (1967), p.35), we have
\begin{gather*}
\abs(\det M\{Z\})^{-2}=\det(\Iim((M<Z>)_{0})/(\det
Y_{0}P(\lambda)\text{ \ where \ } P(\lambda):=\\
\det(\lambda
Y^{-1}_{0}[[Z_{0}^{t}C_{3}+{}^{t}D_{3}]]+E_{j}[[i\lambda{}^{t}C_{4}+{}^{t}D_{4}]]) 
\end{gather*}
with ${}^{t} \bar{S}RS$ abbreviated as $R[[S]]$; we are using here the
relations
\begin{align*}
& \begin{pmatrix}
(Y_{M})_{0} & Y_{M,2}\\
\ast & Y_{M,3}
\end{pmatrix}
^{-1}\{=
\begin{pmatrix}
\ast & \ast\\ 
\ast & (Y_{M,3}-(Y_{M})^{-1}_{0}[Y_{M,2}])^{-1}
\end{pmatrix}\}\\
 &\qquad =
(\Iim(M<Z>))^{-1}=(\Iim(Z))^{-1}[[{}^{t}(CZ+D)]]=\\
&\qquad = 
\begin{pmatrix}
Y^{-1}_{0} & 0\\
0 & \frac{1}{\lambda}E_{j}
\end{pmatrix}
[[
\begin{pmatrix}
\ast & {}^{t}(C_{3}Z_{0}+D_{3})\\
\ast & t(i\lambda C_{4}+D_{4})
\end{pmatrix}
]],
\end{align*}
and
\begin{align*}
& (\abs(\det((CZ+D)^{2})/((\det Y_{0})\lambda^{j})\\
& =1/\det(\Iim(M<Z>))=1/((\det(Y_{M})_{0})(\det
(Y_{M,3}-(Y_{N})^{-1}_{0}[Y_{M,2}])))\\
&=(1/\det(\Iim((M<Z>)_{0}))(\det
(Y^{-1}_{0}[[{}^{t}(C_{3}Z_{0}+D_{0}))]\\
&\qquad +\frac{1}{\lambda}E_{j}[[{}^{t}(i\lambda
      C_{4}+D_{4})]]) 
\end{align*}
Now
$$
\left|
e^{\frac{2\pi
    i}{s}\tr(T_{0}(M<Z>)_{0})}\det(\Iim((M<Z>)_{0}))^{k/2}\right|\leq
\prod_{1\leq \ell\leq n-j}(\lambda^{k/2}_{\ell} e^{-c\lambda_\ell}) 
$$
where $c=c(T_{0})>0$ and $\lambda_{1},\ldots,\lambda_{n-j}$ are the
eigenvalues of $\Iim((M<Z>)_{0})$; hence it is bounded for all $M$,
uniformly as $\lambda$ goes to infinity. We can now conclude from
above that, for fixed $Z_{0}$,
$$
\lim\limits_{\lambda\to \infty}e^{\frac{2\pi i}{s}\tr(TM<Z>)}(\det M\{Z\})^{-k}=0,
$$
unless $P(\lambda)$ is a constant. Next we determine, for what $M$,
$P(\lambda)$ can turn out to be a constant. The relation above
connecting $P(\lambda)$ and $\abs\break(\det M\{Z\})^{-2}$ shows that
$P(\lambda)>0$ while each of $\lambda
Y^{-1}_{0}[[Z_{0}{}^{t}C_{3}+{}^{t}D_{3}]]$ and
$E_{j}[[i\lambda{}^{t}C_{4}+{}^{t}D_{4}]]$ is non-negative definite.
Hence,\pageoriginale for all $\lambda$,
\begin{align*}
& \det (\lambda^{2}C_{4}{}^{t}C_{4}+D^{t}_{4}D_{4})=\det
(E_{j}[[i\lambda{}^{t}C_{4}+{}^{t}D_{4}]])\\
& \leq \det (\lambda Y^{-1}_{0}[[Z^{t}_{0}C_{3}+{}^{t}D_{3}]]+E_{j}[[i\lambda{}^{t}C_{4}+{}^{t}D_{4}]]).
\end{align*}
If $P(\lambda)$ were constant, both sides have the constant value
$\det D_{4}{}^{t}D_{4}$ and hence $C_{4}=0$; also
$Y^{-1}_{0}[[Z_{0}{}^{t}C_{3}+{}^{t}D_{3}]]$ is necessarily $0$,
implying that $C_{3}=D_{3}=0$. Finally, therefore
$M\in\Delta_{n,n-j}(s)$, under the assumption that $P(\lambda)$ is a
constant. Thus, for $j<n$, $\lim\limits_{\lambda\to
  \infty}e^{\frac{2\pi i}{s}\tr(TM<Z>)}\det M\{Z\}^{-k}=0$ unless $M$
is in $\Delta_{n,n-j}(s)$; in that case, the limit is, in fact,
$e^{\frac{2\pi i}{s}\tr(T_{0}\ub{M}<Z_{0}>)}\break\det
\ub{M}\{Z_{0}\}^{-k}$, since $\det M\{Z\}^{-k}=\det D^{k}_{4}\det
(C_{1}Z_{0}+D_{1})^{-k}=\det \ub{M}\{Z_{0}\}^{-k}$ and $e^{\frac{2\pi
    i}{s}\tr(TM<Z>)}=e^{\frac{2\pi
    i}{s}\tr(T_{0}(M<Z>)_{0})}=e^{\frac{2\pi
    i}{s}\tr(T_{0}\ub{M}<Z_{0}>)}$. To complete the proof for $j<n$,
we need only observe that to any coset
$$\Gamma_{n}(s,\left(\begin{smallmatrix} T_{0} & 0\\ 0 & 0
\end{smallmatrix}\right)\left(\begin{smallmatrix} A_{1} & 0 & B_{1} &
  \ast\\ \ast & U & \ast & \ast\\ C_{1} & 0 & D_{1} & \ast\\ 0 & 0 & 0
  & {}^{t}U^{-1}\end{smallmatrix}\right),$$ 
if we make correspond the
coset $\Gamma_{n-j}(s,T_{0})\left(\begin{smallmatrix} A_{1} &
  B_{1}\\  C_{1} & D_{1}\end{smallmatrix}\right)$, this mapping is
clearly well-defined and surjective on
$\Gamma_{n-j}(s,T_{0})\backslash \Gamma_{n-j}(s)$; it is also easily
checked to be injective, since
\begin{gather*}
\begin{pmatrix}
A_{1} & 0 & B_{1}  & \ast\\
\ast & U_{1} & \ast & \ast\\
C_{1} & 0 & D_{1} & \ast\\
0 & 0 & 0 & {}^{t}U^{-1}_{1}
\end{pmatrix}
\begin{pmatrix}
A_{1} & 0 & B_{1} &\ast\\
\ast & U_{2} & \ast & \ast\\
C_{1} & 0 & D_{1}  & \ast\\
0 & 0 & 0 & {}^{t}U^{-1}_{2}
\end{pmatrix}^{-1}\\
=
\begin{pmatrix}
E_{n-j} & 0 & 0 & \ast\\
\ast & U_{1} U^{-1}_{2} & \ast & \ast\\
 & & E_{n-j} & \ast\\
0 & & & {}^{t}U_{1}^{-1}{}^{t}U_{2}
\end{pmatrix}
\in\Gamma_{n}(s,
\begin{pmatrix}
T_{0} & 0\\
0 & 0
\end{pmatrix}
).
\end{gather*}
Thus
$$
\Phi^{j}(g_{k}(Z,
\begin{pmatrix}
T_{0} & 0\\
0 & 0
\end{pmatrix};
\Gamma_{n}(s))
\Gamma_{n}(s))=g_{k}(Z_{0}, T_{0}, \Gamma_{n-j}(s)), \text{ \  for \ }
j<n. 
$$
\end{proof}

The\pageoriginale proof for the case $j=n$ is immediate on putting
$Z=i\lambda E_{n}$ in the Fourier expansion of the Eisenstein series
$g_{k}(Z,0;\Gamma_{n}(s))$ the only term surviving in the limit is the
constant term $1$.

\begin{sublemma}\label{c1:lem-1.6.6}
For $N_{1}$, $N_{2}$ in $\Gamma_{n}$ with
$N_{1}\mathop{\nmid}\limits_{j,s}N_{2}$ and $j<n$, we have
$\Phi^{j}(p_{k}(Z,
\left(\begin{smallmatrix} T_{0}^{(n-j)} & 0\\ 0 & 0
\end{smallmatrix}\right)$, $N_{1}$, $\Gamma_{n}(s))|N_{2}))=0$.
\end{sublemma}

\begin{proof}
Indeed $\Phi^{j}(p_{k}(Z,\left(\begin{smallmatrix} T^{n-j}_{0} & 0\\ 0
  & 0\end{smallmatrix}\right)$, $N_{1}$, $\Gamma_{n}(s)|N_{2})=$
\begin{align*}
& \Lim\limits_{\lambda\to \infty}g_{k}(
\begin{pmatrix}
Z_{0} & 0\\
0 & i\lambda E_{j}
\end{pmatrix}, T,\Gamma_{n}(s))|N^{-1}_{1}N_{2})\quad (\text{with \ }
T=
\begin{pmatrix} 
T^{(n-j)}_{0} & 0\\
0 & 0
\end{pmatrix}).\\
&= \sum_{M'\in\Gamma_{n}(s,T)\backslash
  \Gamma_{n}(s)}\Lim\limits_{\lambda\to \infty}e^{\frac{2\pi
    i}{e^{s}}\tr(T(M'N^{-1}_{1}N_{2})<
\begin{pmatrix}
Z_{0} & 0\\
0 & i\lambda E_{j}
\end{pmatrix}
>)}\\
&\qquad(\det (M'N^{-1}_{1}N_{2})\{
\begin{pmatrix}
Z_{0} & 0\\
0 & i\lambda E_{j}
\end{pmatrix}
\})^{-k}\\
&=0,
\end{align*}
since, for no $M'$ in $\Gamma_{n}(s)$,
$M'N^{-1}_{1}N_{2}=N^{-1}_{1}\cdot
(N_{1}M'N^{-1}_{1})N_{2}\in\Delta_{n,n-j}$, by the hypothesis
$N_{1}\mathop{\not\sim}\limits_{j,s}N_{2}$ and so the limit of every
term is $0$, by the same arguments as in the proof of Lemma
\ref{c1:lem-1.6.5}.
\end{proof}

We now recall the structure of the finite dimensional space $\gamma$
of cusp forms in $(n,k,s)$. As we know, given $f$, $g$ in $\{n,k,s\}$
at least one of\pageoriginale which is a cusp from, the scalar product
$(f,g)$ is defined by
$$
\frac{1}{v}\int\limits_{T_{n}(s)\backslash
  \mathscr{G}_{n}}f(Z)\ob{g}(Z)\frac{dXdY}{(\det Y)^{n+1-k}}
$$
with the customary (invariant) volume element $dv=(\det
Y)^{-(n+1)}dX\ dY$. Corresponding to $Z=X+iY$ in $\mathscr{G}_{n}$ and
$v:=\int\limits_{\Gamma_{n}(s)\backslash
  \mathscr{G}_{n}}dv<\infty$. If
$f(Z)=\sum\limits_{T>0}a(T)e^{\frac{2\pi i}{s}\tr(TZ)}$ is a cusp form
in $\{n,k,s\}$, the scalar product $(f(Z),g_{k}(Z,S;\Gamma_{n}(s))$
is, upto a constant factor, equal to $(\det S)^{\frac{n+1}{2}-k}a(S)$
for $S>0$ and $0$ if $\det S=0$. If $\mathcal{I}$ denotes the subspace
of $\gamma$ generated by $g_{k}(Z,T;\Gamma_{n}(s))$ for semi-integral
$T^{(n)}>0$. Then, using the (non-degenerate) scalar product ( , ) in
$\gamma$, there exists an orthogonal complement $\mathfrak{n}$ for
$\mathcal{I}$ in $\gamma$ \ie $\gamma=\mathcal{I}\oplus
\mathfrak{n}$. We claim that $\mathfrak{n}=\{0\}$; in fact, any $f$ in
$\mathfrak{n}$ is orthogonal to $g_{k}(Z,S;\Gamma_{n}(s))$ for every
semi integral $S>0$ and hence the Fourier expansion of $f$ has all
coefficients equal to $0$ \ie $f=0$.

\begin{sublemma}\label{c1:lem-1.6.7}
Suppose that, for $f\in \{n,k,s\}$, $\Phi^{j}(f|R)$ is a cusp form for
$R$ in $\Gamma_{n}$, whenever $j<n$. Then there exists
$$
\varphi_{R,j}(Z)=\varphi_{R,j}(Z;f):=\sum_{\nu}C_{\nu}p_{k}(Z;
\begin{pmatrix}
T_{R,\nu} & 0\\
0 & 0
\end{pmatrix};
R; \Gamma_{n}(s))
$$
such that $\Phi^{j}((f-\varphi_{R,j})|R)=0$, for every $R$ in $\Gamma_{n}$.
\end{sublemma}

\begin{proof}
First,\pageoriginale let $j<n$. Since $\Phi^{j}(f|R)$ is a cusp form,
there exist, by the above remarks, finitely many $T^{(n-j)}_{R,\nu}>0$
and constants $c_{\nu}=c_{\nu}(R;f)$ such that 
\begin{align*}
(\Phi^{j}(f|R))(Z_{0}) &=
  \sum_{\nu}c_{\nu}g_{k}(Z_{0};T_{R,\nu};\Gamma_{n-j}(s))\qquad
  (Z_{0}\in\mathscr{G}_{n-j})\\
&=\sum_{\nu}c_{\nu}\Phi^{j}(g_{k}(Z;
\begin{pmatrix}
T_{R,\nu} & 0\\
0 & 0
\end{pmatrix};
\Gamma_{n}(s))), \quad\text{by Lemma 5}\\
&= \sum_{\nu}c_{\nu}\Phi^{j}(p_{k}(Z;
\begin{pmatrix}
T_{R,\nu} & 0\\
0 & 0
\end{pmatrix};
R;\Gamma_{n}(s))|R),\quad \text{by Lemma 4}
\end{align*}
which proves the lemma for $j<n$. For $j=n$, we need only to take
$\varphi_{R,n}(Z)=a(0,R)p_{k}(Z,0;R;\Gamma_{n}(s))$, since
$$
\Phi^{n}(f|R)=\Phi^{n}(\sum a(T,R)e^{\frac{2\pi
    i}{s}\tr(TZ)})=a(0,R)\quad\text{and}
$$
$\Phi^{n}(p_{k}(Z,0;R;\Gamma_{n}(s))|R)=\Phi^{n}(g_{k}(Z,0;\Gamma_{n}(s))=1$.
\end{proof}

From Lemma \ref{c1:lem-1.6.2}, we know that $\Phi^{j}(f|R)=0$ for every
$R$ in $\Sp(n,\mathbb{Q})$, if already $\Phi^{j}(f|M_{i})=0$ for
finitely many $M_{1},\ldots,M_{t}$ in $\Gamma_{n}$. From these
$M_{i}$, we pick a maximal set of representatives, say
$M'_{1},\ldots,\break M'_{u_{j}}$ which are mutually
$\tilde{j,s}$-inequivalent. Let now $f$ satisfy the conditions
stated in Lemma \ref{c1:lem-1.6.7}. For fixed $j$, let us consider
$$
\psi_{j}(Z):=\sum_{1\leq \ell\leq
  u_{j}}\varphi_{M'_{\ell},j}(Z)=\sum_{1\leq \ell\leq
  u_{j}}\sum_{\gamma}c_{\ell,\nu}p_{k}(Z;
\begin{pmatrix}
T_{M'_{\ell},\nu} & 0\\
0 & 0
\end{pmatrix};
M'_{\ell};\Gamma_{n}(s))
$$
with the same notation as in Lemma \ref{c1:lem-1.6.7}. Now any
$M_{i}(1\leq i\leq t)$ is $\underset{j,s}{\sim} M'_{m}$
for\pageoriginale some $m$ with $1\leq m\leq u_{j}$; we have then,
$$
\Phi^{j}((f-\psi_{j})|M_{i})=\Phi^{j}(f|M_{i}-\varphi_{M'_{m},j}|M_{i})=\Phi^{j}((f-\varphi_{M'_{m},j})|M_{i})=0,
$$
in view of Lemmas \ref{c1:lem-1.6.6}, \ref{c1:lem-1.6.7} and
\ref{c1:lem-1.6.3}, giving us

\begin{sublemma}\label{c1:lem-1.6.8}
For $\varphi$ in $\{n,k,s\}$, suppose that, whenever $j<n$,
$\Phi^{j}(\varphi|M)$ is a cusp form, for every $M$ in
$\Gamma_{n}$. Then there exists $\psi_{j}$ in
$\{n,k,s\}_{n-j}:=\{\text{linear-combinations of \ }
p_{k}(Z;\left(\begin{smallmatrix} T^{(n-j)}_{0} & 0\\ 0 & 0
\end{smallmatrix}\right)$, $M$, $\Gamma_{n}(s))\}$ such that
$\Phi^{j}((\varphi-\psi_{j})|M)=0$ for every $M$ in $\Gamma_{n}$ and
$1\leq j\leq n$.
\end{sublemma}

Finally we state and prove the following Representation Theorem for
modular forms.

\setcounter{subtheorem}{8}
\begin{subtheorem}\label{c1:thm-1.6.9}
For even integral $k>2n+1$, every $f$ in $\{n,k,s\}$ is a finite
linear combination of the Poincar\'e series
$p_{k}(Z,T,N,\Gamma_{n}(s))$ for semi-integral $T\geq 0$ and $N$ in
$\Gamma_{n}$. 
\end{subtheorem}

\begin{proof}
First we need to formulate an inductive statement, following H.\@
Braun. Let $2\leq j\leq n$ and $R=\left(\begin{smallmatrix} A & B\\ C
  & D\end{smallmatrix}\right)\in\Gamma_{n-j+1}$ with $(n-j+1, n-j+1)$
  submatrices $A$, $B$, $C$, $D$. Then
$$
R':=
\begin{pmatrix}
A & 0 & B & 0\\
0 &  E_{j-1} & 0 & 0\\
C & 0 & D & 0\\
0 & 0 & 0 & E_{j-1}
\end{pmatrix}
$$
is in $\Gamma_{n}$ and further for any $f'$ in
$\{n,k,s\} \; \Phi^{j}(f'|MR')=\Phi(\Phi^{j-1}(f'|MR'))$ for any $M$ in
$\Gamma_{n}$ and from the special form of $R'$, we\pageoriginale have
$\Phi^{j-1}(f|MR')=(\Phi^{j-1}(f'|M))|R$. Thus we have, for any $M$ in
$\Gamma_{n}$ and $R$ in $\Gamma_{n-j+1}$,
\begin{equation*}
\Phi^{j}(f'|MR')=\Phi((\Phi^{j-1}(f'|M))|R).\tag{$\ast$}
\end{equation*}
Now, from Lemma \ref{c1:lem-1.6.7}, there exists $\psi_{n}$ in
$\{n,k,s\}$ such that
$$
\Phi^{n}((f-\psi_{n})|M)=0\quad\text{for every}\quad
M\quad\text{in}\quad \Gamma_{n}.
$$
Assume now that, for any fixed $j$ with $1\leq j\leq n$ and for the
given $f$ denoted as $f_{0}$, we have already constructed $f_{n-j}$ in
$\{n,k,s\}$ so that $\Phi^{j}(f_{n-j}|M))$ is a cusp form for every
$M$ in $\Gamma_{n}$. Then by Lemma \ref{c1:lem-1.6.8}, there exists
$\psi_{j}$ (corresponding to $\varphi=f_{n-j}$) such that
$$
\Phi^{j}((f_{n-j}-\psi_{j})|M)=0\quad\text{for every}\quad
M\quad\text{in}\quad \Gamma_{n},(\ast\ast)_{j}
$$
where $\psi_{j}\in \{n,k,s\}_{n-j}$, is a linear combination of the
Poincar\'e series 
$$
p_{k}(Z;
\begin{pmatrix}
T^{(n-j)} & 0\\
0 & 0
\end{pmatrix};
M'; \Gamma_{n}(s)).
$$
Note that for $j=n$, $a\psi_{n}$ with $\Phi^{n}((f_{0}-\psi_{n})|M)=0$
for every $M$ in $\Gamma_{n}$ already exists. From $(\ast\ast)_{j}$
and $(\ast)$, we obtain
\begin{align*}
\Phi((\Phi^{j-1})((f_{n-j}-\psi_{j})|M))|R)&=0\quad\text{for
  every}\quad M\quad\text{in}\\ 
&\qquad\qquad\Gamma_{n}\quad\text{and
  every}\quad R\quad\text{in}\quad \Gamma_{n-j+1}
\end{align*}
whenever $2\leq j\leq n$. If we set $f_{n-j+1}=f_{n-j}-\psi_{j}$, the
last relation means that, for every $M$ in $\Gamma_{n}$,
$\Phi^{j-1}(f_{n-j+1}|M)$ is a cusp form. Applying Lemma
\ref{c1:lem-1.6.8} to $f_{n-j+1}$ in\pageoriginale place of $f$ and $j-1$
in place of $j$ (for which we had the condition $2\leq j\leq n$),
there exists $\psi_{j-1}$ in $\{n,k,s\}_{n-j+1}$ as defined above,
such that $\Phi^{j-1}((f_{n-j+1}-\Psi_{j-1})|M)=0$ for every $M$ in
$\Gamma_{n}$, which is just $(\ast\ast)_{j-1}$. Thus the inductive
argument is complete, giving us the validity of $(\ast\ast)_{1}$, \ie 
$$
0=\Phi((f_{n-1}-\Psi_{1})|M)=\Phi((f-\sum_{1\leq \ell \leq
  n}\psi_{\ell})|M)\quad\text{for every}\quad M\text{ \  in
  \ }\Gamma_{n}.
$$
In other words, $f-\sum\limits_{1\leq \ell\leq n}\psi_{\ell}$ is a
cusp form. Since the space of cusp forms is generated by
$p_{k}(Z;T^{(n)};E_{2n};\Gamma_{n}(s))$ with semi-integral $T>0$, the
proof of the theorem is complete.
\end{proof}

Let us now identify the Poincar\'e series $g_{k}$ in terms of
``lifts'' (\ie\break Eisenstein series $E(Z;f)$, in the sense of Klingen,
arising) from cusp forms $f$ of degree $\leq n$. Let $f$ be a cusp
form in $\{r,k,s\}$. Then for every $k>n+r+1$, we define, after
Klingen,
$$
E^{k}_{n,r}(Z;f):=\sum_{M\in\Delta_{n,r}(s)\backslash
  \Gamma_{n}(s)}f((M<Z>)^{\ast})(\det M\{Z\})^{-k}
$$
where, for any $(n,n)$ matrix $A$, we denote its top(leftmost) $(r,r)$
submatrix by $A^{\ast}$. The series is well-defined, since for $M$,
$N$ in $\Gamma_{n}(s)$ with $M$ in $\Delta_{n,r}(s)N$, we can easily
verify that $f(M<Z>^{\ast})(\det M\{Z\})^{-k}=f(N<Z>^{\ast})(\det
N\{Z\})^{-k}$; further it represents an element of $\{n,k,s\}$. If
$T^{(n)}=\left(\begin{smallmatrix} T^{(r)}_{0} & 0\\ 0 & 0 
\end{smallmatrix}\right)$ with $T_{0}>0$, then we know already that
the\pageoriginale correspondence
\begin{align*}
\Gamma_{n}(s,T)M&=\Gamma_{n}(s,T)
\begin{pmatrix}
A^{(r)}_{0} & 0 & B^{(r)}_{0} & \ast\\
\ast & U & \ast & \ast\\
C^{(r)}_{0} & 0 & D^{(r)}_{0} & \ast\\
0 & 0 & 0 & {}^{t}U^{-1}
\end{pmatrix}
\mapsto \Gamma_{r}(s,T_{0})
\begin{pmatrix}
\Lambda_{0} & B_{0}\\
C_{0} & D_{0}
\end{pmatrix}\\
&=\Gamma_{r}(s,T_{0})\ub{M}
\end{align*}
from the coset space $\Gamma_{n}(s,T)\backslash \Delta_{n,r}(s)$ to
the coset space $\Gamma_{r}(s,T_{0})\backslash \Gamma_{r}(s)$ is a
bijection. From the coset decompositions
$\Gamma_{n}(s)=\coprod\limits_{M_{\ell}}\Delta_{n,r}(s)M_{\ell}$,
$\Delta_{n,r}(s)=\coprod\limits_{N_{j}}\Gamma_{n}(s,T)N_{j}$, we get
$\Gamma_{n}(s)=\coprod\limits_{M_{\ell},N_{j}}\Gamma_{n}(s,T)N_{j}M_{\ell}$. 

Now
\begin{gather*}
e^{\frac{2\pi i}{s}\tr(T(N_{j}M_{\ell})<Z>)}=e^{\frac{2\pi
    i}{s}\tr(T_{0}((N_{j}M_{\ell})<Z>)^{\ast})}=\\
=e^{\frac{2\pi
    i}{s}\tr(T_{0}(N_{j}<M_{\ell}<Z>>)^{\ast})}=e^{\frac{2\pi
      i}{s}\tr(T_{0}\ub{N_{j}}<(M_{\ell}<Z>)^{\ast}>)} 
\end{gather*}
with $\ub{N_{j}}$ in $\Gamma_{r}(s)$ corresponding to $N_{j}$ in
$\Delta_{n,r}(s)$ in the sense explained already. Moreover, 
\begin{align*}
(\det
(N_{j}M_{\ell})\{Z\})^{-k}&=(\det N_{j}\{M_{\ell}<Z>\})^{-k}\times
(\det M_{\ell}\{Z\})^{-k}\\
&=(\det
\ub{N_{j}}\{(M_{\ell}<Z>)^{\ast}\})^{-k}(\det M_{\ell}\{Z\})^{-k}.
\end{align*}
Now\pageoriginale we have
{\fontsize{10}{12}\selectfont
\begin{align*}
g_{k}(Z,T;\Gamma_{n}(s)) &=
\sum_{M\in\Gamma_{n}(s,T)\backslash\Gamma_{n}(s)} e^{\frac{2\pi
    i}{s}\tr(TM<Z>)}(\det M\{Z\})^{-k}\\
&=
\sum_{\substack{M_{\ell}\in\Delta_{n,r}(s)\backslash\Gamma_{n}(s)\\ N_{j}\in\Gamma_{n}(s,T)\backslash
\Delta_{n,r}(s)}}e^{\frac{2\pi i}{s}\tr(T(N_{j}M_{\ell})<Z>)}(\det
(N_{j}M_{\ell})\{Z\})^{-k}\\
&= \sum_{M_{\ell}}(\det M_{\ell}\{Z\})^{-k}\sum_{N_{j}}e^{\frac{2\pi
    i}{s}\tr(T_{0}\ub{N_{j}}<(M_{\ell}<Z>)^{\ast}>)}\\
&\qquad\qquad(\det
\ub{N_{j}}\{(M_{\ell}<Z>)^{\ast}\})^{-k}\\ 
&= \sum_{M_{\ell}\in\Delta_{n,r}(s)\backslash \Gamma_{n}(s)}(\det
M_{\ell}\{Z\})^{-k}\sum_{\ub{N_{j}}\in\Gamma_{r}(s,T_{0})\backslash
  \Gamma_{r}(s)}e^{\frac{2\pi
    i}{s}\tr(T_{0}\ub{N_{j}}<(M_{\ell}<Z>)^{\ast}>)}\\
&\qquad(\det
\ub{N_{j}}\{(M_{\ell}<Z>)^{\ast}\})^{-k}\\
&= E^{k}_{n,r}(Z;g_{k}(\ast,T_{0}; \Gamma_{r}(s))).
\end{align*}}
We may reformulate the theorem above as the following assertion: for
even integral $k>2n+1$, the space $\{n,k,s\}$ is generated by
$E^{k}_{n,r}(Z;g)|M$ as $g$ varies over cusp forms of degree $r(\leq
n)$ and $M$ over $\Gamma_{n}$.

Using the above Representation Theorem for $\{n,k,s\}$ in terms of the
Eisenstein series $E^{k}_{n,j}(Z;f)$ constructed from cusp forms $f$
in $\{j,k,s\}$ and the estimate for Fourier coefficients of cusp forms
(analogous to Theorem \ref{c1:thm-1.1.1}), we proceed now to derive an
estimate for the Fourier coefficients of modular forms in $\{n,k,s\}$
for even integral $k\geq 2n+2$. To this end, we shall prove, following
Kitaoka \cite{key10}, a series of lemmas and propositions. 

We\pageoriginale decompose any $M$ in $\Gamma_{n}$ as
$M=\left(\begin{smallmatrix} A_{M} & B_{M}\\ C_{M} & D_{M}
\end{smallmatrix}\right)$ with $(n,n)$ submatrices $A_{M}$, $B_{M}$,
$C_{M}$, $D_{M}$. For any $(p,q)$ matrix $F$ and any $s$ with $1\leq
s\leq p$, we denote the $(s,q)$ matrix formed from the last $s$ rows
of $F$ by $\lambda_{s}(F)$. For $0\leq r\leq n$, $\{M\in\Gamma_{n}|$
the first $n+r$ columns of $\lambda_{n-r}(M)$ are $0\}$ is just the
group $\Delta_{n,r}(1)$ introduced earlier. Indeed, for any such $M$, 
$$
A_{M}=
\begin{pmatrix}
A_{1} & A_{2}\\
A_{3} & A_{4}
\end{pmatrix},
B_{M}=
\begin{pmatrix}
B_{1} & B_{2}\\
B_{3} & B_{4}
\end{pmatrix},
C_{M}=
\begin{pmatrix}
C_{1} & C_{2}\\
0 & 0
\end{pmatrix},
D_{M}=
\begin{pmatrix}
D_{1} & D_{2}\\
0 & D_{4}
\end{pmatrix}
$$
with $A_{1}$, $B_{1}$, $C_{1}$, $D_{1}$ of size $(r,r)$ and further
$D_{4}$ is in $GL_{n-r}(\mathbb{Z})$, $A_{4}{}^{t}D_{4}=E_{n-r}$,
$A_{2}{}^{t}D_{4}=0$, $C_{2}{}^{t}D_{4}=0$, and therefore $A_{2}=0$,
$C_{2}=0$, $A=\left(\begin{smallmatrix} A_{1} & 0\\ A_{3} & A_{4}
\end{smallmatrix}\right)$. Moreover, $\Delta_{n,r}(s)=\Delta_{n,r}\cap
\Gamma_{n}(s)$. If we write $M_{1}=\left(\begin{smallmatrix} A_{1} &
  B_{1}\\ C_{1} & D_{1}\end{smallmatrix}\right)$ for any (such) $M$ in
$\Delta_{n,r}$, then $M_{1}$ is in $\Gamma_{r}$. If $Z_{1}$ is the
leading $(r,r)$ submatrix of $Z$ in $\mathscr{G}_{n}$, then it is easy
to see that $M_{1}<Z_{1}>$ is the leading $(r,r)$ submatrix of $M<Z>$
and further $\det M\{Z\}=(\det M_{1}\{Z_{1}\})$. $\det D_{4}$, where
$N\{Z\}:=C_{N}Z+D_{N}$ for any $N$ in $\Gamma_{n}$.

\setcounter{sublemma}{9}
\begin{sublemma}\label{c1:lem-1.6.10}
For $M$, $N$ in $\Gamma_{n}$, $\Delta_{n,r}M=\Delta_{n,r}N$ if and
only if $\lambda_{n-r}(M)\in GL_{n-r}(\mathbb{Z})\lambda_{n-r}(N)$. 
\end{sublemma}

\begin{proof}
From\pageoriginale the form of the elements of $\Delta_{n,r}$, clearly
$\Delta_{n,r}M=\Delta_{n,r}N$ implies that
$\lambda_{n-r}(M)=V\lambda_{n-r}(N)$ for some $V$ in
$GL_{n-r}(\mathbb{Z})$. On the other hand, if $\lambda_{n-r}(M)\in
GL_{n-r}(\mathbb{Z})\lambda_{n-r}(N)$, we may already suppose, without
loss of generality, that $\lambda_{n-r}(M)=\lambda_{n-r}(N)$ after
replacing $M$ by $\left(\begin{smallmatrix} {}^{t}U^{-1} & 0\\ 0 & U
\end{smallmatrix}\right)M$ for $U=\left(\begin{smallmatrix} E_{r} &
  0\\ 0 & V\end{smallmatrix}\right)$ with a suitable $V$ in
  $GL_{n-r}(\mathbb{Z})$. But then we have evidently
  $\lambda_{n-r}(MN^{-1})=(0^{(n-r,n+r)}E_{n-r})$ and we are through.
\end{proof}

\begin{sublemma}\label{c1:lem-1.6.11}
For any $M$ in $\Gamma_{n}$ with rank
$(\lambda_{s}(C_{M}))<s=n-r(<n)$, there exists $N$ in $\Delta_{n,n-1}$
such that $\Delta_{n,r}M\ni N\left(\begin{smallmatrix} U & 0\\ 0 &
  {}^{t}U^{-1}\end{smallmatrix}\right)$ for some $U$ in $GL_{n}(\mathbb{Z})$.
\end{sublemma}

\begin{proof}
From the hypothesis, there exist $V$ in $GL_{s}(\mathbb{Z})$ and $W$
in $GL_{n}(\mathbb{Z})$ such that
$\lambda_{1}(V\lambda_{s}(C_{M})W)=0$. Then, for
$K:=\left(\begin{smallmatrix} \ast & 0 & \\ & E_{r} & 0\\ 0 & 0 & V
\end{smallmatrix}\right)M\left(\begin{smallmatrix} W & 0\\ 0 &
  {}^{t}W^{-1}
\end{smallmatrix}\right)$, we have $\lambda_{1}(C_{K})=0$ and hence
the elements of $\lambda_{1}(D_{K})$ are relatively prime. It is clear
that $D_{K}=\left(\begin{smallmatrix} \ast \\ 0\ldots 01
\end{smallmatrix}\right)F$ for some $F$ in $GL_{n}(\mathbb{Z})$. If we
set $N=K\left(\begin{smallmatrix} {}^{t}F & 0\\ 0 & F^{-1}
\end{smallmatrix}\right)$, then $\lambda_{1}(N)=(0\ldots 01)$ and
consequently $N$ is in $\Delta_{n,n-1}$. The lemma follows on taking
$U=W{}^{t}F$. 
\end{proof}

Let\pageoriginale $f$ be a cusp form in $\{r,k,\ell\}$ for fixed
$r\leq n-1$ and even integral $k\geq n+r+2$. Let us denote, in the
sequel, the leading $(r,r)$ submatrix $P_{1}$ of
$P^{(n,n)}=\left(\begin{smallmatrix} P_{1} & P_{2}\\ P_{3} & P_{4}
\end{smallmatrix}\right)$, by $P^{\ast}$. For any $M$ in $\Gamma_{n}$,
let us abbreviate $f((M<Z>)^{\ast})$ $(\det (C_{M}Z+D_{M}))^{-k}$ as
$(f|M)(Z)$. For any given $R$ in $\Gamma_{n}$, we split the
(absolutely convergent) Eisenstein series $E^{k}_{n,r}(Z;f)|R$ as the
sum of two subseries $\sum\limits_{i}=\sum\limits_{N}(f||N)(Z)$,
$i=1,2$ where $N$ runs over a complete set of elements $N_{1}$,
$N_{2},\ldots$ in $\Gamma_{n}(\ell)R$ such that $N_{i}\not\in
\Delta_{n,r}(\ell)N_{j}$ for $i\neq j$ and the rank of
$\lambda_{n-r}(C_{N})$ is $n-r$ for $N$ occurring in $\sum\limits_{1}$
and $<n-r$ for $N$ in $\sum\limits_{2}$. Now $C_{M}=C_{N}$ for
$M:=N\left(\begin{smallmatrix} E_{n} & \ell_{S}\\ 0 & E_{n}
\end{smallmatrix}\right)$ and any integral symmetric $S^{(n,n)}$. Thus
the subseries $\sum\limits_{i}$ represent functions invariant under
all translations $Z\to Z+\ell S$ and admit Fourier expansions
$\sum_{T\geq 0}a_{i}(T)\exp(2\pi i\tr(TZ)/\ell)$. 

Lemmas \ref{c1:lem-1.6.10}, \ref{c1:lem-1.6.11} lead to the following

\setcounter{subprop}{11}
\begin{subprop}\label{c1:prop-1.6.12}
For a cusp form $f$ in $\{r,k,\ell\}$ as above and $R$ in
$\Gamma_{n}$, all the Fourier coefficients $a_{2}(T)$ for $T>0$ of 
$$
\sum\limits_{2}=\sum_{\substack{N\in\Delta_{n,r}(\ell)\backslash
    \Gamma_{n}(\ell)R\\ \rank (\lambda_{n-r}(C_{N}))<n-r}}(f||N)(Z)
$$ 
vanish.
\end{subprop}

\begin{proof}
By Lemma \ref{c1:lem-1.6.11}, there exist $K$ in $\Delta_{n,r}$, $M$ in
$\Delta_{n,n-1}$ and $U$ in $GL_{n}(\mathbb{Z})$ such that
$N=KM\left(\begin{smallmatrix} U & 0\\ 0 & {}^{t}U^{-1}
\end{smallmatrix}\right)$. Let $K^{\ast}:=\left(\begin{smallmatrix}
  A_{1} & B_{1}\\ C_{1}  & D_{1}\end{smallmatrix}\right)$ in
$\Gamma_{r}$ formed from the leading $(r,r)$ submatrices of $A_{K}$,
$B_{K}$, $C_{K}$, $D_{K}=\left(\begin{smallmatrix} \ast & \ast\\ 0 &
  D_{4}
\end{smallmatrix}\right)$. It is easy to\pageoriginale verify that
\begin{align*}
\det N\{Z\} &=(\det(KM)\{UZ{}^{t}U\})/(\det U)\\
&= (\det((C_{K}A_{M}+D_{K}C_{M})UZ{}^{t}U+C_{K}B_{M}+D_{K}D_{M}))\det
U\\
&= \det(C_{K}M<UZ{}^{t}U>+D_{K})\det (C_{M}UZ{}^{t}U+D_{M})\det U\\
&= \det (C_{1}M<UZ{}^{t}U>)^{\ast}+D_{1})\det D_{4}\cdot \det
(M\{UZ{}^{t}U\})\det U\\
&= \det (K^{\ast}\{(M<UZ{}^{t}U>)^{\ast}\}\det (M\{UZ{}^{t}U\})\det
D_{4}\cdot \det U
\end{align*}
and moreover,
\begin{align*}
(N<Z>)^{\ast}&=((KM)<UZ{}^{t}U>)^{\ast}
=(K<MUZ{}^{t}U>>)^{\ast}\\
&=K^{\ast}(M<UZ{}^{t}U>)^{\ast}>. 
\end{align*}
On the other hand, there exist constants
$\alpha_{1},\ldots,\alpha_{m}$ (depending on $f$ and $K$) such that
$$
f(K^{\ast}<W>)(\det K^{\ast}\{W\})^{-k}=\sum_{1\leq j\leq
  m}\alpha_{j}f_{j}(W)\quad\text{for}\quad W\in\mathscr{G}_{r}
$$
where $f_{1},\ldots,f_{m}$ form a basis of the space of cusp forms in
$\{r,k,\ell\}$. Hence
$(f||N)(Z)=f(K^{\ast}<(M<UZ{}^{t}U>)^{\ast}>)(\det
K^{\ast}\{(M<UZ{}^{t}U>)^{\ast}\})^{-k}(\det
M\{UZ{}^{t}U\})^{-k}=\sum\limits_{j}\alpha_{j}f_{j}((M<UZ{}^{t}U>)^{\ast})(\det
M\{UZ{}^{t}U\})^{-k}=\sum\limits_{j}\alpha_{j}(f_{j}||M)(UZ{}^{t}U)$. 
Decomposing $Z=X+iY$ in $\mathscr{G}_{n}$ as
$\left(\begin{smallmatrix} Z_{1} & Z_{2}\\ {}^{t}Z_{2} & Z_{3}
\end{smallmatrix}\right)$ with $Z_{1}$ in $\mathscr{G}_{n-1}$ and
writing 
$$
A_{M}=
\begin{pmatrix}
A'_{1} & 0\\
A'_{3} & a_{4}
\end{pmatrix}
, B_{M}=
\begin{pmatrix}
B'_{1} & B'_{2}\\
B'_{3} & b_{4}
\end{pmatrix},
C_{M}=
\begin{pmatrix}
C'_{1} & 0\\
0 & 0
\end{pmatrix},
D_{M}=
\begin{pmatrix}
D'_{1} & D'_{2}\\
0 & d_{4}
\end{pmatrix}
$$
with $A'_{1}$, $B'_{1}$, $C'_{1}$, $D'_{1}$ of size $(n-1,n-1)$, we
have $\det M\{Z\}=\det (C'_{1}Z_{1}+D'_{1})d_{4}$ and\pageoriginale
$M<Z>$ has $(A'_{1}Z_{1}+B'_{1})(C'_{1}Z_{1}+D'_{1})^{-1}$ as its
leading $(n-1,n-1)$ submatrix. Thus $(f_{j}||M)(Z)$ is independent of
the variables $Z_{2}$ and $z_{3}$.

For $Y=(y_{pq})=\Iim Z$, let us write $\dfrac{\partial}{\partial
  Y}=\left(\varepsilon_{pq}\dfrac{\partial}{\partial y_{pq}}\right)$ with
$\varepsilon_{pq}=1$ or $1/2$ according as $p=q$ or $p\neq q$ and denote
by $D_{Y}$ the differential operator $(\det Y)(\det
\dfrac{\partial}{\partial Y})$ known to be invariant under $Y\mapsto
VY{}^{t}V$ for all $V$ in $GL_{n}(\mathbb{R})$. Then it is clear that 
\begin{align*}
D_{Y}((f||N)(Z)) &= \sum_{j}\alpha_{j}D_{Y}((f_{j}||M)(UZ{}^{t}U))\\
&= \sum_{j}\alpha_{j}D_{Y}((f_{j}||M)(UX{}^{t}U+iY))\quad (\text{using
  \ } Y\mapsto UY{}^{t}U)\\
&= 0
\end{align*}
and so $D_{Y}(\sum\limits_{2})=0$. On the other hand, we know that
$$
(\det \frac{\partial}{\partial Y})(\exp (2\pi i\tr(TZ)/\ell)=\det
(-(2\pi/\ell)T)\exp (2\pi i\tr (TZ)/\ell).
$$
Thus, on applying $D_{Y}$ termwise to the Fourier expansion of
$\sum\limits_{2}$ (as is indeed permissible), it follows that
$$
\sum_{T\geq 0}a_{2}(T)\det (-(2\pi/\ell)T)\exp(2\pi i\tr(TZ)/\ell)=0.
$$
Consequently, for all $T>0$, we have $a_{2}(T)=0$ and the proposition
is proved.
\end{proof}

Our objective being to get an estimate for the Fourier coefficients of
Eisenstein series for $T>0$ or (indeed) for $a_{1}(T)$, in view of
Proposition \ref{c1:prop-1.6.12} above, we should first get a system of
representatives of the right cosets of $\Gamma_{n}(\ell)$
modulo\pageoriginale $\Delta_{n,r}(\ell)$ containing $N$ with $\rank
(\lambda_{n-r}(C_{N}))=n-r$. The next few lemmas tackle this question
for $\ell=1$.

\setcounter{sublemma}{12}
\begin{sublemma}\label{c1:lem-1.6.13}
For any $n$-rowed symmetric pair $(C,D)$ there exists a coprime
symmetric pair $(P,Q)$ such that $C \; {}^{t}P+D \; {}^{t}Q=0$.
\end{sublemma}

\begin{proof}
If $C=0$, we can trivially take $P=E^{(n)}$, $Q=0$. Let then $C\neq 0$
and first, let $\det C\neq 0$. Then there exist $U$ in
$GL_{n}(\mathbb{Z})$ and $V=\left(\begin{smallmatrix} V^{(n)}_{1} &
  V_{2}\\ V_{3} & V_{4}\end{smallmatrix}\right)$ in
$GL_{2n}(\mathbb{Z})$ such that $U(CD)V^{-1}=(G0)$ with $(n,n)$
integral non-singular $G$. Hence $(G^{-1}UCG^{-1}UD)=(V_{1}V_{2})$;
evidently $(V_{1},V_{2})$ is a symmetric pair, which being primitive
is coprime as well. The lemma follows on taking $P=V_{2}$,
$Q=-V_{1}$. If $0<r=\rank C<n$, there exist $U_{1}$, $U_{2}$ in
$GL_{n}(\mathbb{Z})$ with $U_{1}CU_{2}=\left(\begin{smallmatrix} C_{1}
  & 0\\ 0 & 0\end{smallmatrix}\right)$ and $\det C_{1}\neq 0$. Now
  $U_{1}CU_{2}$ and $U_{1}D{}^{t}U^{-1}_{2}=\left(\begin{smallmatrix}
    D_{1}^{(r,r)} & D_{2}\\ D_{3} & D_{4}
  \end{smallmatrix}\right)$ form a symmetric pair again implying that
  $(C_{1},D_{1})$ is a symmetric pair; further $C_{1}{}^{t}D_{3}=0$
  and so $D_{3}=0$. By the earlier case, there an $r$-rowed coprime
  symmetric pair $(P_{1},Q_{1})$ with
  $C_{1}{}^{t}P_{1}+D_{1}{}^{t}Q_{1}=0$. The lemma is now immediate,
  on taking $P=\left(\begin{smallmatrix} P_{1} & 0\\ 0 & E_{n-r}
  \end{smallmatrix}\right){}^{t}U_{2}$, $Q=\left(\begin{smallmatrix}
    Q_{1} & 0\\ 0 & 0  \end{smallmatrix}\right)U^{-1}_{2}$. The next
  lemma is quite vital for the sequel.
\end{proof}

\begin{sublemma}\label{c1:lem-1.6.14}
For any $M$ in $\Gamma_{n}$ with $\rank (\lambda_{n-r}(C_{M}))=n-r$,
there exists $N$ in $\Delta_{n,r}M$ such that $\det C_{N}\neq 0$ and
further $(A_{N}C^{-1}_{N})^{\ast}$ is integral. 
\end{sublemma}

\begin{proof}
First,\pageoriginale there exist $U_{4}$ in $GL_{n-r}(\mathbb{Z})$ and
$V$ in $GL_{n}(\mathbb{Z})$ such that
$U_{4}\lambda_{n-r}(C_{M}){}^{t}V=(0 \; C_{4}^{(n-r,n-r)})$; necessarily
then, $\det C_{4}\neq 0$. Then for 
$$
U:=
\begin{pmatrix}
E_{r} &0\\
0 & U_{4}
\end{pmatrix},
K:=
\begin{pmatrix}
{}^{t}U^{-1} & 0\\
0 & U
\end{pmatrix}
M
\begin{pmatrix}
{}^{t}V & 0\\
0 & V^{-1}
\end{pmatrix}
\text{ \  is in \ } \Delta_{n,r}M
\begin{pmatrix}
{}^{t}V & 0\\
0 & V^{-1}
\end{pmatrix}
$$
and moreover, $C_{K}=\left(\begin{smallmatrix} C_{1} & C_{2}\\ 0 &
  C_{4}
\end{smallmatrix}\right)$. Correspondingly, if
$A_{K}=\left(\begin{smallmatrix} A_{1} & A_{2}\\ A_{3} & A_{4}
\end{smallmatrix}\right)$, then, from the relation
${}^{t}C_{K}A_{K}={}^{t}A_{K}C_{K}$, we get
${}^{t}C_{1}A_{1}={}^{t}A_{1}C_{1}$. Applying Lemma \ref{c1:lem-1.6.13}
to the $r$-rowed symmetric pair $({}^{t}C_{1},{}^{t}A_{1})$, there
exists an $r$-rowed coprime symmetric pair $(R_{1},S_{1})$-and
consequently, some $\left(\begin{smallmatrix} Q_{1} & P_{1}\\ S_{1} &
  R_{1}
\end{smallmatrix}\right)$ in $\Gamma_{r}$-such that
${}^{t}C_{1}{}^{t}R_{1}+{}^{t}A_{1}{}^{t}S_{1}=0$ \ie
$R_{1}C_{1}+S_{1}A_{1}=0$. 
\end{proof}

Now $L:\left(\begin{smallmatrix} Q_{1} & 0 & P_{1} & 0\\ 0 &
  {}^{t}U^{-1}_{4} & 0 & 0\\ S_{1} & 0 & R_{1} & 0\\ 0 & 0 & 0 & U_{4}
\end{smallmatrix}\right)$ is in $\Delta_{n,r}$ and further, clearly,
for $H:=LM\left(\begin{smallmatrix} {}^{t}V & 0\\ 0 & V^{-1}
\end{smallmatrix}\right)$, we have $A_{H}=\left(\begin{smallmatrix}
  A'_{1} & A'_{2}\\ A_{3} & A_{4}\end{smallmatrix}\right)$ and
$C_{H}=\left(\begin{smallmatrix} 0 & C'_{2}\\ 0 & C_{4}
\end{smallmatrix}\right)$.

From ${}^{t}C_{H}A_{H}={}^{t}A_{H}C_{H}$, we obtain
${}^{t}A'_{1}C'_{2}+{}^{t}A_{3}C_{4}=0$ \ie
$A_{3}=-{}^{t}C^{-1}_{4}{}^{t}C'_{2}A'_{1}$. Since the rank of the
matrix formed by the first $r$ columns of $H$ is $r$, the last
relation implies that $A'_{1}$ has necessarily rank $r$ \ie\break $\det
A'_{1}\neq 0$. Now $N:=\left(\begin{smallmatrix} E_{n} & & 0\\ E_{r} &
  0 & E_{n}\\ 0 & 0
  &\end{smallmatrix}\right)H\left(\begin{smallmatrix} {}^{t}V^{-1} &
  0\\ 0 & V\end{smallmatrix}\right)$ is evidently in $\Delta_{n,r}M$
  and moreover, $C_{N}=\left(\begin{smallmatrix} A'_{1} &
    A'_{2}+C'_{2}\\ 0 &
    C_{4}  \end{smallmatrix}\right){}^{t}V^{-1}$\pageoriginale 
  is indeed non-singular. Since $A_{N}=\left(\begin{smallmatrix}
    A'_{1} & A'_{2}\\ A_{3} &
    A_{4}  \end{smallmatrix}\right){}^{t}V^{-1}$,
  $(A_{N}C^{-1}_{N})^{\ast}=E_{r}$, which proves the lemma.

Let $(C_{4},D_{4})$ be an $(n-r)$-rowed integral symmetric pair with\break
$\det C_{4}\neq 0$ and $D_{3}$ an $(n-r,r)$ integral matrix such that
$F:=(C_{4}D_{3}D_{4})$ is primitive. To $F$, we associate a unique
right coset of $\Gamma_{n}$ modulo $\Delta_{n,r}$ as follows (and
denote it by $\Delta_{n,r}M\{C_{4},D_{3},D_{4}\}$. Indeed, there
exists $V$ in $GL_{2(n-r)}(\mathbb{Z})$ such that
$(C_{4}D_{4})V^{-1}=(0 \; G)$ for an integral $(n-r,n-r)$ nonsingular
matrix $G$. Now $(G^{-1}C_{4},G^{-1}D_{4})=(0,E_{n-r})V$ is an
integral symmetric pair which (being primitive) is a coprime pair as
well. Further, since $(D_{3}C_{4}D_{4})=(D_{3}(0G)V)$ is primitive, so
are $(D_{3}0G)$ and $(D_{3}G)$. Thus there exists $U$ in
$GL_{n}(\mathbb{Z})$ with $\lambda_{n-r}(U)=(D_{3}G)$. Now it is clear
that $C:=U\left(\begin{smallmatrix} 0 & 0\\ 0 & G^{-1}C_{4}
\end{smallmatrix}\right)$, $D:=U\left(\begin{smallmatrix} E^{(r)} &
  0\\ 0 & G^{-1}D_{4}\end{smallmatrix}\right)$ form a coprime
symmetric pair and $\lambda_{n-r}(C)=(0C_{4})$,
$\lambda_{n-r}(D)=(D_{3}D_{4})$. Choose any $M$ in $\Gamma_{n}$ with
$\lambda_{n}(M)=(CD)$; then, clearly
$\lambda_{n-r}(M)=(0C_{4}D_{3}D_{4})$. By Lemma \ref{c1:lem-1.6.14},
there exists $N$ in $\Delta_{n,r}M$ such that $\det C_{N}\neq 0$ and
$(A_{N}C^{-1}_{N})^{\ast}$ is integral. Now there exists $W_{4}$ is
$GL_{n-r}(\mathbb{Z})$ such that
$W_{4}\lambda_{n-r}(N)=\lambda_{n-r}(M)$ and we take, for
$M\{C_{4},D_{3},D_{4}\}$, the matrix $P=\left(\begin{smallmatrix}
  {}^{t}W^{-1} & 0\\ 0 & W\end{smallmatrix}\right)N$ where
  $W:=\left(\begin{smallmatrix} E_{r} & 0\\ 0 & W_{4}
  \end{smallmatrix}\right)$. Clearly
  $\lambda_{n-r}(P)=W_{4}\lambda_{n-r}(N)=\lambda_{n-r}(M)=(0C_{4}D_{3}D_{4})$,
  $\det C_{p}\neq 0$ and $(A_{P}C^{-1}_{p})^{\ast}$ is integral. Any
  such $P$ is denoted as $M\{C_{4},D_{3},D_{4}\}$; by Lemma
  \ref{c1:lem-1.6.10}, $\Delta_{n,r}M\{C_{4},D_{3},D_{4}\}$ is uniquely
  determined by $(C_{4}D_{3}D_{4})$ from which we started above.

Denote\pageoriginale by $\mathscr{C}_{n,r}$ the set of
$F=(C_{4}D_{3}D_{4})$ as described at the beginning of the last
paragraph and define two such matrices $F$, $F'$ to be equivalent (in
symbols, $F \sim F'$) if $F=WF'$ for some $W$ in
$GL_{n-r}(\mathbb{Z})$. Let
$P(n,r;\mathbb{Z})=\{U=\left(\begin{smallmatrix} U^{(r,r)}_{1} &
  U_{2}\\ 0 & U_{4}\end{smallmatrix}\right)\in
GL_{n}(\mathbb{Z})\}$. In $\mathscr{C}_{n,r}$, introduce also another
equivalence relation $F=(C_{4}D_{3}D_{4})=(C'_{4}D'_{3}D'_{4})=F'$ by
the condition $WF'=(C_{4}D_{3}+C_{4}S_{3}D_{4}+C_{4}S_{4})$ for some
$W$ in $GL_{n-r}(\mathbb{Z})$, integral $(n-r)$-rowed symmetric
$S_{4}$ and $(n-r,r)$ integral $S_{3}$. It is easily verified $F-F'$
if and only if 
\begin{align*}
\Delta_{n,r}M\{C'_{4},D'_{3},D'_{4}\}&=\Delta_{n,r}M\{C_{4},D_{3},D_{4}\}P\text{
  \  for \ }\\
P &=
\begin{pmatrix}
E^{(n)} & 0^{(r,r)} & {}^{t}S_{3}\\
0 & S_{3} & S_{4}\\
0 & E^{(n)}
\end{pmatrix}
\text{ \  in \ } \Gamma_{n}.
\end{align*}
We now prove the following crucial

\begin{sublemma}\label{c1:lem-1.6.15}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\rm(\theenumi)}
\item $$\coprod\limits_{\substack{M\in\Gamma_{n}\\ \rank
    (\lambda_{n-r}(C_{M}))=n-r}}\Delta_{n,r}M=\coprod
  \Delta_{n,r}M\{C_{4},D_{3},D_{4}\}\left(\begin{smallmatrix} {}^{t}U
    & 0\\ 0 & U^{-1}  \end{smallmatrix}\right)$$

where, on the right hand side, $(C_{4}D_{3}D_{4})$ runs over a
complete set $\tilde{\mathscr{C}}$ of representatives of the -
equivalence classes in $\mathscr{C}_{n,r}$ and ${}^{t}U$ runs over a
complete set $\mathscr{U}$ of representatives of the right cosets
$P(n,r;\mathbb{Z})\backslash GL_{n}(\mathbb{Z})$

\item $\coprod\limits_{\substack{M\in\Gamma_{n}\\ \rank
    (\lambda_{n-r}(C_{M}))=n-r}} \Delta_{n,r}M=\coprod
  \Delta_{n,r}M\{C_{4},D_{3}D_{4}\}\left(\begin{smallmatrix} E_{n} &
    S\\ 0 & E_{n}  \end{smallmatrix}\right)\left(\begin{smallmatrix}
    {}^{t}U & 0\\ 0 & U^{-1}  \end{smallmatrix}\right)$

where, on the right hand side, $(C_{4}D_{3}D_{4})$ runs over a
complete set $\tilde{\tilde{\mathscr{C}}}$ of representatives
of the $\approx$ -equivalence classes in $\mathscr{C}_{n,r}$,
${}^{t}U$ runs over $\mathscr{U}$ as in\pageoriginale {\rm(i)} and $S$
runs over all $(n,n)$ integral symmetric matrices of the form
$\left(\begin{smallmatrix} 0^{(r,r)} & \ast\\ \ast & \ast
\end{smallmatrix}\right)$. 
\end{enumerate}
\end{sublemma}

\begin{proof}
Given $M$ in $\Gamma_{n}$ with rank $(\lambda_{n-r}(C_{M}))=n-r$, we
can find, as in the proof of Lemma \ref{c1:lem-1.6.4}, $H$ in
$\Gamma_{n}$ with $\lambda_{n-r}(H)=(0C_{4}D_{3}D_{4})$ for some
$(C_{4}D_{3}D_{4})$ in $\mathscr{C}_{n,r}$ and $W$ in
$GL_{n}(\mathbb{Z})$ such that
$\Delta_{n,r}M=\Delta_{n,r}H\left(\begin{smallmatrix} {}^{t}W & 0\\ 0
  &
  W^{-1}\end{smallmatrix}\right)=\Delta_{n,r}M\{C_{4},D_{3},D_{4}\}\left(\begin{smallmatrix}
  {}^{t}W & 0 \\ 0 & W^{-1}\end{smallmatrix}\right)$. To get the
chosen representatives in $\tilde{\mathscr{C}}$ and $\mathscr{U}$,
we need only to take $\left(\begin{smallmatrix} \ast & & 0\\ & E_{r} &
  0\\ 0 & 0 &
  U'_{4}\end{smallmatrix}\right)M\{C_{4},D_{3},D_{4}\}\left(\begin{smallmatrix}
  {}^{t}W't_{W} & 0\\ 0 & (W'){}^{-1}W^{-1}
\end{smallmatrix}\right)$ for  suitable $U'_{4}$ in
$GL_{n-r}(\mathbb{Z})$ and $W'$ in $P(n,r;\mathbb{Z})$. To prove (i),
we have therefore only to prove that the cosets on the right hand side
are all disjoint. Let, if possible,
$$
\Delta_{n,r}M\{C_{4},D_{3},D_{4}\}
\begin{pmatrix}
{}^{t}U & 0\\
0 & U^{-1}
\end{pmatrix}
=\Delta_{n,r}M\{C'_{4},D'_{3},D'_{4}\}
\begin{pmatrix}
{}^{t}U' & 0\\
0 & (U')^{-1}
\end{pmatrix}
$$
for the chosen representatives from $\mathscr{C}$ and
$\mathscr{U}$. Writing $M$, $M'$ instead of $M\{C_{4},D_{3},D_{4}\}$,
$M\{C'_{4},D'_{3},D'_{4}\}$ for the moment, we know that
$A_{M}=\left(\begin{smallmatrix} A_{1} & A_{2}\\ A_{3} & A_{4}
\end{smallmatrix}\right)$, $C_{M}=\left(\begin{smallmatrix} C_{1} &
  C_{2}\\ 0 & C_{4}\end{smallmatrix}\right)$ and
$C_{M'}=\left(\begin{smallmatrix} C'_{1} & C'_{2}\\ 0 & C'_{4}
\end{smallmatrix}\right)$. Taking $V=1$ in the proof of Lemma
\ref{c1:lem-1.6.14}, we may find a suitable $L'$ in $\Delta_{n,r}$ so
that for $H':=L'M$, the first $r$ columns of $C_{H'}$ are $0$. Since
the coset $\Delta_{n,r}M$ is unchanged in the process, we may suppose
already that the first $r$ columns of $C_{M}$ and likewise of $C_{M'}$
are $0$. Now, for some $K$ in $\Delta_{n,r}$, we have
$KM=M'\left(\begin{smallmatrix} {}^{t}V & 0\\ 0 & V^{-1}
\end{smallmatrix}\right)$ with $V=U^{-1}U'=\left(\begin{smallmatrix}
  V_{1}^{(r,r)} & V_{2}\\ V_{3} & V_{4}
\end{smallmatrix}\right)$. 

Also\pageoriginale
$$
C_{M}=
\begin{pmatrix}
0 & C_{2}\\
0 & C_{4}
\end{pmatrix},
C_{M'}=
\begin{pmatrix}
0 & C'_{2}\\
0 & C'_{4}
\end{pmatrix},
A_{M}=
\begin{pmatrix}
A_{1} & A_{2}\\
A_{3} & A_{4}
\end{pmatrix},
C_{K}=
\begin{pmatrix}
C^{\ast}_{1} & 0\\
0 & 0,
\end{pmatrix}
$$
$D_{K}=\left(\begin{smallmatrix} D^{\ast}_{1} & D^{\ast}_{2}\\ 0 &
  D^{\ast}_{4}
\end{smallmatrix}\right)$ with $\det C_{4}$. $C'_{4}\neq 0$. Further,
$C_{KM}=C_{M'}{}^{t}V$ gives $C'_{4}{}^{t}V_{2}=0$, so that $V_{2}=0$,
${}^{t}U'\in P(n,r;\mathbb{Z}){}^{t}U$ and so $U'=U$. Hence $KM=M'$,
$D^{\ast}_{4}(C_{4}D_{3}D_{4})=(C'_{4}D'_{3}D'_{4})$ with
$D^{\ast}_{4}$ in $GL_{n-r}(\mathbb{Z})$ and so
$(C_{4}D_{3}D_{4})=(D'_{4}D'_{3}D'_{4})$. This proves assertion
(i). We omit the proof of (ii), since it is similar to that of (i).
\end{proof}

As an immediate generalization of the well-known formula
$\int\limits_{\mathbb{R}}\exp\break(-ax^{2}+2bx)dx=\sqrt{\pi/a}\exp(b^{2}/a)$
for $\text{Re\,}(a)>0$, with $\sqrt{\pi/a}>0$ for $a\in\mathbb{R}$, we
know that for $(m,m)$ complex $A={}^{t}A$ with $\text{Re\,}A>0$ and
any $m$-rowed column $b$,
$\int\limits_{\mathbb{R}^{m}}\exp(-{}^{t}xAx+2{}^{t}bx)dx=(\det \pi
A^{-1})^{1/2}\exp({}^{t}bA^{-1}b)$ with $(\det \pi A^{-1})^{1/2}>0$
for $A>0$. As a further generalization, we have

\begin{sublemma}\label{c1:lem-1.6.16}
Let $W^{(r,r)}=W={}^{t}W>0$, $A$ an $(n-r,n-r)$ complex symmetric
matrix with $\text{Re\,}A>0$ and $Q$ a complex $(n-r,r)$ matrix. Then
\begin{multline*}
\int\limits_{X^{(r,n-r)}}\exp(-2\pi \tr(WXA{}^{t}X)+2\pi \tr(XQ))dX\\
=(\det W)^{(r-n)/2}2^{r(r-n)/2}(\det A^{-1})^{r/2}\exp(\pi
\tr({}^{t}QA^{-1}QW^{-1})/2) 
\end{multline*}
where the integration with respect to $X=(x_{ij})$ is over the space
of $(r,n-r)$ real matrices, $dX=\prod\limits_{i,j}dx_{ij}$ and $(\det
A^{-1})^{r/2}>0$ for real $A={}^{t}A>0$.
\end{sublemma}

\begin{proof}
Writing\pageoriginale ${}^{t}X=({}^{t}x_{1},\ldots,{}^{t}x_{r})$ where
$x_{1},\ldots,x_{r}$ are the $r$ rows of $X$ with $n-r$ entries each,
we have $\tr(XA{}^{t}X)={}^{t}xBx$ with ${}^{t}x:=(x_{1}\ldots x_{r})$
and $B=\left(\begin{smallmatrix} A & 0 & 0\\ \cdot & \cdot & \cdot\\ 0
  & \cdot & A\end{smallmatrix}\right)$ being the $((n-r)r,(n-r)r)$
  matrix whose $r$ blocks of size $(n-r,n-r)$ on the diagonal are all
  equal to $A$ and other $(n-r,n-r)$ matrix blocks are $0$. If $W_{0}$
  is the positive square root of $W$ and $QW^{-1}_{0}=(y_{1}\ldots
  y_{r})$ with columns $y_{i}$, we have
  $\tr(XQW^{-1}_{0})=({}^{t}y_{1}\ldots {}^{t}y_{r})x$. Thus the
  integral on the left hand side becomes
\begin{align*}
& (\det W_{0})^{r-n}\int\limits_{X}\exp(-2\pi \tr(XA{}^{t}X)+2\pi
  \tr(XQW^{-1}_{0}))dX\\
&= (\det W)^{(r-n)/2}\int\limits_{\mathbb{R}^{(n-r)r}} \exp(-2\pi
     {}^{t}xBx+2\pi({}^{t}y_{1}\ldots {}^{t}y_{r})x)dx\\
&= (\det W)^{(r-n)/2}2^{r(r-n)/2}(\det A^{-1})^{r/2}\exp (\pi
     \tr({}^{t}W^{-1}_{0}{}^{t}QA^{-1}QW^{-1}_{0})/2) 
\end{align*}
on using the formula preceding this lemma.
\end{proof}

\begin{sublemma}\label{c1:lem-1.6.17}
For $Z$ in $\mathscr{G}_{n}$, $N$ in $\Gamma_{n}$ with $\det C_{N}\neq
0$ and $(A_{N}C^{-1}_{N})^{\ast}$ integral, $\left(\begin{smallmatrix}
  W^{(r,r)}_{1} & W_{2}\\ {}^{t}W_{2} & W_{3}
\end{smallmatrix}\right)=W:=(C_{N}Z+D_{N}){}^{t}C_{N}$ and cusp form
$f$ in $\{r,k,s\}$, we have
$$
f((N<Z>)^{\ast})(\det (C_{N}Z+D_{N}))^{-k}=(\det C_{N})^{k}(\det
W_{3})^{-k}\sum\limits_{1\leq j\leq m}\alpha_{j}f_{j}(W_{4}) 
$$
where $\{f_{1},\ldots,f_{m}\}$ is a basis for the space of cusp forms
in $\{r,k,s\}$, $\alpha_{1},\ldots,\alpha_{m}$ are (bounded) constants
depending on $f$, $(A_{N}C^{-1}_{N})^{\ast}$ and
$W_{4}=W_{1}-W_{2}W^{-1}_{3}{}^{t}W_{2}$. 
\end{sublemma}

\begin{proof}
Dropping the suffix $N$ from $A_{N}$, $B_{N}$, $C_{N}$, $D_{N}$, we
note that $N<Z>=AC^{-1}-{}^{t}C^{-1}(CZ+D)^{-1}=AC^{-1}-W^{-1}$, in
view of the relations
$B-AC^{-1}D=B-A{}^{t}D{}^{t}C^{-1}=(B{}^{t}C-A{}^{t}D){}^{t}C^{-1}=-{}^{t}C^{-1}$.
From\pageoriginale
the Babylonian identity 
$$
W=
\begin{pmatrix}
E_{r} & W_{2}W^{-1}_{3}\\
0 & E_{n-r}
\end{pmatrix}
\begin{pmatrix}
W_{4} & 0\\
0 & W_{3}
\end{pmatrix}
\begin{pmatrix}
E_{r} & 0\\
W^{-1}_{3}{}^{t}W_{2} & E_{n-r}
\end{pmatrix},
$$
we have $(W^{-1})^{\ast}=W^{-1}_{4}$. On the other hand, there exist
constants $\alpha_{1},\ldots,\break\alpha_{m}$ depending on $f$ and the
residue class of $(A_{N}C^{-1}_{N})^{\ast}$ modulo $s$ such that
\begin{align*}
(f|
\begin{pmatrix}
(A_{N}C^{-1}_{N})^{\ast} -E_{r}\\
E_{r} & 0
\end{pmatrix}
)(Z_{1})&=f((A_{N}C^{-1}_{N})^{\ast}-Z^{-1}_{1})(\det
Z_{1})^{-k}\\
&=\sum_{1\leq j\leq m}\alpha_{j}f_{j}(Z_{1})
\text{for\quad} Z_{1}\in\mathscr{G}_{r}.
\end{align*}
Thus
\begin{align*}
(f||N)(Z) &= f((A_{N}C^{-1}_{N})^{\ast}-W^{-1}_{4})(\det
  W{}^{t}C^{-1})^{-k}\\
&=\sum_{1\leq j\leq m}\alpha_{j}f_{j}(W_{4})(\det W_{4})^{k}(\det
  W)^{-k}(\det C)^{k}\\
&= \sum_{j}\alpha_{j}f_{j}(W_{1}-W_{2}W^{-1}_{3}{}^{t}W_{2})(\det
  W_{3})^{-k}(\det C)^{k}. 
\end{align*}
Since the number of residue classes of $(r,r)$ integral $S={}^{t}S$
modulo $s$ is finite, $|\alpha_{j}|\leq \nu$ for $1\leq j\leq m$ and a
constant $\nu=\nu(f)$.
\end{proof}

Let us now {\em fix} $N$ in $\Gamma_{n}$ as in Lemma \ref{c1:lem-1.6.17}
with $\det C_{N}\neq 0$ $C_{N}=\left(\begin{smallmatrix} C_{1}^{(r,r)}
  & \ast\\ 0 & \ast\end{smallmatrix}\right)$,
  $(A_{N}C^{-1}_{N})^{\ast}$ integral and a natural number $c_{0}$
  with $c_{0}C^{-1}_{N}$ integral. We shall study more closely the
  subseries
$$
\mathscr{S}(f;N):=\sum_{S}(f||N
\begin{pmatrix}
E_{n} & C^{-1}S{}^{t}C^{-1}\\
0 & E_{n}
\end{pmatrix}
)(Z)
$$
where $S=\left(\begin{smallmatrix} 0^{(r,r)} & S_{2}\\ {}^{t}S_{2} &
  S_{3}
\end{smallmatrix}\right)$ runs over all matrices of this form in\pageoriginale
$a\Lambda_{n}:=\{aT^{(n,n)}|T={}^{t}T$ integral$\}$ and
$a:=sc^{2}_{0}$. Recall
$$
\Lambda^{\ast}_{n}=\{T^{(n,n)}=(t_{ij})|t_{ii},2t_{ij}=2t_{ji}\in
\mathbb{Z}\}, 
$$
the lattice dual to $\Lambda_{n}$. Let us further write $eta_{s}(\ast)$
for $\exp(2\pi i\ast|s)$ and $\eta$ for $\eta_{1}$. As usual, let
${}^{t}BAB$ be abbreviated as $A[B]$. In view of Lemma
\ref{c1:lem-1.6.17}, 
\begin{align*}
\mathscr{S} (f;N)&=\sum_{j}\alpha_{j}\sum_{S_{2},S_{3}}(\det C_{N})^{k}(\det
(W_{3}+S_{3}))^{-k}f_{j}\\
&\qquad(W_{1}-(W_{3}+S_{3})^{-1}[{}^{t}(W_{2}+S_{2})]) 
\end{align*}
where $S_{2}$, $S_{3}$ have entries divisible by $a$. As a first step,
we note that, for $T_{0}>0$ in $\Lambda^{\ast}_{r}$,
\begin{align*}
& \sum_{S^{(r,n-r)}_{2}\equiv 0(\rm{mod} \;
    1)}n_{s}(-\tr(T_{0}W^{-1}_{3}[{}^{t}(w_{2}+aS_{2})])\\
&=
  \sum_{S_{2}^{(r,n-r)}}\int\limits_{X^{(r,n-r)}}\eta_{s}(-\tr(T_{0}W^{-1}_{3}[{}^{t}(W_{2}+aX)])\eta(\tr(S_{2}{}^{t}X))dX\\
&\qquad\qquad\text{(Poisson formula)}\\
&=\eta_{s}(-\tr(T_{0}W^{-1}_{3}[{}^{t}W_{2}]))\sum_{S_{2}}\int\limits_{X}\eta(-\tr((a^{2}|s)T_{0}W^{-1}_{3}[{}^{t}X]))\\
&\qquad(-\tr\left(\frac{2a}{s}XW^{-1}_{3}{}^{t}W_{2}T_{0}\right)+\tr(X{}^{t}S_{2}))dX\\
&= \eta_{s}(-\tr(T_{0}W^{-1}_{3}[{}^{t}W_{2}]))\sum_{S_{2}}\left(\det
    \left(\frac{2a^{2}}{s}T_{0}\right)\right)^{(r-n)/2}\\
&\qquad(\det
    (-iW_{3}))^{r/2}\eta\left(-\frac{s}{4a^{2}}\tr\left({}^{t}QW_{3}QT^{-1}_{0}\right)\right) 
\end{align*}
where $Q:=-\dfrac{2ia}{s}W^{-1}_{3}{}^{t}W_{2}T_{0}+i{}^{t}S_{2}$. Now 
$$
\tr({}^{t}QW_{3}QT^{-1}_{0})=-\dfrac{4a^{2}}{s^{2}}\tr(T_{0}W^{-1}_{3}[{}^{t}W_{2}])+\frac{4a}{s}\tr(S_{2}{}^{t}W_{2})-\tr(W_{3}[{}^{t}S_{2}]T^{-1}_{0})
$$
and so 
\begin{align*}
& \sum_{S^{(r,n-r)}_{2}\equiv 0(\rm{mod} \;
  a)}\eta_{s}(-\tr(T_{0}W^{-1}_{3}[{}^{t}(W_{2}+S_{2})])=\left(\frac{2a^{2}}{s}\right)^{r(r-n)/2}\\
&\qquad(\det
T_{0})^{(r-n)/2}(\det(-iW_{3}))^{r/2}\\
& \sum_{S_{2}^{(r,n-r)}\text{ integral}}\eta\left(-\frac{1}{a}\tr(S_{2}{}^{t}W_{2})+\frac{s}{4a^{2}}\tr(W_{3}[{}^{t}S_{2}]T^{-1}_{0})\right)
\end{align*}

For\pageoriginale the Fourier coefficients $b_{j}(T_{0})$ of the cusp
form
$$f_{j}(Z^{\ast})=\sum\limits_{0<T_{0}\in\Lambda^{\ast}_{r}}b_{j}(T_{0})\eta_{n}(T_{0}Z^{\ast}),$$ 
we know from an analogue \cite{key19} of Theorem \ref{c1:thm-1.1.1} (Hecke)
that $b_{j}(T_{0})=O((\det T_{0})^{k/2})$. Using this Fourier
expansion, we prove

\begin{sublemma}\label{c1:lem-1.6.18}
For a cusp form $f$ in $\{r,k,s\}$ and $Z$ in $\mathscr{G}_{n}$, we
have, with the same notation as in Lemma \ref{c1:lem-1.6.17},
\begin{align*}
& \sum_{S=\left(\begin{smallmatrix} 
0^{(r)} & S_{2}\\
{}^{t}S_{2} & 0
  \end{smallmatrix}\right)\in a\Lambda_{n}}(f||N
\begin{pmatrix}
E_{n} & C^{-1}_{N}S & {}^{t}C_{N}\\
0 & E_{n} &
\end{pmatrix})
(Z)=(\det C_{N})^{k}\left(\dfrac{2a^{2}}{s}\right)^{r(r-n)/2}\\
&\qquad\qquad(\det
W_{3})^{-k}(\det (-iW_{3}))^{r/2}\times\\
&\times
\sum_{j}\alpha_{j}\sum_{\substack{0<T_{0}\in\Lambda^{\ast}_{r}\\ S_{2}^{(r,n-r)}\text{ integral}}} (\det
T_{0})^{(r-n)/2}b_{j}(T_{0})\eta_{s}(\tr(T_{0}W_{1}))\eta\\
&\qquad\qquad\left(-\frac{1}{a}\tr(W_{2}{}^{t}S_{2})+\dfrac{s}{4a^{2}}\tr(W_{3}[{}^{t}S_{2}]T^{-1}_{0})\right) 
\end{align*}
the series over $T_{0}$ and $S_{2}$ being absolutely convergent.
\end{sublemma}

\begin{proof}
In view of the arguments preceding this lemma, for its proof we need
only to insert the Fourier expansion for each $f_{j}(1\leq j\leq m)$
and show the resulting (double) series over $T_{0}$ and $S_{2}$ to be
absolutely convergent.
\end{proof}

Let us observe that the matrix $P$ defined, for real $X^{(r,n-r)}$, by 
\begin{align*}
P&=\Iim(W_{1}-W^{-1}_{3}[{}^{t}(W_{2}+X)])+(\Iim(W^{-1}_{3}))\\
&\quad[{}^{t}(X+\text{Re\,}W_{2}+\Iim(W_{2})(\text{Re\,}(W^{-1}_{3}))(\Iim(W^{-1}_{3}))^{-1})] 
\end{align*}
is actually independent of $X$, since the terms involving $X$ give 
\begin{align*}
&-(\text{Re\,}(W_{2})+X)(\Iim(W^{-1}_{3}))({}^{t}X+\text{Re\,}({}^{t}W_{2}))\\
&-\Iim(W_{2})\text{Re\,}(W^{-1}_{3})\cdot({}^{t}X+\text{Re\,}({}^{t}W_{2}))\\
& -(\text{Re\,}(W_{2})+X)\text{Re\,}W^{-1}_{3}\cdot
  \Iim{}^{t}W_{2}\\
&+(\text{Re\,}(W_{2})+X)(\Iim(W^{-1}_{3}))({}^{t}X+\text{Re,\,}({}^{t}W_{2}))\\
& +\Iim(W_{2})\text{Re\,}(W^{-1}_{3})({}^{t}X+\text{Re\,}({}^{t}W_{2}))+(\text{Re\,}W^{-1}_{3}\cdot \Iim{}^{t}W_{2}=0.
\end{align*}
On\pageoriginale the other hand, for any real $X^{(r,n-r)}$, clearly
$W+\left(\begin{smallmatrix} 0^{(r)} & X\\ {}^{t}X & 0
\end{smallmatrix}\right)\in\mathscr{G}_{n}$ and hence the imaginary
part of the leading $(r,r)$ submatrix 
$$
(W_{1}-W^{-1}_{3}[{}^{t}(W_{2}+X)])^{-1}\text{ \ of \ } (W+
\begin{pmatrix}
0^{(r)} & X\\
{}^{t}X & 0
\end{pmatrix}
)^{-1}
$$
is negative definite. Thus
$P=P(X_{0})=\Iim(W_{1}-W^{-1}_{3}[{}^{t}(W_{2}+X_{0})])>0$ taking
$X_{0}=-\text{Re\,}(W_{2})-\Iim(W_{2})\cdot
\text{Re\,}(W^{-1}_{3})(\Iim(W^{-1}_{3}))^{-1}$. Now
\begin{align*}
&|\eta_{s}(\tr(T_{0}(W_{1}-W^{-1}_{3}[{}^{t}(W_{2}+S_{2})])|\\
&\qquad =\exp(-\frac{2\pi}{s}\tr(T_{0}(P-(\Iim(W^{-1}_{3}))[{}^{t}(S_{2}-X_{0})]))\\
&\qquad <\exp\left(-\frac{2\pi\rho}{s}\tr(T_{0}+(S_{2}-X_{0}){}^{t}(S_{2}-X_{0}))\right) 
\end{align*}
where $\rho>0$ is such that $P-\sqrt{\rho}E^{(r)}$,
$\Iim(-W^{-1}_{3})-\sqrt{\rho}E^{(r)}$ and $T_{0}-\sqrt{\rho}E^{(r)}$
are all $>0$. The series on the left hand side of the asserted
identity
$$
=O\left(\sum_{\substack{0<T_{0}\in
    \Lambda^{\ast}_{r}\\ S_{2}^{(r,n-r)}\equiv 0(\rm{mod} \; a)}}(\det T_{0})^{k/2}|\eta_{s}(\tr(T_{0}(W_{1}-W^{-1}_{3}[{}^{t}(W_{2}+S_{2})])))\right)
$$
and is now easily seen to be absolutely convergent.

To prove the absolute convergence of the double series on the right
hand side, it suffices to prove that
\begin{align*}
& \sum_{S_{2}^{(r,n-r)}\text{
    integral}}|\eta_{s}(\tr(T_{0}W_{1})-\frac{s}{a}
  \tr(W_{2}{}^{t}S_{2}) +
  \frac{s^{2}}{4a^{2}}\tr(W_{3}[{}^{t}S_{2}]T^{-1}_{0}))\\
&= O((\det T_{0})^{n-r}\exp(-2\pi \rho \tr(T_{0}))\text{ \  for some
  \ } \rho>0.
\end{align*}
Now\pageoriginale
\begin{align*}
\Iim(\tr(T_{0}W_{1}) &-
\frac{s}{a}\tr(W_{2}{}^{t}S_{2}) + \frac{s^{2}}{4a^{2}}
\tr(W_{3}[{}^{t}S_{2}]T^{-1}_{0}))=\\
&= \tr(\Iim(W)
\begin{pmatrix}
T_{0} \quad -(s/2a)S_{2}\\
-(s/2a){}^{t}S_{2} \quad (s^{2}/4a^{2})T^{-1}_{0}[S_{2}]
\end{pmatrix}
)\\
&= \tr((\Iim(W))
\begin{bmatrix}
E_{r} & 0\\
-\frac{s}{2a}{}^{t}S_{2}T^{-1}_{0} & E_{n-r}
\end{bmatrix}
\begin{pmatrix}
T^{(r)}_{0} & 0\\
0 & 0
\end{pmatrix}
)
\end{align*}
and further taking $\rho_{1}>0$ with $\Iim(W)>\rho_{1}E_{n}$, we see
that the above series over $S_{2}$ is 
$$
O\left(\sum_{S_{2}}\exp\limits_{\text{integral}}\left(-\frac{2\pi\rho_{1}}{s}\tr\left(T_{0}+\frac{s^{2}}{4a^{2}}S_{2}{}^{t}S_{2}T^{-1}_{0}\right)\right)\right).
$$
To complete the proof of the lemma, we have only to show that for
$\rho'=2\pi s\rho_{1}/(4a^{2})$ and for every $T_{0}>0$ in
$\Lambda^{\ast}_{r}$,
$$
\sum_{S_{2}^{(r,n-r)}}\exp\limits_{\text{integral}}(-\rho'\tr(S_{2}{}^{t}S_{2}T^{-1}_{0}))=O((\det
T_{0})^{n-r}). 
$$
For this purpose, we may assume, without loss of generality that
$T^{-1}_{0}$ is $M$-reduced, so that
$T^{-1}_{0}=\left(\begin{smallmatrix} t_{1} & \ldots & 0\\ \vdots &
  \ddots & \vdots\\ 0 & \ldots & t_{r}
\end{smallmatrix}\right)\left[\begin{smallmatrix} 1 & & \ast\\ &
    \ddots & \\ 0 & \ldots & 1
  \end{smallmatrix}\right]$ and for $\rho_{2}=\rho_{2}(r)>0$,
$\rho_{3}=\rho_{3}(r)>0$, 
\begin{align*}
\rho_{2}T'_{0} &:= \rho_{2}
\begin{pmatrix}
t_{1} & \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots & t_{r}
\end{pmatrix}
<T^{-1}_{0}<\rho_{3}
\begin{pmatrix}
t_{1} & \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots & t_{r}
\end{pmatrix},\\
&\quad(\Lambda^{\ast}_{r}\ni)T_{0}<\rho^{-1}_{2}
\begin{pmatrix}
t^{-1}_{1} & \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots & t^{-1}_{r}
\end{pmatrix}
\end{align*}
and hence $t_{i}<\rho^{-1}_{2}(1\leq i\leq r)$. Thus, as a majorant
for the last mentioned series over $S_{2}$, we\pageoriginale have
\begin{align*}
&\quad
  \sum_{S_{2}^{(r,n-r)}}\exp\limits_{\text{integral}}(-\rho'\rho_{2}\tr(S_{2}{}^{t}S_{2}T'_{0}))\\
&= \prod_{1\leq i\leq
    r}\left(\sum_{\ell\in\mathbb{Z}}\exp(-\rho'\rho_{2}t_{i}\ell^{2})\right)^{n-r}\\
&\leq \prod_{1\leq i\leq
    r}\left(1+\frac{2\exp(-\rho'\rho_{2}t_{i})}{1-\exp(-\rho'\rho_{2}t_{i})}\right)^{n-r}\\
&<
  \prod_{i}\left(1+\frac{2}{\rho'\rho_{2}t_{i}}\right)^{n-r}=\prod_{i}\left(\frac{2+\rho'\rho_{2}t_{i}}{\rho'\rho_{2}t_{i}}\right)^{n-r}\\
&< \prod(\rho'+2)/\rho'\rho_{2})^{n-r}(\det T_{0})^{n-r}
\end{align*}
which proves our claim above and the lemma as well.

\begin{sublemma}\label{c1:lem-1.6.19}
For $0<T_{0}$ in $\Lambda^{\ast}_{r}$, we have
\begin{align*}
&\sum_{S_{3}\in a\Lambda_{n-r}}(\det (W_{3}+S_{3}))^{-k}(\det
(-i(W_{3}+S_{3}))^{r/2}\eta((s/4a^{2})\tr((W_{3}+S_{3})\\
&T^{-1}_{0}[S_{2}])=i^{(r-n)k}(2\pi)^{(n-r)(k-r/2)}2^{(r-n)(n-r-1)/2}a^{(r-n)(n-r+1)/2}\\
&(4a^{2}t_{0})^{(r-n)(2k-n-1)/2}(1/\Gamma_{n-r}(k-r/2))\eta((s/a^{2})\tr(W_{3}T^{-1}_{0}[S_{2}])\\
&\sum_{T}(\det T)^{k(n+1)/2}\eta((1/4a^{2}t_{0})\tr(TW_{3})) 
\end{align*}
where $t_{0}$ is a fixed natural number with $t_{0}T^{-1}_{0}$
integral, $S_{2}^{(r,n-r)}$ is integral,
$\Gamma_{m}(\ell):=\pi^{m(m-1)/4}\prod\limits_{0\leq \nu\leq
  m-1}\Gamma(\ell-\nu/2)$ and $T$ runs over
$\{T\in\Lambda^{\ast}_{n-r}|T>0,\dfrac{1}{4a}(sT^{-1}_{0}[S_{2}]+t^{-1}_{0}T)\in\Lambda^{\ast}_{n-r}\}$. 
\end{sublemma}

\begin{proof}
The\pageoriginale left hand side is just
\begin{align*}
&i^{k(r-n)}\eta((s/4a^{2})\tr(W_{3}T^{-1}_{0}[S_{2}]))\\
&\qquad\qquad\sum_{S_{3}\in\Lambda_{n-r}}\det
(-i(W_{3}+aS_{3}))^{r/2-k}\eta((s/4a)\tr(S_{3}T^{-1}_{0}[S_{2}]))\\
&=i^{k(r-n)}\eta((s/4a^{2})\tr(W_{3}T^{-1}_{0}[S_{2}]))\sum_{\Lambda_{n-r}\ni
  S'_{3}\rm{mod} \;  4at_{0}}\eta(s/4a)\tr(S'_{3}T^{-1}_{0}[S_{2}]))\\
&\qquad\sum_{S_{3}\in\Lambda_{n-r}}\det(-i(W_{3}+aS'_{3}-4a^{2}t_{0}S_{3}))^{r/2-k}\\
&=i^{k(r-n)}\eta((s/4a^{2})\tr(W_{3}T^{-1}_{0}[S_{2}]))\\
&\qquad\sum_{S'_{3}\rm{mod} \; 4at_{0}}\eta((s/4a)\tr(S'_{3}T^{-1}_{0}[S_{2}]))\left(\frac{\pi}{2a^{2}t_{0}}\right)^{(n-r)(k-r/2)}\\
&\times 2^{(r-n)(n-r-1)/2}(1/\Gamma_{n-r}(k-r/2))\\
&\qquad\sum_{0<T\in\Lambda^{\ast}_{n-r}}(\det
T)^{k-(n+1)/2}\eta((1/4a^{2}t_{0})\tr(T(W_{3}+aS'_{3}))), 
\end{align*}
on using the well-known formula (for $\text{Re\,}Y_{1}>0$ and
$\rho>m+1$)
\begin{align*}
& 2^{m(m-1)/2}\Gamma_{m}(\rho)\sum_{F\in\Lambda_{m}}(\det (Y_{1}+2\pi
iF))^{-\rho}\\
&\qquad =\sum_{0<T\in\Lambda^{\ast}_{m}}(\det
T)^{\rho-(m+1)/2}\exp(-\tr(TY_{1})). 
\end{align*}
The lemma now follows from
\begin{align*}
&\sum_{\Lambda_{n-r}\geq S'_{3}\rm{mod} \;
  4at_{0}}\eta((s/4a)\tr(S'_{3}T^{-1}_{0}[S_{2}])+(1/4at_{0})\tr(TS'_{3}))\\
&\qquad=\begin{cases}
(4at_{0})^{(n-r)(n-r+1)/2} & \text{if \ } sT^{-1}_{0}[S_{2}]+\\
 & +t^{-1}_{0}T\in 4a\Lambda^{\ast}_{n-r}\\
0 & \text{otherwise}
\end{cases}
\end{align*}
\end{proof}

Going back to $\mathscr{S}(f;N)$, we have, in view of Lemma
\ref{c1:lem-1.6.18} and \ref{c1:lem-1.6.19}, 
\begin{gather*}
\mathscr{S}(f;N)=\sum_{j}\alpha_{j}\sum_{S_{3}\in a\Lambda_{n-r}}(\det
C_{N})^{k}\left(\frac{2a^{2}}{s}\right)^{r(r-n)/2}\\
\sum_{\substack{0<T_{0}\in\Lambda^{\ast}_{r}\\ S_{2}^{(r,n-r)}\text{
      integral}}}(\det
T_{0})^{(r-n)/2}b_{j}(T_{0})n_{s}(\tr(T_{0}W_{1})-\frac{s}{a}\tr(W^{t}_{2}S_{2}))\times
\\
\times (\det
(W_{3}+S_{3}))^{-k}(\det(-i(W_{3}+S_{3}))^{r/2}\eta\left(\frac{s}{4a^{2}}\tr((W_{3}+S_{3})T^{-1}_{0}[S_{2}])\right)\\
=(\det
C_{N})^{k}\frac{2^{(r-n)(n-1)/2}i^{(r-n)k}(2\pi)^{(n-r)(k-r/2)}a^{(r-n)(r+n+1)/2}}{s^{r(r-n)/2}\Gamma_{n-r}(k-r/2)}\times\\
\times \sum_{1\leq j\leq m}\alpha_{j}\sum_{T_{0},S_{2},T}(\det
T_{0})^{(r-n)/2}(4a^{2}t_{0})^{(r-n)(2k-n-1)/2}b_{j}(T_{0})(\det
T)^{k-\frac{n+1}{2}}\times\\
\times \eta_{s}(\tr(T_{0}W_{1})-\frac{s}{a}\tr(W_{2}{}^{t}S_{2})+\frac{s}{4a^{2}t_{0}}\tr(TW_{3})+\frac{s^{2}}{4a^{2}}\tr(W_{3}T^{-1}_{0}[S_{2}]))
\end{gather*}\pageoriginale
where $0<T_{0}\in\Lambda^{\ast}_{r}$, $S^{(r,n-r)}_{2}$ is integral,
$0<T\in \Lambda^{\ast}_{n-r}$, $sT^{-1}_{0}[S_{2}]+t^{-1}_{0}T\in
4a\Lambda^{\ast}_{n-r}$. Let
$$
P:=
\begin{pmatrix}
T_{0} & -\frac{s}{2a}S_{2}\\
-\frac{s}{2a}{}^{t}S_{2} & \frac{s}{4a^{2}}(t^{-1}_{0}T+sT^{-1}_{0}[S_{2}])
\end{pmatrix}
=
\begin{pmatrix}
P^{(r)}_{1} & P_{2}\\
{}^{t}P_{2} & P^{(n-r)}_{3}
\end{pmatrix}
,\text{ \  say.}
$$
Then from
$$
P=
\begin{pmatrix}
T_{0} & 0\\
0 & \frac{s}{4a^{2}t_{0}}T
\end{pmatrix}
\begin{bmatrix}
E_{r} & -\frac{s}{2a}T^{-1}_{0}S_{2}\\
0 & E_{n-r}
\end{bmatrix},
$$
we see that $P>0$ and further $\det
P=\left(\dfrac{s}{4a^{2}t_{0}}\right)\det T_{0}\cdot \det T$.

Out assumptions above on $T_{0}$, $S_{2}$ and $T$ mean precisely that
$P_{1}\in\Lambda^{\ast}_{r}$, $\dfrac{2a}{s}P_{2}$ is integral,
$\dfrac{a}{s}P_{3}\in\Lambda^{\ast}_{n-r}$ and
$\dfrac{4a^{2}}{s}t_{0}P_{3}-st_{0}T^{-1}_{0}[S_{2}]$ is in
$\Lambda^{\ast}_{n-r}$ (the last condition being superfluous). Now
$\{T_{0},S_{2},T\}$ is in bijective correspondence with $P$ as above
and
$$
\tr(WP)=\tr(W_{1}T_{0})-\dfrac{s}{a}\tr(W_{2}{}^{t}S_{2})+\dfrac{s}{4a^{2}}(\tr(t^{-1}_{0}W_{3}T)+\tr(sW_{3}T^{-1}_{0}[S_{2}])).
$$
We\pageoriginale have thus proved

\begin{sublemma}\label{c1:lem-1.6.20}
For a cusp form $f$ in $\{r,k,s\}$ and $N$ in $\Gamma_{n}$ as in Lemma
\ref{c1:lem-1.6.17}, 
\begin{gather*}
\mathscr{S}(f;N)=\frac{(\det
  C_{N})^{k}2^{(r-n)(n-1)/2}i^{(r-n)k}(2\pi)^{(n-r)(k-r/2)}}{a^{(n-1)(n+r+1)/2}s^{(n-r)(k-(n+r+1)/2)}\Gamma_{n-r}(k-r/2)}\times\\
\times\sum_{j}\alpha_{j}\sum_{0<P}b_{j}(P_{1})(\det
P_{1})^{\frac{r+1-2k}{2}}(\det P)^{\frac{2k-n-1}{2}}\eta_{s}(\tr(PW))
\end{gather*}
where $P_{1}\in\Lambda^{\ast}_{r}$, $2c^{2}_{0}p^{(r,n-r)}_{2}$ is
integral and $c^{2}_{0}P_{3}\in\Lambda^{\ast}_{n-r}$.
\end{sublemma}

We recall that for $N$ in $\Gamma_{n}$ fixed above, $C=C_{N}=\left(
\begin{smallmatrix} C^{(r)}_{1} & C_{2}\\ 0 & C_{4}
\end{smallmatrix}\right)$ and
$c_{0}C^{-1}=c_{0}\left(\begin{smallmatrix} C^{-1}_{1} &
  -C^{-1}_{1}C_{2}C^{-1}_{4}\\ 0 & C^{-1}_{4}\end{smallmatrix}\right)$
is integral. Let us define $G$, $G'$ by 
\begin{align*}
G &= \{\lambda_{n-r}(S{}^{t}C^{-1})|S=
\begin{pmatrix}
0^{(r)} & S_{2}\\
{}^{t}S_{2} & S_{3}
\end{pmatrix}
\in a\Lambda_{n}\},\\
G' &= \{\lambda_{n-r}(CS)|S=
\begin{pmatrix}
0^{(r)} & S_{2}\\
{}^{t}S_{2} & S_{3}
\end{pmatrix}
\in s\Lambda_{n}\}.
\end{align*}
Clearly $G'=\{(C_{4}{}^{t}S_{2},
C_{4}S_{3})|\dfrac{1}{s}S_{2}^{(r,n-r)}$ integral, $S_{3}\in
s\Lambda_{n-r}\}$ is a subgroup of index $\abs(\det C_{4})^{r}$ in the
(additive) group $G_{0}=\{({}^{t}S'_{2},
C_{4}S_{3})|\dfrac{1}{s}\break S'_{2}$ integral and of size $(n-r,r)$,
$S_{3}\in s\Lambda_{n-r}\}$. Moreover,
$G=\{({}^{t}S_{2}{}^{t}C^{-1}_{1}-S^{t}_{3}(C^{-1}_{1}C_{2}C^{-1}_{4})$,
$S_{3}{}^{t}C^{-1}_{4})|\dfrac{1}{a}S_{2}^{(r,n-r)}$ integral,
$S_{3}\in a\Lambda_{n-r}\}\subset G'$. As representatives of $G_{0}/G$, we
can take representatives of $\{C_{4}S_{3}|S_{3}\in
s\Lambda_{n-r}\}/\break\{S_{3}{}^{t}C^{-1}_{4}|S_{3}\in a\Lambda_{n-r}\}$
together with representatives of $\{{}^{t}S'_{2}|S'_{2}$ of size
$(r,n-r)$ and with entries in
$s\mathbb{Z}\}/\{{}^{t}S_{2}{}^{t}C^{-1}_{1}|S_{2}$ of size $(r,n-r)$
and with entries in $a\mathbb{Z}\}$. Hence
\begin{align*}
[G_{0}:G] &=
[s\Lambda_{n-r}:aC^{-1}_{4}\Lambda_{n-r}{}^{t}C^{-1}_{4}]\abs(\det
(a/s)^{t}C^{-1}_{1})^{n-r}\\
&= \abs(\det c_{0}c^{-1}_{4})^{n-r+1}\abs(\det
c^{2}_{0}C^{-1}_{1})^{n-r} 
\end{align*}
and\pageoriginale so
$$
[G':G]=c_{0}^{(n-r)(n+r+1)}\abs((\det C_{1})^{r-n}/(\det
C_{4})^{n+1})=\nu_{0}(C_{N}),\text{ \ say}.
$$
Let
$$
S'_{j}=
\begin{pmatrix}
0 & S_{j,2}\\
{}^{t}S_{j,2} & S_{i,3}
\end{pmatrix}\in s\Lambda_{n}
\text{ \  for \ } 1\leq j\leq \nu_{0}(C_{N})
$$
be chosen such that $\lambda_{n-r}(C_{N}S'_{j})$ are representatives
for $G'/G$. We now claim that for ${}^{t}S=S\equiv 0(\rm{mod} \; s)$ and
$M=N\left(\begin{smallmatrix} E_{n} & S\\ 0 & E_{n}
\end{smallmatrix}\right)$, $(f||M)(Z)$ is determined already by
$\lambda_{n-r}(M)$. Indeed, let
$$
M'=N
\begin{pmatrix}
E_{n} & S'\\
0 & E_{n}
\end{pmatrix},
M''=N
\begin{pmatrix}
E_{n} & S''\\
0 & E_{n}
\end{pmatrix}
$$
with integral symmetric $S'$, $S''\equiv O(\rm{mod} \; s)$ and let
$\lambda_{n-r}(M')=\lambda_{n-r}\break(M'')$. Then, by Lemma
\ref{c1:lem-1.6.10}, $M'=KM''$ for some $K$ in $\Delta_{n,r}$ and the
hypothesis on $S'$, $S''$ forces $K$ to be in $\Delta_{n,r}(s)$ and
the associated $K^{\ast}$ in $\Gamma_{r}$ to lie in $\Gamma_{r}(s)$;
hence we have
$(f||M')(Z)=(f||KM'')(Z)=((f||K^{\ast})||M'')(Z)=(f||M'')(Z)$. Writing
therefore $(f||(\lambda_{n-r}(C_{M}),\lambda_{n-r}\break(D_{M}))(Z)$ for
$M=N
\left(\begin{smallmatrix} E_{n}  & S\\ 0 & E_{n}
\end{smallmatrix}\right)$ in $\Gamma_{n}$ as above, we have
\begin{align*}
\mathscr{T}(f,N): &=\sum_{S=\left(\begin{smallmatrix} 0^{(r)} &
    S_{2}\\ {}^{t}S_{2} & S_{3}
  \end{smallmatrix}\right)\in s\Lambda_{n}}(f||N
\begin{pmatrix}
E_{n} & S\\
0 & E_{n}
\end{pmatrix})(Z)\\
&=\sum_{H\in G'}
(f||(\lambda_{n-r}(C_{N}),\lambda_{n-r}(D_{N})+H))(Z)\\
&=\sum_{i}\sum_{H\in
  G}(f||\lambda_{n-r}(C_{N}),\lambda_{n-r}(D+CS'_{i}))+H))(Z)\\
&=\sum_{i}\sum_{S=\left(\begin{smallmatrix} 0^{(r)} &
    S_{2}\\ {}^{t}S_{2} & S_{3}
  \end{smallmatrix}\right)\in a\Lambda_{n}}\left(f||N
\begin{pmatrix}
E_{n} & S'_{i}\\
0 & E_{n}
\end{pmatrix}
\begin{pmatrix}
E_{n} & C^{-1}_{N}S{}^{t}C^{-1}_{N}\\
0 & E_{n}
\end{pmatrix}\right)(Z).
\end{align*}
For $M=N\left(\begin{smallmatrix} E^{(n)} & S'_{i}\\ 0 & E^{(n)}
\end{smallmatrix}\right)$, we have, however,
$C_{M}=C_{N}=\left(\begin{smallmatrix} C^{(r)}_{1} & C_{2}\\ 0 & C_{4}
\end{smallmatrix}\right)$,\break $D_{M}=CS'_{i}+D_{N}$, 
$(A_{M}\; C^{-1}_{M})^{\ast}$\pageoriginale is integral and
$C_{M}Z{}^{t}C_{M}+D_{M}{}^{t}C_{M}=W+C_{M}S'_{1}{}^{t}C_{M}$. In view
of Lemma \ref{c1:lem-1.6.20}, we have
\begin{align*}
& \mathscr{T} (f,N)=\beta(\det C_{N})^{k}a^{(r-n)(n+r+1)/2}\\
& \sum_{j,i}\alpha_{j}\sum_{0<P}b_{j}(P_{1})(\det
  P_{1})^{(r+1-2k)/2}(\det P)^{k-(n+1)/2}\times \\
&\times \eta_{s}(\tr(P(W+S'_{i}[{}^{t}C_{N}])))
\end{align*}
where $P=\left(\begin{smallmatrix} P^{(r)}_{1} & P_{2}\\ {}^{t}P_{2} &
  P^{(n-r)}_{3}\end{smallmatrix}\right)>0$ runs over all such matrices
with $P_{1}\in\Lambda^{\ast}_{r}$, $2c^{2}_{0}P_{2}$ integral,
$c^{2}_{0}P_{3}\in\Lambda^{\ast}_{n-r}$ and
\begin{align*}
\beta: &= i^{(r-n)k}2^{(r-n)(n-1)/2}(2\pi)^{(n-r)(k-r/2)}\times\\
&\quad s^{(r-n)(k-(n+r-1)/2)}/\Gamma_{n-r}(k-r/2).
\end{align*}
For any such $P$ and any $H=(H_{1}^{(n-r,r)}, H_{2}^{(n-r,n-r)})$ in
$G'$, let $\chi(H):=\eta_{s}(2\tr(H_{1}{}^{t}C_{1}P_{2})+2tr(H_{2}
{}^{t}C_{2}P_{2})+\tr(H_{2}{}^{t}C_{4}P_{3}))$. Then 
it is not hard to prove that $\chi(H)=1$ for all $H$ in $G$ and 
$\eta_{s}(\tr(PS[{}^{t}C_{N}]) =
\chi((C_{4}{}^{t}S_{2},\break C_{4}S_{4}))=\chi(\lambda_{n-r}(CS))$ 
for $S=\left(\begin{smallmatrix} 0 & S_{2}\\ {}^{t}S_{2} & S_{3}
\end{smallmatrix}\right)\in s\Lambda_{n}$. Therefore, in view of our
choice of $S'_{i}$, we have
$\sum\limits_{i}\eta_{s}(\tr(PS'_{i}[{}^{t}C]))=\sum_{H\in
  G'/G}\chi(H)=\nu_{0}(N)$ or $0$ according as $\chi$ is trivial or
not. Now, $\chi$ is clearly trivial if and only if
$2{}^{t}C_{1}P_{2}C_{4}\equiv O(\rm{mod} \; 1)$ and
${}^{t}C_{2}P_{2}C_{4}+{}^{t}C_{4}{}^{t}P_{2}C_{2}+P_{3}[C_{4}]\in\Lambda^{\ast}_{n-r}$. 

\begin{sublemma}\label{c1:lem-1.6.21}
For $P$ as above and $T:=P[C_{N}]=\left(\begin{smallmatrix}
  T^{(r)}_{1} & T_{2}\\ {}^{t}T_{2} & T_{3}
\end{smallmatrix}\right)$, we have
{\fontsize{10}{12}\selectfont
\begin{equation*}
\left.
\begin{split}
& P_{1}\in\Lambda^{\ast}_{r,}2c^{2}_{0}P_{2}\equiv 0(\rm{mod} \; 1),
c^{2}_{0}P_{3}\in\Lambda^{\ast}_{n-r}\\
& 2{}^{t}C_{1}P_{2}C_{4}\equiv 0(\rm{mod} \; 1),\\
&{}^{t}C_{2}P_{2}C_{4}+{}^{t}C_{4}{}^{t}P_{2}C_{2}+P_{3}[C_{4}]\in\Lambda^{\ast}_{n-r}
\end{split}
\right\}
\Longleftrightarrow
\begin{cases}
T\in \Lambda^{\ast}_{n}\\
T_{1}[C^{-1}_{1}]=(T[C^{-1}_{N}])^{\ast}\in\Lambda^{\ast}_{r}
\end{cases}
\end{equation*}}
\end{sublemma}

\begin{proof}
$T=P[C_{N}]$\pageoriginale is equivalent to the conditions
\begin{align*}
T_{1}&=P_{1}[C_{1}], T_{2}={}^{t}C_{1}P_{1}C_{2}+{}^{t}C_{1}P_{2}C_{4},\\
T_{3}&=P_{1}[C_{2}]+{}^{t}C_{4}{}^{t}P_{2}C_{2}+{}^{t}C_{2}P_{2}C_{4}+P_{3}[C_{4}]. 
\end{align*}
From the assumptions on $P$, we see that
$T_{1}=P_{1}[C_{1}]\in\Lambda^{\ast}_{r}$,
$2T_{2}=2{}^{t}C_{1}P_{1}C_{2}+2{}^{t}C_{1}P_{2}C_{4}\equiv 0(\rm{mod} \; 1)$
and $T_{3}\in\Lambda^{\ast}_{n-r}$, proving the implication
$\Longrightarrow$. We uphold next the reverse implication. From
$T\in\Lambda^{\ast}_{n}$ and
$T_{1}[C^{-1}_{1}]$. $(T[C^{-1}_{N}])^{\ast}\in\Lambda^{\ast}_{r}$, we
have
$$
P_{1}=T_{1}[C^{-1}_{1}]\in\Lambda^{\ast}_{r},
{}^{t}C_{2}P_{2}C_{4}+{}^{t}C_{4}{}^{t}P_{2}C_{2}+P_{3}[C_{4}]=T_{3}-P_{1}[C_{2}]\in\Lambda^{\ast}_{n-r}. 
$$
Further
\begin{align*}
&2{}^{t}C_{1}P_{2}C_{4}=2T_{2}-2{}^{t}C_{1}P_{1}C_{2}\equiv 0 (\rm{mod} \; 1),\\
&2c^{2}_{0}P_{2}=2c_{0}{}^{t}C_{1}^{-1}(2{}^{t}C_{1}P_{2}C_{4})c_{0}C^{-1}_{4}\equiv 0(\rm{mod} \; 1),\\
&P_{3}=T_{3}[C^{-1}_{4}]-P_{1}[C_{2}C^{-1}_{4}]-{}^{t}C^{-1}_{4}({}^{t}T_{2}-{}^{t}C_{2}P_{1}C_{1})C^{-1}_{1}C_{2}C^{-1}_{4}\\
&\qquad\qquad-{}^{t}(C^{-1}_{1}C_{2}C^{-1}_{4})(T_{2}-{}^{t}C_{1}P_{1}C_{2})C^{-1}_{4} 
\end{align*}
and so $c^{2}_{0}P_{3}\in \Lambda^{\ast}_{n-r}$, in view of
$c_{0}C^{-1}=c_{0}\left(\begin{smallmatrix} C^{-1}_{1} &
  -C^{-1}_{1}C_{2}C^{-1}_{4}\\ 0 & C^{-1}_{4}
\end{smallmatrix}\right)$ being integral.
\end{proof}

Putting together the results above, we have, for $f$ and $N$ as above,
\begin{align*}
\mathscr{T}(f,N)&=\beta\frac{(\det C_{1})^{k}/(\det
  C_{4})^{k}}{s^{(n-r)(n+r+1)/2}}\sum_{j}\alpha_{j}\\
&\qquad\sum_{0<T\in\Lambda^{\ast}_{n}}b_{j}([T[C^{-1}_{N}])^{\ast})(\det
  T^{\ast})^{(r+1)/2-k}(\det T)^{k-(n+1)/2}\\
&\qquad\times \eta_{s}(\tr(TC^{-1}_{N}D_{N}))\eta_{s}(\tr(TZ))
\end{align*}

\begin{sublemma}\label{c1:lem-1.6.22}
The number of $(D_{3},D_{4})$ such that $F=(C_{4}D_{3}D_{4})$ runs
over a set of representatives of $\approx$ -equivalence classes in
$\mathscr{C}_{n,r}$ for fixed $C_{4}$ with $\det C_{4}\neq 0$ is at
most $\abs(\det C_{4})^{r}\delta^{n-r}_{1}\ldots\delta_{n-r}$ where
$\delta_{1}|\ldots|\delta_{n-r}$ are elementary divisors of $C_{4}$.
\end{sublemma}

\begin{proof}
For fixed $C_{4}$, the number of $\approx$ -inequivalent $F$ is at
most the index of $\{C_{4}H|H=H^{(n-r,r)}$ integral$\}$ in
$\{H|H^{(n-r,r)}$ integral$\}$ multiplied by the index of
$\{C_{4}L|L\in\Lambda_{n-r}\}$ in $\{D_{4}^{(n-r,n-r)}$ integral
$|C^{-1}_{4}D_{4}$ is symmetric$\}$ and hence at most equal to
$\abs(\det C_{4})^{r}\cdot \sigma_{n-r}(C_{4})$ where
$\sigma_{n-r}(C_{4})$ is the index\pageoriginale of $\Lambda_{n-r}$ in
$\{{}^{t}S=S^{(n-r,n-r)}$ with entries in $\mathbb{Q}|C_{4}S$
integral$\}$. Now there exist $U_{1}$, $U_{2}$ in
$GL_{n-r}(\mathbb{Z})$ such that $U_{1}C_{4}U_{2}=\delta$ is a
diagonal matrix with diagonal entries $\delta_{1},\ldots,\delta_{n-r}$
for which $\delta_{1}|\ldots|\delta_{n-r}$. For calculating
$\sigma_{n-r}(C_{4})$, there is no loss of generality in taking
$C_{4}$ to be already equal to $\delta$ and so
$\sigma_{n-r}(C_{4})=\delta^{n-r}_{1}\ldots \delta_{r}$, proving the
lemma.
\end{proof}

We are finally in a position to state 

\setcounter{subtheorem}{22}
\begin{subtheorem}[\cite{key10}, \cite{key20}]\label{c1:thm-1.6.23}
Let $f$ be a cusp form of degree $r$, (even) weight $k\geq n+r+1$ and
stufe $s$, for $1\leq r\leq n-1$. Then for
$T^{(n,n)}=\left(\begin{smallmatrix} T^{\ast(r,r)} & \ast\\ \ast &
  \ast
\end{smallmatrix}\right)>0$ in $\Lambda^{\ast}_{n}$, the Fourier
coefficients $a(T,f;M)$ of the transform $E^{k}_{n,r}(Z,f)|M$ of the
Eisenstein series, for $M$ in $\Gamma_{n}$, we have the estimate
$$
a(T,f;M)=O((\det T)^{k-(n+1)/2}/(\det T^{\ast})^{k-(r+1)/2})
$$
the $O$-constants depending on $f$, $n$, $s$ and $k$ and being uniform
as long as $T$ lies in a fixed Siegel domain.
\end{subtheorem}

\begin{proof}
Now $E^{k}_{n,r}(Z,f)|M:=\sum\limits_{N\in\Delta_{n,r}(s)\backslash
  \Gamma_{n}(s)M}(f||N)(Z)$ and in view of\break Proposition~\ref{c1:prop-1.6.12}, 
contributions to $a(T,f;M)$ arise only from terms
for which rank $(\lambda_{n-r}(C_{N}))=n-r$. By Lemma~\ref{c1:lem-1.6.15}, 
we have 
\begin{align*}
&\Gamma_{n}(s)M=\coprod\limits_{(C_{4}D_{3}D_{4})\in\overset{\approx}
{\mathscr{C}}_{n,r}}\Delta_{n,r}(s)KM\{C_{4},D_{3},D_{4}\}\\
&\qquad\qquad\begin{pmatrix}
E_{n} & S'\\
0 & E_{n}
\end{pmatrix}
\begin{pmatrix}
E_{n} & sS\\
0 & E_{n}
\end{pmatrix}
\begin{pmatrix}
{}^{t}U & 0\\
0 & U^{-1}
\end{pmatrix}\\
&
\begin{pmatrix}
0^{(r,r)} & \ast\\
\ast & \ast
\end{pmatrix}
=S'={}^{t}S'\rm{mod} \; s, {}^{t}S=S=
\begin{pmatrix}
0^{(r,r)} & \ast\\
\ast & \ast
\end{pmatrix}\text{ \ integral}\\
& K\in\Delta_{n,r}(s)\backslash \Delta_{n,r}, {}^{t}U\in
P(n,r,\mathbb{Z})\backslash GL_{n}(\mathbb{Z})
\end{align*}
where\pageoriginale the accent on $\coprod$ indicates that only the
$(C_{4}D_{3}D_{4})$, $K=K(S',U)$ and ${}^{t}U$ relevant for the
decomposition of the left hand side appear. (Indeed
$(KM(C_{4},D_{3},D_{4})\left(\begin{smallmatrix} E_{n} & S'\\ 0 &
  E_{n}
\end{smallmatrix}\right))^{-1}M\left(\begin{smallmatrix} {}^{t}U^{-1}
  & 0\\ 0 & U\end{smallmatrix}\right)$ must be in $\Gamma_{n}(s)$,
  this condition clearly being independent of the matrices
  $sS$). Applying the formula for $\mathscr{T}(f,N)$, stated just
  prior to Lemma \ref{c1:lem-1.6.22}, to $f|K^{\ast}$ instead of $f$,
  $Z+S'$ instead of $Z$ (since $\left(\begin{smallmatrix} E_{n} &
    S'\\ 0 & E_{n}  \end{smallmatrix}\right)$ commutes with
  $\left(\begin{smallmatrix} E_{n} & sS\\ 0 & E_{n}
  \end{smallmatrix}\right)$ and $N=M\{C_{4},D_{3},D_{4}\}$ we get, for
  the Fourier coefficient $a(T,f;M)$ corresponding to $T>0$ in
  $\Lambda^{\ast}_{n}$ an expression of the form
\begin{align*}
& \gamma
\sum_{j}\alpha'_{j}
{\mathop{\sum}'}_{\substack{(C_{4}D_{3}D_{4})\in\overset{\approx}{\mathscr{C}}_{n,r}\\ K,S',{}^{t}U}}
((\det C_1)^k/(\det 
C_{4})^{k})b_{j}((T[{}^{t}U^{-1}C^{-1}_{N}])^{\ast})\\
& \qquad (\det
(T[{}^{t}U^{-1}])^{\ast})^{\frac{r+1}{2}-k}(\det
T)^{k-\frac{n+1}{2}}\times\\
& \hspace{1.3cm} \times
\eta_{s}(\tr(TS'))\eta_{s}(\tr(T[{}^{t}U^{-1}]C^{-1}D))
\end{align*}
with a similar connotation for the account on $\sum$ as for $\coprod'$
earlier and further $\gamma=\beta/s^{(n-r)(n+r+1)/2}$ and bounded
constants $\alpha'_{j}(1\leq j\leq m)$. By Lemma \ref{c1:lem-1.6.22}, we
know that the number of $(D_{3},D_{4})$ such that $(C_{3}D_{3}D_{4})$
runs over $\overset{\approx}{\mathscr{C}}_{n,r}$, for fixed
(non-singular) $C_{4}$ with $\delta_{1},\ldots,\delta_{n-r}$ as
elementary divisors is at most $(\abs\det
C_{4})^{r}\delta^{n-r}_{1}\ldots \delta_{n-r}$. Under the equivalence
$\approx$, $C_{4}$ and $VC_{4}$ for $V$ in $GL_{n-r}(\mathbb{Z})$ have
to be identified and hence, in order to estimate the number of
integral invertible $C_{4}$ with $\delta_{1},\ldots,\delta_{n-r}$ as
elementary divisors, we may assume $C_{4}=\left(\begin{smallmatrix}
  c_{1} & \ldots & \vdots\\ \vdots & \ddots & c_{ij}\\ 0 & \ldots &
  c_{n-r}
\end{smallmatrix}\right)$ in triangular form with
$c_{1},\ldots,c_{n-r}>0$ and $0\leq c_{ij}<c_{i}$ for $j\geq i$. Since
$\delta^{n-r}_{1}\ldots
\delta_{1}=\delta_{1}(\delta_{1}\delta_{2})\ldots(\delta_{1}\ldots\delta_{n-r})\leq
c_{1}(c_{1}c_{2})\ldots(c_{1}\ldots c_{n-r})$ and the number of such
$C_{4}$ for fixed $c_{1},\ldots,c_{n-r}$ is evidently $\leq
c_{2}\ldots c^{n-r-1}_{n-r}$, we may now conclude, in view also of the
estimate for Fourier coefficients of cusp derived\pageoriginale
earlier, the finiteness of the number of $K$, $S'$ and the boundedness
of $\alpha'_{j}$, that
\begin{align*}
a(T,f;M) &= O(\sum_{\substack{(C_{4}D_{3}D_{4})\in
    \overset{\approx}{\mathscr{C}}_{n,r}\\ {}^{t}U\in
    P(n,r;\mathbb{Z})\backslash GL_{n}(\mathbb{Z})}}(\det C_{1}/\det
C_{4})^{k}(\det (T[{}^{t}U^{-1}])^{\ast}/\\
&\qquad\det C^{2}_{1})^{k/2}(\det
(T[{}^{t}U^{-1}])^{\ast})^{\frac{r+1}{2}-k}
\times(\det T)^{k-(n+1)/2}\\
&= O((\sum_{1\leq c_{1},\ldots,c_{n-r}<\infty} (c_{1}\ldots
c_{n-r})^{-k+r}c^{n-r}_{1}\ldots c_{n-r}c_{2}\ldots
c^{n-r-1}_{n-r})\\
&\qquad (\sum_{t_{U\in P(n,r;\mathbb{Z})\backslash
    GL_{n}(\mathbb{Z})}}(\det
(T[{}^{t}U^{-1}])^{\ast})^{\frac{r+1-k}{2}}(\det
T)^{k-\frac{n+1}{2}})\\
&= \zeta(k-n)^{n-r}O((\det T^{\ast})^{(r+1-k)/2}(\det T)^{k-(n+1)/2}) 
\end{align*}
since, for the sum over ${}^{t}U$ which is a Selberg zeta function, we
have the above $O$-estimate involving $\det T^{\ast}$ and $\det T$, as
long as $T$ stays in a fixed Siegel domain (see page 143 and the
Theorem on page 144, \cite{key17}). This completes the proof of Theorem
\ref{c1:thm-1.6.23}. 
\end{proof}

\begin{remarks*}
\begin{enumerate}
\renewcommand{\labelenumi}{\theenumi)}
\item The case of half-integral $k\geq n+r+1$ can also be dealt with
  similarly.

\item Let $f(Z)=\sum\limits_{T}a(T)\eta_{s}(\tr(TZ)\in\{n,k,s\}$ for
  even $k\geq 2n+2$, such that the constant term of the Fourier
  expansions at all the cusps vanish. Then, for $T>0$, and
  $\min(T)\geq \mathscr{X}>0$, we have $a(T)=O((\det
  T)^{k-(n+1)/2}/(\min (T))^{k/2-1})$. (This is just
  Theorem \ref{c1:alphthmD} stated on page 7 and it follows from
  the reformulation of Theorem \ref{c1:thm-1.6.9} given immediately
  thereafter and Theorem \ref{c1:thm-1.6.23}, on noting that $(\det
  T^{\ast})^{(r+1-k)/2}\ll ((\min(T^{\ast}))^{-r(k-r-1)/2}\leq (\min
  (T))^{-r(k-r-1)/2}<(\min T))^{1-k/2}$, since $r(k-r-1)/2\geq k/2-1$
  for $1\leq r\leq n\leq (k/2)-1$.

\item Applying the theorem above to the theta series
  $$\vartheta_{n}(Z,S):=\sum_{G^{(m,n)}}\exp(2\pi i\tr(S[G]Z))$$
  associated with an integral $(m,m)$ positive-definite matrix $S$, we
  get, for the number $r(S,T)$ of integral representations of
  $T=T^{(n,n)}>0$\pageoriginale by $S$, an `asymptotic formula' for
  $m\geq 4n+4$: 
\begin{gather*}
r(S,T)=2^{n(m-n+1)/2}\prod^{n-1}_{j=0}\frac{\pi(m-j)/2}{\Gamma((m-j)/2)}(\det
T)^{\frac{m-n-1}{2}}\prod_{p}\alpha_{p}(S,T)+\\
+O((\det T)^{(m-n-1)/2}/(\min T)^{\frac{m}{4}-1})
\end{gather*}
as $\min(T)$ tends to infinity.
\end{enumerate}
\end{remarks*}

\section{Primitive Representations}\label{c1:sec-1.7}\pageoriginale

We fix a natural number $n$. For $G_{P}=GL_{n}(Q_{P})\cap
\mathscr{M}_{n}(\mathbb{Z}_{P})$ and $U_{P}=GL_{n}(\mathbb{Z}_{P})$,
$L(U_{P},G_{P})$ stands for a vector space over $\mathbb{Q}$ spanned
by left cosets $U_{P}g$, $g\in G_{P}$. $U_{P}$ acts canonically from
the right on $L(U_{P},G_{P})$ and we denote by $H(U_{P},G_{P})$ the
set of all invariant elements of $L(U_{P},\break G_{P})$ under this
action. The abbreviation $U_{P}gU_{P}(g\in G_{P})$ denotes an element
$\sum U_{P}g_{i}$ of $H(U_{P},G_{P})$ where $U_{P}gU_{P}=\coprod
U_{P}g_{i}$ is a left coset decomposition. It is easy to see that the
set $\{U_{P}gU_{P}|g\in G_{P}\}$ is a basis of $H(U_{P},G_{P})$. If we
introduce a product in $H(U_{P},G_{P})$ by $(\sum
a_{i}U_{p}g_{i}))\cdot (\sum b_{j}U_{p}h_{j})$:
$$
=\sum a_{i}b_{j}U_{p}g_{i}h_{j}(a_{i},b_{j}\in\mathbb{Q},
g_{i},h_{j}\in G_{p}), 
$$
it is well defined. Let
\begin{gather*}
\pi_{p}(i):=U_{p}[\underbrace{p,\ldots,p}_{i},1,\ldots 1]U_{p} \;
(i=0,1,\ldots n),\\
T_{p}(k):=\sum_{\substack{r_{1}+\cdots+r_{n}=k\\ r_{1}\geq\ldots\geq
    r_{n}\geq 0}}U_{p}[p^{r_{1}},\ldots,p^{r_{n}}]U_{p}\text{ \ if \ }
k\geq 0, \text{ \ and \ }
\end{gather*}
$T_{p}(k):=0$ if $k<0$. Then the following is a fundamental result of
Tamagawa [ \ ]:

\begin{sublemma}\label{c1:lem-1.7.1}
$H(U_{p},G_{p})$ is a commutative ring and
$$
\sum^{n}_{h=0}(-1)^{h}p^{h(h-1)/2}T_{p}(k-h)\pi_{p}(h)=0\text{ \ for
  \ } k\geq 1.
$$
\end{sublemma}

Let $V$ be a vector space over $\mathbb{Q}$ with $\dim V=n$. By a {\em
  lattice} in $V$ we mean a finitely generated $\mathbb{Z}$-submodule
$L$ of $V$ with rank $L=n$. Let $\tilde{V}$ be the vector space
over $\mathbb{Q}$ whose basis is the set of all lattices on $V$. Then
any element of $\tilde{V}$ is a formal sum of lattices on $V$ with
rational coefficients. If\pageoriginale we consider a lattice $L$ on
$V$ as an element of $\tilde{V}$, then we denote it by $[L]$. Now
$\tilde{V}$ becomes a $H(U_{P},G_{P})$ module as follows: Let $L$
be a lattice in $V$ and $g\in G_{P}$. For a fixed basis $\{u_{i}\}$ of
$\mathbb{Z}_{p}\otimes L$, let $L'_{p}$ be lattice in
$\mathbb{Q}_{p}\otimes V$ spanned by
$(u_{1},\ldots,u_{n})g^{-1}$. Then we define $gL=V\cap
(\bigcap\limits_{q\neq p}\mathbb{Z}_{q}\otimes L\cap L'_{p})$. For a
left coset decomposition $U_{p}gU_{p}=\coprod U_{p}g_{i}$,
$\sum_{i}[g_{i}L]$ is independent of the choice of the basis
$\{u_{i}\}$ and determined uniquely by $U_{p}gU_{p}$, and $L$. Hence
we can set $U_{p}gU_{p}[L]=\sum\limits_{i}[g_{i}L]$ where
$U_{p}gU_{p}=\coprod U_{p}g_{i}$.

If $\{p^{e_{1}},\ldots,p^{e_{n}}\}$ are elementary divisors of $g\in
G_{p}$, then $U_{p}gU_{p}[L]$ is a sum in $\tilde{V}$ of lattices
$M$ in $V$ such that $M/L\simeq
\mathbb{Z}/(p^{e_{1}})\oplus\ldots\oplus\mathbb{Z}/(p^{e_{n}})$. If
$U_{p}gU_{p}=\coprod U_{p}G_{i}$, $U_{p}hU_{p}=\coprod U_{p}h_{j}$,
then $U_{p}hU_{p}(U_{p}gU_{p}[L])=U_{p}hU_{p}(\sum
[g_{i}L])=\sum_{ij}[h_{j}g_{i}L]=((U_{p}hU_{p})(U_{p}gU_{p}))[L]$. 

Thus $V$ becomes a $H(U_{p},G_{p})$-module.

\setcounter{subtheorem}{1}
\begin{subtheorem}\label{c1:thm-1.7.2}
Let $V$ be a regular quadratic space over $\mathbb{Q}$ with $\dim
V=n$, and $B(,)$ the bilinear form on $V$. Let $P$ be a linear mapping
from $\tilde{V}$ to $\mathbb{C}$ such that $P([L])=0$ unless
$d(L):=\det B(x_{i},x_{j})\in\mathbb{Z}$ where $\{x_{i}\}$ is a basis
of $L$.

Putting
\begin{align*}
R(L): &= \sum_{M\supset L}P(M), \text{  \  we have}\\
P(L) &=\sum_{M\supset L}\pi (M,L)R(M)\text{ \ where}
\end{align*}
$\pi(M,L)$\pageoriginale is defined as follows: Suppose
$\mathbb{Z}_{p}M/\mathbb{Z}_{p}L=\underbrace{\mathbb{Z}/(p)\oplus\ldots\oplus\mathbb{Z}/(p)}_{h_{p}}$
for every prime $p$; then
$\pi(M,L)=\prod\limits_{p}(-1)^{h_{p}}p^{h_{p}(h_{p}-1)/2}$ and
otherwise, $\pi(M,L)=0$. 
\end{subtheorem}

\begin{proof}
If $M\supset L$, then clearly $d(L)=[M:L]^{2}d(M)$ and so $R(L)$ is a
finite sum of nonzero $P(M)$.

By Lemma \ref{c1:lem-1.7.1}, we have
\begin{align*}
P([L]) &=
P(\sum^{\infty}_{k=0}\sum^{n}_{h=0}(-1)^{h}p^{h(h-1)/2}T_{p}(k-h)\pi_{p}(h)[L])\\
&=
\sum^{n}_{h=0}(-1)^{h}p^{h(h-1)/2}P(\sum^{\infty}_{k=0}T_{p}(k-h)\pi_{p}(h)[L])\\
&= \sum^{n}_{k=0}(-1)^{h}p^{h(h-1)/2}P(\sum^{\infty}_{k=0}T_{p}(k)\pi_{p}(h)[L])\\
&= \sum_{0\leq h_{1}\ldots,h_{t}\leq
  n}\sum^{t}_{i=1}(-1)^{h_{i}}p^{h_{i}(h_{i}-1)/2}P(\sum_{k_{1},\ldots,k_{t}\geq
  0}T_{p_{1}}(k_{1})\ldots\\
&\qquad \ldots T_{p_{t}}(k_{t})\pi_{p_{1}}(h_{1})\ldots
\pi_{p_{t}}(h_{t})[L])\\
&=\sum_{0\leq h_{1},\ldots,h_{t}\leq
  n}\prod^{t}_{i=1}(-1)^{h_{i}}p^{h_{i}(h_{i}-1)/2}R(\pi_{p_{1}}(h_{1})\ldots
\pi_{p_{t}}(h_{t})[L]), 
\end{align*}
where $p_{1},\ldots,p_{t}$ are prime divisors of $d(L)$, since
$R(L)=p(\prod\limits_{p}\sum\limits_{k}T_{p}(k)\break[L])$. Since
$\pi_{p_{1}}(h_{1})\ldots\pi_{p_{t}}(h_{t})[L]$ is a sum in\pageoriginale
$\tilde{V}$ of lattices $M$ such that
$\mathbb{Z}_{p_{i}}M/\mathbb{Z}_{p_{i}}L=\underbrace{\mathbb{Z}/(p_{i})\oplus\ldots\oplus
  \mathbb{Z}(P_{i})}_{h_{i}}$, the proof is complete.
\end{proof}

In the following, we fix a positive definite quadratic space $W$ over
$\mathbb{Q}\dim W=m\geq n$ and a lattice $S$ on $W$ such that
$B(x,y)\in\mathbb{Z}$, $B(x,x)\in 2\mathbb{Z}$ for every $x$, $y\in S$
where $B$ is a bilinear form on $W$. For a lattice $L$ on a positive
definite quadratic space on $V$ with $\dim V=n$, we denote by $R(L)$
and $P(L)$ the number of isometries from $L$ to $S$ and the number
isometries $\sigma$ from $L$ to $S$ such that $S/\sigma(L)$ is
torsion-free. An isometry $\sigma$ from $L$ to $S$ induces canonically
an isometry from $V$ to $W$ and we denote the extension by the same
letter $\sigma$. Considering $\sigma\mapsto a$ pair $(\sigma|M,M)$
where $M=\sigma^{-1}(\sigma(V)\cap S)$, we obtain
$R(L)=\sum\limits_{M\supset L}P(M)$. Hence we have
$P(L)=\sum\limits_{M\supset L}\pi(M,L)R(M)$.

Let $\{S_{i}\}$ be a complete system of representatives of the
(finitely many) classes in the genus of $S$ and $E(S_{i})$ the order
of the group of isometries of $S_{i}$. Denote by $SW(L)$ (= Siegel's
weighted sum)
$$
(\sum_{i}E(S_{i})^{-1}\sum_{i}\frac{R(L;S_{i})}{E(S_{i})}
$$
where $R(L;S_{i})$ is the number of isometries from $L$ to $S_{i}$,
and put $A(L)=R(L)-SW(L)$. If $T$ is an $(n,n)$ matrix corresponding
to $L$, then $A(L)$ is the Fourier coefficient of $e(\tr(TZ))$ for a
Siegel modular form of degree $n$, weight $m/2$ and some level whose
constant term vanishes at every cusp. Put
$SW_{P}(L)=\sum\limits_{M\supset L}\pi(M,L)SW(M)$ and
$A_{p}(L)=\sum_{M\supset L}\pi(M,L)\break A(M)$; then
$P(L)=SW_{p}(L)+A_{p}(L)$. It is known that
{\fontsize{10}{12}\selectfont
$$
SW_{p}(L)=(\text{some constant depending on } n,S)\times
d(L)^{(m-n-1)/2}\prod_{p}d_{p}(L,S), 
$$}
where\pageoriginale $d_{p}(L,S)$ is a so-called primitive density and
for a fixed prime $p$ the number of possible values of $d_{p}(L,S)$ is
finite when $L$ runs over regular lattices with rank $L=n$. Moreover
if $m\geq 2n+3$ and $SW_{p}(L)\neq 0$, then $SW_{p}(L)\gg
d(L)^{(m-n-1)/2}$, and if $m=2n+2$, $SW_{p}(L)\neq 0$, then
$SW_{p}(L)\gg \ub{n}(L)^{-\varepsilon}d(L)^{(m-n-1)/2}$ for any
$\varepsilon>0$, where $\ub{n}(L)$ is a natural number defined by
$\ub{n}(L)\mathbb{Z}=\mathbb{Z}\{Q(x)|x\in L\}$.

\begin{subtheorem}\label{c1:thm-1.7.3}
Suppose that, for every Siegel modular form $f(z)=\sum a(T)e(\tr(Tz))$
of degree $n$, weight $m/2$ and some level, whose constant term
vanishes at each cusp, the estimate $a(T)=O(\min(T)^{-\varepsilon}\break(\det
T)^{(m-n-1)/2})$ holds for $\min T\geq \mathscr{X}$ (= an absolute
constant independent of $f$). If $m\geq 2n+2$ and $\varepsilon$ is a
sufficiently small positive number, then
$A_{p}(L)=O((\min(L))^{-\varepsilon}(d(L)^{(m-n-1)/2})$. 
\end{subtheorem}

\begin{proof}
Let $a\geq \mathscr{X}(a\in\mathbb{Z})$, and without loss of
generality we may suppose $B(x,y)\equiv 0\rm{mod} \; a$ for any $x$, $y\in
S$. If, then $\min (L)<\mathscr{X}$, $SW(L)=R(L)=A(L)=0$. Hence we may
suppose that the estimate for $a(T)$ holds without the restriction
``$\min (T)\geq \mathscr{X}$''. For a positive definite matrix $T$ and
integral non-singular matrix $G$, $\min (T[G^{-1})=\min (\det
  G^{-2}.T[\det G\cdot G^{-1}]>\det G^{-2}\min(T)$. Hence, for
  $M\supset L$, we have $\min(M)\geq [M:L]^{-2}\min(L)$. From this, we
  have
\begin{align*}
A_{p}(L) &= \sum_{M\supset L}\pi(M,L)A(M)\\
&= \sum_{\substack{M\supset L\\ d(M)\in\mathbb{Z}}}|\pi
(M,L)|O((\min(M))^{-\varepsilon}(d(M))^{(m-n-1)/2}\\
&= \sum_{\substack{M\supset
    L\\ d(M)\in\mathbb{Z}}}|\pi(M,L)|O([M:L]^{2\varepsilon}(\min
(L))^{-\varepsilon}\times\\
&\qquad \times ([M:L]^{-2}d(L))^{(m-n-1)/2})\\
&\ll (\min(L))^{-\varepsilon}(d(L))^{(m-n-1)/2}\sum_{\substack{M\supset
    L\\ d(M)\in\mathbb{Z}}}|\pi(M,L)|[M:L]^{-(m-n-1)+2\varepsilon},
\end{align*}
where\pageoriginale the last sum is bounded by
\begin{align*}
&\quad \prod_{p|d(L)}(1+\sum_{1\leq h\leq
    n}p^{h(h-1)/2-h(m-n-1)+2h\varepsilon+h(n-h)+\alpha)})\\
&\qquad \text{for any } \alpha>0 \quad \text{ by Lemma \ref{c1:lem-1.4.7}.}\\
&\leq \prod_{P}(1+\sum_{1\leq h\leq
    n}p^{h(-h/2-3/2+2\varepsilon)+\alpha})(m\geq 2n+2)\\
&\leq \prod_{P}(1+np^{-1.5})\ll 1.
\end{align*}

If $n=1$ and $m\geq 4$, then the supposition in Theorem
\ref{c1:thm-1.7.3} is valid and leads us to an asymptotic formula for
$A_{p}(L)$; we can thus conclude that if a natural number $t$ is
primitively represented by $S$ at every prime, then $t$ is primitively
represented globally by $S$ if $t$ is sufficiently large. A similar
assertion is also true for $n=2$, $m\geq 7$. Let $n=2$, $m=6$. The
error term is $O((\min
(L))^{-\varepsilon}\log\dfrac{\sqrt{d(L)}}{\min(L)}(d(L))^{3/2})$ by
Theorem \ref{c1:thm-1.5.13} under the Assumption $(\ast)$. Since
$\dfrac{\sqrt{d(M)}}{\min(M)}\leq \dfrac{\sqrt{d(L)}}{\min(L)}[M:L]$
for $M\supset L$, $\log\dfrac{\sqrt{d(M)}}{\min(M)}\leq
\log\dfrac{\sqrt{d(L)}}{\min(L)}+O([M:L]^{\alpha})$ for any
$\alpha>0$. Similarly, we get
$A_{p}(L)=O((\min(L))^{-\varepsilon}\log\dfrac{\sqrt{d(L)}}{\min(L)}(d(L))^{3/2})$. 

\end{proof}
