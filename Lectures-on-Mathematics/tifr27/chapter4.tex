\chapter{Classical Linear Groups over $p$-adic Fields}\label{part2:chap2}

\section{General Definitions}\label{part2:chap2:sec1}

We\pageoriginale shall study the following types of classical linear groups over
field $P$ or over a division algebra. 
\begin{enumerate}[(I)]
\item 
  \begin{enumerate}[(a)]
  \item \underbar{$G L_n(P)$}- The group of all non-singular n x n
    matrices with coefficients from $P$ is called the general linear
    group 
  \item \underbar{$\Pr G L_n(P)$} Let $C L_n (P)$ be the centre of the
    group $G L_N (P)$. The group pr $G L_n (P)= G L_n (P)/C L_n (P)$
    is called the projective linear group. 
  \item $S L_n (P)$-The subgroup of $G L_n (P)$ consisting of all the
    matrices of determinant 1 is called the special linear group or
    the unimodular group. It can be proved that $\Pr S L_n (P)=S L_n
    (P)/ C(S L_n(P))$ is a simple group 
  \end{enumerate}
\item -Let $E= P^n$ and $\varphi$ a non-degenerate bilinear form over $E$ 
  \begin{enumerate}[(a)]
  \item \underbar{$S p_n(P)$}-If $\varphi$ is an alternating form,then
    the the set of all matrices in $G L_n(P)$ which leave this
    bilinear form invariant is a group called the linear symplectic
    group. We shall denote the by $Sp_n(P)$. This group is independent
    of the choice of the alternating bilinear form because any two
    such bilinear forms are equivalent. 
  \item If $\varphi$ is a symmetric non-degenerate bilinear form, then
    the set of elements in $G L_n(P)$ leaving $\varphi$ invariant is
    group called the linear orthogonal group. 
  \end{enumerate}
\item Let $\widetilde{P}$ be a separable quadratic extension of
  $P$. Let $\xi \to \bar{\xi}$ be the unique nontrivial automorphism of
  $\widetilde{P}$. If $\varphi$ is a non-degenerate
  Hermitian\pageoriginale bilinear 
  form over $E$ \iec  $\varphi(y,x)= \varphi \overline{(x,y)}$, then
  the set $U_n(\varphi,P)$ of elements of $G L_n(P)$ leaving $\varphi$
  invariant is a group called the unitary group. 
\item Let $\widetilde{P}$ be a division algebra of finite rank
  over $P$, such that $P$ is the centre of $\widetilde{P}$. We define
  $G L_n(P)$ as in I (a). The group $S L_n(P)$ can be defined as the
  kernel of the map $\sigma$(determinant of Dieudonne) from $G L_n(P)$
  to $\widetilde{P}^*/C$ where $C$ is the commutator subgroup of
  $P^*$. 
\item Let $\widetilde{P}$ be the algebra of quaternions over $P$. In
  this case there exists an involution in $\widetilde{P}$ \iec  an
  anti automorphism of $\widetilde{P}$ of order 2. So we can define as
  in (3) the group $U_n(\varphi, P)$ which leaves invariant the
  bilinear form $\varphi$ over $\widetilde{P}^n$. As in (1) one can
  define $S 0_n(\varphi,  P)$ and $S V_n(\varphi,  P)$ and prove that
  their projective groups are in general simple. 
\end{enumerate}

Suppose that $P$ is a locally compact $p$-adic field. All the groups of
type (1), (2) and (3) are locally compact, because on
$M_n(P)$ (the set of all $n \chi n$ matrices with coefficients from $P$)
we have the topology of $P^{n^2}$ and $G L_n (P)$ is an open subset of
$M_n(P)$ and the groups $S L_n(P)$ etc.are closed subgroups of $G L_n
(P)$. 

Let us assume that the rank of $\widetilde{P}$ over $P$ in ($4$) is
$r$. Then $M_n(\widetilde{P})$ may be imbedded in $M_{nr}(P)$, as
$\widetilde{P}^n$ can be considered as a space of dimension nr over
$P$, since a matrix is inversible in $M_n(\widetilde{P})$ if and only
if it is invertible in $M_{nr}(P)$, we have 
$$
G L_n(\widetilde{P}) = G L_{nr}(P) \cap M_n(\widetilde{P})
$$

But $G L_{nr}(P)$ is an open subset of $M_{nr}(P)$, therefore $G
L_n(\widetilde{P})$ is an open subset\pageoriginale of $M_n(\widetilde{P})$. Since
$M_n(\widetilde{P})$ is locally compact, because it has the same
topology as the $\widetilde{P}^{n^2},G L_n(\widetilde{P})$ is locally
compact. $U_n(\varphi, \widetilde{P})$ is locally compact,
because it is a closed subgroup of $G L_n(\widetilde{P})$. 

\section {Study of $G L_n(\widetilde{P})$}\label{part2:chap2:sec2}

By $\widetilde{P}$ we shall mean a division algebra of finite rank
over $P$, which is a locally compact valuated field, contained in the
centre of $\widetilde{P}$. Let $\tilde{\mathscr{O}}$ denote the ring of
integers of $\widetilde{P}$ 

As we have already seen that $\tilde{\mathscr{O}}$ is a compact subset of
$\widetilde{P}$, therefore $M_n(\tilde{\mathscr{O}})$ which is homeomorphic to
$\mathscr{O}^{n2}$ is compact in $ M_n\tilde{(\widetilde{P})}$. Let $G
L_n(\tilde{\mathscr{O}})$ be the set of elements
$M_n(\tilde{\mathscr{O}})$ which are 
invertible in $M_n(\tilde{\mathscr{O}})$. Obviously$G L_n(\widetilde{P})$
contains $G L_n(\tilde{\mathscr{O}})$. Therefore  
$$
G L_n(\tilde{\mathscr{O}})=G L_n(\widetilde{P})\cap
M_n(\tilde{\mathscr{O}}) \cap [G 
  L_n(\widetilde{P}) \cap M_n(\tilde{\mathscr{O}})]^{-1} 
$$
Since $\tilde{\mathscr{O}}$ is open in
$\widetilde{P}, M_n(\tilde{\mathscr{O}})$ is open in
$M_N(\tilde{P})$. Therefore $G L_n(\tilde{\mathscr{O}})$ is 
open in $M_n(\tilde{\mathscr{O}})$. Similarly $G
L_n(\tilde{\mathscr{O}})$ is open in 
$G L_n(\widetilde{P})$. Moreover $G L_n(\tilde{\mathscr{O}})$ is closed in
$M_n(\tilde{\mathscr{O}})$. For, let $(X_p)$ be a sequence of elements in $G
L_n(\tilde{\mathscr{O}})$ such that $X_p$ tends to $X \in
M_n(\tilde{\mathscr{O}})$ as 
$p$ tends to infinity. Because $M_n(\tilde{\mathscr{O}})$ is compact, we can
assume that $X^{-1}_p$ has a limit $Z$ in $M_n(\tilde{\mathscr{O}})$. But then
$ZX = XZ = I$, therefore $X$ belongs to $G L_n(\tilde{\mathscr{O}})$. Hence $G
L_n(\tilde{\mathscr{O}})$ is compact. 

We define in the following some subgroups of $G L_n(\widetilde{P})$,
which will be of use later on. 
\begin{enumerate}[(i)]
\item
  \begin{equation*}
    \Gamma = \left \{ \gamma = 
    \begin{pmatrix}
      a_1 & & *\\
      & \ddots &\\
      0 & & a_n
    \end{pmatrix} 
    | \gamma \in G L_n (\widetilde{P})
    \right \}
  \end{equation*}
  where(*) indicates that there may be some non-zero entries.
\item 
  \begin{equation*}
    T = \left \{ t = 
    \begin{pmatrix}
      \widetilde{\pi}^{\alpha}& & *\\
      & \ddots &\\
      0 & & \widetilde{\pi}^{\alpha}& & \\
    \end{pmatrix} 
    | t \in G L_n (\widetilde{P}), \alpha_i \in Z
    \right \}
  \end{equation*}\pageoriginale
 $\widetilde{\pi}$ being a uniformising parameter in $\tilde{P}$
\item  
  \begin{equation*}
    N = \left \{ \underbar{n} = 
    \begin{pmatrix}
      1& & *\\
      & \ddots &\\
      1 & & 1& & \\
    \end{pmatrix} 
    \right \}
  \end{equation*}
\item
  \begin{equation*}
    D = \left \{ \underbar{d} = 
    \begin{pmatrix}
      a_{11}& & 0\\
      & \ddots &\\
      0 & & a_{1 n}& & \\
    \end{pmatrix} 
    \Bigg | a_{ij} \in \widetilde{P}, a_{i j} \neq 0
    \right \}
  \end{equation*}
\item
  \begin{equation*}
    \Delta= \left \{  
    \begin{pmatrix}
      \widetilde{\pi}^{\alpha_1}& & 0\\
      & \ddots &\\
      0 & & \widetilde{\pi}^{\alpha_n}& & \\
    \end{pmatrix} 
    \Bigg | \alpha_i \in Z
    \right \}
  \end{equation*}
\end{enumerate}

We see immediately that $T = \Delta N$ and  $\Gamma=DN$. Moreover $T$
is a solvable group$,\Gamma$ is solvable if $\widetilde{P}$ is
commutative and $T$ (respectively $\Gamma$) is a semi direct product of
$\Delta$ and $N$ (respectively $D$ and N).  

\setcounter{proposition}{0}
\begin{proposition}\label{part2:chap2:sec2:prop1}
  $G L_n(\tilde{P}) = G =TK, \text{ where } K=G L_n(\tilde{\mathscr{O}})$. 
\end{proposition}

\begin{proof}
  When $n=1$, the proposition is trivially true. Suppose that it is
  true for all $G L_s(\widetilde{P})$ for $s \leq n-1$. We shall prove
  it for $G L_n(\widetilde{P})$.    
  Let $g = (g_{ij})$ be an element of $G$. We can find integers
  $(k_{j1})_{1 \leq j \leq n}$ such that  
\begin{align*}
  \sum ^{n}_{j=1} g_{ij} k_{j1}& = 0 ~\text{for}~ 2 \leq i \leq n \\
  & = a_{11} \neq 0 \text{ for }i =1.
\end{align*}
By\pageoriginale multiplying on the right with a suitable element of
$\widetilde{P}$ 
we can take atleast one of $k_{j1}$ to be 1. Let $k = (\gamma_{ij})$
be a matrix, where $\gamma_{i1}=k_{i1} \text{ for } i = 1,2,\ldots,n
\text{ with } k_{ji}=1, \gamma_{jr}=0 \text{ for } r= 2, \ldots,n$ and
the other $\gamma_{ij}$ are so determined that $k$ belongs to $K$. 

So we get 
\begin{equation*}g_k=
  \begin{pmatrix}
    a_{11}&*\\
    0&*
  \end{pmatrix}
  =
  \begin{pmatrix}
    \widetilde{\pi}&0\\
    0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & *\\
    0 & g'
  \end{pmatrix}
  \begin{pmatrix}
    y & 0 \\
    0 & 1
  \end{pmatrix}
\end{equation*} 
where $g'$ is $n- 1~ \times ~n -1$ matrix and $a_{11}=
\tilde{\pi}^{\alpha}y, y \in \mathscr{O}^*$. 
\end{proof}

But by induction hypothesis $g'= t' k'$ where $t'$ belongs to $T'$ and
$k' \in K'$ the subgroups $T'$ and $K'$ defined in $G
L_{n-1}(\widetilde{P})$ in the same way as $T$ and $K$ in $G$ Thus we
get 
\begin{equation*}
  \begin{pmatrix}
    1&*\\
    0&g'
  \end{pmatrix}
  =
  \begin{pmatrix}
    1&*k'^{-1}\\
    0 & t'
  \end{pmatrix}
  \begin{pmatrix}
    1 & 0\\
    0 & k'
  \end{pmatrix}
\end{equation*}
This implies that
\begin{align*}
  gk & =
  \begin{pmatrix}
    \widetilde{\pi}^{\alpha}&0\\
    0&1
  \end{pmatrix}
  \begin{pmatrix}
    1&*k'^{-1}\\
    0 & t'
  \end{pmatrix}
  \begin{pmatrix}
    1 & o\\
    0 & k'
  \end{pmatrix}
  \begin{pmatrix}
    y&0\\
    0&1
  \end{pmatrix}\\
  & =t_1 k_1,t_1 \in T ~\text{and}~ k_1 \in K. 
\end{align*}
Hence our result follows:

We shall now prove an analogue of Elementary divisors theorem. Let $A$
be a ring with unity (but without any other condition). Let us
consider the following assertions(where module signifies left module): 

(I a) any finitely generated module is isomorphic to a direct sum
$\overset{i=r}{\underset{i=1}{\oplus}} A/ \underbar{a}_i$,\pageoriginale where
$\underbar{a}_i$ are left ideals with $A \neq {a}_1 \supset
\cdots \supset \underbar{a}_r$ 
\begin{enumerate}
\item[(I b)] Such a decomposition, if it exists, is unique.

\item[(II a)] if $M$ is a free module of finite type and $N$ a finitely
  generated submodule of $M$, there exists a basis $e_1, \ldots,e_r$
  and r elements $\alpha_1, \ldots,\alpha_r$ of $A$ such that
  $\alpha_{i+1} \subset A \alpha_i$ and such that $N$ is the direct sum
  of submodules $A \alpha_i e_i$. 

\item[(II b)] if such elements $e_i \text{ and } \alpha_i$ exist, the ideal
  $A\alpha_i$ are independent of the choice of the $e_i$  and
  $\alpha_i$ satisfying (II a).   

\item[(III a)] if $g$ is a $m \times n$ matrix with coefficients in A,
  there exists 
  two $m \times m$ and $m x n$ invertible matrices $p$ and $q$ such that
  $d=pgq$ is a $m \times n$ ``diagonal" matrix(\iec  $d_{ij}= \text{ for } i
  \neq j$) and $\alpha_i = d_{ii}\in A \alpha_{i+1}$. 

\item[(III b)] if such matrices $p$ and $q$  exist, the ideals $A \alpha_i$ are
  independent of the choice of $p$ and $q$ (satisfying(III b)). 
\end{enumerate}
It is obvious that (III a) implies (II a): consider a basis $x_1,
\ldots, x_n$ of $M$ and a system of generators $y_1, \ldots,y_m$ of
$N$ and define the matrix g by $y_j = \sum g_{ij} x_i$. Then $e_i =
\sum (q^{-1})_{ik}x_k$ is basis of $M$ and the $\alpha_i e_i = \sum
p_{ik}y_k$ generate $N$. if a is left Noetherian then (II a) implies
(I a), for any finitely generated module is a quotient M/N, with $M$
free of finite type and $N$ finitely generated. Conversely,it is
obvious that (I a) implies (II b) and (II b) implies (III b). 

It is well known that all these six assertions are true if A is a
commutative principal ideal ring (without zero divisors) (see for
instance Bourbaki, Alg., ch VII, \S\ 4). We shall now prove the following
extension: 

\begin{theorem*}
  Let\pageoriginale $A$ be a ring with unity (but $A$ may be non-commutative and
    may have zero divisors),which satisfies the following conditions: 
  \begin{enumerate}[\rm 1)]
  \item any left or tight ideal is two sided (equivalently, $Ax
    =xA$ for any $x \in A$ 
  \item the set of the principal ideals is totally ordered by
    inclusion (hence any finitely generated ideal is principal). 
  \end{enumerate}
\end{theorem*}

\textit{Then, the assertions (III a) and (I a) are true (hence
  also (II a), and (III b). If moreover $A$ dis Noetherian (that
  is if any ideal is principal), then (I a) is also true} 

\textbf{Proof of (III a):}
the result is obviously true form $n=m=1$. Assume it is proved for
$(m-1)\times (n-1)$ matrices. Let us consider the ideals $A$ $g_{ij}:$ by (2)
they are all contained in one of them, and we can assume without loss
of generality, that $g_{ji}\in Ag_{11}$ for any indices $i$, $j$. Let
$g_{i1}=c_i g_{11}$ for $2 \leq i \leq m$. By multiplying $g$ on the
left by a $m \times m$ matrix $k$ where 
\begin{equation*}
k=
  \begin{pmatrix}
    i & 0 \cdots 0\\
    -c_2 & 1~ 0 \cdots 0\\
    & \cdots \\
    -c_m & o \cdots 1
  \end{pmatrix}
\end{equation*}
we get a matrix kg with $(kg)_{11}=g_{11}$  and  $(kg)_{i1}=0$  for
$i \geq 2$. Moreover, the matrix $k$ is invertible. Similarly, using
the fact that $g_{ij}\in g_{11} A$. We find a $n \times n$ inversible
matrix $h$ such that  
\begin{equation*} 
  kgh=
  \begin{pmatrix}
    g_{11} & 0 &\cdots 0\\
    0 & & \\
    & g'& \\
    0 & &
  \end{pmatrix}
\end{equation*}

Now,\pageoriginale we have just to apply the induction hypothesis to $g'$ (remember
that all the coefficients of $g$, hence of $g'$ belong to $A
g_{11})$. 
\begin{proof}
  (I b): more generally, we shall prove that assumption 1) alone
  implies (I b). 
\end{proof}

Let $M= \sum\limits^{n}_{i=1} A/ \ub{a_i} =\sum \limits^m_{j=1}
A/ \ub{b_i}$, with $\ub{a_1} \supset \ub{a_2}
\supset \cdots \supset \ub{a_n}$  and $\ub{b_1} \supset
\ub{b_2} \supset \cdots \supset \ub{b_m}, \ub{a_i} \neq A$ 
and $\ub{b_i} \neq A$  for any $i$. Then $m=n$ and
$\ub{a_i}=\ub{b_i} ~\text{ for }~ i = 1, 2, \ldots, n$.
 
\begin{proof}
  Let $x'_i$(respectively $y'_j$) be the canonical generator of $ A /
  \ub{a_i}$ (respectively $A/ \ub{b_j}$) and
  $x_i$ (respectively $\ub{y_j}$) the canonical image of
  $x'_i$ (respectively $y'_j$) in $M$. Then $y_j = \sum \limits^n_{i=1}
  a_{ij}x_i$, where $a_{ij} \in A$ and is determined completely modulo
  $\ub{a_i}$ and therefore modulo
  $\ub{a_i}$. Similarly $x_i=\sum\limits^m_{k=1} b_{ki}y_k$,
  where $b_{ki} \in A$ and is completely determined modulo $b_1$. Let $m$ be
  a maximal left ideal containing $b_1$. We see immediately that $m$ is
  a two sided ideal and $A/ \underbar{m}$  is a division
  algebra. Since $y_j = \sum \limits^n_{i=1} a_{ij} = \sum
  \limits^n_{i=1} b_{ki}y_k$, we have  
  $$
  \sum^n_{i=1} a_{ij} \equiv \delta_{kj} \pmod{\ub{m}} 
  $$

  But this is possible only when $n \geq m$, because if $V^m \text{ and
  } V^n$ are two vector spaces over a division ring of dimension $m$ and $n$
  respectively such that $\varphi \text{ and }\psi$ are two linear
  transformations from $V^m \text{ to } V^n$ and $V^n \text{ to } V^m$
  respectively. then $\varphi \psi = I$ implies that $\psi$ is an
  isomorphism of $V^m$ onto a subspace of $V^n$. In the same way we get
  that $m \geq n$. Hence $m=n$. 
\end{proof}

If possible let us suppose that $a_{\ub{i}} \neq b_{\ub{i}}$ for some $i$.
Let\pageoriginale us suppose that there exists an element a in $a_{\ub{i}}$
which does not belong to $b_{\ub{i}}$. Consider the set $aM$, it is
a submodule of $M$. Therefore 
$$
a M = \sum^n_{i=1} a A/ a A \cap a_{\ub{i}}=\sum ^n_{i=1} a A/ a
\cap a_i 
$$
because every left principal ideal in $A$ is a right principal ideal
in $A$. Let $x \in A \to \overline{xa} \in Aa$ be a map from $A$ to $Aa/
a A \cap \underline{a_{i}}$, its kernel is the set $\{x|xa \in
\underline{a_{i}}\}=B$. Therefore we get that $Aa/a A \cap
\underline{a_{i}}$ is isomorphic to $A/B$. Moreover $A/B = (0)$ if and
only if a belongs to $\underline{a_{i}}$. Now rank of aM=number of
$\underline{a_{i}}$ such that a does not belong to
$\underline{a_{i}}$. Since a belongs to $a_{i}$, a belongs
to ${a_{j}}$  for $j \leq i$, therefore rank of a $M \leq
n-i$. On the other hand rank of $a M =$ number of
$\underline{b_{j}}$, such that a does not belong to
$\underline{b_{j}}$. Since a does not belong to $\underline{b_{i}}$,
rank $a M> n-i$. Hence we arrive at a contradiction. Thus
$\underline{a_{i}} = \underline{b_{i}}$ and our result is proved.  

\begin{remark*}
  It can be shown that the six assertions (I a ) to (III b) are
  true if the ring $A$ satisfies the conditions 1) and:  
  \begin{enumerate}[1)]
  \item any ideal is principal;
  \item $A$ has no zero divisors.
  \end{enumerate}
  The proof works exactly as in the commutative case (see Bourbaki, loc. cit.)
\end{remark*}

Obviously, the ring $\tilde{\mathscr{O}}$ of the integers of any valuated
non - commutative field satisfies 1)and 2).Moreover we have in this case
$d_{ii}= \tilde{(\pi)}^{\beta_i}$ with $y_i \in \tilde{\mathscr{O}}^\ast$  and $1
\leq i \leq r$ and  $d_{ii}=0$ for  $i >r$. The diagonal $n \times n $
matrix $y$ defined by $y_{ii}=y_i$  for  $1 \leq i \leq r \text{ and }
y_i =1 \text{ for } i > r$ is invertible and multiplying d on the
right by $y^{-1}$ and $q$ on\pageoriginale the left by $y$, we get a decomposition
$g=p'd'q'$ where $p'$ and $q'$ are invertible and $d'$ is a diagonal
matrix whose diagonal coefficients $\tilde{\pi}^{\beta_i} $ are positive powers of the
uniformising parameter $\widetilde{\pi}$  with  $\beta_1
\leq \cdots \leq \beta_r$, and the $\beta_i$ are completely determined
by these conditions (we used the fact that ideal in $\tilde{\mathscr{O}}$ is
generated by one and only one power of $\widetilde{\pi}$).  

Now, let us return to the group $G$. For any $n$-tuple of
rational integers, $\alpha= (\alpha_1, \ldots, \alpha_n)$, let
$d_{\alpha}$ be the diagonal $n \times n$ matrix with diagonal coefficients
$\tilde{\pi}^{\alpha_i}$ and let $\Delta^+$ be the subset of the
subgroup $\Delta$ consisting of the matrices $d_{\alpha} \text{ with }
\alpha_1 \leq \cdots \leq \alpha_n$. 

\begin{proposition}\label{part2:chap2:sec2:prop2}
  In each double coset $K g K$ modulo $K$, there exists one and only
    one element of $\Delta^+$. 
\end{proposition}

\begin{proof}
  Let $g = (g_{ij})$ be any element of $G$. Multiply $g$ by a diagonal
  matrix $(a_{ii})$, where $a_{ii}= a^k$, $a \in P, v(a)> 0$ and $k$ is a
  sufficiently large integer so chosen that the matrix $g' =
  g{(a_{ii})}$belongs to $K$. Then by the above theorem there exist
  matrices $p'$ and $q'$ in $K$ such that  
  $$
  g(a_{ii}) = g' =p' d \beta q'  ~\text{ with }~  d \beta \in \Delta^+
  $$
  Let us take $\alpha_i = \beta_i -k v(a)$. Then we have $g = p
  d \alpha q$ with $q$, $p$ in $K$ and $d_{\alpha}$ in
  $\Delta^+$. Conversely if $g$ belongs to $K d_{\alpha} K$. then $g'$
  belongs to $K d_{\beta}K$. But $d_{\beta}$ is unique, therefore
  $d_{\alpha}$ is unique.  
\end{proof}

\setcounter{corollary}{0}
\begin{corollary}\label{part2:chap2:sec2:coro1}
  $K$ is a maximal compact subgroup of $G$. 
\end{corollary}

If possible let $H \supset K$ be a compact subgroup of $G$. Obviously
there exists $\alpha \neq 0$ such that $d {\alpha}$ belongs to
$H$. Then  
\begin{equation*}
  (d {\alpha})^r= 
  \begin{pmatrix}
    \widetilde{\pi}^{\alpha_1 {r}}& &  0\\
      & \ddots &\\
    0 & & \widetilde{\pi}^{\alpha_nr}
  \end{pmatrix} 
\end{equation*}

If\pageoriginale $\alpha_i \neq 0$, then $v(\pi^{r \alpha_i} \to \pm \infty$ as $r \to
\pm \infty$, which is a contradiction as $v$ is a continuous function
form $\widetilde{P}$ to $R$. Hence $H = K$. 

Let $E$ be a vector space over $\widetilde{P}$. Let $I$ be a lattice
in $E$ \iec  a finitely generated $\tilde{\mathscr{O}}$ module such that its
basis generate $E$. Since $I$ has no torsion, basis of $I$ is a basis
of $E$. In particular if we take $E=\widetilde{P}^n$ and
$I=\widetilde{\mathscr{O}}^n$ and if we identify $G$ with the group of
endomorphisms of $E$, then $g \in K$ and only if $g(I)=I$. Moreover if
we take any lattice $L$, then the subgroup of $G$ which leaves $L$
invariant is a conjugate subgroup of $K$. 

Let $H$ be a compact subgroup of $G$. Let $e_1, \ldots, e_n$ be a basis
of $E$. Let $J$ be an $\tilde{\mathscr{O}}$-module generated by the elements
$h(e_j)$, $1 \leq j \leq n$ and $h \in H$. Evidently we have  
\begin{enumerate}[(1)]
\item $J$ is invariant by $H$
\item $J \supset I$
\item The map $h\to h(e_j)$ is a continuous map from $H$ to $E$.
\end{enumerate}
But $H$ is compact, therefore the image of $H$ in $E$ by the map defined
in (3) is compact and hence bounded. Therefore there exists an integer
$k$ such that $J\subset \widetilde{\pi}^{-k} I$, which shows that $J$
is finitely generated, but $J \supset I$, therefore $J$ is 
generated by a finite set of element which generate $E$. Hence $J$ is a 
lattice. Thus $H$ is contained in a conjugate subgroup of $K$ namely
the subgroup of $G$ which leaves $J$ invariant. Hence we have proved
the the following. 

\begin{corollary}
  Any two maximal compact subgroups of $G$ are conjugates and any
  compact subgroup of $G$ is contained in a maximal compact subgroup
  of $G$. 
\end{corollary}

\begin{remark}
  Any\pageoriginale double coset  $K x K, x  \in G$, is a finite union of left
  cosets modulo $K$, because $K$ is open and compact, therefore every
  double coset and left coset modulo $K$  is open and compact. 
\end{remark}	

We introduce $a$ total ordering in $Z^n$ by  the lexicographic order
\iec  if $ \alpha  = ( \alpha  _1,  \cdots, \alpha  _n )$ and $\beta =
( \beta_1,  \cdots,  \beta_n)$    are two elements of $Z^n$, then we
say that $\beta > \alpha  $ if $\beta_i > \alpha_i$, for the least
index $i$ for which $\beta_i \neq \alpha_i$. 

\begin {proposition}\label{part2:chap2:sec2:prop3}
  If $Nd _\beta K \cap Kd_ \alpha  K  \neq  \phi$, where $\alpha \cdot
   \beta$ are in  $Z^n$ and  $d_\alpha  \in \Delta^+$ then $\beta \ge
  \alpha $ and $Nd_\alpha  K \cap K d_\alpha  K = d_\alpha  K$. 
\end {proposition}

\begin{proof}
  Let $n d _ \beta $ belongs to $N d_\beta$ $K \cap $ $Kd_\alpha  K$, where
  \begin {equation*}
    \ub{n}=
    \begin{pmatrix}
      1 \quad  *\\
      \ddots\\
      0 \quad  1
    \end{pmatrix}, 
    d_\beta=
    \begin{pmatrix}
       \tilde{\pi}^{\beta_1} \qquad 0\\
             \ddots\\
      0 \qquad  \tilde{\pi}^{\beta_n}
    \end{pmatrix}
  \end{equation*}
  
  Then $n d_\beta$  belongs to $K d_\alpha  K$. But   $\underline{n} d
  _\beta $ belongs to $ K d_\alpha  K $ if and only if the
  invariant factors of $\underline{n}  d_ \beta $ are $
  \overset{\alpha  _1} {\tilde {\pi}}, \cdots \overset{\alpha  _n}
          {\tilde {\pi}} $. Therefore we get that $\tilde{\pi}^{\alpha
            _1}$ divides $\tilde{\pi}^{\beta_i}$ for  $i = 1, 2,
          \cdots,  n$. If $\alpha_1 < 
          \beta_1$, our assertion is proved. If $ \alpha  _1 = \beta_1$,
          then we multiply the matrix $\underline n d_\beta$ on the
          right by $a$ matrix $\delta$, where 
   \begin{equation*}
     \delta =
     \begin{pmatrix}
       1& - \tilde{\pi}^ {\alpha _1} a _{12}& \cdots & -
       \tilde{\pi}^ {\alpha _1} a_{1n}\\ 
       0& 1 & \cdots& 0\\
       0& 0 & \cdots& 1
     \end{pmatrix}
   \end{equation*}
   if
   \begin{equation*}
     \underbar{n}d_\beta =
     \begin{pmatrix}
       \tilde{\pi}^{\alpha _1}& a_{12} & \cdots a_{1n}\\
       0 & \underset{\ddots}{\tilde{\pi}^{\beta _2}} & \cdots *\\
       0 & 0 & \tilde{\pi}^{\beta_n}
     \end{pmatrix}
   \end{equation*}\pageoriginale
   So we get
   \begin{equation*}
     \underbar{n}d_\beta \delta =
     \begin{pmatrix}
       \tilde{\pi}^{\alpha _1}& 0& \cdots 0\\
       0 & \underset{\ddots}{\tilde{\pi}^{\beta _2}} &  *\\
       0 & 0 & \tilde{\pi}^{\beta_n}
     \end{pmatrix}
     = 
     \begin{pmatrix}
       {\tilde{\pi}^{\alpha _1}}& 0\\
       0 & g'
     \end{pmatrix}
   \end{equation*}
   
   It is obvious that $\delta$ belongs to $K$. Therefore
   $\underbar{n}d_ \beta \delta$ is in $Kd_\alpha  K$, which means
   that its invariant  factors are $ {\tilde{\pi}^{\alpha _1}},
   \cdots, {\tilde{\pi}^{\alpha _n}}$. Thus
   ${\tilde{\pi}^{\alpha _2}},  \cdots {\tilde{\pi}^{\alpha
       _n}}$ are the invariant factors for $g'$, which implies
   that $g'$ belongs  to $K_{n-1} d _\alpha - K_{n-1}$ with obvious
   notations. Our assertion is trivially true for$ n= 1$. If we assume
   that it is true for all  groups  $ G L _r \tilde{(P)}$ for
   $r\leq n-1$, we get $\overline \alpha \leq \overline\beta$. But
   $\alpha = \beta$,
   therefore $ \alpha  \leq \beta$. We  prove the second assertion
   also by induction on $n$. For $n=1$, it is trivially true. Let us
   assume that  the results is true for all groups $GL_r(P)$ for $r
   \leq n-1$. We have to show that $d^{-1}_\alpha \underbar{n}d \alpha  $
   belongs to $K$ if $ \underbar{n} d_\alpha $ belongs to $K d_\alpha
   K$  Let us suppose that  
   \begin{equation*}
     n =
     \begin{pmatrix}
       1& a_{12}& \cdots a_{1n}\\
       0 & \underset{\ddots}1&  *\\
       0 & 0 &0
     \end{pmatrix}
   \end{equation*}
   
   Since $\underbar{n} d_\alpha$ belongs to $Kd_\alpha K
   \tilde{\pi}^{\alpha _1}$ divides $a_{1i}$ for $i = 2,
   \cdots,n$. Obviously 
   \begin{equation*}
     d^{-1}_{\alpha } \underbar{n}d_\alpha  =
     \begin{pmatrix}
       1& x_{12}& \cdots x_{1n}\\
       0 & 1&y\\
       0 & 0 &1
     \end{pmatrix}
     = 
     \begin{pmatrix}
       1&X\\
       0&g'
     \end{pmatrix}
   \end{equation*}\pageoriginale
   where $X$ consists of integers $x_{ij} = \tilde{\pi}^{-\alpha
     _1}  a_{ij}$, and $ g'$ is $a \,(n -1 \times n -1)
   $ mat rix of the form $d_{\alpha ^-}^ {-1}  \underbar{n}' d_{\alpha
     ^-}$ and the invariant factors of $ n'd _{\alpha}-$ are $
   \tilde{\pi}^{\alpha_2}, \cdots, \tilde{\pi}^{\alpha
     _n}$. Therefore by induction hypothesis $g'$ belongs to
   $K_{n-1}$. This shows that $ d^{-1}_{\alpha }  \underbar{n}d_\alpha$ 
   belongs to $K$. 
\end{proof}

\section{Study of $O_n (\varphi, P)$}\label{part2:chap2:sec3}

In this section we shall prove some of the results of
\S \ref{part2:chap2:sec2} for the
group $G = O_n(\varphi, P)$.The same results can be proved for other
such groups of $G L_n(P)$ namely $S L_n(P)$ etc. with obvious
modifications. Throughout our discussion $P$ will denote $a$ locally
compact $p$-adic field such that $K= \mathscr{O}_P | \mathscr{Y}_P$ has
characteristic different from $2$. 

\setcounter{definition}{0}
\begin{definition}\label{part2:chap2:sec3:def1}
  Let $E$  be $a$ vector space of dimension $n$ over $P$. $A$ subspace $F
  \subset E$ is called {\em isotropic with respect to $\varphi$} (a
  bilinear form as $E$) if there exists an element $x$ in $F$ such
  that $\varphi (x,y)=0$ for every  $y$ in $F$, in other words the
  bilinear form when restricted to $F$ is degenerate. 
\end{definition}

\begin{definition}\label{part2:chap2:sec3:def2}
  $A$ subspace $F \subset E$ is called  {\em totally isotropic with
    respect to} $\varphi $ if the restriction of $\varphi$ to $F$ is
  zero \iec  $\varphi(x,y)=0 $ for every   $x,y$ in $F$. 
\end{definition}

It is obvious from the definition that the set of totally isotropic
subspaces of $E$ is inductively ordered. Therefore there exist maximal
totally isotropic subspaces of $E$. They are of the same dimension, 
which\pageoriginale we call the index of $\varphi$. If index of
$\varphi=0, \varphi$  is called $a$ non-isotropic form. 

\textit{Witt's decomposition}. Let $E_1, E_2$ and $E_3$  be three
subspaces of $E$ such that 
\begin{enumerate}[(1)]
\item $E=E_1 \oplus  E_2 \oplus  E_3$
\item $E_1$ and $E_3$ are  totally isotropic.
\item $E_1 +E_3$ is not isotropic.
\item $E_2$ is orthogonal to $E_1 +E_3$ \iec for $x$ in $E_2, \varphi
  (x,y)= 0$ for every $y \in E_1 +E_3$. 
\end{enumerate}

It can be proved that for the vector space $E=P^n$, there exists $a$
Witt decomposition and we can find $a$ basis $ e_1, e_2, \cdots,  e_r$ 
of $E_1, e_{r+1},\cdots,  e_{r+q}$ of $E_2$ and $e_{r+q+1}, \cdots, 
e_n$ of $E_3$, where $2r+q = n$, in such $a$ way that 

$\varphi (e_i, e_j)= \delta_{i,n+1-j}$  for $1 \leq i \leq r$ and 
$r+q < j \leq n$. (I) and that $ r_{r+1}, \cdots, e_{r+q}$  is an
orthogonal basis for $E_2$. Clearly the matrix of the bilinear form
$\varphi$ with respect to this basis of $E$ is  
\begin{equation*}
  \Phi=
  \begin{pmatrix}
    O&O&S\\
    O&A&O\\
    S&O&O
  \end{pmatrix}
  ~\text{where}~ S = 
  \begin{pmatrix}
    0&0&1\\
    0&1&0\\
    1&0&0
  \end{pmatrix}
\end{equation*}
and $A$ is a $q \times q$  matrix, which is the matrix of $\varphi$
restricted to $E_2$. 

We shall now  completely determine the restriction of $\varphi$ to the
non - isotropic part. For simplicity we assume that $r=0$ and $q=n$.
Let $ e_1, \cdots, e_q$ be an orthogonal basis  of $E$. If $x= (x_1,
\cdots, x_q) $ is $a$  point of $E$  with respect to these basis. Then
$\varphi (x,x)= \sum \limits _{i=1}^q a_i x_i ^2$ with 
$a_i \in P$. If $ \dfrac{-a_j}{a_i}$ for  $i \neq  j$ is in $
(P^*)^2$,\pageoriginale then the vector $(o, \cdots,  a_j,  \cdots,  a_i,  \cdots, 
o) $ is an isotropic vector of $ \varphi$,  which is not
possible. Therefore $ a_i \nequiv a_j \pmod {P^{*2}}$, which implies
that $q \leq 4$. 
We shall say that two bilinear forms $\varphi$  and $ \varphi '$ are
equivalent if there  exists $a$ linear isomorphism of the space of
$\varphi $ onto the space of $\varphi'$ and $a$  constant $c \neq 0$,
such that $\varphi' \circ \lambda = \circ \varphi$. Then it can be proved
that every non-isotropic bilinear form over $E$ is equivalent to one
and only one of the following type: 
\begin{enumerate}[(1)]
\item q=4
  \begin {enumerate}[(a)]
  \item  $x _1^2 - C x_2^2 - \pi x _3^2 + C\pi x_4 ^2 $\\
  \end {enumerate}
\item q=3	
  \begin {enumerate}[(a)]
  \item  $x _1^2 - C x_2^2 - \pi x _3^2 $
  \item  $x _1^2 - C x_2^2 - C\pi x _3^2 $
  \end {enumerate}
\item q=2
  \begin {enumerate}[(a)]
  \item  $x _1^2 - C x_2^2  $
  \item  $x _1^2 - \pi x_2^2  $
  \item  $x _1^2 - C\pi x_2^2  $
  \end {enumerate}	
\item q=1
  \begin {enumerate}[(a)]
  \item  $x _1^2  $
  \end {enumerate}
\item q=o
  \begin {enumerate}[(a)]
  \item The $O$-form as where\pageoriginale $(1, C, \pi, C ~\pi)$ is a set of
    representatives of $P^\ast$ modulo $(P^\ast)^2$ as obtained in
    Corollary \ref{part1:chap2:sec1:coro2} of Hensel's Lemma. 
  \end {enumerate}	
\end{enumerate}

We shall say that $a$ basis $e_i,\ldots, e_n$ is a Witt basis for
$\varphi$ if the relations in (I) are satisfied and if the restriction
of $\varphi$ to $E_2$ has one of the above forms. It is obvious that
for $\varphi$ or for a constant
multiple fo $\varphi$, we can always find a Witt besides and the
matrix of $\varphi$ with respect to $a$  Witt basis is independent of
the choice of  the Witt basis.  

\begin{proposition}\label{part2:chap2:sec3:coro4}
  If $M=M_q(P) $ is a matrix such that $M' AM$ belongs to $M_q
  (\mathscr{O})$  ($M'$ denotes the transpose of the matrix $M$ and
  $A$  denotes the matrix of the restriction of $ \varphi$ to $E_2)$, 
  then $M$ belongs to $ M_q (\mathscr{O})$.   
\end{proposition}

\begin{proof}
  We prove first that if for $ x \in E$, $\varphi(x,x)$ is in
  $\mathscr{O}$, then 
  the co-ordinates of $x$ are in $\mathscr{O}$. Let us assume for
  instance that $q=4$.  If possible let $ v(x_1) < 0 $ and  $
  v(x_1)\leq \min (v(x_2), v(x_3), v(x_4))$. Suppose that $ v(x_1)=
  \alpha $.  Since $ v(x_1^2 -c x_2 ^2- \pi x_3^2 - c \pi x_4^2) \ge 0$
  we have $ x_1^2 - Cx_2^2 \equiv o \pmod {\mathscr{Y}^{2r+1}} $, where
  $ r= max (0,\alpha)$. Therefore  $(\pi^{- \alpha }x_1)^2 - s(\pi
  ^{- \alpha }x_2)^2  \equiv 0 \pmod {\mathscr{Y}}$. 
  
  But  this is impossible, because $ \overline {C}$  is not $a$ square
  in $k$. Thus our result is established. The other cases can be
  similarly dealt with.  
\end{proof}

Let $M=(m_{ij})$, then $M' A M = (\gamma_{ij}) $ where  $ \gamma_{ij}
= \varphi (m_{1i}, \cdots, m_{qi } m_{qj})$, If $M' A M$ belongs to 
$M_q(\mathscr{O})$ then $\gamma _{ii}$ belongs to $ \mathscr{O}$,
which implies that $ m_{ri}$ belongs to $ \mathscr{O}$ for $ i, r =
1,2,\cdots,  q$. It is obvious that  it is sufficient to
assume that only the diagonal elements of $M' A M $ are in
$\mathscr{O}$. 

In\pageoriginale the following we shall be dealing with $a$ fixed Witt basis of the
space $E$.  We shall adhere to the following notations throughout our
discussion. 
\begin{gather*}
  K^o = G \cap K, T^o = G \cap T, N^o = G \cap N, \Delta^o = G \cap
  \Delta^+\qquad \text{and}\\
d^\circ_\alpha=
\begin{pmatrix}
    \pi^{-\alpha_1}&&&&&& \\
    & \ddots &&&&&\\
    &&\pi^{-\alpha_r} 0& && &\\
    & & &{}^1 \ddots_1 & &&\\
    &&&& \pi^{\alpha_r} &&\\
    &&&&&\ddots &\\
    &&&&&& \pi^{\alpha_1}
  \end{pmatrix}
\end{gather*}
where $ \alpha  =( \alpha  _1,  \cdots, \alpha _r )$

\begin{proposition}\label{part2:chap2:sec3:prop5}
  $G= T^o K^o$
\end{proposition}

\begin{proof}
  We have already proved that $GL_n(P)= TK$. Therefore $g \in  G $
  implies that $g = tk$ where $t $ and $K$ belong to $T$  and $K$
  respectively.  We know that det $(g) = \pm 1$ and det $(k)$
  belongs to $\mathscr{O}^*$. So  det $(t)$  belongs to $\mathscr{O}
  ^*$. But det $(t)$  is $a$ power of $\pi$, therefore det
  $(t)=1$. Now $g$ belongs to $G$ if and only if $g' \Phi g= \Phi$
  \iec  $t' \Phi t = k^{-1'} \Phi K^{-1}$. Since $k^{-1'}  \Phi k
  ^{-1}$  belongs to $ M_n ( \mathscr{O} ),t' \Phi  t$ belongs to
  $M_n( \mathscr{O})$.  
\end{proof}

Let us suppose that
$$
\displaylines{\hfill
  t =
  \begin{pmatrix}
    a_1 & X & Z\\
    O& a_2& Y\\
    O&O& a_2
  \end{pmatrix}\hfill \cr
\text{then}\hfill 
  t' \Phi t=
  \begin{pmatrix}
    o&O&a'_1 S a_3\\
    o & a'_2 A a_2 &  X'S a_3 +a'_2 AY\\
    a'_3Sa_1 & Y' A a_2+ a_3 S X & Z' S a_3 +Y' A Y + a_3' S Z.
  \end{pmatrix}\hfill }
$$

This\pageoriginale shows that $ a'_1 S a_3$   and $a_3$ and $ a'_2 A a_2$ belong to
$M_n( \mathscr{O} )$. Moreover, we have $ 1=  \det t =  (\det
a_1) (\det a_2)  (\det a_3)$ and $(\det a_2)$ and\break $(\det a_1)$. $(\det a_3)$
belong to $\mathscr{O} $ (for,  $a'_1 S a_3$ belongs to $ M_n$ 
($\mathscr{O}))$. So det $a_2$  belongs to $\mathscr{O} ^*$
implying $a_2$ belongs to $K$. By above proposition we get that the
matrix $a_2$ has coefficients from $\mathscr{O} $. We shall find $a$
matrix $\delta$ in $T \cap  K $ such that $t \delta$ belongs to
$G$. Then $g = t K =  t \delta \delta^{-1}K$  implies that
$\delta^{-1}K$  belongs to $ K^o$ and our result will be
proved. Multiply the matrix $t$ by the matrices  $h$ and $h'$  on
the right.  where
$$
\displaylines{\hfill h= 
   \begin{pmatrix}
     b& 0 & 0 \\
     0 & a^{-1}_2& 0\\
     0& 0& 1
   \end{pmatrix}, ~~h'= 
   \begin{pmatrix}
     1& \xi &  \zeta\\
     0&1&0\\
     0&0&1
   \end{pmatrix}\hfill \cr
\text{we get}\hfill~ 
   t~  h~ h' =
   \begin{pmatrix}
     a_1 b & a_1 b+ a_2^{-1} X & a_1 b \zeta +z\\
     0&1&Y\\
     0&0&a_3
   \end{pmatrix}\phantom{we get}\hfill} 
   $$
We shall determine  the matrices $b$,  $\xi$ and $\zeta$ in such $a$
way that $ t~  h~ h'$ belongs to $G $. Now $t~ h~ h'$ belongs to $G$ if
and only if  
  
 $ (t~ h~ h')' \Phi (t ~h~ h') = \Phi$  \iec  if and only if the
following conditions are satisfied 
\begin{align*}
  &b' a'_1 S a_3 = S\tag{1}\label{part2:chap2:sec3:eq1}\\
  &AY + X' a_2 ^{-1} S a_3 + \xi ' b' a'_1 S a_3 =0
  \tag{2}\label{part2:chap2:sec3:eq2}\\ 
  & a'_3 S a_1 b \zeta + a'_3 S Z + y ' AY + \zeta' b' a' _1 S a_3 +
  Z' s a_3 = 0 \tag{3}\label{part2:chap2:sec3:eq3} 
\end{align*}
 
 Let us take $b' = S(a'_1 S a_3) ^{-1}$. Then $h$ belongs to $K
 \cap  T $ and the  
 conditions\pageoriginale (2) and (3) reduce to 
 \begin{gather*}
   AY + X' (a_2^{-1})'  S a_3 + \xi ' S =0\\
   S \zeta + a'_3 S Z + Y' A Y + \zeta' S +Z' S a_3 =0
 \end{gather*}
 
 So if we take $S \xi '= -A Y - X' a_2 ^{-1} S a_3 $ and $s \zeta=
 -\dfrac{1}{2}V $ where $ V= a'_3 S Z + Y' A Y + Z' S a_3, $ we see
 that the matrix $t h h' $ belongs to $G$. It is obvious that the
 matrix $h h'$ belongs to $T \cap K$.Hence we get $ g=t h h'.  (h
 h')^{-1} k = t_\circ k_\circ $, which  proves our result completely. 

 \begin{defi*}
   Let $I$ be $a$ lattice in $E$.  The $\mathscr{O}$ module $
   \mathfrak{N}(I)$  generated by the set of elements $\varphi (x,y)$
   for $x,y$ in $I$ is called the  {\em norm } of the lattice $I$. 
 \end{defi*} 
 
$A$ lattice $I$ is called a maximal lattice if it is maximal among the
 lattices of norm $\mathfrak{N} (I) $. It is easy to see that any
 lattice of a given norm is contained in $a$ maximal lattice of the
 same norm. The lattice $I_o$ generated by the Witt basis $(e_1
, \cdots,  e_n )$ of $E$   is a maximal lattice of norm $\mathscr{O}_n
 $.  Let $I$ be $a$ lattice of norm $\mathscr{O}$ containing
 $I_o$. Let $ x= \sum \limits _{i=1}^n  x_i e_i $ be any element in
 $I$.  Then $ \varphi (x,e_i)= \pm x_{n+1-i}$ for $ 1 \leq i \leq r $
 and $ r + q < i \leq n$. let y$=\sum \limits _{i=r+1}^{r+q} x_i e_i,
 $  since $ \varphi (y,e_j)$ is  an integer for $r+1 \leq j \leq r +q,
 x_j$ is an integer for $ r+1 \leq q +r $.  Hence $x$ belongs to
 $I_o$. Therefore $I_o$ is $a$ maximal lattice.  

\setcounter{theorem}{1}
\begin{theorem}\label{part2:chap2:sec3:thm2}
  Let $I_1$ and $I_2$ be two maximal lattices of norm $\mathscr{O}
  $, then there exists $a$ Witt basis $ (f_1, f_2, \cdots,  f_n)$ of
  $E$  and $r$ integers 

  $\alpha_i \ge \cdots \ge \alpha_r \ge 0$, such that ($r = $index
  $\varphi $) 
  \begin{enumerate}[\rm(1)]
  \item  $I_1$\pageoriginale is generated by $ (f_1, f_2, \ldots, f_n)$
  \item $I_2$ is generated by 
    $$
    \left(\overset{- \alpha_1}{\pi}f_1,\ldots
   ,  \overset{-\alpha_r}{\pi}f_r, f_{r+1 },  \ldots,  f_{r+q},
    \overset{\alpha_r}{\pi} f_{r+q+1}, \ldots,
    \overset{\alpha_1}{\pi}f_n\right).
    $$
  \end{enumerate}
\end{theorem}

\begin{proof}
  We shall prove the theorem by induction on $r$. When
  $r=0$, $\varphi$ is non-isotropic and there exists only one maximal
  lattice of norm $ \mathscr{O}$ which is generated by any witt basis
  of $E$.  Let us assume that the theorem is true for all bilinear
  forms of index $< r$. We first prove the following result. 
\end{proof}

If $I$ is $a$ maximal lattice of norm $\mathscr{O}$ and $X$ is an 
isotropic vector in $I$ such that $\pi^{-1} X$ does not belong to $I$,
then there exists an isotropic vector $X' \in I $ such that $
\varphi(X,X')=1$. 

If possible let us suppose that the result is not true. Let us assume
that $ \varphi(X,Y)$ belongs to $\mathscr{Y} $ for every $Y$ in
$I$. Then $\varphi (\pi^{-1}X,Y)$ belongs to
$\mathscr{O}$. Consider $I'= I + \mathscr{O} {\pi^{-1}} X$. It
is $a$  lattice because $I'$ is finitely generated $\mathscr{O} $
module containing $I$. Moreover 

$ \varphi (Y+ \alpha  {\pi^{-1}} X,Z + \beta
{\pi^{-1}}X) = \varphi(Y,Z) + \alpha \varphi ({\pi^{-1}} X,Z) +
\beta \varphi ({\pi^{-1}}X,Y)$  is an integer for every
$\alpha ,  \beta $ in $ \mathscr{O} $.  Therefore norm of $I'$ is
$\mathscr{O}$. But   this is $a$  contradiction because $I$ is $a$
maximal lattice of norm $\mathscr{O}$. Therefore there exists $a$
vector $Y$ in $I$ such that $\varphi(X,Y) $ belongs to $\mathscr{O}^\ast
$. By multiplying $Y$ by some inversible element of $\mathscr{O} $, we
get $a$ vector $Y'$ in $I$ such  that $\varphi (X,Y')= 1$. Let us
take 

$X'=Y' - \dfrac {1}{2} \varphi (Y', Y' ) X$. Obviously $\varphi (X,X')
= 1$ and  $\varphi (X', X')=O$. 

Now we shall prove the theorem. For every isotropic vector $X \in
I_1$
(respectively\pageoriginale $I_2$) let $t(X)$  (respectively u(X)) denote the smallest
integer such that $\overset{t(x)}{\pi}X$ (respectively $ \pi
^{u(X)}X)$  belongs to $ I_2$(respectively$ I_1)$. Such an integer
exists.  because $I_1$ is an $\mathscr{O}$-module  of finite
type and $I_2$ generates $E$,  therefore there exists an integer $t$
such that $ \pi^t I_1 \subset I _2$. Thus $t(X) \leq t$ always. Let
$X$ be an isotropic vector in $I_1$ such that $ \pi ^{-1} X$  does not
belong to $I_1$. Then	$ Y= \pi^{t(X)} X $ belongs to $I_2$ and $ \pi
^{-1}Y $ does not belong to $I_2$. Since $\pi^{-1} X $ does not
belong to $I_1$, it is obvious that $u(Y)= -t(X)$. By the above result
there exists $a$ vector $X'$  in $I_1$ such that $ \varphi(X,X')=1$
and $\varphi(X',X')=0$. This shows that $\pi ^{-1}X'$ does not  belong
to $I_1$. By the definition of $t(X)$ and $t(X)'$ we get that 
$$
\varphi \left(\pi^{t (X)} X, \pi^{t(X')}X'\right)= \pi^{t(X)+t(X')}
$$ 

Since  $ \varphi (\pi^{t(X)}X, \pi^{t(X)}X, \pi^{t(X')}X)$ belongs
to $ \mho$, we get that  
\begin{equation*}
  t (X) + t(X') \ge 0. \tag{1}
\end{equation*}

Similarly there exists an isotropic vector $Y'$ in $I_2$ such that 
$$
\varphi(Y,Y')=1 \text {and } u(Y) +u(Y')\ge 0.
$$

Let $ Z= \pi^{u(Y')}Y'$, then $ t(Z)= -u(Y')$

Therefore we get
\begin{equation*}
  t(X)+t(Z)\leq 0\tag{2}
\end{equation*}
obviously $Z$ is isot ropic and $\pi^1 Z$ does not belong to
$I_1$. Therefore  there exists  $a$ vector $Z'$ in $I_1$  such that
$\varphi (Z,Z')=1$ and $\varphi (Z',Z')=0$ and  
\begin{equation*}
  t(Z) + t(Z') \ge 0\tag{3}
\end{equation*}

Let us suppose that the vector $X$ is so chosen that $t(X)$ is of
maximum value, which exists because $t(X) \leq t $ for every $X$ for
some integer $t$. 

Therefore\pageoriginale in particular we get $t(Z') \leq t (X)$. From (2) and
(3) it follows that  
\begin{gather*} 
  t(X) +t(Z)=0\\
  t(X) +t(Z')=0
\end{gather*}

Thus we have found two vectors $X$ and $Z$ in $I_1 $  such that $\pi
^{\alpha_1}X$ and $ \pi^{-\alpha_1}Z$ where $\alpha_1 =t(X) $
belong to $I_2 $ and  
$$
\varphi (Z,X)= \varphi (\pi^{-t(Z)}Y', \pi^{t(X)}Y)=1.
$$

Let $F$ denote the subspace of $E$ orthogonal to the subspace of $E$
generated by the vectors $X$ and $Z$.Obviously $\varphi$ restricted to
$F$ is non - de- generate and its index is $r-1$.  Moreover $ I_1=
\mathscr{O} X \oplus \mathscr{O} Z \oplus F \cap I_1$, because for
any $a$ in $I_1$ we  have 

$ a= \lambda X + \mu~ Z +b$, where $\lambda$ and
$\mathscr{O}$ belong to $\rho$ and $b$ belongs to $F$. 

But  $\varphi (a,X) = \mu$, therefore it is an integer,
similarly $\lambda $ is an integer. Thus $b$ belongs to $I_1$ and the
assertion is proved. Similarly   we have $ I_2 = \mathscr{O} \pi
^{\alpha_1} X \oplus \mathscr{O} \pi ^{-\alpha _1}Z \oplus I_2 \cap
F$. It can be easily sen that $ I_j \cap F(j=1,2)$ is a maximal
lattice of norm $\mathscr{O} $. Hence by induction hypothesis there
exists $a$ Witt basis $f_2, f_3, \cdots, f_{n-1}$ of $F$ and there
exist $r-1$ integers $\alpha _2 \ge - \alpha _r \ge 0$ such that
\begin{enumerate}[(1)]
\item $f_1, f_2, \cdots,  f_{n-1}$ generate $I_1 \cap F$.
\item $\overset{-\alpha_2}{\pi}  f_2,
  \ldots,\overset{-\alpha_r}{\pi} f_r, f_{r+1}, \ldots,
  f_{r+q}, \overset{\alpha_r}{\pi} f_{r+q+1'} \overset{\alpha_2}{\pi}
  f_{n-1}$ generate $ I_2\cap F$.
\end{enumerate}
If we take $f_1=  Z,  f_n= X$ and $\alpha _1 = t(X)$   we get $a$ Witt
basis 
$(f_1, \cdots,  f_n )$\pageoriginale of $E$ and $r$ integers $\alpha  _1,\ldots, 
\alpha _r$ satisfying the requirements of the theorem because 
$\alpha _2 = t(f_{n-1)} \leq \alpha_1$. 
\begin{corollary}
The group $G$ acts transitively on the set of lattices of norm
$\mathscr{O}$. 
\end{corollary}

The mapping $g$ defined by 
\begin{align*}
  g(f_i) &= \pi^ \gamma f_i, \text{ where }\\
  \gamma &= \alpha _i ~\text{ for } 1 \leq i \leq r\\
  &= O ~\text{for } r+1 \leq i \leq r+q \\
  &=2r+q-i+1~ \text {for } r+q+1 \leq i \leq 2r+q.
\end{align*}
leaves $\Phi$ invariant. Therefore $g$ belongs to $G$. 

\begin{proposition}\label{part2:chap2:sec3:prop6}
  In each double coset of $G$  modulo $K^o$ there exists one and only
  one element $d_\alpha$ of $\Delta^0_+$. 
\end{proposition}

\begin{proof}
  Let $g$ be any element of $G $. We shall denote by $g$ itself the
  automorphism of $E$ with respect to the initial Witt basis 
  $(e_1,\cdots, e_n)$. The lattice $ g(I_o)$ is obviously $a$ maximal
  lattice of norm $\mathscr{O}$.  Therefore by the above theorem we
  get $a$ Witt basis $(f_1, \cdots,  f_n )$ of $E$ such that  
  \begin{enumerate}[(1)]
  \item $I_o$ is generated by $ f_1,\cdots, f_n $,
  \item $ g(I_o)$ is generated by $ g_1,\cdots, g_n $ where $ g_i =
    \pi ^\gamma f_i $  with  
  \end{enumerate}
  $ \gamma $ as defined  in the corollary of above
  theorem. Let $\ub{k_1}$ (respectively $\ub{k_2}$) be
  the matrix with respect to the basis $e_1, \ldots, e_n)$
  (respectively $g_!, g_2,\break \ldots g_n$) of the
  automorphism $k_1$ (respectively $k_2) $ defined by $k_1(e_i)=f_i$
  (respectively $k_2(g_i)=g(e_i))$ for $i=1,2,\cdots, n$. We see
  immediately that the matrix $ K_{\underbar{1}}$ and $K_{\underbar
    {2}}$ are in $ K^o$. Moreover the matrix of the automorphism $f_i
  \rightarrow g_i$ with respect to the basis $f_i$  is $ d^o_\alpha $
  where $ \alpha  = (\alpha _1, \alpha _2, \cdots \alpha  _r)$. 
\end{proof} 

It\pageoriginale is obvious that
\begin{align*}
  g(e_i) & = \sum_j \underline{(k_2)}_{ji}~ g_j\\
  & = \sum_{j,k}\underline{(k_2)}_{ji} (d^{o}\alpha)_{kj} ~f_k\\
  & = \sum_{j,k,l}\underline{(k_2)}_{ji}(d^o\alpha)_{kj}~
  \underline{(k_1)}_{lk} ~ e_l 
\end{align*}

Thus we get $g = \underline{k_2}~ d^0_{\alpha} ~\underline{k_1}$,
which means $d_{\alpha}$ belongs to $K^0 g K^0$.The uniqueness part of
the propositional follows from the uniqueness of $d^o_\alpha$ in $K
~x~ K $ for $x$ in $GL_n(P)$. 

We introduce a total ordering in $Z^n$ which is inverse of the
lexicographic ordering. 

\begin{proposition}\label{part2:chap2:sec3:prop7}%proposition 7
  Let $\alpha$ and $\beta$ be two elements in $Z^r$ such that
  $d^0_{\alpha}\in \triangle^0_+$. If $N^0~d^0_{\beta}~K^0\cap
  K^0~d^0_{\alpha}~K^0 \neq \phi$ then $\beta \geq \alpha$. Moreover
  $N^0~d^0_\alpha ~ K^0 ~\cap~ K^0~ d^0_\alpha ~ K^0 = d^0_\alpha ~
  K^0$. 
\end{proposition}

\noindent \textit{Proof.}%pro
  Since $N^0~d^0_{\beta}~K^0$ and $K^0~d^0_\alpha~K^0$ are contained
  in $N~d'_\beta K$ and $K~d'_{\alpha}~K$ respectively with 
  \begin{align*}
    \alpha' & = (-\alpha_1,-\alpha_2,\ldots,-\alpha_r,0 \cdots
    0,\alpha_r,\alpha_{r-1},\ldots,\alpha_1)\\ 
    \beta' & = (-\beta_1,-\beta_2,\ldots,-\beta_r,0 \cdots
    0,\beta_r,\beta_{r-1},\ldots,\beta_1) 
  \end{align*}
  we have $N~d_{\beta},K~\cap~K~d_\alpha,~K \neq \phi$. Therefore
  $\beta' \geq \alpha'$ for the lexicographic ordering introduced in
  $Z^n$ before proposition \ref{part2:chap2:sec2:prop3} in this
  chapter. It is obvious that 
  $\beta\geq\alpha$ for the new ordering of $Z^r$. The other assertion
  follows trivially from the fact that 
  \begin{equation*}
    d^0_{\alpha} K \cap G = d^0_\alpha K^0.\tag*{$\Box$}
  \end{equation*}

\section{Representations of $p$-adic Groups}\label{part2:chap2:sec4}

We\pageoriginale prove here an analogue of the theorem about the representations of
semisimple Lie Groups in chapter $I$ of this part. We shall give the
proof of the theorem for the general linear group $GL_n(P) = G$,
though the same theorem could be proved for other classical linear
groups with obvious modifications. We shall adhere to the notations
adopted in the earlier chapter. 

Let $\lambda$ denote a character of $T$ which is trivial on $N$. Since
$\triangle$ is isomorphic to $T/N,\lambda$ can be considered as a
character of $\triangle$. Let us assume that $U^{\lambda}_{f}=0$ for
every $\lambda$ in $\triangle^*$ (the group of characters of
$\triangle$) and $f\in L(G)$ such that $f \neq 0$. We first try to
find the condition under which our assumptions are valid. Let
$\varphi$ be an element of $C^\lambda$ (the space of the
induced representation of $\lambda$). Then $\varphi(tx) =
(\rho(t))^{\frac{1}{2}}\lambda(t)\varphi(x)$ for $x \in G$ and $t \in
T$. Moreover 
\begin{equation*}
  U^{\lambda}_{f}\varphi(e) = \int_G \varphi(y)f(y)dy =
  0,~\text{because}~ U^\lambda_{f} = 0 \tag{I}\label{part2:chap2:sec4:eqI}  
\end{equation*}

Since $\sum$ the support of $f$ is a compact set, it intersect only a
finite number of double cosets modulo $K$. Let 
\begin{align*}
  S & = S(f) = \left[\alpha| d\alpha \in \triangle_+,\sum \cap K~d_\alpha~K
    \neq \phi\right]\\ 
  \alpha & = \alpha(f) = \min_{\beta}~ \left\{ \beta \in S(f)\right\}.
\end{align*}

The set $S$ is a finite non-empty set because $f \neq 0 $. Therefore
$\alpha$ exists. For any $d_{\alpha}$ in $\triangle_+$ the coset
$K~d_{\alpha}~K$ is a finite union of left cosets modulo $K$, the
representatives for which could be found in $T$,because $G = TK$. Let
$I_{\alpha}$ be the set of left cosets $C$ modulo $K$ such that
$K~d_{\alpha}~K = \bigcup\limits_{C\in I_\alpha} C$, where $C =
t(C)K,t(C)\in T$. But\pageoriginale we know that $T = N\triangle$, therefore $t(C) =
n(C)~d_{\gamma}(C)$ where $n(C)$ and $d_{\gamma}(C)$ belong to $N$ and
$\triangle$ respectively. Since $n(C)~d_{\gamma}(C)$ belongs to
$K~d_{\alpha}~K$ proposition 3 implies that $\gamma(C)\geq\alpha$,
Thus we get that $K~ d_\alpha~ K =m\bigcup\limits_{C \in ~I_\alpha}
n(C) ~K$, $\gamma (C) \geq \alpha$ and if $\gamma (c) = \alpha$, then
$C= d_{\alpha}~K$ and we can take $t(c) =
d_\alpha$. Let us assume that the right invariant Haar measure on $G$
is such that its restriction to $K$ is normalised \iec $\int_k~d_k =
1$. Then for any left coset $C = t(C)K$, we have 
$$
\int_G f(g) d_g = \triangle(t(C)) ~ \int_K f(t(C)k) ~ dk
$$
and the equation (I) gives
\begin{align*}
0 = \int_G \varphi(y)f(y)~dy & = \sum_{\beta\in S} \sum_{C \in
I_{\beta}}\triangle(t(C))~\int_K \varphi(t(C)k)f(t(C)k)dk\\ 
& = \sum_{\beta}\sum_C \sigma (t(C))~\int_K \varphi^0(k)~f(t(C)k)~ dk
\end{align*}
with $\sigma(t) = [\delta(t)\triangle(t)]^{\dfrac{1}{2}}$ and where
$\varphi^0$ denotes the restriction of $\varphi$ to $K$. 

We have shown earlier that $\varphi^0(tx) = \lambda(t)~\varphi^0(x)$
for $t \in T \cap K = N \cap K$, but $\lambda(N) = 1$, therefore the
space $C^\lambda$ is independent of $\lambda$. Moreover there is only
one term corresponding to $\beta = \alpha$ in the summation, since for
others $\gamma(c) \ge \alpha$. Separating the term for $\beta =
\alpha$ we get $U_f^{\lambda}\varphi(e) =
\sigma(d_\alpha)^{\dfrac{1}{2}}\lambda(d_\alpha)~\int_K\varphi^o(k)
f(d_\alpha ~ k)dk + \underset{\gamma\ge
  \alpha}{\sum}Q_{\gamma}(f,\varphi)\lambda(d\gamma)$ with  
\begin{equation}
  Q_{\gamma}(f,\varphi) = \sum_{C\in I_\beta \gamma(C) = \alpha}
  \sigma(t(C))^{\frac{1}{2}}~\int_K\varphi^0(k)f(t(C)k)dk
  \tag{II}\label{part2:chap2:sec4:eqII}  
\end{equation} 

It\pageoriginale is obvious that $Q_\gamma(f,\varphi)$ is independent of
$\lambda$. For every $\gamma\in Z^n$, the mapping $d_\gamma \in
\triangle\longrightarrow \chi_{\gamma} \in \triangle^{*^{*}}$ given by
$\chi_\gamma(\lambda) = \lambda(d_\gamma)$is an isomorphism of the
groups $\triangle$ and $\triangle^{*^{*}}$. But the characters of an
abelian group are linearly independent, therefore
(\ref{part2:chap2:sec4:eqII}) gives us 
$Q_\gamma(f,\varphi) = 0$ for every $\gamma $ and in particular
$Q_\alpha(f,\varphi) = 0$. Thus we obtain 
\begin{equation}
\int_K\varphi(k)f(d_\alpha k)dk = 0, \text{ for every } \varphi~
\text {with } \varphi(nk) = \varphi(k) \text{ for } n \in N \cap
K. \tag{III}\label{part2:chap2:sec4:eqIII} 
\end{equation}

The equation (\ref{part2:chap2:sec4:eqIII}) is true for left and right
translations of $f$ by 
elements of $K$ because $U^{\lambda}_{\sigma_{x^f}}=
U^{\lambda}_{\varepsilon_{x_n * f}} = U^{\lambda}_x   U^{\lambda}_f = 0
$ and 
$$
U^{\lambda}_{\tau_{x^f}} = U^{\lambda}_{f*\varepsilon_x} =
U^{\lambda}_f~ U^{\lambda}_x = 0. 
$$

So if $g(x)  = f(k^{-1}x)$ for $k$ in $K$, we have $U^{\lambda}_g =
0$. Obviously $S(f) = S(g)$ and $\alpha(f) = \alpha(g)$. Let
$K'_{\alpha} = K \cap d_\alpha K~d^{-1}_\alpha$ and $K_\alpha = K \cap
d^{-1}_{\alpha}~ K~ d_\alpha$ be two subgroups of $K$. Now 
$$
\int_K\varphi(k)f(d_\alpha(d^{-1}_\alpha~hd_\alpha~k))dk = \int_K
\varphi(d^{-1}_\alpha~hd_\alpha ~k) f(d_\alpha k)dk = 0 
$$

Thus the function $k \rightarrow f(d_\alpha~k)$ is orthogonal to all
the functions $\varphi$ in $C^{\lambda} = C$ and their left translates
by the elements of $K_\alpha$, where $\varphi$ is invariant on the
left by the elements of $N \cap K$. 

\begin{lemma*}%lem
  For every $\alpha \in Z^n$ such that $d_{\alpha}\in \triangle_+$,
  the subgroup $K_\alpha$ contains $N' \cap K$ where $N'$ is the group
  consisting of the transpose of elements of $N$. 
\end{lemma*}

\begin{proof}%pro
  By definition

  $d_\alpha =$
  $\begin{pmatrix}
    \pi^{\alpha_1} && 0\\
    & \ddots & \\
    0 & & \pi^{\alpha_n}
  \end{pmatrix}$
  with $\alpha_1 \leq \alpha_2 \leq, \ldots,\leq \alpha_n$.
\end{proof}\pageoriginale

Let $h = (h_{ij})$ be an element of $K$. Then
$(d_\alpha~h\,d^{-1}_\alpha)_{ij} =
\pi^{\alpha_i-\alpha_{j}} h_{ij}$ which shows that the groups
$K_\alpha$ consist of matrix $h$ in $K$ such that
$\pi^{\alpha_i-\alpha_j} {h_{ij}}$ is integral. If we take $h
\in N'\cap K$, obviously $h$ belongs to $K_\alpha$. Thus $K_\alpha$
contains $N'\cap K$. This lemma shows that the groups $K_\alpha$ and
$K'_\alpha$ are sufficiently big. 

 In addition to the above assumption about $f$, let us further assume
 that $f$ belongs to $L_M(G)$ where $M$ is some irreducible
 representation of $K$. Clearly $M$ is a subrepresentation of left
 regular representation of $K$ in $L^2(K)$. Let $E \subset L^2(K)$ be
 an invariant subspace of the left regular representation $\sigma$ of
 $K$ such that $\sigma$ when restricted to $E$ is of class
 $M$. Therefore $E\subset L_M(K)$. Define $F(k) = f(d_\alpha~k)$. We
 can assume that $F \neq 0$. Since $F$ is transformed following
 $\bar{M}$ by the right regular representation of $K,F$ belongs to
 $L_M(K)$. But $F$ is orthogonal to all the functions $\varphi$ in $C$
 invariant on the left by the elements of $N \cap K$, the left
 translates of $\varphi$ by the elements of $K$ and the right
 translates of $\varphi$ by the elements of $K$. Hence if $M$
 satisfies the condition ($S$) i.e. The smallest subspace of $E$
 invariant by $N'$ and which contains elements invariant on the left
 by the elements of $N \cap K$ is $E$. Then $F$ is orthogonal to
 $L_M(K)$, because $L_M(K)$ is generated by the right translates
 of\pageoriginale 
 $E$. But this is a contradiction, because $F \in L_M(K)$. Thus we get
 the following 
\begin{theorem}\label{part2:chap2:sec4:thm3}%theo 3
  The representations $U^\lambda$ for $\lambda\in \triangle^*$ form a
  complete system of representations of the algebra $L_M(G)$ if the
  irreducible representation $M$ satisfies the condition ($S$). 
\end{theorem} 

\setcounter{corollary}{0}
\begin{corollary}\label{part2:chap2:sec4:coro1}% cor 1
  If $M$ satisfies ($S$) then $M$ occurs atmost ($\dim M$) times in
  any completely irreducible representation of $G$. 
\end{corollary}

Since $U^\lambda$ for any $\lambda$ in $\triangle^*$ when restricted
to $K$ is a subrepresentation of the left regular representation of
$K,C\subset L_M(K)$ which is a subspace of dimension $(\dim M)^2$,
thus $M$ is contained at most $(\dim M)$ times in $U^\lambda$. Our
result follows from proposition 1.3. 

\begin{corollary}\label{part2:chap2:sec4:coro2}% cor 2
  The identity representation of $K$ occurs at most once in any
  completely irreducible representation of $G$. 
\end{corollary}

This follows from Corollary \ref{part2:chap2:sec4:coro1} as the
identity representation satisfies the condition $(S)$. 

\begin{corollary}\label{part2:chap2:sec4:coro3}%co 3
  If $M$ is the identity representation of $K$, then the algebra
  $L_M(G)$ is commutative. 
\end{corollary}

The algebra $L_M(G)$ has complete system of representations of $\dim
1$. Therefore if $x$ and $y$ are any two elements of $L_M(G)$, then
$U^\lambda(x~y) = U^\lambda(y ~x)$ for every $\lambda\in \triangle^*$,
because $U^\lambda$ is of dimension $1$. Therefore $U^\lambda(xy - yx)
= 0$ for every $\lambda$ in $\triangle^*$. But this is possible only
if $xy - yx = 0 $ \iec  the algebra $L_M(G)$ is commutative. 

 Finally we try to find out what are the various representations of
 $K$ which satisfy the condition $(S)$. It is obvious that a
 representation which satisfies the condition $(S)$ when restricted to
 $N \cap K$ contains the\pageoriginale identity representation of $N \cap K$. It is
 not known whether there exist or not representations of $K$ which
 when restricted to $N \cap K$ contain the identity representation but
 which do not satisfy the condition $(S)$. However in this connection
 we have the following result. 
 
\begin{theorem}\label{part2:chap2:sec4:thm4}%the 4
   Every irreducible representation $M$ of $K$ which comes from a
   representation of $GL_n(\mathscr{O} /\mathscr{Y})$ and the
   restriction of which to $N \cap K$ contains the identity
   representation of $N\cap K$ satisfies the condition $(S)$. 
\end{theorem} 
 
 It can be easily proved that $GL_n(\mathscr{O}/\mathscr{Y})$ is
 isomorphic to $K/H$, where $H$ is a normal subgroup $K$ consisting of
 the matrices $(\delta_{ij}+a_{ij})$ where $a_{ij}$ belongs to
 $\mathscr{Y}$. Therefore a representation of
 $GL_n(\mathscr{O}/\mathscr{Y})$ gives rise to a representation of
 $K$. 

\begin{remark*}%Rem
  We have proved that in the case of real or complex general linear
  group the representations induced by the unitary characters of $T$
  form a complete system of representations of algebra $L(G)$. But in
  the case of general linear groups over $p$-adic fields the
  representations induced by the characters of $\triangle$ do not form
  a complete system. In fact the algebra $L(K)$ is a sub-algebra of
  $L(G)$, because $K$ is open and compact in $G$. Therefore if the
  representations $U^\lambda$ form a complete system for $L(G)$, their
  restrictions to $K$ will form a complete system of representations
  of $L(K)$. But the restriction of $U^\lambda$ to $K$ is a
  representation of $K$ induced by the unit character of $N\cap K$,
  therefore by Frobenius reciprocity theorem the irreducible
  representations of $K$ which occur in $U^\lambda$ are precisely
  those which when restricted to $N\cap K$ contain the identity
  representation. But there exist representations of $K$ for which
  this property is not satisfied. 
\end{remark*} 

\section{Some Problems}\label{part2:chap2:sec5}

\heading{I.}

For\pageoriginale any classical group, we have found a maximal
compact sub-group 
$K$. If $G$ is the general linear group, it is easy to see that: 

(i) \textit{any maximal compact subgroup is conjugate to $K$ by an
  inner automorphism;} 

(ii) \textit{any compact subgroup is contained in a maximal compact
  subgroup}. (For, let $H$ be a compact subgroup of $GL(n,\tilde{P})$
  let $e_1,\ldots,e_n$ be the canonical basis of $\tilde{P}^n$. Let
  $I_0$ be the $\tilde{O}$-module generated by the $e_i$ and let $I$
  be the $\tilde{O}$-module generated by the $h~e_i$ for $h \in H$:
  because $H$ is compact, the coordinates of the $h.e_i$ are bounded
  and there is an integer $n\geq 0$ such that $I \subset
  \tilde{\pi}^{-n}I_0$. Hence $I$ is a lattice and $H$ is contained in
  the maximal compact subgroup $K_1$ formed by the $g \in G$ such that
  $g.I = I$. Moreover, if $g \in G$ is such that $g.I_o =I$, then $K_1
  = gKg^{-1}.)$ 

  But for the other types of classical groups, it is not known if the
  results (i) and (ii) are true or not. Actually, one cannot hope that
  (i) is true: already in $SL(n,P)$, we have only: 

  (i bis) \textit{any maximal compact subgroup is conjugate to $K$ by
    an (not necessarily inner) automorphism.} 
  
  It seems possible that there exist several but a finite number of
  class\-es of maximal compact subgroups: for instance, it seems
  unlikely that the maximal compact subgroup $K'$ of the orthogonal
  group $0(n,P)$ which leaves invariant a maximal lattice of norm
  $\mathscr{P}$ is conjugate to $K$. But perhaps, any maximal compact
  subgroup of $0(n,P)$ is conjugate to $K$ or to $K'$. 

  It may be noted that (i) and (ii) are not both true in the
  \textit{projective} group\pageoriginale $G = PGL(2,P)$: a maximal compact subgroup
  $K$ is the canonical image of $GL(2,0)$ in $G$; the determinant
  defines a map $d$ from $G$ to the quotient group $P^*/(P^*)^n$ and
  the image of any conjugate of $K$ is contained in the image $D$ of
  $0^*$ in $P^*/(P^*)^2$. Now, let $u$ be the image of
  $\begin{pmatrix}0 & \pi\\ 1 & 0 \end{pmatrix}$ in $G$: we have $u^2
  = 1$ and $d(u)\notin D$. Hence, $u$ generates a compact subgroup which
  is \textit{not} contained in any conjugate of $K$. 

\heading{II.} 

It seems very likely that our results about classical groups are
  valid for any semi-simple algebraic linear group over $P$ (at least
  if char $P = 0$). The general meaning of the subgroups $N,D,T~
  \Gamma$ is clear: $N$ is a maximal unipotent, $D$ is a maximal
  decomposed torus (a decomposed torus is an algebraic group
  isomorphic to $(P^*)^r$), which normalised $N$. Then $D$ can be
  written as $D = \triangle.U$, where $\triangle\approx Z^r$ and
  $U\approx (0^*)^r$ and we have $T=\triangle.N$. The subgroup
  $\Gamma$ is the normaliser of $N$. It can be proved
  (A. Borel, unpublished) that $D$ and $N$ exist in any such $G$ (at
  least if the base field $P$ is perfect) and are unique, upto an
  inner automorphism. Now the problems are: 
  \begin{enumerate}[(i)]
  \item define a maximal compact subgroup $K$;
  \item prove that $G = T.K$;
  \item prove that $G = K.\triangle.K$ and define $\triangle_+$ (which
    is certainly related with the Weyl group and the Weyl chambers); 
  \item prove the key Lemma about the intersection $Nd_\alpha~K \cap
    Kd_{\beta} K$. For (i), the simplest idea is to take a lattice I
    in the vector space in which $G$ acts, and to put $K = \{ g|g \in
    G,g.I = I\}$. Then we get a compact subgroup. But it is obvious
    that $K$ will be maximal and satisfy (ii)\pageoriginale and (iii)
    only if I is conveniently chosen. 
\end{enumerate}

  Assume that char $P =0$: then we may consider the Lie algebra
  $\mathscr{G}$ of $G$ and the adjoint representation. Then we can
  choose a lattice $I$ in $\mathscr{G}$ such that $[I,I] \subset I$
  (in other words, $I$ is a Lie algebra over $0$);such a lattice
  always exists: take a basis $\mathscr{G}$ and multiply it by a
  suitable power of $\pi$ in such a way that the constants of
  structure become integral. Now there exist such lattices which are
  maximal, because $[I,I]\subset I$ implies that $I$ is a lattice of
  norm $\subset 0$ for the Killing form of $\mathscr{G}$. As this form
  is non-degenerate, it is impossible to get an indefinitely growing
  sequence of such lattices. Hence we can choose such a maximal
  lattice $I$ and put $K = \{ g | g \in G, g.I = I\}$. 

  But let us look at the \textit{compact} case: it can be shown that $G$
  is compact if any only if the Lie algebra $\mathscr{G}$ has no
  nilpotent elements. In this case, we should have $K = G'$. So we are
  led to the following conjectures: 
  \begin{Conjecture}\label{part2:chap2:sec5:conj1}%cor 1
    there is a unique lattice in $\mathscr{G}$ which is a maximal Lie
    subalgebra over $0$; 
  \end{Conjecture}
  
  \begin{Conjecture}\label{part2:chap2:sec5:conj2}%cor 2
    the set $I$ if the $X \in \mathscr{G}$ such that the characteristic
    polynomial of the operator $ad ~X$ has its coefficients in $0$, is a
    Lie subalgebra over $0$; 
  \end{Conjecture}
  
  \begin{Conjecture}\label{part2:chap2:sec5:conj3}%cor 3
    (A.Weil): any algebraic simple compact group over a locally compact
    $P$-adic field of characteristic zero is (up to finite groups) the
    quotient of the multiplicative group of a {\em division algebra}
    $Q$ over $P$ by its center. 
  \end{Conjecture}
  
  It\pageoriginale is easy to prove that (\ref{part2:chap2:sec3:eq3})
  implies (\ref{part2:chap2:sec3:eq2}): the Lie algebra 
  $\mathscr{G}$ is the quotient of the Lie algebra $Q$ by its center and
  the $X \in I$ are exactly the images of the integers of $Q$. It is
  obvious that (\ref{part2:chap2:sec3:eq2}) implies
  (\ref{part2:chap2:sec3:eq1}), because any 
  Lie subalgebra over $0$ is 
  contained in $I$. Moreover, (\ref{part2:chap2:sec3:eq3}) is true for the classical groups: we
  have only for compact groups the groups $PGL_1(\tilde{P})\approx
  \tilde{P}^* $ / center and the orthogonal and unitary groups for an
  anisotropic form; but $0_1$ and $0_2$ are abelian $0_3$ gives the
  quaternion field, $0_4$ is not simple, etc. But one does not know a
  general proof of (\ref{part2:chap2:sec3:eq3}). 
  
  On the other hand, we can look at the ``anticompact'' case, that is the
  case of the groups defined by Chevalley in (12). Then the results (i)
  to (iv) can be proved (for the definition of $K$ and proof of (ii),
  see Bruhat (10); for (iii) and (iv), my results are not yet
  published). 
  
  Then \textit{if} one can prove one of the above conjectures, one can
  hope to generalize these results to any semi-simple group by an
  argument by induction on the dimension of a maximal nilpotent
  subalgebra of $\mathscr{G}$. 
  
 \heading{III. Extension to the representations of $K$  which do not
  satisfy the condition $(S)$}. 

  This problem is related with the construction of other
  representations of $G$: we have seen that the representation
  $U^\lambda$ do not form a complete system. Hence, by the
  Gelfand-Raikov theorem, there certainly exist other irreducible
  unitary representations of $G$. 

  We have two indications: first the case of a real semi-simple Lie
  group $G$. It seems very likely that to any class of Cartan subgroups
  $H$ of $G$, corresponds a series of representations of $G$, indexed by
  the characters of $H$. This has been verified in some particular cases
  (of.Harish-Chandra and\pageoriginale Gelfand-Graev). In particular, assume that
  there exists a compact Cartan subgroup $H$: then in many cases (more
  precisely in the cases where $G/K$ is a bounded homogeneous domain in
  the sense of $E$. Cartan ($K$ is a maximal compact subgroup)), we can
  get irreducible unitary representations of $G$ in the following way:
  take a character $\lambda$ of $H$. take the unitary induced
  representations $U^\lambda$ in the space $\mathscr{H}^\lambda$ ; this
  representation is not irreducible. But we have a complex-analytic
  structure on $G/H$ and we can look at the subspace of
  $\mathscr{H}^\lambda$ formed by the functions which correspond to
  \textit{holomorphic} functions on $G/H$. Then we get an irreducible
  representation (of (22) or (21). This is in particular true for
  compact semi-simple Lie groups (after Borel-Well,of (32)). 
  
  On the other hand, in the case of classical linear groups over a
  \textit{finite} field, for instance for the special linear group $G$
  with $2,3$ or $4$ variables, one knows all the irreducible
  representations of $G$ and one sees that to each class of Cartan
  subgroup $H$, corresponds a series of representations indexed by the
  characters of $H$ (of Steinberg (33)). But one does not know how
  exactly this correspondence works. It seems likely that the
  representation $U(\lambda)$ associated with character $\lambda$ of $H$
  is a subrepresentation of the induced representation $U^\lambda$, and
  it would be extremely interesting to get a ``geometric" definition of
  $U(\lambda)$. 
  
  If one could get such a definition, it would perhaps be possible to
  generalize it to the algebraic simple linear groups (or at least to
  the classical groups) over a $p$-adic field. 
  
\heading{IV. Study of the algebra of spherical functions}
  
  Let\pageoriginale $M$ be the unity representation of $K$ and Let $A$ be the
  algebra $L_M(G)$: by our results,this is a \textit{commutative}
  algebra. It seems possible to determine completely the structure of
  $A$. The representations $U^\lambda$ likely give all the characters
  $\hat{\lambda}$ of $A$. The $\lambda$ describe a space isomorphic to
  a space $C^r$ and the map a $\rightarrow (\hat{\lambda}(a))$ is
  probably an isomorphism of $A$ onto the algebra of polynomials on
  $C^r$ which are invariant by the Weyl group of $G$. (It seems that a
  recent work by Satake (unpublished) gives a positive answer). 
  
\heading{V. Computation of the ``characters'' of the $U^\lambda$}.

  The representations $U^\lambda$ are ``in general'' irreducible (of
  (10)). Moreover, if $f$ is a continuous function on $G$, with
  carrier contained in $K$, and if $f$ belongs to some $L_M(K)$, then
  it is trivial to show that the operator $U^\lambda_f$ if of
  \textit{finite rank}, and hence has a \textit{trace}. The same is
  obviously true if $f$ is a finite linear combination of translates
  of such functions. But the space of those $f$ is exactly what $I$
  called the space of ``regular" functions of $G$ (space $D(G)$) and
  the map $f \rightarrow \Tr ~U^\lambda_f$ is a ``distribution" on
  $G(of(10))$. A problem is to compute more or less explicitly this
  distribution (which is the ``character" of $U^\lambda$. It seems
  likely that, at least on the open subset of the ``regular" elements
  $g$ of $G$ it is a simple function of the  proper values of $g$ (by
  analogy with the case of complex or real semi-simple Lie groups, of
  works of Harsih-Chandra and Gelfand-Naimark).
