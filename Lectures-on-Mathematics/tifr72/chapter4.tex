\part{Topics in Prime Number Theory}\label{part2}

\chapter{Zero-Free Regions for $\zeta (s)$ and $L(s, \chi)$}\label{chap4}

WE\pageoriginale NOW TURN to the applications of the results obtained in the
preceding chapters to some basic problems in the theory of the
distribution of prime numbers. As the first of such applicatons we
shall show in this chapter that, to some extent, the sieve method can
take the place which has long been occupied solely by the complex
variable method in the investigations of the fundamental properties of
$\zeta (s)$ and $L (s, \chi)$. More precisely, we shall demonstrate
that, by employing, instead, the Selberg sieve for multiplicative
functions, the classical function-theoretical convexity argument can
be dispensed with in deducing Vinogradov's zero-free region and
Page-Landau-Siegel-Linnik's theorem from the relevant elementary
estimates of the zeta-and L-funtions. 

We shall     also dwell on the Brun-Titchamarsh theorem; this is
included here, because of its relation with the exceptional zeros of
L-functions. 

\section{Vinogradov's Zero-Free Region for $\zeta
  (s)$}\label{chap4-sec4.1} %sec 4.1 

In order to extract the informations on the distribution of prime
numbers from the Euler product for $\zeta (s)$ which connects prime
numbers with a fairly smooth analytical expression, we need
to\pageoriginale extend 
the zero-free region of $\zeta (s)$ as far as possible to the left of
the line $\sigma = 1$. Probably the simplest way to get an effective
zero-free region of this sort is the one due to Landau; he deduced,
for $t > 2$, 
\begin{equation*}
  \frac{\zeta '}{\zeta} (s) = O((\log t)^7) \text{ for } \sigma > 1 - c
  (\log t)^{-9} \tag{4.1.1} \label{eq4.1.1}
\end{equation*}
from de la Vallee Poussin's inequality 
\begin{equation*}
  \zeta^3 (\sigma ) | \zeta (\sigma + it)^4 \zeta (\sigma + 2 it) | >
  1 (\sigma > 1). \tag{4.1.2} \label{eq4.1.2}
\end{equation*}

For the sake of a later purpose, we stress that \eqref{eq4.1.1} is an
elementary result in the sense that in deriving it we do not need to
appeal to the complex variable method. 

Although \eqref{eq4.1.2} already yields a relatively good estimate of the
error-term in the prime number theorem, to get finer results we have
to seek for a wider zero-free region. And a general theorem of Landau
which is a consequence of Hadamard-Borel-Caratheodory's convexity
theorem, thus much involve in the complex variable method, converts
our problem to the one of estimating $\zeta (s)$ in the vicinity of
$\sigma = 1$. Further, the elementary formula 
\begin{equation*}
  \zeta (s) = \sum_{n < N} n^{-s} + \frac{N^{1-s}}{s-1} + O
  (N^{-\sigma}) \tag{4.1.3}\label{eq4.1.3} 
\end{equation*}
which holds uniformly for $\sigma > 0$, $| t | < N$ reduces it to that
of\pageoriginale the sum 
$$
\sum_{n<N} n^{it}.
$$

And for our purpose, it is desirable to have an estimate which is
particularly effective for those $N$ much smaller than $| t |$. In
this context, the following \textit{purely elementary} result of
Vinogradov is the hitherto best one: 

\begin{Lemma}\label{chap4-lem17}%lemma 17
For $N < ct, ~ t > 2$, we have
  $$
   \sum_{n<N} n^{it} < < N \exp \left(-c \frac{\log^3 N}{\log^2 t}\right).
  $$
\end{Lemma}

This yields, via the general theorem of Landau mentioned above, 
\begin{equation*}
\begin{split}
\frac{\zeta '}{\zeta} (s) &= 0\left((\log t)^{2/3} (\log \log
  t)^{1/3} \right)\\ 
&\quad\text{for } \sigma > 1 - \frac{c}{(\log t)^{2/3} (\log \log t)^{1/3}}
(t \geq 3),
\end{split}\tag{4.1.4}\label{eq4.1.4}
\end{equation*}
which is the deepest zero-free region for $\zeta (s)$ known at
present, and has had profound influence on diverse problems involving
prime numbers. 

Since its discovery, it has long been maintained that Vinogradov's
zero-free region \eqref{eq4.1.4} represents one of the most important
analytical properties of $\zeta (s)$, partly because only Hadamard's
global theory of integral functions and the convexity principle of the
Bor\'el-Carath\'eodory type have been able to derive it\pageoriginale
from the result 
stated in LEMMA \ref{chap4-lem17}. We can, however, break away from
this prevalent 
notion in the theory of the zeta-function, for, as we shall show
below, there exists an elementary argument with which we can deduce a
result of the same depth as \eqref{eq4.1.4} from LEMMA \ref{chap4-lem17}. 

Our proof of the last assertion depends largely on a special instance
of the Selberg sieve for multiplicative functions as well as an
auxiliary result (\eqref{eq4.1.6}  below) from the theory of elementary
proofs of the prime number theorem with remainder term. 

We begin our discussion by making the second point explicit. We shall
require a special case of THEOREM \ref{part1-chap1:sec1.3:thm4}, and as we have
remarked in \S~ \ref{part1-chap1:sec1.3} the necessary upper bound for $R_d
(x) $ (cf. \eqref{eq1.3.8}) can also 
be obtained in an elementary manner. Here we prove this fact for the
case $k = 1$ only, since this is sufficient for our present purpose. 

We see from \eqref{eq1.3.8} (with $k = 1$) that, retaining the notations of
\S~ \ref{part1-chap1:sec1.3}, it suffices to deal with the sums  
\begin{equation*}
  \sum_{\substack{u < x \\ (u, d) = 1}} \mu (u)u^{-\eta},
  \sum_{\substack{u < x \\ (u, d) = 1}} \mu (u) u^{-\eta} \log u,
  \tag{4.1.5} \label{eq4.1.5}
\end{equation*}
where $x < z$ can be assumed to be sufficiently large. A routine
argument transforms the first sum into the expression: 
\begin{multline*}
  \sum_{\substack{uf < x \\ f | d^{\infty}  \\ f< \sqrt{x}}} \mu (u)
  (uf)^{-\eta} + 0 \left(\sum_{\substack{uf < x \\ f | d^\infty \\ f \geq
      \sqrt{x}}} (uf)^{-\eta}\right) \\ 
  = \sum_{\substack{_f | d^{\infty}  \\ f< \sqrt{x}}} f^{-\eta} \left\{
  \zeta (\eta )^{-1} - \sum_{u > x/f} \mu (u) u^{-\eta}  \right\} + 0
  \left(x^{-\frac{1}{10}} \prod_{p | d} \left(1 +
  \frac{1}{\sqrt{p}}\right)\right).  
\end{multline*}\pageoriginale

Then we appeal to the elementary estimate
\begin{equation*}
  \sum_{n < y} \mu (n) n^{-1} < < (\log y)^{-1} (y \geq
  2). \tag{4.1.6}\label{eq4.1.6} 
\end{equation*}

This implies
$$
\sum_{u > x/f} \mu (u) u^{-\eta} << \left(\frac{x}{f}\right)^{1-\eta}
(\log x)^{-1}, 
$$
whence the first sum in \eqref{eq4.1.5} is 
$$
<< (\log x)^{-1} \prod_{p | d} \left(1 + \frac{1}{\sqrt{p}}\right).
$$

In much the same way, we can show that the second sum in \eqref{eq4.1.5} is 
$$
<< \prod_{p | d} \left(1 + \frac{1}{\sqrt{p}}\right).
$$

Inserting these into \eqref{eq1.3.8}, $k = 1$, we immediately obtain an
elementary account of THEOREM \ref{part1-chap1:sec1.3:thm4} for the case $k = 1$. 

Now, to make explicit the first point, i.e. the sieve aspect of our
argument we have to make a rather lengthy preparation;  the complexity
is caused mainly by our elementary treatment of various estimates. 

First,\pageoriginale we introduce two parameters $\delta$ and $B$ such that 
\begin{equation*}
  (\log t)^{-10} \leq \delta \leq (\log t)^{-2/3}, (\log \log g)^{1/3}
  \geq B > 0 \tag{4.1.7} \label{eq4.1.7}
\end{equation*}
and we shall assume always in the sequel that $B$ and $t$ are
sufficiently large. We shall use the notations 
\begin{align*}
  Y(t) &= (\log t)^{2/3} (\log \log t)^{1/3},\\
  Q(t) &= \exp (Y(t)) (\log \log t)^{2/3},\\
  E(t) &= (\log t)^c | \zeta (1 - \delta + it) | + (\log t)^{-cB^3};
\end{align*}
the last one allows us to use the convention:
$$
0\{ (\log t)^c E(t) \} = 0 \{ E(t) \}.
$$

Further, we introduce the multiplicative function
$$
f(n) = | \sigma (n, - \delta - it) |^2,
$$
where $\sigma (n, a)$ is the sum of the a-th powers of divisors of n;
here, we should note that as a special case of an identity due to
Ramanujan we have, for $\sigma > 1$, 
\begin{equation*}
  \sum_{n=1}^\infty f(n)n^{-s} = \zeta (s) \zeta (s + 2 \delta ) \zeta
  (s + \delta - it) \zeta (2(s + \delta
  ))^{-1}. \tag{4.1.8} \label{eq4.1.8} 
\end{equation*}

Afterwards, we shall apply the Selberg sieve to $f$, and for this
sake, we prove first the following lemma; the argument employed in the
proof is the one common in the problems pertaining to sums of divisor
functios, and so we may be brief. 
\begin{Lemma}\label{chap4-lem18}%lemma 18
  Let\pageoriginale
  $$
  H(x) = \sum_{n<x} f(n) n^{-1 + 2\delta}
  $$
  and
  $$
  \mathscr{F} = \zeta (1+ 2\delta) | \zeta (1+\delta + it) |^2 \zeta
  (2(1+\delta ))^{-1}. 
  $$
\end{Lemma}

Then we have 
$$
H(x) = (2\delta)^{-1} \mathscr{F} \times^{2 \delta}(1+O(E(t)))
$$
provided
$$
Q(t) \geq \times \geq \exp (BY(t)).
$$

To show this, we note that \eqref{eq4.1.8} implies
$$
f(n) = \sum_{d^2 d_1 k_2 d_3 d_4 = n} \mu (d) d^{-2 \delta} d_2^{-2
  \delta} d_3^{-\delta + it} d_4^{-\delta - it} 
$$
so
\begin{equation*}
  H(x) = \sum_{d < x^{\frac{1}{4}}} \mu (d) d^{-2(1- \delta )} H_1
  (xd^{-2}) + O(x^{-1/5}), \tag{4.1.9} \label{eq4.1.9}
\end{equation*}
where
$$
H_1 (y) = \sum_{d_1 d_2 d_3 d_4 < y} d_1^{-1} d_2^{-1 + 2 \delta }
d_3^{-1 + \delta + it} d_4^{-1 + \delta - it}.  
$$

We decompose $H_1 (y)$ into three parts as follows:
\begin{align*}
  H_1(y) &= \sum_{u v \leq \sqrt{y}} u^{-1} v^{-1 + 2 \delta}  K\left(1-
  \delta it, ~ 1- \delta - \text{it}; \frac{y}{uv}\right) \\ 
  &+ \sum_{u v \leq \sqrt{y}} u^{-1+\delta + \text{it}} v^{-1 + \delta - it} K
  \left(1- 2 \delta, 1; \frac{y}{uv}\right) \\ 
  &- K(1- \delta + it, ~ 1-\delta - it; \sqrt{y}) K (1-2\delta,  1;
  \sqrt{y} ),\tag{4.1.10}\label{eq4.1.10} 
\end{align*}
where\pageoriginale
$$
K (s, w; y) = \sum_{mn \leq y} m^{-s} n^{-w}.
$$

Similarly, we have
\begin{gather*}
  K (s, w; y) = \sum_{m \leq \sqrt{y}} m^{-s} U \left(w, \frac{y}{m}\right) +
  \sum_{n \leq \sqrt{y}} n^{-w} U \left(s, \frac{y}{n}\right)\\ 
  - U (w, \sqrt{y}) U (s, \sqrt{y}). \tag{4.1.11}\label{eq4.1.11}
\end{gather*}
where
$$
U (s, y) = \sum_{n \leq y} n^{-s}.
$$

Hence the problem is reduced to an asymptotic evaluation of $U (s,
y)$ at the points $s = 1$, $1 - 2 \delta$,  $1 - \delta \pm$ it. The
first two cases give no difficulty; we have 
\begin{gather*}
  U (1, y) = \log y + \gamma + O\left(\frac{1}{y}\right),\\
  U (1 - 2\delta,  y) = \frac{1}{2 \delta} y^{2 \delta} + \zeta (1
  - 2 \delta ) + O(y^{-1 + 2 \delta}), \tag{4.1.12}\label{eq4.1.12}
\end{gather*}
where $\gamma$ is the Euler constant. As for the points $s = 1- \delta
\pm $ it  we require LEMMA \ref{chap4-lem17}. We set $N = t$ in
\eqref{eq4.1.3}, getting   
$$
U (\sigma + it, y) = \zeta (\sigma + it) - \sum_{y < n \leq t}
n^{-\sigma - it} + O(t^{-\sigma}), 
$$
but the lemma implies
$$
\sum_{y < n \leq t} n^{-\sigma - it } << \max_{y \leq \times \leq t}
\times^{1 - \sigma} \exp \left(-c \frac{(\log x)^3}{(\log t)^2}\right); 
$$
thus\pageoriginale we get
\begin{equation*}
  U(\sigma + \text{ it }, y)  =  \zeta (\sigma + \text{ it })+ o ((
  \log t)^{-cB^3}), \tag{4.1.13} \label{eq4.1.13}
\end{equation*}
provided
$$
\sigma \geq 1 -  \delta, Q(t) \geq y \geq \exp (BY (t)).
$$

In particular, we have, for $\sigma \geq 1 - \delta$,
\begin{equation*}
\zeta (\sigma + \text {it}) = o (\log^c t). \tag{4.1.14}\label{eq4.1.14}
\end{equation*}

Inserting \eqref{eq4.1.12} into \eqref{eq4.1.11} (but with $s =1 - 2
\delta, w = 1)$ we get  
\begin{align*}
  K(1-2 \delta, 1; y) & = \frac{1}{2} \log y \,U (1-2 \delta, \sqrt{y})
  + \left(\frac{1}{2} \log y + \gamma \right) \zeta (1-2 \delta )\\ 
  & + \frac{y^{2 \delta}}{2 \delta} U (1+2 \delta, \sqrt{y}) 
  + U' (1-2 \delta, \sqrt{y}) + 0 (y^{- \frac{1}{2}} \log^c t),
\end{align*}
where $U' (s, y) = \frac{d}{ds} U(s, y)$. But we have, for $y \leq Q(t)$,
$$
U' (1-2 \delta, \sqrt{y}) = \zeta ' (1-2 \delta) - \frac{y^ \delta}{4
  \delta} \log y + \frac{y ^ \delta}{4 \delta^2} + o (y ^{-
  \frac{1}{2}} \log^c t). 
$$

Hence we have, for $y \leq Q (t)$,
\begin{multline*}
  K(1- 2 \delta, 1; y) \\
  = \frac{1}{2 \delta} y^{2 \delta} \zeta (1+2 \delta) + (\gamma + \log
  y) \zeta (1-2 \delta) + \zeta ' (1-2 \delta) + o(y^{- \frac{1}{2}}
  (\log t)^c). 
\end{multline*}

On the other hand, noting \eqref{eq4.1.14}, we see that \eqref{eq4.1.13} gives
$$
K(\sigma + \text{it}; \sigma - \text{it}; y) = | \zeta (\sigma +
\text{it} |^2 + o(( \log 
t)^{-c B^3}) 
$$
on\pageoriginale the same condition as that for
\eqref{eq4.1.13}. Inserting these into 
\eqref{eq4.1.10}, we obtain 
\begin{align*}
  H_1 (y) & = (2 \delta)^{- 1} \zeta (1+2 \delta) | \zeta (1+ \delta +
  \text {it} |^2 y^{2 \delta}\\ 
  & - \zeta (1-2 \delta ) \sum_{mn \leq \sqrt{y}} m^{-1 + \delta +
    \text{it}} n^{-1 + \delta - \text{it}} \log mn + 0 (E (t)) 
\end{align*}
for $Q (t) \geq y \geq \exp (BY(t))$. But this sum over $m, n$ admits
the similar decomposition as \eqref{eq4.1.11}, and again by virtue of LEMMA
\ref{chap4-lem17} we can readily estimate it to be $0 (E(t))$ Hence we have 
$$
H_1 (y) = (2 \delta)^{-1} \zeta(1+ 2 \delta) | \zeta (1+ \delta + it)
|^2 y^{2 \delta} + 0(E(t)) 
$$
for $Q(t) \geq y \geq \exp (BY (t))$ Then by \eqref{eq4.1.19}, we obtain the
assertion of the lemma. 

Also we shall need
\begin{Lemma}\label{chap4-lem19}%lemma 19
  Let 
  $$
  I_D(x) = \sum_{\substack{n < x \\ (n, D) = 1}} f(n),
  $$
  and
  $$
  F_p = \sum_{m = 0}^{\infty} f(p^m ) p^{-m}.
  $$ 


Then we have
$$
 I_D(x) = \mathscr{F} \times \prod_{p | D} F^{-1}_p ( 1 + 0(E(t)))
$$
provided
$$
 \log \times >> \log D, Q(t) \geq \times \geq \exp (BY (t)).
$$
\end{Lemma}

This\pageoriginale corresponds to the condition $C_3)$ of  \S~
\ref{part1-chap1:sec1.4}.  The proof is 
quite similar to that of the preceding lemma, so we omit it. We should
remark, however, that at a point in the proof, we require the
following observation: 
\begin{multline*}
  F_p = \left(1-\frac{1}{p}\right)^{-1} \left(1 - \frac{1}{p^{1+2
      \delta}}\right)^{-1} | - 
  \frac{1}{p^{1+\delta + \text{it}}}|^{-2} \left(1-
  \frac{1}{p^{2(1+\delta)}}\right)\\
   > \left(1- \frac{1}{p^{2(1 + \delta )}}\right)^{-1},
\end{multline*}
whence
$$
F_p - 1 > p^{-2 (1 + \delta)}.
$$

This corresponds precisely to $(C_2) $ of \S~ \ref{part1-chap1:sec1.4}.

Now we consider the Selberg sieve for the multiplicative function $f$:
$$
\sum_{n \leq N} f(n) \left(\sum_{\substack{d | n \\ d < R}}
\Theta_d\right)^2 (\Theta_1 = 1). 
$$

By LEMMA \ref{chap4-lem19} and the general theory developed in \S~
\ref{part1-chap1:sec1.4}, we see 
that the optimal choice of $\Theta_d$ is given by 
\begin{equation*}
  \Theta_d = \mu (d) \frac{G_d (R/d)}{G_1 (R)} \prod_{p/d} F_p,
  \tag{4.1.15} \label{eq4.1.15}
\end{equation*}
where
$$
G_d(y) = ~ \sum_{\substack{r \leq y \\ (r, d) = 1}} \mu^2  (r)
\prod_{p | r} (F_p - 1). 
$$

And\pageoriginale this yields
\begin{equation*}
  \sum_{n \leq N} f(n) \left( \sum_{d | n} \Theta_d \right)^2 = \mathscr{F} G_1
  (R)^{-1} N (1 + o (E(t))) \tag{4.1.16} \label{eq4.1.16}
\end{equation*}
provided $Q(t) \geq N \geq R^{40} \geq \exp (BY(t))$.

On the other hand, the sieve-effect of \eqref{eq4.1.15} is embodied in the
assertion that 
{\fontsize{10pt}{12pt}\selectfont
\begin{equation*}
G_1 (R) \leq (2 \delta )^{-1} \mathscr{F} (1 + o(E(t)))
\tag{4.1.17}\label{eq4.1.17} 
\end{equation*}}\relax
provided $Q(t) \geq R \geq \exp (BY(t))$; this follows immediately
from LEM\-MA \ref{chap4-lem18}, if we note that 
$$
G_1 (R) \geq R^{-2 \delta} \sum_{n < R} f(n) n^{-1 + 2\delta}.
$$

Having these preparations at our hands, we can now proceed to our
elementary proof of Vinogradov's zero-free region. 

So, let us assume that $\zeta (s) $ takes a small value at $s = 1 -
\delta + $ it, or, more precisely, the ineqality 
\begin{equation*}
  |\zeta (1- \delta + \text{ it } | \geq (\log t)^{-A}
  \tag{4.1.18}\label{eq4.1.18} 
\end{equation*}
holds for a $\delta$ satisfying \eqref{eq4.1.7}; the value of $A$ is to be
fixed later but, for a while, let us take it for a large parameter. 

Using $\Lambda_d^{(1)}$ and $\Theta_d$ which are defined in THEOREM
\ref{part1-chap1:sec1.3:thm4} and at\break \eqref{eq4.1.15}, respectively, we put 
$$
\omega_d = \sum_{[d_1, d_2] = d} \Theta_{d_1} \Lambda^{(1)}_{d_2}, 
$$ 
so that, for all $n$,
\begin{equation*}
  \sum_{d | n} \omega_d = \left(\sum_{d | n} \Theta_d \right) \left( \sum_{d | n}
  \Lambda_d^{(1)}\right); \tag{4.1.19} \label{eq4.1.19}
\end{equation*}\pageoriginale
in particular, we have
\begin{equation*}
  \sum_{d | n} \omega_d = 
  \begin{cases}
    1 ~  \text{ if } ~ n = 1\\
    0 ~ \text{ if } ~ n \leq z.
  \end{cases} \tag{4.1.20}\label{eq4.1.20}
\end{equation*}

Further, we set
$$
\vartheta = 1, z = \exp (40 AY(t)), R = \exp (AY(t)), x = \exp
(100 AY(t))
$$ 
where $\vartheta, z$ occur in the definition of
$\Lambda_d^{(1)}$. 

Then we consider the sum
\begin{align*}
  Z & = ~ \sum_{n < x} \sigma (n, - \delta - \text{it}) \left(\sum_{d | n}
  \omega_d \right) n^{-1+ \delta - \text{it}}\\ 
  & = ~ \sum_{d < z^2 R} \omega_d d^{-1 + \delta - \text {it}} \sum_{n
    < \times /d} \sigma (dn, - \delta - \text{it}) n^{-1 + \delta -
    \text{it}}. 
\end{align*}

This inner-sum can be readily estimated to be
$$
0_A \left\{ (\log t)^c | \zeta (1- \delta + \text{ it }) | + (\log t)^{-cA^3}
\right\} 
$$
by appealing to LEMMA \ref{chap4-lem17}. Thus, if $A$ is sufficiently large, the
assumption \eqref{eq4.1.18} implies $Z = o (1)$, whence recalling
\eqref{eq4.1.20}, we have 
$$
\frac{1}{2} < \sum_{z \leq n < \times} f(n)^{\frac{1}{2}} | \sum_{d |
  n} \omega_d | n^{-1+\delta}. 
$$

Hence,\pageoriginale by \eqref{eq4.1.19} and Schwarz's inequality, we get
$$
1 <<_A \times^{2 \delta} \sum_{z \leq n < \times} f(n) \left( \sum_{d | n}
\Theta_d\right)^2 n^{-1} \sum^{\infty}_{n = 1} \left( \sum_{ d | n}
\Lambda_d^{(1)}\right)^2 n^{- \xi}, 
$$
where $\xi = 1 + Y(t)^{-1}$. But, by virtue of THEOREM
\ref{part1-chap1:sec1.3:thm4}, $k = 1$, 
with its elementary account given above, the last infinite sum is $0_A
(1)$. Then \eqref{eq4.1.16} gives 
$$
  1 <<_A \mathscr{F} \times^{2 \delta} Y (t) G_1 (R)^{-1} \{ 1 +
  (\log t)^c | \zeta (1- \delta + \text{it }) | \}. 
$$ 

Hence noting \eqref{eq4.1.17}, we infer that for an appropriately chosen $A$
the assumption \eqref{eq4.1.18} implies 
$$
1 << \delta \times^{2 \delta} Y(t),
$$ 
which is apparently equivalent to 
$$ 
Y(t)^{-1} << \delta.
$$

This and \eqref{eq4.1.1} give rise to the assertion:
\begin{theorem}\label{chap4-thm11}%them 11.
The estimate 
$$
\frac{\zeta'}{\zeta} (\sigma + \text{it}) = o((\log t)^c) \text{for }
\sigma > 1 - \frac{c}{(\log t)^{2/3} (\log \log t)^{1/3}} (t > 3) 
$$
is obtainable without using the theory of functions.
\end{theorem}

\section{The Deuring-Heilbronn Phenomenon}\label{chap4-sec4.2}

Now let us apply similar considerations to Dirichlet's $L$-functions.

The\pageoriginale important point in the study of the distribution of
zeros of $L (s, \chi)$ is that it is required to have results which
hold uniformly 
for varying $\chi$. This raises difficult problems, and the
incompleteness of our knowledge on $L(s, \chi)$ is reflected in the
fact that the following statement, the theorem of Page, Landau and
Siegel, is the best zero-free region for $L(s, \chi)$ known at
present. 

Let us denote by $Z(T)$ the set of all zeros of all $L(s, \chi)$ for
primitive $_\chi \pmod{q}$, $q \leq T$, which are in the region $| t |
\leq T, 0 < \sigma < 1$. Then, except for at most one element $\beta_1$ of
$Z(T)$, we have, for all $\rho \epsilon  Z(T)$, 
\begin{equation*}
  Re \rho < 1 - \frac{c_0}{\log T} \tag{4.2.1}\label{eq4.2.1}
\end{equation*}
where $c_0 > 0$ is effectively computable. This exceptional zero
$\beta_1 = \beta_1 (T)$ which may also be called $T$-exceptional, if
exists,  is real and simple, and comes from  $L(s, \chi_1)$ for a
unique real primitive character $\chi_1$. Further, for any fixed
$\epsilon  > 0$, there exists a $c(\epsilon ) > 0$ such that  
\begin{equation*}
  \beta < 1 - c(\epsilon  ) T^{- \epsilon }. \tag{4.2.2}\label{eq4.2.2}
\end{equation*}

\eqref{eq4.2.1}  is due to Page and Landau, and \eqref{eq4.2.2} to Siegel. The
non-existence of such exceptional zeros has never  been proved, and
indeed this seems to be one of the most difficult problems in analytic
number theory. It can be shown, however,\pageoriginale that if $\beta_1$ ever
exists, then a strange phenomenon occurs among other elements of
$Z(T)$. This was discovered by Deuring and Heilbronn in their effort
to determine the asymptotic behaviour of class numbers of imaginary
quadratic fields. Afterwards, Linnik succeeded in obtaining a
quantitative version of their finding, which he called the
Deuring-Heilbronn phenomenon, and runs as follows. 

There exists an effectively computable constant $c_1 > 0$ such that
for all $ \rho \epsilon  Z(T), \rho \neq \beta_1$, we have 
\begin{equation*}
  Re \rho < 1 - \frac{c_1}{\log T} \log \left(\frac{c_0e}{(1-\beta_1 ) \log
    T}\right). \tag{4.2.3} \label{eq4.2.3}
\end{equation*}

It seems worth remarking that this implies \eqref{eq4.2.2}. We note that, by
an obvious reason, we may assume that there is a zero $\beta_
\epsilon  + i_{\gamma_ \epsilon }$ of an $L(s, \chi_
\epsilon )$, $\chi_ \epsilon  (\mod q_ \epsilon )$, such that
$\beta_ \epsilon  > 1 - \epsilon $. Then let us take $T$ so large
that $ q_ \epsilon  < T$, $| \gamma_ \epsilon  | < T$, $\beta_
\epsilon  < 1 - c_0 (\log T)^{-1}$. This means that we may put $1-
\epsilon $ on the left side of \eqref{eq4.2.3},  and we get
\eqref{eq4.2.2}.  

It should be stressed that Linnik's result has the important feature
that a sieve estimate, i.e. the Brun-Titchmarsh theorem, played a
crucial r$\hat{o}$le in its proof. In this context, perhaps it may not
be surprising that we can show the following statement by means of
Selberg's sieve method. 

\begin{theorem}\label{chap4-thm12}%them 12.
Page - Landau-Siegel's theorem and the Deuring-Heil\-bronn
phenomenon\pageoriginale 
can be proved without appealing to the theory of functions. 
\end{theorem}

The proof is quite similar to that of THEOREM \ref{chap4-thm11}, save
for the point 
that we have to be careful in obtaining an elementary lower bound of
$L(s, \chi)$ for real $\chi$ in the vicinity of $s = 1$. Thus, to
avoid unnecessary repetition, we shall show only the main steps of our
argument. 

We observe first that modifying the reasoning employed in the proof of
\eqref{eq4.1.1} and using the well-known elementary result 
\begin{equation*}
  L(1, \chi ) > c q^{-\frac{1}{2}} (\log q)^{-1}
  \tag{4.2.4}\label{eq4.2.4} 
\end{equation*}
for real $_\chi \pmod{q}$ it can be shown easily that, for all $\rho
\epsilon  Z(T)$, Re $\rho < 1 - T^{-2}$, provided $T$ is
sufficiently large as we assume hereafter. 

Now let $1 -  \delta + i \tau \epsilon  Z(T)$ be a zero of $L (s,
\psi)$; we may assume of course that $T^{-2} \le \delta \le 1/4$,
say. We put 
$$
h(n) = | \sum_{d | n} \psi (d) d^{- \delta - i \tau } |^2.
$$

Then we have
\begin{align*}
  &\sum_{n = 1}^{\infty} h (n) n^{-s} \\
  &= \zeta (s) L(s + 2 \delta,  \psi_0 ) L (s + \delta + i \tau,  \psi
  ) L (s + \delta - i \tau, \bar{\psi}) L (2(s + \delta ), \psi_0
  )^{-1}, 
\end{align*}
where\pageoriginale $\sigma > 1$ and $\psi_0 = \psi \bar{\psi}$. And
we consider the 
Selberg sieve for the multiplicative function $h$: 
$$
\sum_{n < N} h(n) \left( \sum_{\substack{d | n \\ d < R}} \Theta'_d\right)^2
\quad (\Theta'_1 = 1). 
$$

The optimal value of $\Theta'_d$ can be found by the argument of \S~
\ref{part1-chap1:sec1.4}, and we can infer that it yields the estimate 
\begin{equation*}
  <<_B \delta N \tag{4.2.5}\label{eq4.2.5}
\end{equation*}
for the last sum, if $N \geq R^4 \geq T^B$ with sufficiently large
$B$. This is proved in much the same way as in the case of $ \zeta
(s)$; in fact, we need only \eqref{eq4.1.3} and its analogue for $L(s,
\chi)$. 

Then, as before, we define $\omega '_d$ by
$$
\sum_{d | n} \omega '_d = \left(\sum_{d | n} \Theta'_d\right)
\left(\sum_{d | n} \Lambda_d^{(1)}\right). 
$$

And we set $\vartheta = 1, z = T^{4A}, R = T^A, \times = T^{100A}$
with a large constant $A$. After some elementary estimations we have,
for any non-principal $_\chi \pmod{q}, q \leq T$, 
\begin{multline*}
  \sum_{z \leq n < \times}  \chi (n) \left(\sum_{d | n} \psi (d) d^{-
    \delta - \text {it}}\right) \left(\sum _{\ell | n} \omega
  '_\ell\right) n^{-s}\\  
  = -1 + K(s, \chi ) M(s, \chi ) + o (T^{-cA}), \tag{4.2.6}\label{eq4.2.6}
\end{multline*}
provided
\begin{equation*}
  Re(s) \geq 3/4, | Im(s) | \leq T. \tag{4.2.7}\label{eq4.2.7}
\end{equation*}

Here\pageoriginale
$$
K(s, \chi) = L(s, \chi) L(s + \delta + i \tau), \chi \psi)
$$
and
$$
M(s, \chi) = \sum_{d < z^{2} R} \omega '_{d} \chi (d) d^{-s} \prod_{p
  | d} \left(1 + \frac{\psi (p)}{p^{\delta + i \tau}} - \frac{\chi \psi
  (p)}{p ^{s + \delta + i \tau}}\right). 
$$

Now let $\rho = \beta + i \gamma$ be a zero of $K(s, \chi)$ in the
region \eqref{eq4.2.7}.  Then setting $s = \rho$ in \eqref{eq4.2.6}, we get  
$$
\sum_{z \leq n < \times} h(n)^{\frac{1}{2}} | \sum_{\ell | n} \omega
'_\ell | n^{-\beta } \geq \frac{1}{2}. 
$$

Thus, by virtue of \eqref{eq4.2.5} and THEOREM \ref{part1-chap1:sec1.3:thm4}, 
$k = 1$, we obtain 
\begin{equation*}
  1 << \delta T^{20 A (1- \beta )} \log T. \tag{4.2.8}\label{eq4.2.8}
\end{equation*} 
 
 We now observe that either if $\psi$ is complex, if $\psi$ is real,
 non-principal and $\tau \neq 0$, or if $\psi$ is real, non-principal,
 $\tau = 0$ and $1 - \delta$ is a multiple zero of $L(s, \psi)$, then
 we have $K(1-\delta + i \tau, \psi) = 0$. Namely, in these cases, we
 may put $\rho = 1-\delta + i \tau$ in \eqref{eq4.2.8}, getting 
 $$
 \delta > \frac{c}{\log T}.
 $$
 
 But if $\psi$ is trivial we have already proved this, in fact much
 more, in the previous section. Thus, in the remaining case $\psi$ is
 real, non-principal, and $1 -  \delta$ is a simple zero of $L(s,
 \psi)$. Here we may assume obviously that $\delta \le c' (\log
 T)^{-1}$ with a certain small constant $c' > 0$. Then \eqref{eq4.2.8} implies
 that all elements of $Z(T)$\pageoriginale except for $1  - \delta$ are in the
 region 
$$
\sigma < 1 - \frac{c''}{\log T}
$$
with an effectively computable $c'' > 0$. This proves \eqref{eq4.2.1}. Finally,
if $1 -  \delta$ is the T-exceptional zero then \eqref{eq4.2.8} implies the
Deuring-Heilbronn phenomenon \eqref{eq4.2.3}. This ends the proof of the
theorem. 

\section{The Brun-Titchmarsh Theorem}\label{chap4-sec4.3}%sec 4.3

The undesirable possibility of the existence of exceptional zeros
causes much trouble in most applications of Page-Landau's theorem;
thus many attempts to eliminate this defect in the theory of
$L$-functions have been made from various directions. Among them is a
sieve-theoretical one which, despite not much prospect of its
success, seems to be worth describing explicitly because of its
simplicity as well as the completeness of the hypothetical assertion
deduced by it. 

This idea rests on the plausibility of the estimate 
\begin{equation*}
  \pi (\times ; k, \ell ) < (2 -  \eta ) \frac{\times}{\varphi (k)
    \log (\times / k)} (k < \times^\xi ) \tag{4.3.1} \label{eq4.3.1}
\end{equation*}
with some effective constants $\eta, \xi > 0$. From this, we can deduce
the non-existence of exceptional zeros. The proof is quite simple. In
fact, let us assume that $L (s, \chi)$, real $_\chi \pmod{k}$, has a
real zero $1 - \delta$. Then we put  
$$
b (n) = \sum_{d|n} \chi^{(d)}d^{- \delta}
$$
which\pageoriginale is positive and multiplicative. We apply Selberg's sieve to $b(n)$: 
$$
I(N)= \sum_{n < N} b (n) \left(\sum_{\substack{d|n\\ d<r}}
\lambda_d\right)^2 (\lambda_1 = 1). 
$$

By an argument similar to (in fact, much simler than) that of the
preceding section, we can infer that, for $N \ge R^4 \ge k^c$, the
optimal chioce of $\lambda_d$ gives $I(N) \ll \delta N$. On the other
hand, we have 
\begin{align*}
 I(N) & > \sum_{R \le p < N} b(p)\\
 &\ge \pi (N) - \pi (R) - \sum_{\substack{ p<N\\ \chi(p) =-1}} 1.
\end{align*}

Thus the hypothetical estimate \eqref{eq4.3.1} implies
\begin{align*}
  I(N) & > (1- o(1)) \frac{N}{\log N} - \frac{\varphi(k)}{2} (2-\eta)
  \frac{N}{\varphi(k) \log N/k}\\ 
  & > \frac{\eta}{3} \frac{N}{\log N}
\end{align*}
provided $\log N \gg_{\xi,\eta} \log k$, whence $\delta \gg_{\xi,\eta}
(\log k)^{-1}$ or the non-existence of the exceptional zeros as
claimed.  

But we have \eqref{eq1.2.15} which is close to
\eqref{eq4.3.1}. Because of this fact, 
considerable efforts have been spent to improve upon \eqref{eq1.2.15}, and
they are closely connected with the development of the\pageoriginale
sieve method 
itself. And the purpose of this section is to see how far the modern
account of the linear sieve takes us on this matter. 

Precisely speaking, we are going to improve upon \eqref{eq1.2.14} a
generalised version of the Brun-Titchmarsh theorem. For this sake, we
shall first show briefly a special instance of the hybridization of
Iwaniec's linear sieve and the multiplicative large sieve.  

Thus, let $(k,\ell) = 1$ and put
$$
S(x,z, \chi) = \sum_{\substack{ r \equiv \ell \pmod{k} \\ (r,P(z)) = 1
    \\ r < x}} \chi(r)a_r, 
$$
where $\{a_r\}$ are arbitrary complex number, and as usual $P(z)$ is
the product of all primes less then $ z > 2$. And we consider the
estimation of the expression 
$$
\sum_{\chi  \epsilon  \square } |S(x,z, \chi)|^2,
$$
where
$$
\square  = \{ X;  ~\text{primitive}~ \pmod{q}, q < Q, (q,k) = 1\}.
$$

But, by the duality principle (LEMMA \ref{part1-chap1:sec1.2:lem2}),
it suffices to deal with 
$$
  D(x,z) = \sum_{\substack{ r \equiv \ell \pmod{k} \\ (r,P(z)) = 1
      \\ r < x}} |\sum_{\chi \epsilon  \square } \chi(r)b_\chi|^2, 
$$
where\pageoriginale $\{b_\chi\}$ are arbitrary complex numbers. The
argument leading to 
LEMMA \ref{chap2-lem10} gives 
\begin{align*}
  D(x,z) & \le \sum_{K} (-1)^\omega{^{{(K)}}} \Theta_1 (K) \sum_{d
    \epsilon  K} D_d (x,z_1)\\ 
  & + \sum_{\substack{I < K\\ \omega(K) \equiv 0 \pmod{2}}} \Theta_1
  (KI) \sum_{\substack{p,p' \epsilon  I\\ d \epsilon  K}} D_{dpp}'
  (x,z_1), 
\end{align*}
where $2 < z_1 < z $ and
$$
 D_d (x,z_1) = \sum_{\substack{r \equiv \ell \pmod{k} \\ r \equiv \ell
     \pmod{d} \\ (r,p (z_1)) = 1\\r < x}} | \sum_{\chi \in \square } \chi
 (r) b_\chi |^2; 
$$
the mode of the dissection of the interval [$z_1,z)$ is the one given
  at \eqref{eq3.4.10}, and of course $\beta=2$ in the defnition of
  $\Theta_1$. We then follow closely the reasoning of \S~
  \ref{chap3-sec3.4} up to 
  \eqref{eq3.4.20}; by using the notation 
\begin{equation*}
  R_d (x, \chi)=
  \begin{cases}
    \qquad 0  & ~\text{if}~ (d,k) > 1\\
    \sum \limits_{\substack{n \equiv \ell \pmod{k} \\ n \equiv 0
        \pmod{d}\\ n < x}}{\chi(n) -E (X) \chi (d) \frac{\varphi(q)} {dqk}} x&
    ~\text{if}~ (d,k) = 1, 
  \end{cases}
\end{equation*}
where $\chi$ is to $\mod q$,we may express the result as
\begin{align*}
  D (x,z) & \le \frac{x e^{-\gamma}}{\varphi (k) \log z}
  \left(\phi_1\left(\frac{\log y} {\log z}\right) + o (1)\right)
  \sum_{\chi \epsilon  \square } |b_X|^2\\ 
  & + \sum_{\chi, \psi \epsilon  \square } b_\chi \bar{b}_ \psi \left\{
  \sum_K \Theta_1 (K)(-1)^{\omega(k)} \sum_{\substack{d \epsilon 
      K\\ f < z^\tau}} \xi_f^{(1+\omega(K))}{_R{_{df}}} (x, \chi
  \bar{\psi}) \vphantom{\sum_{\substack{d \epsilon  K\\ p,p' \epsilon  I \\ f <
      z^\tau \\ f|P(z_1)}}} \right. \\ 
  & \qquad \qquad \left.+ \sum_{\substack{I < k\\ \omega(k) \equiv 0
      \pmod{2}}} \Theta_1 (KI) \sum_{\substack{d \epsilon  K\\ p,p'
      \epsilon  I \\ f < z^\tau \\ f|P(z_1)}} \xi_f^{(1)} R_{dpp'f}^{(x,\chi
    \bar{\psi})}\right\} 
\end{align*}\pageoriginale
provided $\log kQz \ll \log x \log \ll \log z$, where $\gamma$ is the
Euler constant, and the conventions are just the same as those in
\eqref{eq3.4.20}. 

Next, to this we apply the smoothing device:
$$
D (x,z) \le \frac{1}{\eta} \int\limits_{X}^{xe^\eta} D(w,z)
\frac{dw}{w} \text{ for any } \eta > 0,  
$$
and then appeal to LEMMA \ref{chap3-lem16}. We get
$$
  D (x,z) \le \left\{\frac{x e^{-\gamma}}{\varphi (k) \log z}
  \left(\phi_1\left(\frac{\log MN} {\log z} \right) + o (1)\right)+ o
  ((\log x)^2 E)\right\} 
  \sum_{\chi \epsilon  \square } |b_\chi|^2, 
$$
where $MN \ge z^2$, and
\begin{align*}
  E &=\sup_{w \le 2x} \sup_{\alpha,\beta} \sup_{\psi \epsilon  \square 
  }\sum_{\chi \epsilon  \square }| \sum_{\substack{m < M \\ n < N
      \\ (mn,k) =1}} \alpha_m \beta_n R_{mn}^{(1)} (w, \chi
  \bar{\psi})|;\\ 
  R^{(1)}_d (w, \chi) &= \sum_{\substack{ r \equiv \ell \pmod{k} \\ r
    \equiv 0 \pmod{d} \\ r < w}} \chi(r) \log \frac{w}{r}- E(\chi)
  \chi(d) \frac{\varphi(q)}{dqk} w, \chi \pmod{q}. 
\end{align*}\pageoriginale

Here $\alpha = \{\alpha_m\}, \beta = \{\beta_m\}$ are, as before,
variable vectors such that $|\alpha_m|\le 1,| \beta_n| \le 1$. 

Hence, by LEMMA \ref{part1-chap1:sec1.2:lem2}, we obtain
{\fontsize{10pt}{12pt}\selectfont
\begin{equation*}
\sum_{\chi \epsilon  \square } |S (x,z, \chi)|^2
\le \left\{\frac{xe^{-\gamma}} {\varphi(k) \log z} \phi_1 \left(\frac{\log MN}
    {\log z}\right) + o ((\log x)^2E)\right\} \sum_{\substack{ r
        \equiv \ell \pmod{k}  \\ (r,p (z)) = 1 \\ r < x}} |a_r|^2
    \tag{4.3.2} \label{eq4.3.2} 
\end{equation*}}\relax
provided $\log kQz \ll \log x \ll \log z$, and $MN \ge z^2$.

Now we proceed to the setimation of $E$. For this sake, we quote the
following basic aids. 

\begin{Lemma}\label{chap4-lem20}%lemma 20
For any $\psi \epsilon  \square  $ and $T \ge 1$, we have 
\begin{enumerate}[\rm (i)]
\item $ \sum\limits_{\chi \epsilon  \square } \sum\limits_{\xi \pmod{k}}
  \int\limits_{-T}^T |L \dfrac{1}{2} + \text{it}, \chi \bar{\psi}
  \xi|^4 dt \ll (kQ^2 T)^{1+\epsilon }$, 

and also, for any $H \ge 1$ and $t$,
\item $ \sum \limits_{\chi \epsilon \square} \sum\limits_{\xi
  \pmod{k}}| \sum\limits_{h < H} \chi \bar{\psi} \xi (h) h^{ -\dfrac{1}{2}-
    it} |^4 \ll ((|t| + 1) kQ^2)^{i+\epsilon } $, 
\end{enumerate}
\end{Lemma}

\begin{Lemma}\label{chap4-lem21}%lemma 21
Let $a_n$ be arbitrary complex numbers, and let $G = \sum\limits_{n
  <N} |a_n|^2$. Then we have, for any $V >0$, 
\begin{multline*}
\Bigg|\left\{ (\chi, \xi); \chi \epsilon  \square, \xi \pmod{k} ~\text{such that
}~| \sum\limits_{n <N} a_n \chi \xi (n)| > V \right\}\Bigg| \\
\ll GNV^{-2} + G^3 NV^{-6} (kQ^2) ^{1-\epsilon }.
\end{multline*}
\end{Lemma}

\begin{Lemma}\label{chap4-lem22}%lema 22
Let\pageoriginale $\chi$ be non-principal $\pmod{q}$. Then we have 
$$
\sum_{n < L} \chi(n)n^{it} \ll (|t| + 1)L^{I-\epsilon },
$$
provided $L > q^{3/8 + \eta}$ with $\eta = \eta (\epsilon ) > 0$.
\end{Lemma}

To estimate $E$, it is sufficient to treat
$$
  E_{\psi}(A,B) = \sum_{\chi \epsilon  \square }| \sum_{\substack{A < m
      \le 2A\\ B < n \le 2B\\ (mn,k)=1}} \alpha_m \beta_n R_{mn}^{(1)}
  (w,\chi \bar{\psi})|, 
$$
where $\psi \epsilon  \square $; $A$, $B$ are independent variables, and
$\log ABkQ \ll \log x, w \ll x$, as we shall assume below. To this we
shall apply two methods. 

The \textit{first method} rests on the expression
\begin{multline*}
  \sum_{\substack{A < m \le 2A\\ B < n \le 2B\\ (mn,k)=1}} \alpha_m
  \beta_n R_{mn}^{(1)} (w, \chi \bar{\psi}) 
  =\frac{1}{2 \pi i \varphi(k)} \sum_{\xi \pmod{k}} \bar{\xi}(\ell)\\
  \int\limits_{1/2 - i \infty}^{1/2 + i \infty} L (s, \chi \bar{\psi
    \xi}) A (s,\bar{\psi \xi}) B (s,\bar{\psi \xi}) \frac{w^s}{s^2}
  ds, \tag{4.3.3} \label{eq4.3.3}
\end{multline*}
where
$$
A(s, \chi) =\sum\limits_{A <m \le 2A} \chi (m)
{\alpha{_m}}{^{m{^{-s}}}}, B(s, \chi) = \sum_{B < n \le 2B}
\chi(n)\beta_n n^{-s}. 
$$

Thus using H\"{o}lder's inequality, we get 
\begin{align*}
  E_\psi (A,B) & \ll \frac{x^{1/2+ \epsilon }}{k} \left\{ \int \sum |L(s,
  \chi \bar{\psi} \xi)|^4 \frac{|ds|} {|s|^2}\right\}^{1/4}\\ 
  & \times \left\{ \int \sum  |M  (s, \chi \bar{\psi} \xi)|^4  \frac{|ds|}
       {|s|^2}\right\}^{1/4}  \left\{ \int \sum  |N  (s, \chi \bar{\psi}
       \xi)|^2  \frac{|ds|}{|s|^2}\right\}^{1/2},  
\end{align*}\pageoriginale
where integerals are along the line $\sigma =  \dfrac{1}{2}$, and sums
are over $\chi \epsilon  \square $, $\xi \pmod{k}$. Then the
multiplicative large sieve inequality and (i) of LEMMA \ref{chap4-lem20} give 
$$
E_\psi (A,B) \ll x^{1/2 + \epsilon }
\left(\frac{Q}{\sqrt{k}}\right)^{1/2} 
\left(A^2 + kQ^2\right)^{1/4} (B +kQ^2)^{1/2}. 
$$

Hence, if we set 
$$
M = (\frac{x}{Q\sqrt{k}})^{\frac{1}{3}-\eta}, N=M^2 \ge k Q^2 
$$
with a small fixed $\eta > 0$, we have
$$
E \ll x^{1-\epsilon }/k.
$$

Inserting this into \eqref{eq3.4.2}, we obtain 
\begin{equation*}
\sum_{\chi \epsilon  \square }|S \left(x, D^{\frac{1}{3}},
  \chi\right)|^2  \le \frac{(2+o(1))x} {\varphi(k) \log D}
  \sum_{\substack{r \equiv \ell \pmod{k}\\ (r, p(D^{1/3})) =
      1\\ r<x}} |a_r|^2 \tag{4.3.4}\label{eq4.3.4}
\end{equation*}
for
$$
  D = \left(\frac{x}{Q\sqrt{k}}\right)^{1-\epsilon }, kQ^2 \le
  x^{\frac{1}{2} -\epsilon}.
$$

The \textit{second method} is more involved, and rests on the
observation that, apart from a negligible error, the left side
of\pageoriginale  \eqref{eq4.3.3} is equal to  
$$
\frac{1}{2 \pi i \varphi(k)} \sum_{\xi \pmod{k}} \bar{\xi}(\ell)
\int\limits_{1/2 - \text{ix}^c}^{1/2 + \text{ix}^c} H (s, \chi \bar{\psi \xi}) A
(s, \chi \bar{\psi \xi}) B (s, \chi \bar{\psi \xi}) \frac{w^s}{s^2}
ds,  
$$
where $c$ is sufficiently large, and 
$$
H (s, \chi) = \sum_{h < H} \chi (h)h^{-2}, H=\frac{w}{AB}.
$$

Now, for each pair of $\psi$ and $s$ with $Re(s) = 1/2$, let
$K_{\psi,s}(U,V,W)$ be the number of charecters $\chi \xi, \chi
\epsilon  \square, \xi \pmod{k}$, satisfying simultaneously  
$$
U < |H(s, \chi \bar{\psi \xi})| \le 2U,V < |A(s, \chi \bar{\psi
  \xi})|\le 2U, W< |B(s, \chi \bar{\psi \xi})| \le 2W. 
$$

Here, by an obious reason, we may assume that 
\begin{equation*}
  |\log U|,|\log V|,|\log W| \ll \log x. \tag{4.3.5}\label{eq4.3.5}
\end{equation*}

By virtue of (ii) of LEMMA \ref{chap4-lem20} and LEMMA
\ref{chap4-lem21}, we find readily  
$$
K_{\psi, s} (U,V,W) < x^\epsilon  F
$$
with 
\begin{multline*}
  F=\min  \left\{ \frac{A+kQ^2}{V^2}, \frac{B+kQ^2}{W^2},\frac{kQ^2|s|}{U^4},
  \frac{A}{V^2}+ \frac{kQ^2 A}{V^6}, \frac{B}{W^2}\right.\\ 
  \left.+\frac{kQ^2
    B}{W^6}, \frac{H^2}{U^4} + \frac{kQ^2 H^2}{U^{12}}\right\}; 
\end{multline*}
in particular, we have
$$
  E_\psi (A,B) \ll \frac{x^{1/2+\epsilon }}{k} \int\limits_
  {1/2-\text{ix}^c}^{1/2+\text{ix}^c} \sup_{U,V,W} UVWF \frac{|ds|}{|s|^2}, 
$$
where $U,V,W$ are to satisfy \eqref{eq4.3.5}. 

Then\pageoriginale it suffices to show
\begin{equation*}
  UVWF \le |s| x ^{1/2-\eta} \tag{4.3.6}\label{eq4.3.6}
\end{equation*}
for some $\eta > 0$. Actually, we shall prove this on the assumption
\begin{equation*}
  A,B <\left(\frac{x}{(kQ^2)^{3/8}}\right)^{1/2 -\delta}, kQ^2 <
  x^{\frac{9}{20}-\delta} \tag{4.3.7} \label{eq4.3.7}
\end{equation*}
for some $\delta =\delta (\eta) >0$. For this sake, we shall consider
four cases separately: (i) $F \ll AV^{-2}$, $BW^{-2}$; (ii) $F \gg AV^{-2}$,
$BW^{-2}$; (iii) $F \ll AV^{-2}$, $F \ll BW^{-2}$; (iv) $F \gg
AV^{-2}$, $F \ll BW^{-2}$. But, because of the symmetry, we do not need
to treat the case (iv).  

Now if (i) holds, then we have
$$
 UVWF \ll UVW \min \left(\frac{A}{V^2}, \frac{B}{W^2}\right) \ll U(AB)^{1/2}.
$$

But, by virtue of \eqref{eq4.3.7} we may appeal to LEMMA
\ref{chap4-lem22} which gives $U \ll |s| L^{1/2}{_X}^{-\eta}$, whence
we have \eqref{eq4.3.6}. Before treating 
the case (ii), we remark that for any $\alpha_i > 0$ we have  
$$
\min \{ \alpha_1, \alpha_2,\ldots,\alpha_r\} \le \alpha_1^{e_1} \alpha_2^{e_2} \dots \alpha_r^{e_r}
$$
with any $e_i \ge 0,\sum\limits_{i=1}^{r} e_i = 1$. Thus, in case
(ii), we have  
\begin{align*}
  F  \ll \min & \left\{ \frac{kQ^2}{V^2},
  \frac{kQ^2}{W^2},\frac{AkQ^2}{V^6}, \frac{BkQ^2}{W^6},
  \frac{H^2}{U^4}, \frac{kQ^2|s|}{U^4}\right\} \\ 
  + \min & \left\{ \frac{kQ^2}{V^2}, \frac{kQ^2}{W^2},\frac{AkQ^2}{V^6},
  \frac{BkQ^2}{W^6}, \frac{H^2 kQ^2}{U^{12}}
, \frac{kQ^2|s|}{U^4}\right\} \\ 
  \ll |s| & \left[\left(\frac{kQ^2}{V^2}\right)^\alpha
  \left(\frac{kQ^2}{W^2}\right)^\alpha
  \left(\frac{AkQ^2}{V^6}\right)^\beta
  \left(\frac{BkQ^2}{W^6}\right)^\beta \left\{ \min \left(
  \frac{H^2}{U^4}, \frac{kQ^2}{U^4}\right)\right\}^\gamma \right.\\
  + \min & \left\{\left(\frac{kQ^2}{V^2}\right)^\alpha
  \left(\frac{kQ^2}{W^2}\right)^\alpha
  \left(\frac{AkQ^2}{V^6}\right)^\beta
  \left(\frac{BkQ^2}{W^6}\right)^\beta
  \left(\frac{kQ^2}{U^4}\right)^\gamma\right.\\ 
  & \left.\left.\left(\frac{kQ^2}{V^2}\right)^{\alpha'}
  \left(\frac{kQ^2}{W^2}\right)^{\alpha'}
  \left(\frac{AkQ^2}{V^6}\right)^{\beta'}
  \left(\frac{BkQ^2}{W^6}\right)^{\beta'} \left(\frac{H^2
    kQ^2}{U^{12}}\right) \gamma' \right\}\right]. 
\end{align*}

Here\pageoriginale $\alpha$, $\beta$, $\gamma$, ${\alpha'}$,
${\beta'}$, ${\gamma'}$ are to be 
chosen in such a way that $UVWF$ is bounded by a quantity not
depending on $U,V,W$, and also $2\alpha + 2\beta,+\gamma =
1,{2\alpha'} + {2 \beta'} + {\gamma'} = 1$. We should put, obviously,
$\gamma =\frac{1}{4}, {\gamma'} = \frac{1}{12}$, and $2 \alpha + 6
\beta = 1, {2 \alpha'} + {6 \beta'} = 1$. So we find $\alpha=
\frac{5}{16}, \beta= \frac{1}{16}, {\alpha'}= \frac{7}{16}, {\beta'}=
\frac{1}{48}$. Inserting these into the last expression, we get 
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
  F & \ll |s|(UVW)^{-1} kQ^2 (AB)^{\frac{1}{16}} \left\{ \min
  (1,H^{\frac{1}{2}} (kQ^2)^{-\frac{1}{4}}) + \min
  (1,H^{\frac{1}{6}}(AB)^{-\frac{1}{24}})\right\}\\ 
  F & \ll |s|(UVW)^{-1} kQ^2 (AB)^{\frac{1}{16}} \left\{ \left(H^{\frac{1}{2}}
  (kQ^2)^{-\frac{1}{4}}\right)^{\frac{1}{8}} +
  \left(H^{\frac{1}{6}}(AB)^{-\frac{1}{24}}\right)^{\frac{3}{10}}\right\}\\ 
 & \ll |s| (UVW)^{-1} \left\{ x ^{\frac{1}{16}}(kQ^2)^{\frac{31}{32}} +
  x^{\frac{1}{20}} kQ^2 \right\}. 
\end{align*}}\relax

And this is, by \eqref{eq4.3.7},
$$
\ll |s|(UVW) ^{-1} x^{1/2-\eta},
$$
whence we get \eqref{eq4.3.6} again. In much the same way,  we can
show that\pageoriginale 
if (iii) holds, then 
$$
F \ll |s| (UVW)^{-1} \left\{ x^{\frac{1}{8}}(kQ^2)^{\frac{7}{16}}
N^{\frac{3}{8}} +  x^{\frac{1}{12}} (kQ^2)^{\frac{1}{2}}
N^{\frac{5}{12}}\right\}, 
$$
which is, by \eqref{eq4.3.7},
\begin{align*}
  & \ll |s|(UVW)^{-1} \left(x^{\frac{5}{16}}(kQ^2)^{\frac{47}{128}} +
  x^{\frac{7}{24}}(kQ^2)^{\frac{27}{64}}\right)\\ 
  & \ll |s|(UVW)^{-1} x^{0.49}.
\end{align*}

This amply implies \eqref{eq4.3.6}.

This, we infer that we have
\begin{equation*}
\sum_{ X \epsilon  \square } |S (x,D_1^{\frac{1}{3}}, \chi)|^2 \le
  \frac{(2 + o(1))x}{\varphi(k)\log D_1} \sum_{\substack{r \equiv \ell
      \pmod{k} \\ (r,P(D_1^{1/3})) = 1 \\ r <x}} |a_r|^2
  \tag{4.3.8}\label{eq4.3.8} 
\end{equation*}
if
$$
  D_1 = \left(\frac{x}{(kQ^2)^{3/8}}\right)^{1-\epsilon }, kQ^2 <
  x^{\frac{9}{20}- \epsilon }.
$$

Now we specialize the sequence $\{a_r\}$ by setting $a_r = 1 $ if $r$
is a prime, and $= 0$ otherwise. Introducing  this into \eqref{eq4.3.4} and
\eqref{eq4.3.8}, and estimating trivially the contribution of primes less than
$D$ or $D_1$, we obtain 

\begin{theorem}\label{chap4-thm13}
We have
  $$
  \sum_{\substack{q < Q\\ (q,k) = 1}} \sum\limits_{\chi \pmod{q}}
  |\sum_{\substack{ p \equiv \ell \pmod{k} \\ p < x}} \chi(p)|^2 
  $$
  \begin{equation*}
    \le
    \begin{cases}
      \frac{(2+0(1))x}{\varphi(k) \log \frac{x}{kQ^ {2^{3/8}}}} \pi
      (x;k,\ell) &{\rm if }~ kQ^2 < x^{\frac{9}{20}-\epsilon },\\ 
      \frac{(2+0(1)) x}{\varphi(k) \log \frac{x} {\sqrt{kQ}}} \pi
      (x;k,\ell) &{\rm if }~ kQ^2 < x^{\frac{1}{2}-\epsilon }  
    \end{cases}
  \end{equation*}\pageoriginale

  Specifically, 
  \begin{equation*}
    \pi (x;k,\ell) \le 
    \begin{cases}
      \frac{(2+0(1))x}{\varphi(k) \log \frac{x}{k^ {3/8}}}  & {\rm if }~
      k < x^{\frac{9}{20}-\epsilon },\\ 
      \frac{(2+o(1))x}{\varphi (k) \log \frac{x}{\sqrt{k}}} & {\rm if }~
      k <x^{\frac{1}{2}-\epsilon }.  
    \end{cases}\tag{4.3.9}\label{eq4.3.9}
  \end{equation*}
  
  These are genuine improvement upon \eqref{eq1.2.14} and
  \eqref{eq1.2.15} but far from  \eqref{eq4.3.1}. 
\end{theorem}

\begin{center} 
\textbf{NOTES (IV)} 
\end{center}

A clear-cut proof of Vinogradov's result states in LEMMA
\ref{chap4-lem17} can be found in the text book \cite{key42}. 

Montgomery [\cite{key48}, Chap 11] has proved, by analytical means, that if
there is a zero of $\zeta (s)$ very near the line $\sigma = 1$ there
are other (in fact many) zeros nearby; actually, he obtained a
quantitative account of this phenomenon, from which the zero-free
region of Vinogradov follows. On this matter see also Ramachandra
\cite{key64}.   

For\pageoriginale a proof of \eqref{eq4.1.6}, see Wirsing
\cite{key82}, where much more than what is required here is proved. 

THEOREM \ref{chap4-thm11} is due to Motohashi \cite{key61}. Apart from
Selberg's sieve and 
THEOREM \ref{part1-chap1:sec1.3:thm4} an important ingredinent in his
argument is Ramanujan's 
identity. It seems that Ingham \cite{key27} is the first who exploited the
information about the zeros of $\zeta(s)$ from Ramanujan's
indentitly. Balasubramanian and Ramachandra \cite{key1} discussed Ingham's
idea in detail. In this context, it may be worth remarking that de la
Vallee Poussin's inequality \eqref{eq4.1.2} corresponds to the following
identity of the Ramanujan type: for Re(s)$>1$, 
$$
  \sum^{\infty}_{n=1}| \sigma(n,iu)|^4 n^{-s} = \zeta(s)^6 \zeta(s +
  iu)^4 \zeta(s - iu)^4 \zeta(s + 2iu) \zeta(s - 2iu) G(s, u), 
$$
where $u$ is real, and $G(s, u)$ is regular for Re$(s) > 1/2$. From
this, we can extract interesting information on the relation between
the size of $\zeta(1+ it)$ and the existence of zeros in the vicinity
of the line $\sigma =1$. 

Linnik's proof [\cite{key47}, II] of the Deuring-Heilbronn phenomenon\break
\eqref{eq4.2.3} 
is formidable. Considerable simplifications were made by Kna\-powski
\cite{key43}; in his argument, the power sum method of Turan is
vital. Further simplifications were given by Montgomery \cite{key50}; he
employed a special version due to himself of the power sum
method. Later Jutila \cite{key40} and Motohashi [\cite{key55}, I]
worked out a conceptually much simpler proof of \eqref{eq4.2.3} via the
Selberg\pageoriginale sieve. The 
argument developed in \S~ \ref{chap4-sec4.2} is quoted from Motohashi
\cite{key57}.  

For an elementary proof of \eqref{eq4.2.4}, see e.g. Gel'fond
\cite{key18}. The 
elementary treatment of the basic theory of $L$-functions originates in
Linnik's work \cite{key46}. Pintz made an extensive study on this matter. 

Brun-Titchmarsh's theorem states, in its original form, that there
exists an absolute constant $C > 0$, such that, for any $k < x$, we
have 
$$
\pi (x; k, l) < C \frac{x}{\varphi (k) \log \frac{x}{k}}.
$$

This was obtained as a particular application due to Titchmarsh of the
really revolutionary idea of Brun, and shows clearly the advantage of
his elementary method over analytical methods, for, the latter has
never been able to yield such effective and uniform an estimate of
$\pi (x; k, \ell)$ as this. For a detailed history of the
Brun-Titchmarch theorem, see the relevant part of the text book
\cite{key21}. 

The assertion that \eqref{eq4.3.1} implies the non-existence of exceptional
zeros is due to Motohashi \cite{key59}; formerly, it was known only that, if
\eqref{eq4.3.1} holds, then $L(s, \chi), \chi \pmod{k}$, does not vanish in
the interval  
$$
1- \frac{c}{\log k \log \log k}< s < 1. 
$$

Siebert\pageoriginale \cite{key78} has extended the matter so that any
effective improvements on the main-term in the limear seive applied to
arithmetic progressions would yield a result similar to our
assertion. It should be noted also that, as can be seen easily from
our argument, in order to obtain our result it suffices to have
\eqref{eq4.3.1} for all but $0(\varphi (k))$ residue classes $\pmod{k}$. Such
statistical study of the Brun-Titchmarsh theorem was initiated by
Hooley \cite{key24}. 

In this context, it is quite intersting that the estimate of the sort 
$$
\pi (x + h) - \pi (x)<< \frac{h}{\log h}
$$
which is closely related to the Brun-Titchmarsh theorem can yield an
effective zero-free region for $\zeta (s)$. This was observed by
balasubramanina and Ramachandra in the paper quoted above; in fact,
they obtained  
$$
| \zeta (1+ it) >> (\log(| t | + 2))^{-3}
$$
by using the above result on $\pi (x)$, and moreover, this was
achieved without using de la Vallee Poussin's inequality unlike all
other methods.  

THEOREM \ref{chap4-thm13}  is due to Motohashi [\cite{key54}. IV],
which is a large sieve 
extension of the results of Iwaniec \cite{key32} who showed
\eqref{eq4.3.9}; the estimation of $E$ is done by following the
relevant part of Iwaniec's work. 

(i)\pageoriginale of LEMMA \ref{chap4-lem20} can be proved by
employing an idea of Rama\-chandra 
\cite{key63} (see also \cite{key6}), and (ii) is an easy consequence
of (i). LEM\-MA \ref{chap4-lem21} is a simplified version of the large
value theorem of Huxley 
\cite{key26}; for its quick and elegant proof, see Jutila
\cite{key39}, LEMMA \ref{chap4-lem22} is due to Burgess \cite{key10}. 

In the paper quoted above, Iwaniec proved also the estimate:
$$
\pi(x; k, \ell ) \leq \frac{(2 + 0(1) )}{\varphi (k) \log (x^{3/2}/
  k^{7/4})}x ~\text{for}~ k \leq x^{2/3}. 
$$

This remarkable result was obtained by an ingenious combination of his
linear sieve and a special instance of the dispersion method of
Linnik. These results of  Iwaniec are substantial improvements upon
those due to Motohasi \cite{key52} who using the Selberg sieve proved
\eqref{eq4.3.9} but, for smaller values of $k$. 
