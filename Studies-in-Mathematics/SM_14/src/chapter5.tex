\chapter{THE PREPARATION THEOREM FOR DIFFERENTIABLE FUNCTIONS}\label{chap5}
\pageoriginale

\section[The special preparation theorem]{The special preparation theorem.}\label{chap5-sec1}
The aim of this chapter is to prove the preparation theorem for differentiable functions. We begin by stating the theorem in a special case.

\subsection{The special Preparation Theorem.}\label{chap5-subsec1.1}
Let $x = (x_1, \ldots, x_n) \in \mathbf{R}^n$, $t \in \mathbf{R}^n$, $t \in \mathbf{R}$, and let
$$
\Pi (x,t) = t^p + \sum\limits^{p}_{i=1} a_i (x) t^{p-i}
$$
be a distinguished polynomial in $t$ with coefficients which are analytic functions of $x$ in a neighbourhood of $x =0$ in $\mathbf{R}^n$. Then, for any $f \in \mathscr{E}_{n+1}$, there exists $g \in \mathscr{E}_{n+1}$ and $\rho_i \in \mathscr{E}_n$, $0 \leq i \leq p -1 $, such that 
\begin{equation*}
f (x,t) = \Pi (x,t) g(x,t) + \sum\limits^{p-1}_{i=0} \rho_i (x)t^i. \tag{W}\label{chap5-eqW}
\end{equation*}

We first state a more general theorem which is more convenient to handle. To state this, we introduce some notations.

Let $X$ be an analytic set in a neighbourhood of $0\in \mathbf{R}^n$, $X_0$ its germ at $x=0$. For any set $A \supset \mathbf{R}^n$, denote by $A_0$ the germ of the set $A$ at $x=0$ and $\widehat{A}_0$ the germ of the set $A \times \mathbf{R}$ at $(x,t) = (0,0)$, with a similar convention for germs of sets in $\mathbf{R}^n$. Let $Y$ be an analytic subset of $X$, and let $\mathscr{I}(Y_0; X_0)$ denote the space of germs of Whitney functions on $X_0$ which are flat on $Y_0$, and define  $\mathscr{I}(\widehat{Y}_0, \widehat{X}_0)$ in a similar way. Then we have

\setcounter{theorem}{1}
\begin{theorem}\label{chap5-thm1.2}%%% 1.2
  Let $\Pi$, $X_0$, $Y_0$ be as above. Then for any $f \in \mathscr{I} (\widehat{Y}_0; \widehat{X}_0)$, there exist germs $g \in \mathscr{I}(\widehat{Y}_0; \widehat{X}_0)$ and $\rho_i \in \mathscr{I} (Y_0, X_0)$, $0\leq i \leq p -1$ such that \eqref{chap5-eqW} holds.
\end{theorem}

The special preparation theorem follows on taking $X = \mathbf{R}^n$, $Y=\varnothing$.

We\pageoriginale begin by reducing Theorem \ref{chap5-thm1.2} to a weaker statement. For this purpose, $n$, $\Pi$ being given, let us denote by $Th (Y_0, X_0)$ the statement of Theorem \ref{chap5-thm1.2}. It is clear that if $Z_0 \subset Y_0 \subset X_0$, and $Th (Z_0, Y_0)$ and $Th(Y_0, X_0)$ are true, then $Th(Z_0, X_0)$ is true. The weaker statement referred to is

\textit{$P(X_0)$. Given the germ $X_0$ of an analytic set in $\mathbf{R}^n$, for any analytic germ $Y_0 \subset X_0$, $Y_0 \neq X_0$, there exists an analytic germ $Y'_0$, $Y_0 \subset Y'_0 {\displaystyle\mathop{\subset}_\neq} X_0$ such that $Th (Y'_0, X_0)$ is true.}

We prove that $P(X_0)$ for any $X_0$ implies $Th (Y_0, X_0)$. Let $\mathfrak{H}$ be the set of analytic germs $Z_0$, $Y_0 \subset Z_0 \subset X_0$ such that $Th (Z_0,X_0)$ is true. Clearly $\mathfrak{H}$ is nonempty $(X_0 \in \mathfrak{H})$, so that since any decreasingly filtered family of analytic germs is stationary, $\mathfrak{H}$ contains a minimal element, which we again denote $Z_0$. We prove that $Y_0 = Z_0$. If this were not so, then, by $P(Z_0)$, there is $Z'_0$, $Y_0 \subset Z'_0 {\displaystyle\mathop{\subset}_\neq} Z_0$ such that $Th (Z'_0, Z_0)$ is true. But since $Th (Z_0, X_0)$ is true, it follows that $Th(Z'_0, X_0)$ is true, and $Z_0$ is not minimal.

Thus, we have only to prove $P(X_0)$. We do this in two stages; first, when $X = \mathbf{R}^n$, we will prove a stronger result (which proves an analogue of $P(X_0)$ for a fixed neighbourhood of 0) and then we will reduce the general case to this.

\vspace{-.2cm}

\section[The case $X = \mathbf{R}^n$]{The case \boldmath$X = \mathbf{R}^n$.}\label{chap5-sec2}

Let $V$ be a neighbourhood of $0$, $Y$ an analytic subset of $V$, $Y_0$ its germ at the origin; we suppose that the coefficients of $\Pi$ are analytic in a neighbourhood of $\overline{V}$.


Let $I$ be a bounded open interval in $\mathbf{R}$ such that every real root of $\Pi(x,t)=0$ lies in $I$ if $x \in V$; for any subset $A$ of $V$, we set $\widehat{A} = A \times I$.

We may suppose in Theorem \ref{chap5-thm1.2} that the polynomial $\Pi$ is irreducible in $\mathscr{O}_n [t]$. Hence its discriminant $\Delta \not\equiv 0$ in $V$. Let $\delta = \{x \in V | \Delta (x) = 0\}$. We shall prove that $Th (Y'_0, \mathbf{R}^n)$ is true, where $Y'_0 = Y_0 \cup \delta_0$. More precisely, we have

\begin{proposition}\label{chap5-prop2.1}
  Let $Y' = Y \cup \delta$. For any $f \in \mathscr{I} (\widehat{Y}' ; \widehat{V})$, there exists $g \in \mathscr{I} (\widehat{Y}'_0, \widehat{V})$  and $\rho_i \in \mathscr{I} (Y', V)$, $0 \leq i \leq p-1$ such that $f = \Pi g + \sum \rho_i t^i $ on $\widehat{V}$. 
\end{proposition}

\vfill\eject

\begin{proof}
Let\pageoriginale $U_s$ be the set of points of $V - \delta$ where $\Pi$ has exactly $s$ real roots. Then $U_s$ is open and its boundary is contained in $\delta$. Let $U'_s = U_s - Y$, $F_s = V - U_s$. Let $f^{(s)} = f$ in $\widehat{U}_s=0$ in $\widehat{F}_s$. Then  $f = \sum\limits^{p}_{s=0} f^{(s)}$ and it is enough to prove the proposition for each $f^{(s)}$. Hence we may suppose that $f =0$ outside $U_s$ for some $s$.
\end{proof}

Let $\tau_1 (x) < \ldots < \tau_s (x)$ be the real roots of $\Pi (x,t)$ for $x \in U_s$. Then, $\tau_i (x)$ are quasi-h\"olderian, and belong to the space $\mathscr{M} (F_s; V)$ [Proposition \ref{chap5-eq5.3}.]

Let $f_1 (x,t)$ be the functions defined by
$$
f(x, t) = (t - \tau_1 (x)) f_1 (x,t) + f (x, \tau_1 (x))
$$
in $\widehat{U}_s$, $f_1 =0$ in $\widehat{F}_s$. We assert that $f_1 (x,t)$, $f(x, \tau_1 (x))$ belong to the spaces $\mathscr{I} (\widehat{F}_s \cup \widehat{Y}; \widehat{V})$ and $\mathscr{I}(F_s \cup Y; V)$ respectively. It is clearly sufficient to prove that they belong to the spaces $\mathscr{I} (\widehat{F}_s; \widehat{V})$ and $\mathscr{I}(F_s; V)$  respectively since they are clearly flat on $\widehat{Y} \cap \widehat{U}_s$. We have seen in the proof of Proposition \ref{chap5-eq5.3} that $f(x, \tau_1 (x)) \in \mathscr{I} (F_s; V)$. For $f_1$, we write, for $(x, t, \tau) \in V \times I \times I$,
$$
 f(x, t) - f(x, \tau)  = ( t - \tau) h (x,t, \tau), 
$$
with $h \in \mathscr{I}(\widehat{F}_s \times I ; \widehat{V} \times I)$. Clearly
$$
f_1 (x, t) = h (x, t, \tau_1 (x)), 
$$
and it follows, as before, that $f_1 \in \mathscr{I} (\widehat{F}_s; \widehat{V})$. We apply the same procedure with $f$ replaced  by $f_1$ and  $\tau_1 (x)$ by $\tau_2 (x)$ and obtain
$$
 f(x,t) = (t - \tau_1 (x)) (t - \tau_2 (x)) f_2 (x,t) + (t - \tau_1 (x)) f_1 (x, \tau_1 (x)) + f(x, \tau_1 (x)),
$$
where $f_2 \in \mathscr{I} (\widehat{F}_s \cup \widehat{Y}; \widehat{V})$.  Furthermore we have
$$
 f_1 (x, \tau_2(x)), \tau_1 (x) f_1 (x, \tau_2 (x)) \in \mathscr{I} (F_s \cup Y; V).
 $$

 Repeating this process $s$ times, we find
 $$
f(x, t) = (t - \tau_1 (x)) \ldots (t - \tau_s (x)) f_s (x,t) + \sum\limits^{s-1}_{i=0} \rho_i (x) t^i,
$$
where $f_s \in \mathscr{I} (\widehat{F}_s \cup \widehat{Y}; \widehat{V})$ and $\rho_i \in \mathscr{I} (F_s \cup Y ; V) $.

Now\pageoriginale we have
$$
\Pi (x,t) = \prod\limits^s_{i=1} (t - \tau_i (x)) \Pi' (x,t) , x \in U_s.
$$

The proposition would obviously be proved if we show that the function
$$
g = 
\begin{cases}
f_s  / \Pi'  & \text{~ in ~}  \widehat{U}_s,\\
0   & \text{ ~ in ~ }  \widehat{F}_s
\end{cases}
$$
belongs to $\mathscr{I}(\widehat{F}_s \cup \widehat{Y}; \widehat{V})$. Clearly, since $\Pi'$ does not vanish at any point of $U_s$, we have only to prove that $1/\Pi' \in \mathscr{M} (\widehat{F}_s; \widehat{V})$. By (IV, 4.1) it suffices to prove that
\begin{itemize}
\item[(a)] $\Pi' \in \mathscr{M} (\widehat{F}_s ; \widehat{V})$;

\item[(b)] for any compact $K \subset \widehat{V}$, $|\Pi' (x,t)| \geq c (d (x,\delta))^\alpha$ for suitable $c$, $\alpha >0$ and $(x,t) \in  K - \widehat{\delta}$.
 
\end{itemize}

\medskip
\noindent
Proof of (a).  Let $\lambda_1, \ldots \lambda_s$ be new variables and let us divide the polynomial $\Pi (x,t)$ by $(t - \lambda_1) \ldots (t - \lambda_s)$. This gives us, with $\lambda = (\lambda_1, \ldots, \lambda_s)$,
$$
\Pi (x,t) = (t - \lambda_1) \ldots (t - \lambda_s) \Psi (x; t; \lambda) + \Psi' (x; t; \lambda)
$$
where $\Psi$, $\Psi'$ are polynomials in $t$, $\lambda$ with coefficients which are analytic functions on $V$. Clearly $\Psi' (x; t; \tau) =0$, where $\tau = (\tau_1 (x), \ldots, \tau_s (x))$, so that
$$
\Pi' (x, t) = \Psi (s; t; \tau).
$$
Since $\tau_1 (x) \in \mathscr{M} (F_s, V)$, (a) is proved.

\medskip
\noindent{Proof of (b)}. If $\sigma_j$ are the complex roots of $\Pi (x,t)$ we have for $(x,t) \in K$,
$$
|\Pi' (x, t)| = {\displaystyle\mathop{\Pi}_j}  |t - \sigma_j| \geq {\displaystyle\mathop{\Pi}_j}  |\Iim \sigma_j| \geq c_1  {\displaystyle\mathop{\Pi}_j} |\overline{\sigma}_j - \sigma_j|  \geq c_2 |\Delta (x)|^2,
$$
where $c_1$, $c_2$, $>0$. The result follows from the {\L}ojasiewicz inequality (Theorem IV, 4. 1.).

\begin{remark*}
We\pageoriginale remark here that the uniqueness of the remainder is not guaranteed. The remainder we obtained is of degree $\leq s -1 $ on $U_s$, and it is not difficult to see that we could obtain any function $R$ of degree $\leq p - 1$ as remainder so long as $R \in \mathscr{I}(\widehat{F}_s \cup \widehat{Y}; \widehat{V})$ and $R (x; \tau_i (x)) = f(x ; \tau_i (x))$ for $i =1, \ldots, s$. This shows that we have uniqueness in Proposition \ref{chap5-prop2.1} only in the case when \textit{all the roots of $\Pi$ are real}.
\end{remark*}

\section[The proof of Theorem \ref{chap5-thm1.2} in the  general case]{The proof of Theorem \ref{chap5-thm1.2} in the  general case.}\label{chap5-sec3}
We begin with the following remark. Suppose $X_0$ is as in \S\S \ref{chap5-sec1}, \ref{chap5-sec2}, and $X_0 = X'_0 \cup X''_0$. Suppose the statements $P(X'_0)$ and $P(x''_0)$ are true. Then $P(X_0)$ is true. In fact, we may suppose that $S_0 = X'_0 \cap X''_0 $ is properly contained in both $X'_0$ and $X''_0$; let now $Y_0 \subset X_0$, and $Z'_0 = X'_0 \cap (Y_0 \cup S_0)$, $Z''_0 = X''_0 \cap (Y_0 \cup S_0)$. If $Z'_0 \subset W'_0 \subset X'_0$, $Z''_0 \subset W''_0 \subset X''_0$ are such that $Th(W'_0, X'_0)$ and $Th (W''_0, X''_0)$ are true, then since $X'_0$, $X''_0$ are regularly situated, we conclude that $Th (W'_0 \cup W''_0 , X_0)$ is true.

Hence we may suppose that $X_0$ is irreducible. We now go back to the notations of Chapter \ref{chap4}, \S \ref{chap4-sec3}.

Let the neighbourhood $V$ of 0 be chosen sufficiently small and so as to have all the properties stated in Chapter \ref{chap4}, \S \ref{chap4-sec3}. Let $Y$ be an analytic subset of $X$, (we regard $X$ as a closed subset of $V$ in what follows).

We may suppose that $Y$ is the inverse image under the projection $X \to V' \subset \mathbf{R}^k$ of an analytic set $S \subset V'$, $S \neq V'$. To see this, we have only to use the fact that if $\phi \in \mathscr{O}_n$, $\phi = 0$ on $Y$, $\phi \not\in \mathfrak{J} = \mathfrak{J} (X_0)$, then there is $h \in \mathscr{O}_n-  \mathfrak{I}$ such that $h \phi - \phi_1 \in \mathfrak{J}$, where $\phi_1 \in \mathscr{O}_k$. We assert that there exists a polynomial $\Psi \in mathscr{O}_n[t]$ such that $\Psi$ is a multiple of $\Pi$ in $\mathscr{O}_n[t]$ and $\Psi = \Psi' + \Psi''$, where $\Psi'' \in \mathfrak{J}[t]$, and $\Psi' \in \mathscr{O}_k [t]$ is a distinguished polynomial. To prove this, let $\Psi'$ be the product of the conjugates of the image $\overline{\Pi}$ of $\Pi$ in $(\mathscr{O}_n/\mathfrak{I})[t]$ over $\mathscr{O}_k[t]$, say $\Psi' = \overline{\Lambda}~~ \overline{\Pi}$. Let $\Lambda \in \mathscr{O}_n[t]$ induce $\overline{\Lambda}$, and $\Psi = \Lambda \Pi$. Clearly $\Psi' = \overline{\Lambda}~~ \overline{\Pi}$ is a distinguished polynomial in $\mathscr{O}_k [t]$, and $\Psi'' = \Lambda \Pi - \Psi' \in \mathfrak{J}[t]$.

We\pageoriginale remark that it is enough to prove $P(X_0)$ with $\Pi$ replaced by $\Psi$; in fact if $R(x,t) = \sum\limits^p_{i=0} r_i (x) t^i$ is the remainder of the division of $f$ by $\Psi$, we have only to carry out the standard polynomial division of $R$ by $\Pi$ to prove $P(X_0)$ for $\Pi$, since clearly the coefficients in this polynomial division will be flat wherever the $r_i (x)$ are flat.

We suppose that $\Psi'$ and $\Psi''$ have coefficients analytic on $V$ and that $I$ is a bounded open interval in $\mathbf{R}$ such that any real root of $\Psi(x'; t)$ lies in $I$ for $x'\in V'$. For any subset $A$ of $V'$, we shall write $\widehat{A}_k$ for $A \times I \subset \mathbf{R}^{k+1}$, $\tilde{A}$ for $A \times V'' \subset \mathbf{R}^n$, $\widehat{A}_n$ for $A \times V'' \times I = \tilde{A} \times I \subset \mathbf{R}^{n+1} $.

Applying Proposition 2.1 to the irreducible factors of $\Psi'$, we find that if $V$ is sufficiently small, there is an analytic set $S'$ of $V'$, $S \cup \delta \subset S'$, such that $\dim S' < k$ for which any $f \in \mathscr{I} (\widehat{S}'_k ; \widehat{V}'_k)$ can be written $f = \Psi' g + \sum\limits^{p'}_{i=0} \rho_i t^i$, $p' = \deg. \Psi'$, where $\rho_i \in \mathscr{I} (S'; V')$ and $g \in \mathscr{I}(\widehat{S}'_k; \widehat{V}'_k)$. Let $Y'$ be the set $\tilde{S'} \cap X$. We shall prove the following result, which clearly implies $P(X_0)$ for $\Psi$.

\setcounter{theorem}{3}
\begin{proposition}\label{chap5-prop3.4}
  If $f \in \mathscr{I}(\widehat{Y}'; \widehat{X})$, then there exist $g \in \mathscr{I}(\widehat{Y}'; \widehat{X})$ and $\rho_i \in \mathscr{I} (Y'; X)$, $0 \leq i \leq p' - 1$, such that
  $$
f = \Psi_g + \sum \rho_i t^i.
  $$
\end{proposition}

\begin{proof}
  Since the sets $X_r$ are regularly situated and $X_r \cap X_s \subset Y'$, it is enough to prove the proposition with $X$ replaced by $X_r$. To do this, we remark that if $\pi$ is the isomorphism of $\mathscr{I}(D; X_r)$ onto $[\mathscr{I}(V' - V_r ; V')]^{\mathbf{N}^l}$ given by Chapter \ref{chap4}, Proposition \ref{chap4-prop5.5}, then $\pi$ induces an isomorphism of $\mathscr{I}(Y';X_r)$ onto $[\mathscr{I}(C; V')]^{\mathbf{N}^l}$, where $C = (V' - V_r) \cup S'$ There is further a similar isomorphism $\widehat{\pi}$ of $\mathscr{I}(\widehat{Y}' ; \widehat{X}_r)$ onto $[\mathscr{I} (\widehat{C};\widehat{V}')]^{\mathbf{N}^l}$, defined by $\widehat{\pi}(F) = (G^{\lambda})$, $\lambda \in \mathbf{N}^l$, where $G^\lambda (x,t) = (\pi F_t)^{(\lambda)}(x)$, $F_t$ standing for the function $x \to F (x,t)$. We have therefore only to prove that if $f \in \mathscr{I}(\widehat{Y}'; \widehat{X}_r)$, then there are $\rho_i \in \mathscr{I}(Y';X_r)$, $t \in \mathscr{I}(\widehat{Y}'; \widehat{X}_r)$ with
  \begin{equation*}
    \widehat{\pi} (f) = \Psi \widehat{\pi} (g) + \sum \pi (\rho_i) t^i. \tag{3.1}\label{chap5-eq3.1}
  \end{equation*}
\pageoriginale
  If we write
  $$
\widehat{\pi} (f) = (f^\lambda), \text{~ and ~} \Psi^\lambda = D^\lambda_{x''} \Psi (x', \Phi^r (x'), t),
$$
then $\Psi^{\lambda} \in \mathscr{M} (\widehat{V}' - \widehat{V}_r; \widehat{V}')$, so that the equation \eqref{chap5-eq3.1} is equivalent with the infinite system
\begin{equation*}
  f^\lambda = \Psi' g^{\lambda} + \sum\limits_{\mu < \lambda}  \binom{\lambda}{\mu} \Psi^{\lambda - \mu} g^{\mu} + \sum\limits^{p'-1}_{i=0} \rho^{\lambda}_i t^i, \tag{3.2}\label{chap5-eq3.2}
\end{equation*}
for the functions $\rho^{\lambda}_i \in \mathscr{I} (C; V)$, $g \in \mathscr{I} (\widehat{C}, \widehat{V})$. The existence of the $g^{\lambda}$, $\rho^{\lambda}_i$ is an immediate consequence of Proposition \ref{chap4-prop5.3}. In fact suppose $g^{\mu}$, $\rho^{\mu}_i$ are constructed for $\mu < \lambda$; we have only to solve the equation
$$
h^{\lambda} = \Psi' g^{\lambda} + \sum\limits^{p'-1}_{i=1} \rho^{\lambda}_i t^i
$$
where $h^{\lambda} = f^{\lambda} - \sum\limits_{\mu < \lambda} \binom{\lambda}{\mu} \Psi^{\lambda - \mu} g^{\mu}$ which belongs to $\mathscr{I} (\widehat{C}; \widehat{V})$ by \ref{chap4}, Proposition \ref{chap4-prop1.4}. Note that since the system \eqref{chap5-eq3.2} is infinite, we need Proposition \ref{chap5-prop2.1}; the statement $P(X_0)$ would not suffice.
\end{proof}

\begin{remark*}
Suppose that in the special preparation theorem we add the following condition:
\end{remark*}  
  
At any point near 0, the Taylor expansion of $\Pi$ divides that of $f$ in the ring of formal power series.

The above proof then shows that we may take the $\rho_i = 0$ in the theorem. [The only point is that in \S \ref{chap5-sec3}, we must apply our considerations to $\Lambda f$ since we replace $\Pi$ by $\Lambda \Pi$.] This gives us the following theorem.

\textit{Let $\Omega$ be open in $\mathbf{R}^n$, $\Pi$ an analytic function in $\Omega$. A functions $f \in \mathscr{E}(\Omega)$ is of the form $\Pi_g$, $g \in \mathscr{E} (\Omega)$, if and only if the Taylor expansion of $\Pi$ divides that of $f$ at any point of $\omega$.}

({\L}ojasiewicz \cite{S. Lojasiewicz : 1}, H\"ormander \cite{L. Hormander : 1}.)

In view of the results of Chapter \ref{chap2}, this may be formulated as asserting  that \textit{a principal ideal in $\mathscr{E}(\Omega)$ generated by an analytic function is closed.} We shall prove a generalization of this theorem to arbitrary ideals generated by analytic functions in the next chapter. 

\section[The general preparation theorem]{The general preparation theorem.}\label{chap5-sec4}
\pageoriginale

\begin{theorem}[Malgrange \cite{B. Malgrange : 2}]\label{chap5-thm4.1}
Let $A$ and $B$ be differentiable algebras and $u ; A \to B$ a morphism. If $u$ is quasi-finite, then $u$ is finite. 
\end{theorem}

Before beginning the proof, we remark that we have already proved that the quasi-finiteness of $u$ if equivalent with $\widehat{u}$ being finite, as also with $\widehat{u}$ being quasi-finite. Moreover, we may assume that $A = \mathscr{E}_n$, $B = \mathscr{E}_m$ [$\mathscr{E}_k$ being the ring of germs of differentiable functions at $0 \in \mathbf{R}^k$]; this is proved in the same way as in the analytic case.

We have the following

\begin{lemma}\label{chap5-lem4.2}
Let $u$ be a surjective morphism of a differentiable algebra $A$ onto a differentiable algebra $B$. Let $\widehat{u}$ be the induced morphism of the completions : $\widehat{u} : \widehat{A} \to \widehat{B}$. Then $\ker \widehat{u} = (\ker u + \mathfrak{m}^\infty (A)) / \mathfrak{m}^\infty (A)$.
\end{lemma}

\begin{proof}
Since clearly $\ker \widehat{u} = u^{-1} (\mathfrak{m}^\infty (B)) / \mathfrak{m}^\infty(A)$, we have only to prove that $u^{-1} (\mathfrak{m}^\infty (B)) = \ker u + \mathfrak{m}^\infty (A)$. Now, since $u$ is surjective, we have $u (\mathfrak{m}(A)) = \mathfrak{m}(B)$. Hence, for any $k$, $u (\mathfrak{m}^k (A)) = \mathfrak{m}^k (B)$, so that $u^{-1} (\mathfrak{m}^k (B)) = \mathfrak{m}^k(A) + \ker u$. Hence $u^{-1} (\mathfrak{m}^\infty (B)) = \bigcap\limits_{k \geq 0} (\ker u + \mathfrak{m}^k (A))$. Since the completion $\widehat{A}$ of $A$ is noetherian, it follows from Krull's theorem that $\bigcap\limits_{k \geq 1} (\ker u + \mathfrak{m}^k (A)) = \ker u + \mathfrak{m}^\infty (A)$, and the lemma is proved.
\end{proof}

Let $u : \mathscr{E}_n \to \mathscr{E}_m$ be a quasi-finite morphism, and let $\phi: \mathbf{R}^m \to \mathbf{R}^n$ be a differentiable mapping with $\phi^\ast = u$. We denote the coordinates in $\mathbf{R}^m$ by $x =(x_1, \ldots, x_m)$, those in $\mathbf{R}^n$ by $y = (y_1, \ldots, y_n)$ and we write also $\mathscr{E}_n = \mathscr{E}(y)$, $\mathscr{E}_m = \mathscr{E}(x)$. Now $u$ can be factored
$$
\mathscr{E} (y) \to \mathscr{E} (x,y) \to \mathscr{E} (x),
$$
where the first mapping is the canonical injection (associating to each $f (y) \in \mathscr{E}(y)$ the same function considered as a function of $x$ and $y$) and the second is the mapping
$$
f(x,y) \to f (x,\phi (x))
$$
where $\phi (x) = (\phi_1 (x), \phi_n (x)) = (u(y_1) (x), \ldots, u (y_n)(x))$. Let $N$ be the kernel of the mapping $\mathscr{E} (x,y) \to \mathscr{E}(x)$. To prove that $u$ is finite, it suffices to find an ideal $P\subset N$ such that the composite
$$
\mathscr{E} (y) \to \mathscr{E} (x,y) \to \mathscr{E} (x,y)/ P
$$\pageoriginale
is finite (since $u$ is the composite of this mapping and a surjection of $\mathscr{E}(x,y)/P$ onto $\mathscr{E}(x)$). We denote this composite by $i$.

Let $\widehat{N}$ be the kernel of the map $\widehat{u} : \widehat{\mathscr{E}} (y ) \to \widehat{\mathscr{E}} (x)$. As remarked above, $\widehat{u}$ is finite; hence there exists a finite number of monomials $x^{\alpha} = x^{\alpha_1}_1 \ldots x^{\alpha_m}_m$ which generate $\widehat{\mathscr{E}}(x)$ over $\widehat{\mathscr{E}}(y)$. Clearly, since if certain of these monomials generate $\widehat{\mathscr{E}}(x)/\widehat{\mathscr{E}}(x)\mathfrak{m}(\widehat{\mathscr{E}}(y))$ over $\mathbf{R}$, they generated $\widehat{\mathscr{E}}(x)$ over $\widehat{\mathscr{E}}(y)$, we may suppose that they are linearly independent in $\widehat{\mathscr{E}}(x)/ \widehat{\mathscr{E}}(x) \mathfrak{m}(\widehat{\mathscr{E}}(y))$. Let $r$ be a sufficiently large integer. Then, since the $x^{\alpha}$ are generators of $\widehat{\mathscr{E}}(x)$ over $\widehat{\mathscr{E}}(y)$, there exist elements $c_{i\alpha}(y) \in\widehat{\mathscr{E}}(y)$ such that
$$
x^r_i = \sum\limits_{\alpha} c_{i\alpha} (\phi (x)) x^{\alpha}.
$$
Since this equation holds in $\widehat{\mathscr{E}} (x)$, we conclude that if $r$ is sufficiently large, then $c_{i\alpha} (0) = 0$. By our definition of $\widehat{N}$, we conclude that 
$$
Q_i (x,y) = x^r_i - \sum\limits_{\alpha} c_{i \alpha} (y) x^{\alpha}  \in \widehat{N}.
$$
Let us write $[\alpha] = \max\limits_{j} |\alpha_j|$  if $\alpha = (\alpha_1, \ldots, \alpha_m)$. By introducing series $c_{i\alpha} \equiv 0$ if necessary, we suppose that 
$$
Q_i (x, y) = x^r_i - \sum\limits_{[\alpha] < r} c_{i \alpha } (y) x^{\alpha} \in \widehat{N}, c_{i\alpha} (0) = 0.
$$
Because of Lemma \ref{chap5-lem4.2}, there exist functions $P_i (x,y) \in N$ whose Taylor expansion at 0 coincides with $Q_i$. Let $P$ be the ideal in $\mathscr{E}(x,y)$ generated by the $P_i (1 \leq i \leq m)$. We shall prove that the morphism
$$
i  : \mathscr{E} (y) \to  \mathscr{E} (x, y) / P
$$
is finite. This, as we have already remarked, will terminate the proof.

We now introduce the new variables $t = (t_{i\alpha})$, $1 \leq i \leq m$, $[\alpha] < r$, and the ``generic polynomials''
$$
\Pi_i (x,t) = x^{r}_i - \sum\limits_{[\alpha] < r} t_{i\alpha} x^{\alpha},
$$\pageoriginale
considered as elements of $\mathscr{E}(x,y,t)$. Let $\Pi$ be the ideal generated by $\Pi_i$ in $\mathscr{E} (x,y,t)$. Our next object is to prove the following

\begin{lemma}\label{chap5-lem4.3}
  If $f \in \mathscr{E} (x,y,t)$, then there exist functions $g_i \in \mathscr{E}(x, y, t)$, $h_\alpha  \in \mathscr{E}(y,t)$ ($1 \leq i \leq m$, $[\alpha] < r$) such that
  \begin{equation*}
    f(x,y,t) = \sum\limits^{m}_{i=1} \Pi_i (x,t) g_i (x,y,t) + \sum\limits_{[\alpha] < r} h_\alpha (y, t) x^{\alpha}. \tag{4.1}\label{chap5-eq4.1}
  \end{equation*}
  Moreover, if $f$ is flat at the origin, the $g_i$ and $h$ can be chosen flat at the origin.
\end{lemma}


\begin{proof}
  It is clear that every polynomial in $x$, $t$ is congruent to a sum of monomials $x^\alpha$, $[\alpha] < r$ modulo the ideal $\mathfrak{p}$ generated by the $\Pi_i$ in the ring of polynomials in $x$, $t$. Hence the composite
  $$
\mathbf{R} [t] \to \mathbf{R} [x,t] \to \mathbf{R} [x,t] /\mathfrak{p}
$$
is a finite mapping, so that, fro each $i$, $x_i$ is integral over $\mathbf{R} [x,t]/ \mathfrak{p}$, so that there exists a monic polynomial in $x_i$, $\psi_i (x_i,t) \in \mathfrak{p}$. Since clearly $\psi_i (0,0) = 0$, there exists, by the Weierstrass preparation theorem (or better, the henselian properties of analytic rings), distinguished polynomials
$$
R_i (x_i, t) = x^s_i + \sum\limits^{s-1}_{j=0} \phi_{ij} (t) x^{j}_i,
$$
where the $\phi_{ij}(t)$ are analytic functions of $t$, which belong to the ideal generated by $\mathfrak{p}$ in the ring of analytic functions of $x$, $t$. We may obviously suppose that $s$ is independent of $i$ .

For any $f(x, y, t) \in \mathscr{E} (x,y,t)$, we now apply the special preparation theorem and conclude that
$$
f(x,y,t) = G_1 (x,y,t) R_1 (x_1,t) + \sum\limits_{\alpha_1 < s}  H_{\alpha_1} (x_2, \ldots, x_m ; y; t) x^{\alpha_1}_1
$$
where $G_1$, $H_{\alpha_1} \in \mathscr{E} (x,y,t)$. If now $f$ is flat at the origin, the uniqueness of the division algorithm in the ring of formal power series assures us that $G_1$,  $H_{\alpha_1}$ are automatically flat.

We\pageoriginale repeat the process with $G_1$, $H_{\alpha_1}$ and divide them by $R_2 (x_2; t)$ and so on. This gives us an identity
$$
f(x,y,t) = \sum\limits^{m}_{i=1} G_i (x,y,t) R_i (x_i, t) + \sum\limits_{[\beta] < s} H_\beta (y, t) x^{\beta},
$$
where, if $f$ is flat at 0, so are $G_i$, $H_\beta$. Since the $R_i \in \Pi$, and the $x^{\beta}$ are congruent, modulo $\mathfrak{p}$, to a linear combination of the $x^{\alpha}$, $[\alpha] < r$, this gives us an identity
$$
f(x,y,t) = \sum\limits^m_{i =1 } \Pi_i (x,t) g_i (x,y,t) + \sum\limits_{[\alpha] < r} h_\alpha (y,t) x^{\alpha},
$$
in which the $g_i$, $h_\alpha  \in \mathscr{E} (x,y,t)$, and are flat at 0 if $f$ is. This proves the lemma.

Now, we have
$$
Q_i (x,y) = \Pi_i (x,t) + \sum\limits_{[\alpha] < r} (t_{i \alpha} - c_{i \ alpha} (y) ) x^{\alpha}.
$$
If $\gamma_{i\alpha} \in \mathscr{E} (y)$ has the Taylor expansion $c_{i \alpha} (y)$, then the difference
$$
P_i (x,y) - \Pi_i (x,t) - \sum\limits_{[\alpha] < r} (t_{i\alpha} - \gamma_{i\alpha} (y)) x^{\alpha}
$$
is flat at 0; from Lemma 4.3, it follows that we have a relation
\begin{equation*}
  P_i (x,y) = \Pi_i (x,t) + \sum\limits^{n}_{j=1} \Pi_j (x,t) g_{ij} (x,y,t) + \sum\limits_{[\alpha]<r} k_{i\alpha} (y,t) x^{\alpha} , \tag{4.2}\label{chap5-eq4.2}
\end{equation*}
where the $g_{ij}$ are flat and $k_{i \alpha}$ has, at 0, the Taylor expansion $t_{i\alpha} - c_{i\alpha } (y)$. Consequently the matrix
$$
\left(\frac{\partial k_{i\alpha}}{\partial t_{j \beta}} (0, 0) \right)
$$
is the unit matrix (the number of $k_{i\alpha}$, and that of $t_{i\alpha}$ is the same). By the implicit function theorem, there exist differentiable functions $\theta_{i\alpha} (y)$ such that
$$
k_{i \alpha} (y, t) = 0 \text{ for all } i ,
$$
is equivalent with
$$
t_{i \alpha } = \theta_{i \alpha} (y).
$$
If\pageoriginale we set $t_{i \alpha} = \theta_{i \alpha} (y)$ in \eqref{chap5-eq4.2}, we obtain
\begin{equation*}
  P_i (x,y) = \Pi_i (x; \theta (y)) + \sum\limits^{n}_{=1} \Pi_j (x, \theta(y)) g_{ij} (x,y), \tag{4.3}\label{chap5-eq4.3}
\end{equation*}
where the $g_{ij}$ are flat. Consequently, the equations \eqref{chap5-eq4.3} can be inverted, so that the functions $\Pi_i(x, \theta(y))$ generate the same ideal $P$ as the functions $P_i (x,y)$. If now $f\in \mathscr{E}(x,y)$, we apply \eqref{chap5-eq4.1}, and then substitute $\theta_{i\alpha}(y)$ for $t_{i\alpha}$. We obtain
$$
f(x,y) = \sum\limits^{n}_{i=1} \Pi_i (x, \theta (y)) g_i (x,y, \theta(y)) + \sum\limits_{[\alpha] < r} h_\alpha (y, \theta (y)) x^\alpha
$$
This proves that the mapping
$$
i : \mathscr{E} (y) \to \mathscr{E} (x,y)x / P
$$
is a finite mapping; in fact the $x^{\alpha}$ with $[\alpha] < r$ generate $\mathscr{E} (x,y)/P$ over $\mathscr{E}(y)$. This proves the preparation theorem.
\end{proof}

\begin{corollary}\label{chap5-coro4.4}
  Let $u: A \to B$ be a morphism of the differentiable algebra $A$ and $B$. Let $b_1,\ldots, b_p \in B$ and let $\widehat{b}_1, \ldots, \widehat{b}_p$ be their images in $\widehat{B}$. Then the following conditions are equivalent:
  \begin{itemize}
  \item[(i)] the images of $\widehat{b}_i$ generate $\widehat{B}/\widehat{B} \mathfrak{m}(\widehat{A})$ over $\mathbf{R}$;
    
  \item[(ii)] the $\widehat{b}_i$ generate $\widehat{B}$ over $\widehat{A}$;
    
  \item[(iii)] the images of $b_i$ generate $B/B \mathfrak{m}(A)$ over $\mathbf{R}$;
    
  \item[(iv)] the $b_i$ generate $B$ over $A$.
      \end{itemize}
\end{corollary}
(The deduction of this corollary from Theorem \ref{chap5-thm4.1} has already been given; see Chapter \ref{chap3}, Corollary \ref{chap3-coro1.8}.)

\section[Examples]{Examples.}\label{chap5-sec5}
We give now three examples to illustrate how the preparation theorem, (or rather the corollary above) can be applied.

\medskip
\noindent
\textbf{I. Symmetric Functions.} Let $\sigma_i (x)$ be the $i$th elementary symmetric function of $x_1, \ldots, x_n$, the coordinate functions in $\mathbf{R}^n$. Let $\phi: \mathbf{R}^n \to \mathbf{R}^n$ denote the map
$$
\phi (x) = (\sigma_1 (x), \ldots, \sigma_n (x))
$$
and\pageoriginale $u : \mathscr{E}_n \to \mathscr{E}_n$ the induced morphism. It follows at once from the elementary theorem on the representation of symmetric polynomials as polynomials in $\sigma_1, \ldots, \sigma_n$ that in the ring $\widehat{\mathscr{E}}_n$ of formal power series, the monomials
$$
x^{\alpha_1}_1 \ldots x^{\alpha_n - 1}_{n-1}, ~ 0 \leq \alpha_i \leq n - i,
$$
generate $\widehat{E}_n$ over the subalgebra generated by the images of $\sigma_1, \ldots , \sigma_n$. Hence, by the above corollary, \textit{these monomials generate $\mathscr{E}_n$ over the subalgebra of the differentiable functions of $\sigma_1, \ldots, \sigma_n$.} In particular, if $f \in \mathscr{E}_n$ is symmetric (i.e. invariant under permutations of $x_1, \ldots, x_n$) we see, by averaging over the permutation group, that there exists $g \in \mathscr{E}_n$ such that
$$
f(x_1, \ldots, x_n) = g(\sigma_1, \ldots, \sigma_n).
$$
\textit{Thus, every germ of a differentiable function which is symmetric can be expressed as a differentiable function of the elementary symmetric functions.}

This result is due to G. Glaeser \cite{G. Glaeser: 2}.

\medskip
\noindent
\textbf{II. The Weierstrass preparation Theorem.} Let $F(x_1, \ldots, x_n) \in \mathscr{E}_n$ be \textit{regular} in $x_n$ of order $p$, i.e. $F(0, \ldots, 0, x_n)$ has zero of order exactly $p$ at $x_n = 0$. Let $B$ be the differentiable algebra $\mathscr{E}_n/(F)$, $A$ the algebra $\mathscr{E}_{n-1}$. Let $u: A \to B$ be the composite of the injection $\mathscr{E}_{n-1} \to \mathscr{E}_n$, and the projection $\mathscr{E}_n \to B$. It is clear that the images of $1$, $x_n, \ldots, x^{p-1}_n$ in $\widehat{B}/ \widehat{B} \mathfrak{m}(\widehat{A})$ generate this module over $\mathbf{R}$. By the corollary above, $1$, $x_n, \ldots, x^{p-1}_n$ generate $\mathscr{E}_n/ (F)$ over $\mathscr{E}_{n-1}$; this means that for any $f \in \mathscr{E}_n$, there exist functions $Q \in \mathscr{E}_n$ and $r_i \in \mathscr{E}_{n-1}$ such that
$$
f = Q F + \sum\limits^{p-1}_{i=0} r_i x^i_n.
$$
If we apply this to $f =x^{p}_{n}$, we see that since $F(0,\ldots,0,x_{n})$ has a zero of order exactly $p$ at $x_{n}=0$, we must have $r_{i}(0)=0$, $Q(0)\neq 0$, so that
$$
F=gP;
$$
where $g=\dfrac{1}{Q}$ and $P=x^{p}_{n}-\sum\limits^{p-1}_{i=0} r_{i}x^{i}_{n}$ is a distinguished polynomial. Thus,\pageoriginale {\em any function, regular in $x_{n}$ of order $p$, is equivalent to a distinguished polynomial in $x_{n}$ of degree $p$ with coefficients differentiable functions of $x_{1},\quad,x_{n-1}$.}

\medskip
III.~\textsc{Generic mappings \boldmath$\bfR^{2}\to \bfR^{2}$.}

Let $X$ and $Y$ be two copies of $\bfR^{2}$ with coordinates $(x_{1},x_{2})$ and $(y_{1},y_{2})$ respectively. Let $\Omega$ be an open set $\subset X$ and let $F=(f_{1},f_{2})$ be a $C^{\infty}$ mapping $\Omega\to Y$
\begin{itemize}
\item[(a)] There exists $F'$ as near as we like to $F$ in $\scrE(\Omega;Y)$ and having the following property: at any point $(x_{1},x_{2})\in \Omega$, the rank of the mapping $F'$ (i.e. the rank of its jacobian matrix) is $\geq 1$.

In fact, consider the mapping
$$
\left(\dfrac{\partial f_{1}}{\partial x_{1}}, \dfrac{\partial f_{1}}{\partial x_{2}}, \dfrac{\partial f_{2}}{\partial x_{1}},\dfrac{\partial f_{2}}{\partial x_{2}}\right) \ \Omega \to \bfR^{4}.
$$
By Sard's theorem I, 7.4 its image has measure $0$. Let then 
$$
(\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4})
$$ 
be a point not belonging to the image (which we may choose arbitrarily small). We may take
$$
f'_{1}=f_{1}-\lambda_{1}x_{1}-\lambda_{2}x_{2}, f'_{2}=f_{2}-\lambda_{3}x_{1}-\lambda_{4}x_{2}.
$$

\item[(b)] Suppose that the rank of $F$ is everywhere $\geq 1$. By making $\Omega$ small and making suitable changes of variables in $X$ and $Y$, we may suppose that $f_{1}=x_{1}$. We then set $f_{2}=f$ for simplicity. Let us show that there exists $f'$ arbitrarily close to $f$ in $\scrE(\Omega;\bfR)$ having the following property.
\end{itemize}

\noindent
\thnum{(G)}.\label{chap5-G} At any point $a\in \Omega$ where $\dfrac{\partial f'}{\partial x_{2}}(a)=\dfrac{\partial^{2}f'}{\partial x^{2}_{2}}(a)=0$, we have
$$
\dfrac{\partial^{2}f'}{\partial x_{1}\partial x_{2}}(a)\neq 0, \dfrac{\partial^{3}f'}{\partial x^{3}_{2}}(a)\neq 0.
$$

The proof is as before: using Sard's theorem, one shows that the set of $(\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4})\in \bfR^{4}$ for which
$$
f'=f-\lambda_{1}x_{2}-\lambda_{2}x_{1}x_{2}-\lambda_{3}x^{2}_{2}-\lambda_{4}x^{3}_{2}
$$
does not satisfy \eqref{chap5-G} is of measure zero.

Using (a) and (b) one proves the following (details are left to the reader).

Let\pageoriginale $M$ and $N$ be two $C^{\infty}$ manifolds of dimension 2 which are countable at infinity, and let $K$ be a compact set in $M$. Let $\scrE(M,N)$ be the space of $C^{\infty}$ mappings of $M$ into $N$ with the topology of uniform convergence on any compact set of functions and their derivatives of all orders (in an obvious sense). Then, {\em the set of mappings in $\scrE(M,N)$ all of whose critical points on $K$ satisfy} \eqref{chap5-G} {\em (in a suitable coordinate system) is open and dense.}

We shall now look more closely at these critical points. We place ourselves at $0$ in $X$ and $Y$ for simplicity. There are two types which cannot be reduced to one another.

\smallskip
\textsc{Type \thnum{1}\label{chap5-Type1}}.
\begin{equation}
F=(x_{1},f),\dfrac{\partial f}{\partial x_{2}}(0)=0,\dfrac{\partial^{2}f}{\partial x^{2}_{2}}\neq 0.\label{chap5-eq5.1}
\end{equation}
Let us apply Corollary \ref{chap5-coro4.4} to the mapping $\scrE_{2}\to \scrE_{2}$ defined by $F$. We find, in particular, that there exist $\Phi$, $\Psi\in \scrE_{2}$ such that
\begin{equation}
x^{2}_{2}=\Phi(x_{1},f)+2\Psi(x_{1},f)x_{2}.\label{chap5-eq5.2}
\end{equation}

We obviously have $\Phi(0)=\Psi(0)=0$. Put $x'_{2}=x_{2}-\Psi(x_{1},f)$, $y'_{2}=\Phi(y_{1},y_{2})+\Psi^{2}(y_{1},y_{2})$. We deduce from \eqref{chap5-eq5.1} and \eqref{chap5-eq5.2} that $(x_{1},x'_{2})$, $(y_{1},y'_{2})$ are local coordinates at $0$. In this coordinate {\em system, our mapping takes the canonical form of Type \ref{chap5-Type1} : $f_{1}=x_{1}$, $f_{2}=x^{2}_{2}$.}

\smallskip
\textsc{Type \thnum{2}\label{chap5-Type2}}.
\begin{align}
F=& (x_{1},f), \dfrac{\partial f}{\partial x_{2}}(0)=\dfrac{\partial^{2}f}{\partial x^{2}_{2}}(0)=0;\label{chap5-eq5.3}\\
&\dfrac{\partial^{2}f}{\partial x_{1}\partial x_{2}}(0)\neq 0, \dfrac{\partial^{3}f}{\partial x^{3}_{2}}(0)\neq 0.\notag
\end{align}
Applying again Corollary \ref{chap5-coro4.4}, we can find functions $\Phi$, $\Psi$, $\Theta\in \scrE_{2}$ such that
\begin{equation}
x^{3}_{2}=\Phi(x_{1},f)+\Psi(x_{1},f)x_{2}+3\Theta (x_{1},f)x^{2}_{2}.\label{chap5-eq5.4}
\end{equation}
Clearly, $\Phi(0)=\Psi(0)=\Theta(0)=0$. Replacing $x_{2}$ by $x_{2}-\Theta(x_{1},f)$, we see that we have again local coordinates on $X$ for which \eqref{chap5-eq5.3} is satisfied, so that we may suppose that $\Theta=0$.

This\pageoriginale being so, the conditions \eqref{chap5-eq5.3} and \eqref{chap5-eq5.4} show that we may take as coordinates
\begin{align*}
\text{on } X: \ x'_{1} &= \Psi(x_{1},f), \ x'_{2}=x_{2}\\
\text{on } Y: \ y'_{1} &= \Psi(y_{1},y_{2}), y'_{2}=\Phi(y_{1},y_{2})
\end{align*}
as is easily verified. We obtain finally the {\em canonical form of Type \ref{chap5-Type2}:}
$$
f_{1}=x_{1}, f_{2}=-x_{1}x_{2}+x^{3}_{2}.
$$

The preceding results are due to $H$. Whitney \cite{H. Whitney : 3}. The idea of proving them using the preparation theorem is due to $R$. Thom. A generalization is to be found in B. Morin \cite{B. Morin : 1}.
