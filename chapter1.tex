\chapter{Cauchy-Kowalewaski Theorem and Characteristics Real Analytic Functions}\label{chap1}

\chaptermark{Cauchy-Kowalewaski Theorem...}

\section*{Real analytic functions of one variable}\pageoriginale

Let $\Omega$ be an open set in $\mathbb{R}\cdot f : \mathbb{R}\to \mathbb{R}$ be a real valued function. We say that $f$ is analytic in $\Omega$ if for any $c\in \Omega$ there is a power series $\Sigma a_{n}(x-c)^{n}$ which converges to $f(x)$ for every $x$ in a rhd of $c$.

\begin{prop*}
Let the power series $\sum\limits^{\infty}_{n=0}a_{n}x^{n}$ converge for some $x_{0}\neq 0$. Then the series is absolutely and uniformly convergent for $|x|\leq b$, $0<b<|x_{0}|$.
\end{prop*}

\begin{proof}
Since $\sum a_{n}x^{n}_{0}$ is convergent $|a_{n}x^{n}_{0}|\leq M$ for some positive const. $M$. If $x$ satisfies the above condition, then
$$
|a_{n}x^{n}|=\left|a_{n}\left(\frac{x}{x_{0}}\right)^{n}x^{n}_{0}\right|\leq M \left(\frac{x}{x_{0}}\right)^{n}\leq M\left(\frac{b}{|x_{0}|}\right)^{n}.
$$

Since $\left|\frac{b}{x_{0}}\right|<1$, the geometric series $\sum \left(\frac{b}{x_{0}}\right)^{n}$ is convergent. This proves the result.
\end{proof}

\begin{coro*}
An analytic function is continuous.
\end{coro*}

\begin{exer*}
Suppose the power series is convergent in $|x|<R$. Show that the function $f(x)=\sum a_{n}x^{n}$ is analytic. (i.e., show at any $c\in R$, $f(x)$ can expanded in a power series).
\end{exer*}

\noindent
{\bf Ex.}~ $f$ and $g$ are analytic so are $f+g$ and $\lambda f$ for a Scalar $\lambda$.

\begin{prop*}
A real anal. fn. is differentiable. If
$$
f(x)=\sum a_{n}x^{n},\quad f'(x)=\sum\limits^{\infty}_{0}na_{n}x^{n-1}
$$
\end{prop*}

\begin{proof}
We shall show that if $\sum a_{n}x^{n}$ converges in an interval $I$ around $0$, the series $\sum n a_{n}x^{n-1}$ also converges in the interval. Let $x\in I$. Choose $x_{0}\in I$ s.t. $|x|<|x_{0}|$ with the power series converging at $x_{0}$. Then 
$$
na_{n}x^{n-1}=\frac{nx^{n-1}}{x^{n}_{0}}\cdot a_{n}x^{n}_{0}.
$$\pageoriginale

The series
$$
\sum\limits^{\infty}_{n=0}\frac{nx^{n-1}}{x^{n}_{0}}
$$
is convergent as
$$
\left| \frac{(n+1)x^{n}}{x^{n+1}_{0}}\right| / \left|\frac{nx^{n-1}}{x^{n}_{0}}\right|=\left| \frac{(n+1)x^{n}}{x^{n+1}_{0}}\cdot \frac{x^{n}_{0}}{nx^{n-1}}\right|=\left| \left(1+\frac{1}{n}\right)\frac{x}{x_{0}}\right|\to \left|\frac{x}{x_{0}}\right|<1
$$
and $a_{n}x^{n}_{0}$ is bounded. Hence the series $\sum n a_{n}x^{n-1}$ is convergent. The prop. follows by last prop and
\begin{exer*}
Let $f_{n}$ be a sequence of continuously differentiable functions s.t. $f_{n}\to f$ and $f'_{n}\to g$ uniformly on compact sets. Show that $f$ is different and $f'=g$.
\end{exer*}

\noindent
{\bf Ex.}~ Show that analytic $fr$ is $C^{\infty}$. If $f=\sum a_{n}x^{n}$ then $a_{n}=\dfrac{f^{n}(0)}{n!}$.
\end{proof}

\begin{remark*}
If the Taylor series of a $C^{\infty}$ function at $a$ pt. is convergent, the function need not be analytic. In fact $\exists c^{\infty}$ functions $\nequiv 0$ in any nhd of $0$ s.t. all derivatives vanish at $0$. e.g.
$$
f(x)=
\begin{cases}
e^{-1/x^{2}} & x\neq 0\\
0 & x=0
\end{cases}
$$ 
\end{remark*}

\begin{prop*}
\begin{enumerate}
\item If an analytic function in a connected open set has all its derivatives at a point = 0 (including 0$^{\text{th}}$ order, namely the value of a function at the point) then $f\equiv 0$ in $\Omega$.

\item If $\Omega$ is connected and $f\equiv 0$ in a non-empty open subset of $\Omega$ then $f(x)=0\forall x\in \Omega$.
\end{enumerate}
\end{prop*}

\begin{proof}
$1\Rightarrow 2$\pageoriginale as all the derivatives at a point in the open subset are zero. To prove 1 consider the set
$$
A=\left\{x\in \Omega / f^{n}(x)=0,n=0,1,2,\ldots\right\}
$$
Since
$$
A=\bigcap\limits^{\infty}_{n=0}\left\{x\in \Omega / f^{n}(x)=0\right\}
$$
is an intersection of closed sets it is closed. We claim that $A$ is also open. Let $x_{0}\in \Omega$, then in a rhd of $x_{0}$, $f(x)=\sum \dfrac{f^{n}(x_{0})}{n!}(x-x_{0})^{n}$ as $f$ is analytic at $x_{0}$. Since $f^{n}(x_{0})=0$, $f$ is zero is a rhd. Thus $A$ is open and non-empty by hypothesis. So $A=\Omega$, and $f(x)=0\forall x\in \Omega$.
\end{proof}

\section*{Majorant or dominating series}

Let $F=\sum a_{n}x^{n}$ be a formal power series. If $G=\sum b_{n}x^{n}$ is a formal power series with $b_{n}\geq 0$, and such $|a_{n}|\leq b_{n}$, we say that $G$ is a majorant or a dominant series for $F$. If $G$ is convergent in $|x|<R$ it is $G$ clear that $F$ is also convergent in $|x|<R$. We also say, in the convergent case that the function $G$ dominates the function $F$.

Suppose $P$ is a polynomial in $P(y_{0}\ldots y_{n})$ whose coeffs. are real and $\geq 0$. We then have
$$
\left| P\left(a_{0_{1}}\ldots a_{n}\right)\right|\leq P\left(b_{0_{1}}\ldots b_{n}\right).
$$
This remark will be useful later.

Given a power series which is convergent, there exist dominant power series which are convergent e.g. $\sum a_{n}x^{n}_{0}$, $x_{0}\neq 0$ is convergent, $|a_{n}|\leq \dfrac{M}{|x_{0}|^{n}}$. So
$$
\sum \dfrac{M}{|x_{0}|^{n}}x^{n}=\left(\dfrac{M}{1-\dfrac{x}{|x_{0}|}}\right)=\dfrac{M}{\left(1-\frac{x}{a}\right)}
$$
is a dominant convergent series, converging for $|x|<|x_{0}|$.

\noindent
{\bf Ex.}~ If\pageoriginale $G_{1}$ (resp. $G_{2}$) dominates $F_{1}$ (resp. $F_{2}$) show that $G_{1}G_{2}$ dominates $F_{1}F_{2}$.

\noindent
{\bf Ex.}~ If $f$ and $g$ are analytic functions show that $fg$ is analytic.

\section*{Composition of analytic functions. Substitution of a power series in another.}

Let $f:\Omega \to \mathbb{R}$ be an analytic function and $g$ an analytic function defined in an open set containing image of $f$. Then $g\circ f:\Omega \to \mathbb{R}$ is an analytic function. If
\begin{align*}
& y = f(x)=a_{0}+a_{1}(x-x_{0})+\cdots\\
& g(y)=b_{0}+b_{1}(y-y_{0})+\cdots
\end{align*}
and suppose $f(x_{0})=y_{0}$, i.e., $a_{0}=y_{0}$. Then the power series expansion for $g\circ f(x)$ is obtained by substituting for $y-y_{0}$ the series
$$
a_{1}(x-x_{0})+\cdots
$$
or by substituting for $y$ the series
$$
a_{0}+a_{1}(x-x_{0})+\cdots
$$
See Goursat Mathematical Analysis Ch.IX, \S\ 182. In fact, see for a good account of analytic functions, Ch. IX \S\ 177 - \S\ 191.

\begin{exer*}
Prove that the functions $\log (1-x)$ and $\sqrt{1+x}$ are analytic in $|x|<1$. What are the Taylor expansions at 0?.
\end{exer*}

\begin{exer*}
Let $f$ be a $C^{\infty}$ function in $\Omega$. Show that $f$ is analytic if and only if for every compact set $K\subset \Omega$, there exists positive constraints $M$ and $\mathbb{C}$ such that
$$
\displaystyle{\mathop{\text{Sup\,}}\limits_{K}}|f^{n}(x)|\leq MC^{n}.n!
$$
\end{exer*}

\section*{Analytic functions of several variables.}

We shall deal with two variables $(x,y)$. Let $\Omega$ be an open subset of $\mathbb{R}$ and $f:\Omega\to \mathbb{R}^{1}$ is a function. We say $f$ is analytic if given any point $(x_{0},y_{0})$ in $\Omega$ there exists real numbers $a_{m,n}$, $m$, $n$ integers $\geq 0$ such that the power series
$$
\sum\limits_{\substack{m,n\in \mathbb{Z}\\ m\geq 0, n\geq 0}}a_{m,n}(x-x_{0})^{m}(y-y_{0})^{n}
$$\pageoriginale
converges absolutely to $f(x,y)$ for $(x,y)$ in a neighbourhood of $(x_{0},y_{0})$. 

As in the case of one variable, $f$ is $C^{\infty}$ i.e., $f$ has continuous partial derivatives of all orders. Moreover, $a_{m,n}=\dfrac{\partial^{m+n}f}{\frac{\partial x^{m}\partial y^{n}}{m!\,n!}}(x_{0})$ and the principle of analytic continuation is valid. A power series $\sum b_{m,n}x^{m}y^{n}$, with $b_{m,n}\geq 0$ is a majorant for $\sum a_{m,n}x^{m}y^{n}$ if $|a_{m,n}|\leq b_{m,n}$. If the series is convergent, say at $(x_{0},y_{0}) x_{0}\neq 0$, $y_{0}\neq 0$, then
$$
|a_{mn}|\leq \frac{M}{|x_{0}|^{m}|y_{0}|^{n}}.
$$
Let
$$
\frac{1}{|x_{0}|}=a, \ \frac{1}{|y_{0}|}=b
$$
then the function
$$
\frac{M}{\left(1-\frac{x}{a}\right)\left(1-\frac{y}{b}\right)}
$$
is a (convergent) dominant series. The function
$$
\frac{M}{\left[1-\left(\frac{x}{a}+\frac{y}{b}\right)\right]}
$$
is another dominant function. For the coefficient of $x^{m}y^{n}$ of $\left(\frac{x}{a}+y/b\right)^{m+n}$ is equal to that of $M/(1-x/a)(1-y/b)$ and hence the coefficient of $x^{m}y^{n}$ in $M/1-(x/a+y/b)$ is $\geq$ to that of $M/(1-x/a)(1-y/b)$.

\section*{Cauchy's Existence Theorem for Ordinary Differential Equation.}\pageoriginale

\begin{theorem*}
Let $f$ be an analytic function in a nhd of $(x_{0},y_{0})$ in $\mathbb{R}^{2}$. There exists a unique analytic solution of the ordinary differential equation $dy/dx=f(x,y)$ in a nhd of $x_{0}$ with $y(x_{0})=y_{0}$ i.e. there exists a unique analytic function $u$ in a connected nhd of $x_{0}$ in $\mathbb{R}$ with $u(x_{0})=y_{0}$ and such that
$$
\frac{du}{dx}(x)=f(x,u(x))\quad\text{for}\quad x\quad \text{in the nhd.}
$$
\end{theorem*}

\begin{proof}
We may assume $x_{0}=0$, $y_{0}=0$; for write $y-y_{0}=y'$ and $x-x'_{0}=x'$ and solve the differential equation
$$
\frac{dy'}{dx'}=f(x'+x_{0}, \ y'+y_{0})\quad\text{with}\quad y'(0)=0.
$$
The idea is that the coefficients of the power series expansion are uniquely determined by the coefficients of power series expansion of $f(x,y)$. Suppose
$$
f(x,y)=b_{0,0}+b_{1,0}x+\cdots+b_{0,1}y+b_{20}x^{2}+b_{11}xy+b_{0,2}y^{2}+\cdots
$$
Let us seek for $u$ in the form
$$
u(x)=a_{0}+a_{1}x+a_{2}x^{2}+\cdots
$$
where the $a_{i}$ are to be determined.

Since $u(0)=0$ we have $a_{0}=0$.
\begin{align*}
& \frac{du}{dx}=a_{1}+2a_{2}x+3a_{3}x^{2}+\cdots\\[3pt]
& f(x,u(x))=b_{0,0}+b_{1,0}x+b_{0,1}(a_{1}x+a_{2}x^{2}+\cdots)\\[3pt]
&\qquad\qquad + b_{2,0}x^{2}+b_{1,1}x(a_{1}x+a_{2}x^{2}+\cdots)\\[3pt]
&\qquad\qquad +b_{0,2}(a_{1}x+a_{2}x^{2}+\cdots )^{2}\\[3pt]
&\qquad\qquad +\cdots\cdots\cdots
\end{align*}

Since we need
$$
\frac{du}{dx}=F(x,u(x)),\quad\text{let us equate the coefficients.}
$$

We get
\begin{gather*}
a_{1}=b_{0,0}\\[3pt]
2a_{2}=b_{1,0}+b_{0,1}a_{1}=b_{1,0}+b_{0,1}b_{0,0}
\end{gather*}
\begin{align*}
3a_{3} &= b_{0,1}a_{2}+b_{2,0}+b_{1,1}a_{1}+b_{0,2}a^{2}_{1}\\[3pt]
&= \frac{1}{2}b_{0,1}(b_{1,0}+b_{0,1}b_{0,0})+b_{2,0}+b_{11}b_{00}+b_{02}b^{2}_{0,0}
\end{align*}\pageoriginale

Generally we get
$$
a_{n}=\frac{P_{n}}{n}\left(b_{0,0},\ldots\ldots b_{0,n-1}\right)
$$
where $P_{n}$ is a univereal polynomial in $b_{ij}i+j\leq n-1$ with positive coefficients and $P_{n}$ is a polynomial independent of $f$ (if $u(0)=0$). Since the coefficients $a_{n}$ are determined by $b_{ij}$, the function $u$, if it exists, is unique by the principle of unique continuation. It remains to prove the convergence of the series $\sum a_{n}s^{n}$ in nhd of $0$. The idea of Cauchy to prove this, is the following. Suppose that we have a function $g(x,y)$ majorising $f(x,y)$ and that the equation
$$
\dfrac{dy}{dx}=g(x,y)
$$
has an analytic solution $v$ in nhd of 0 with $v(0)=0$ such that the coefficients of $v$ in the power series expansion around zero are nonnegative.

If
$$
g(x,y)=\sum b_{ij}x^{i}y^{i}
$$
and
$$
v=a'_{1}x_{1}+\cdots+a'_{n}x^{n}+\cdots
$$

We have
$$
a'_{n}=\frac{P_{n}}{n}(b'_{ij})\quad\text{where $P_{n}$ is the universal}
$$
polynomial considered above with non-negative coefficients. We then have
$$
|a_{n}|\leq \left| \frac{P_{n}}{n}(b_{ij})\right|\leq \frac{P_{n}}{n}(b'_{ij})=a'_{n}.
$$
Thus $v$ would be a majorising convergent series for the formal power series $\sum a_{n}x^{n}$. Hence $\sum a_{n}x^{n}$ would be convergent in a nhd of 0 and will represent the required solution of the differential equation.

We\pageoriginale naturally take for the majorising series the function
$$
\frac{M}{(1-x/a)(1-y/b)}M,a,b>0
$$
and consider the differential equation
$$
\frac{dy}{dx}=\frac{M}{(1-x/a)(1-y/b)}\quad\text{with}\quad y(0)=0.
$$
We have to show that this differential equation has a solution in a nhd of 0 with all its Taylor coefficients $\geq 0$. We shall write down explicitly the solution. Write formally, separating variables.
$$
(1-y/b)dy=\dfrac{M}{(1-x/a)}dx
$$
We obtain
$$
y-\dfrac{y^{2}}{2b}=-aM\log (1-x/a)
$$

Solving the quadratic equation we get
$$
y=b-b/\left(1-2a\frac{M}{b}\log (1-x/a)\right)
$$
(The minus sign is taken to make $y(0)=0$)

Let us check that $y$ is an analytic function in a neighbourhood of 0 (and makes sense as a real valued function) 

analytic). Now

since $\log 1=0$, we can find a nhd of 0 such that
$$
\left| \frac{2aM}{b}\log (1-x/a)\right|<1
$$
for $x$ in this nhd. This shows that $y$ is analytic in a nhd of 0 as $\log (1-xa)$ is analytic in $|x|<a$ and $1+z$) is analytic in $|z|<1$ (use the fact that composite of analytic functions is analytic). The coefficients of $y$ being polynomials (with non-negative coefficients) in the coefficients of
$$
\frac{M}{(1-x/a)(1-y/b)}
$$
are non-negative. Thus $y$ majorises the formal power series expansion for $u$.

Hence\pageoriginale $u$ is convergent in a nhd of zero.
\end{proof}

\begin{remark*}
The power series for $u$ is convergent in $|x|<\gamma$ where $\gamma=a(1-\exp(b/(2aM)))<a$. (This value is obtained by considering the equation
$$
\frac{2aM}{b}\log (1-x/a)=-1)
$$
We claim that for $|x|<\gamma$, $\left|\dfrac{2aM}{b}\log (1-x/a)\right|<1$. To see this, if $x\geq 0$ we have $\log (1-x/a)\leq 0$ and strictly decreasing so that
$$
\left| \frac{2aM}{b}\log (1-x/a)\right|<|2aM/b\log (1-\gamma/a)|=|-1|=1
$$

If $x<0$, we have
\begin{align*}
\left| \frac{2aM}{b}\log (1-x/a)\right| &= \left|\frac{2aM}{b}\left(-\frac{x}{a}-\frac{x^{2}}{2a^{2}}-\cdots\right)\right|\\[3pt]
&\leq \left| \frac{2aM}{b}\left(\frac{|x|}{a}+\frac{|x|^{2}}{a}+\cdots\right)\right.\\[3pt]
\left.=\frac{2aM}{b}\log \left(1-\frac{x}{a}\right)\right| &< 1 \text{~~ as~~ } |x|\geq 0\quad\text{and}\quad |x|<\gamma
\end{align*}
\end{remark*}

\section*{Systems of Ordinary Differential Equations.}

Consider the system of ordinary differential equations
$$
\dfrac{dy_{i}}{dx}= f_{i}(x,y_{1},\ldots,y_{n})i=1,\ldots,n
$$
where $f_{i}$ are analytic in a nhd of $(x^{\circ}_{0},y^{\circ}_{1}\ldots y^{\circ}_{n})$.

\begin{theorem*}
There exist uniquely determined analytic functions $u_{i}(x)$, $i=1,\ldots,n$, defined in a nhd of $x_{0}$ such that the $u_{i}(x_{0}=y^{\circ}_{i}$ and
$$
\frac{du_{i}(x)}{dx}=f_{i}(x,u_{1}(x),\ldots u_{n}(x))\quad\text{in the nhd of}\quad x_{0}.
$$
\end{theorem*}

\noindent
{\em Sketch of Proof.} Consider the system
$$
\dfrac{dy_{1}}{dx}=\ldots=\frac{dy_{n}}{dx}=\frac{M}{(1-x/a)(1-y_{1}/b)(1-y_{n}/b)}
$$
($f_{i}$ are analytic in $|x|\leq a$, $|y_{i}|\leq b$ and $|f_{i}|\leq M$ in this closed set) with $y_{i}(0)=0$. The solution of this system is given by the solution $y_{1}(x),\ldots y_{n}(x)=y(x)$ of the single differential equation
$$
\frac{dY}{dx}=\frac{M}{(1-x/a)(1-y/b)^{n}}
$$\pageoriginale

This equation has the solution
$$
y=b-b^{n+1}\surd \left(1+\frac{(n+1)}{b}Ma\log (1+x/a)\right)
$$
which is analytic in
$$
|x|<a\left(1-e^{-b/(n+1)Ma}\right)
$$

\begin{remark*}
We can replace the system by an `equivalent' system in which the functions on the right do not contain the variable $t$ i.e., are independent of $t$. In fact introduce a new variable $y_{n+1}$ and consider the system
\begin{align*}
& \frac{dy_{i}}{dx}=f(y_{n+1},y_{1}\ldots y_{n})i=1,\ldots n\\[3pt]
& \frac{dy_{n+1}}{dx}=1
\end{align*}

In this case, we have to consider the majorising equation
$$
\dfrac{dy}{dx}=\dfrac{1}{(1-y)^{n+1}},\quad y(0)=0
$$
and the solution is given by
$$
y(x)=1-(1-(n+2)x)^{1/n+2}
$$
and thus does not contain logorithm.
\end{remark*}

\begin{exer*}
Write out in full the proof of the above theorem.
\end{exer*}

\begin{remark*}
A single equation of $n^{\text{th}}$ order
$$
\frac{d^{n}y}{dx^{n}}=F\left(x,y,\dfrac{dy}{dx}, \ \dfrac{d^{n-1}}{dx^{n-1}}y\right)
$$
can be replaced by the equivalent system
\begin{align*}
& \dfrac{dy}{dx}=y_{1},\ldots \dfrac{dy_{n-2}}{dx}=y_{n-1}\\[3pt]
& \dfrac{dy_{n-1}}{dx}=F(x,y,y_{1},\ldots y_{n-1})
\end{align*}\pageoriginale

We see from this that if $F$ is analytic in a nhd of $x_{0}$, $y_{0}$, $y'_{0},\ldots y_{0}^{(n-1)}$ there exists an analytic solution $u(x)$ in a nhd of $x_{0}$ such that $u(x_{0})=y_{0}$,
$$
\frac{dn}{dx}(x_{0})=y'_{0}\ldots \dfrac{d^{n-1}}{dx}u(x_{0})=y_{0}^{(n-1)}
$$
\end{remark*}

\section*{Exercise}

\section*{System of Linear Ordinary Differential Equations.}

Consider the system
$$
\dfrac{dy_{i}}{dx}=a_{i_{1}}(x)y_{1}+a_{i_{2}}(x)y_{2}+\cdots+ a_{in}y_{n}+b_{i}(x)(i=1,\ldots,n)
$$
where $a_{j}$ and $b_{i}$ have convergent power series expansion in $|x|<R$. Given $y^{\circ}_{1}\ldots y^{\circ}_{n}$ show that there is a solution of this equation with $y_{i}(x_{0})=y^{\circ}_{1}$ such that the power series expansions for $y_{i}$ converge in $|x|<R$.

\section*{Cauchy-Kowalawsky Existence Theorem}\pageoriginale

We now wish to prove the existence of solution of the Cauchy problem for first order partial differential equation of the form
$$
\frac{\partial u}{\partial t}=H\left(t,x_{1},\ldots x_{n},u,\frac{\partial u}{\partial x_{1}},\ldots \frac{\partial u}{\partial x_{n}}\right)
$$

\begin{theorem*}
Let $H$ be an analytic function of variables
$$
(t,x_{1},\ldots x_{n},u,p_{1},\ldots,p_{n})
$$
in a nhd of
$$
(t^{\circ},x^{\circ}_{1},\ldots x^{\circ}_{n},u^{\circ},p^{\circ}_{1},\ldots p^{\circ}_{n}).
$$
Let $\varphi$ be an analytic function in - a nhd of $(x^{\circ}_{1},\ldots,x^{\circ}_{n})=x^{\circ}$ such that $\phi(x_{0})=u^{\circ}\dfrac{\partial \phi}{\partial x_{i}}(x_{0})=p^{\circ}_{i}$. Then there exists a unique analytic function $U$ in a nhd $V$ of $(t^{\circ},x^{\circ})$ such that
\begin{itemize}
\item[\rm(a)] $\dfrac{\partial U}{\partial t}=H(t,x(t,x),\ldots,\dfrac{\partial u}{\partial x_{i}}(t,x),)$ \  in the nhd and

\item[\rm(b)] $U(t_{0},x)=\phi(x)$, \ for \ $x\underline{\in} V\cap (t=t_{0})$.
\end{itemize}
\end{theorem*}

\begin{proof}
We may clearly assume $t^{\circ}=x^{\circ}_{i}=0$. We may also assume $u^{\circ}=p^{\circ}_{1}=0=p^{\circ}_{n}$, by writing
$$
v=U-\phi (0)-\sum \left(\frac{\partial \phi}{\partial x_{i}}\right)_{0}x^{i}
$$
(In particular we assume $\phi(0)=0\dfrac{\partial \phi}{\partial x_{i}}(0)=\phi$)

Let seek $U$ in the form
\begin{align*}
U(t,x) &= \sum\limits_{\substack{k\geq 0\\ \alpha\geq 0}}a_{k}, \ t^{k}x^{\alpha}\\[3pt]
&= \left(\sum a_{0,\alpha}x^{\alpha}\right)+\cdots+\left(\sum a_{k\alpha}x^{\alpha}\right)t^{k}+\cdots
\end{align*}
Since $\phi(x)$ is given we know already $a_{0,\alpha}$. Substituting $U(t,x)$ in the right hand side of the differential equation, we obtain an expansion of the form
$$
\sum \ b_{k,\alpha} t^{k}x^{\alpha}
$$
where\pageoriginale $b_{k,\alpha}$ is a polynomial $P_{k,\alpha}$ (with non-negative coefficients) in the coefficients of the power series expansion of $H$ in all the variables and in the $x$, $a_{j,\alpha}$ with $j\leq k$. We have
\begin{align*}
\frac{\partial U}{\partial t} &= \sum\limits_{\alpha\geq 0}ka_{k,\alpha}t^{k-1}x^{\alpha}\\[3pt]
&= \sum\limits_{\substack{\alpha \geq 0\\ k\geq 0}}(k+1)a_{k+1,\alpha}t^{k}x^{\alpha}
\end{align*}

Equating the coefficients (as we want to satisfy the differential equation) we get 
$$
a_{k+1,\alpha}=\frac{1}{k+1}P_{k,\alpha}(\quad , a_{j,\alpha}),j\leq k
$$

Thus $a_{k+1,\alpha}$ is determined inductively.

We may also assume that $\phi(x)\equiv 0$. We have only to let $v(t,x)=U(t,x)-\phi(x)$ and consider the differential equation
$$
\frac{\partial v}{\partial t}=H\left(t,x,v+\phi,\frac{\partial (v+\phi)}{\partial x_{i}},\qquad\right)
$$
In this situation it is sufficient to prove the existence of a solution where the R.H.S. is replaced by a majorant $H'$ of $H$ and the initial datum has non-negative coefficients i.e., majorised zero and such that $H'$ and $\partial H'/\partial x_{i}$ are zero at $(0,0)$. For if $A_{k,\alpha}$ are the coefficients of this solution, we will have $|a_{k,\alpha}|\leq A_{k,\alpha}$, since $P_{k,\alpha}$ is a polynomial with $\geq 0$ coefficients in $b_{k,\alpha}$ and $a_{0,\alpha}$. We shall now write down a majorant for $H$ such that the majorised equation has a solution all of whose coefficients are $\geq 0$, and such that the solution and its first partial derivatives are zero at $(0,\ldots,0)$. We may also suppose that $H(0,\ldots,0)=0$. For if $H(0,\ldots,0)=c$, we can consider $v=U=0t$, (This satisfies $v\equiv 0$ in $t=0$), Since $H(0)=0$ we take for $H$ the majorant
$$
\frac{H}{\left(1-\dfrac{t/\alpha +x_{1}+\cdots+x_{n}+u}{a}\right)\left(1-\frac{P_{1}+\cdots+P_{n}}{b}\right)}-M\quad a, b>0
$$\pageoriginale
for a constant $\alpha$ with $0 < \alpha < 1$ which will be chosen presently (considering a majorant of this type is due to, apparently, Kowalowsky and the introduction of $\alpha$ is due to Goursat. We thus consider the differential equation
$$
\frac{\partial U}{\partial t}=\frac{M}{\left(1-\frac{t/\alpha+x_{1}+\cdots+x_{n}+U}{a}\right)\left(1-\frac{\partial u\partial x_{1}+\cdots+\partial u/\partial x_{n}}{b}\right)}-M
$$

We seek a solution depending only on
$$
x=t/\alpha+x_{1}+\cdots+x_{n},\quad \text{say}\quad z(x).
$$
Since $\partial u/\partial t=1/\alpha \ dz/dx$ and $\partial u/\partial x_{i}=dz/dx$, we have to consider the differential equation
$$
\frac{1}{\alpha}\frac{dz}{dx}=\frac{M}{\left(1-\frac{x+z}{a}\right)\left(1-\frac{n}{b}\frac{dz}{dx}\right)}-M
$$
or
$$
\frac{1}{\alpha}\frac{dz}{dx}\left(1-\frac{n}{b}\frac{dz}{dx}\right)=\frac{M}{\left(1-\frac{x+z}{a}\right)}-M\left(1-\frac{n}{b}\frac{dz}{dx}\right)
$$
or
$$
(1/\alpha - Mn/b)\frac{dz}{dx}=\frac{n}{\alpha b}\left(\frac{dz}{dx}\right)^{2}+\frac{M}{1-\frac{x+z}{a}}-M
$$
so that
\begin{align*}
\frac{dz}{dx} &= \frac{\left(\frac{1}{\alpha}-\frac{Mn}{b}\right)\pm \sqrt{\left(\left(\frac{1}{\alpha}-\frac{Mn}{b}\right)-\frac{4n}{\alpha b}\left(\frac{M}{1-x+z/a}-M\right)\right)}}{\frac{2n}{\alpha b}}\\
&= \psi (x,z)\quad\text{say}.
\end{align*}
Choose $\alpha$ so small that $(1/\alpha - Mn/b)>0$.

If put $x=z=0$ and take the minus sign in $\pm$ we get $\psi (0,0)=0$ and the quantity under the radical sign is $>0$ for $x=z=0$ and hence in a nhd of $(0,0)$. Hence $\psi(x,z)$ is a real valued analytic function in a nhd of $(0,0)$. By\pageoriginale the existence theorem for ordinary differential equations we have an analytic solution $z(x)$ in a neighbourhood of $0$ with $z(0)=0$. Note that $dz/dx(0)=\psi (0,0)=0$. If we show that all the coefficients of the power series expansion of $z$ at $0$ are $\geq 0$ then function
$$
U(t,x_{1},\ldots,x_{n})=z(t/\alpha +x_{1}+\cdots+x_{n})
$$
will have its Taylor coefficients non-negative; moreover we have
$$
U(0)=z(0)=0\quad\text{and}\quad \frac{\partial u}{\partial x_{i}}(0)=\dfrac{dz}{dt}(0)=0.
$$
The proof is completed by

\begin{lemma*}
Let $z(x)$ be an analytic function in a nhd of with $z(0)=\dfrac{dz}{dx}(0)=0$ and satisfying
$$
\frac{dz}{dx}=A\left(\frac{dz}{dx}\right)^{2}+\eta (x,z)
$$
where $A>0$ and the Taylor coefficients of $\eta (x,z)$ are $\geq 0$. Then the Taylor coefficients $z$ are $\geq 0$ ($\eta(x.z)$ is assumed to be analytic in a nhd of $(0,0)$.
\end{lemma*}

\begin{proof}
Writing $z=a_{2}x^{2}+a_{3}x^{3}+\cdots$ we find that
$$
a_{n}=P(a_{2},\ldots,a_{n-1},b_{ij}), \ i+j\leq n - 1, \ a_{2}=\dfrac{b_{0,1}}{2}
$$
where $b_{ij}$ are the Taylor coefficients of $\eta(x,z)$, $P$ being a polynomial with non-negative coefficients. 
\end{proof}
\end{proof}

\begin{theorem*}[(Cauchy-Kowalewsky for First Order System)~]
Let $H_{1},\ldots,H_{N}$ be analytic functions in the variables
$$
\left(t,x_{1},\ldots,x_{n},u_{1},\ldots,u_{N},p^{j}_{i}\right)^{i=1,\ldots,n}_{j=1,\ldots,N}
$$
in a nhd of
$$
(t^{\circ},x^{\circ}_{i},u^{\circ}_{j},(p^{j}_{i})^{\circ}).
$$
Let
$$
(\phi_{j}),j=1,\ldots,N
$$
be\pageoriginale analytic functions of $x=(x_{1},\ldots,x_{n})$ in a nhd of $x^{\circ}=(x^{\circ}_{1},\ldots,x^{\circ}_{n})$ such that $\phi_{j}(x_{0})=u^{\circ}_{j}$ and
$$
\frac{\partial \phi_{j}}{\partial x_{i}}(x_{0})=(p^{j}_{i})^{\circ}.
$$
Then there exists uniquely determined analytic function $U_{j}(j=1,\ldots,N)$ of $(t,x)$ in a nhd $V$ of $(t^{\circ},x^{\circ})$ such that
\begin{itemize}
\item[\rm(a)] $\dfrac{\partial u_{j}}{\partial t}=H_{j}\left(t,x,U_{j}(t,x),\dfrac{\partial u_{j}}{\partial x_{i}}\right)$, \ for \ $j=1,\ldots,N$

\item[\rm(b)] $U_{j}(t_{0},x)=\phi_{j}(x)$ \ for \ $x$ such that

$(t_{0},x)\in V(j=1,\ldots,N)$.
\end{itemize}
\end{theorem*}

\begin{proof}
Exercise.
\end{proof}

\section*{Higher Order Equations.}

The Cauchy problem for a single higher order equation\footnote[1]{of the form
$$
\left(\frac{\partial}{\partial t}\right)^{k}u=H\left(t,x,D^{\alpha}_{r}\left(\frac{\partial}{\partial t}\right)^{m}u\right)\quad\text{with}\quad |\alpha | + m< k\quad\text{and}\quad m<k
$$
({\em Cauchy type})}
(or for higher order systems) can be reduced to the Cauchy problem for a first order system. For simplicity of notation let us consider the Cauchy problem for a single second order equation of `Cauchy type'.

Thus consider the Cauchy problem
$$
\frac{\partial^{2}u}{\partial t^{2}}=H\left(t,x,u,\frac{\partial u}{\partial t}, \frac{\partial^{2}u}{\partial x_{i}\partial x_{j}}, \ \frac{\partial^{2}u}{\partial x_{i}\partial t}\right)
$$
with
\begin{align*}
u(0,x) &= \phi_{1}(x)\\[3pt]
\frac{\partial u}{\partial t}(0,x) &= \phi_{2}(x).
\end{align*}
Then if $u$ is a solution, the functions $u$, $u_{0}=\dfrac{\partial u}{\partial t}$, $u_{i}=\dfrac{\partial u}{\partial x_{i}}(i=1,\ldots,n)$ satisfy the equations:
\begin{align*}
& \frac{\partial u_{0}}{\partial t} =H\left(t,x,u,u_{0},u_{i},\frac{\partial u_{j}}{\partial x_{i}}, \ \frac{\partial u_{0}}{\partial x_{i}}\right)\\[3pt]
& \frac{\partial u_{i}}{\partial t}=\frac{\partial u_{0}}{\partial x_{i}}(i=1,\ldots,n)\\[3pt]
& \frac{\partial u}{\partial t}=u_{0}
\end{align*}\pageoriginale
with initial conditions
\begin{align*}
u(0,x) &= \phi_{1}(x)\\[3pt]
u_{0}(0,x) &= \phi_{2}(x)\\[3pt]
u_{i}(0,x) &= \frac{\partial \phi_{1}}{\partial x_{i}}(x)\quad\text{for}\quad i=1,\ldots,n.
\end{align*}
Now consider the above Cauchy problem for this system with unknowns $u$, $u_{0}$, $u_{i}$. We shall show that if $(u,u_{0},u_{i})$ is a solution for this problem, then $u$ is a solution for the original problem in a nhd of $(t_{0},x_{0})$. Since $u_{0}=\dfrac{\partial u}{\partial t}$, we have only to show that $u_{i}(t,x)=\dfrac{\partial u}{\partial x_{i}}(t,x)$ for $i=1,\ldots,n$, in a nhd of $(t_{0},x_{0})$. To prove this, we have to use the special nature of the initial conditions. Now the equation
$$
\frac{\partial u_{i}}{\partial t}=\frac{\partial u_{0}}{\partial x_{i}}
$$
gives, using
$$
\frac{\partial u}{\partial t}=u_{0},\quad\text{that}\quad \frac{\partial u_{i}}{\partial t}=\frac{\partial u}{\partial t\partial x_{i}}\quad\text{or}\quad \frac{\partial}{\partial t}\left(u_{i}-\frac{\partial u}{\partial x_{i}}\right)=0.
$$
Thus $u_{i}-\dfrac{\partial u}{\partial x_{i}}$ is independent of $t$. Thus, if we take a nhd around $(0,x_{0})$ which is a ball or a cube, we have, for $(t,x)$ in this nhd,
\begin{align*}
u_{i}(t,x) - \frac{\partial u}{\partial x_{i}}(t,x) &= u_{i}(0,x)-\frac{\partial u}{\partial x_{i}}(0,x)\\[3pt]
&= \frac{\partial \phi_{1}}{\partial x_{i}}(0,x)-\frac{\partial \phi_{1}}{\partial x_{i}}(x)\\[3pt]
&= 0,
\end{align*}
by\pageoriginale the initial conditions, $u(0,x)=\phi_{1}(x)$ and $u_{i}(0,x)=\dfrac{\partial \phi_{1}}{\partial x}(x)$.

\begin{remarks*}
\begin{enumerate}
\item In a sense the system has more solutions than the original equation as we have used only special Cauchy data, while the system can be solved for arbitrary Cauchy data.

\item By introducing $v=\dfrac{\partial^{2}u}{\partial t^{2}}$ as a new independent variable, we can reduce the equation to a quasi-linear system, i.e., to an equation of the form
$$
\begin{cases}
\frac{\partial}{\partial t}Y=\sum^{n}_{i=1}A_{i}(t,x,Y)\frac{\partial Y}{\partial x_{j}}+B(t,x,Y)\\[4pt]
Y(0,x)=\phi(x)
\end{cases}
$$
where $Y$ and $B(t,x,Y)$ are functions with values in $\mathbb{R}^{N}$ and $A_{i}$ a function with values in $N\times N$ matrices. We may also assume $\phi(x)\equiv 0$ and $A_{i}$ and $B$ do not involve $t$ be introducing a new variable $w$ and adding the equation $\dfrac{dw}{dt}=1$. The additional equation to be added to earlier ones is obtained by differentiating $\dfrac{\partial^{2}u}{\partial t}=H$ w.r.t. to $t$.\footnote[1]{This reduction to quasi-linear system is valid for any operator of Cauchy type.}
\end{enumerate}
(See Folland: Introduction to partial differential equations Ch. I, D). In this quasi-linear form the proof Cauchy-Kowalewski simplifies somewhat. If the original equation were linear the system would also be linear, i.e. $A_{i}$ and $B$ would not involve $Y$.

By reducing a single equation to a first order system, the following Cauchy problem can be solved in the analytic case.
\begin{itemize}
\item[(a)] $\left(\dfrac{\partial}{\partial t}\right)^{k}u=H\left(t,x,D^{\alpha}_{x}\left(\dfrac{\partial}{\partial t}\right)^{m}u\right)$ \ where \ $|\alpha | + m\leq k$ and $m<k$

\item[(b)] $\left(\dfrac{\partial}{\partial t}\right)^{m}u(0,x)=\phi_{m}(x)$ \ for \ $0\leq m<k$.
\end{itemize}
More\pageoriginale generally the Cauchy problem for systems of the following type can be solved (in the analytic case)
\begin{itemize}
\item[(a)] $\left(\dfrac{\partial}{\partial t}\right)^{k_{i}}u_{i}=H_{i}\left(t,x,D^{\alpha}_{x}\left(\dfrac{\partial}{\partial t}\right)^{m}u_{j}\right), i, j=1,2,\ldots N$

where \ $|\alpha|+m\leq k_{j}$ and $m<k_{j}$.

\item[(b)] $\left(\dfrac{\partial}{\partial t}\right)^{m}u_{j}=\phi_{m,j}(x)$  \ for \ $0\leq m<k_{j}$, $j=1,\ldots,N$.
\end{itemize}
\end{remarks*}

\section*{The Generalised Cauchy Problem, Symbol and Characteristics:}\pageoriginale

Let $\Omega$ be an open subset of $\mathbb{R}^{n}$ and $S$ a submanifold of dimension $(n-1)$. Since our considerations will be local, we may assume that $S=\{x\in \Omega|\phi(x)=0\}$ where $\phi$ is a real valued $C^{\infty}$ (or analytic) function on $\Omega$ such that $d\phi \neq 0$ on $S$ i.e., at least one partial derivative does not vanish on $S$. Assume that we are given a nowhere vanishing vector field $Z$ on $\Omega$ such that $Z$ is not tangential to $S$ at any point of $S$. This means that we are given a first order linear differential operator
$$
\sum\limits^{n}_{i=1}a_{i}(x)\frac{\partial}{\partial x_{i}}\quad\text{on}\quad \Omega
$$
such that not all $a_{i}(x)(i=1,\ldots,n)$ vanish at any point $x\in \Omega$ and such that
$$
\sum\limits^{n}_{i=1}a_{i}(x)\frac{\partial f}{\partial x_{i}}\neq 0 \quad\text{on}\quad S.
$$

Let $P=\sum\limits_{|\alpha |\leq m}a_{\alpha}(x)D^{\alpha}$ be a linear differential operator in $\Omega$, where $a_{\alpha}$ are $C^{\infty}$ (or analytic) functions in $\Omega$. We consider the following generalised Cauchy problem:
\begin{itemize}
\item[(a)] $Pu=f$ \ in \ $\Omega$

\item[(b)] $Z^{k}u|_{s}=\psi_{k}$, \ $k=1,\ldots,m-1$
\end{itemize}
where $Z^{k}u=\left(\sum a_{i}\dfrac{\partial}{\partial x_{i}}\right)^{k}u$ and $f$ (resp. $\psi_{k}$) is a given function on $\Omega$ (resp. on $S$) (A function $\psi$ on $S$ is said to be $C^{\infty}$ if for any $s\in S$, there exists a nhd $U$ of $s$ in $\Omega$ and a $C^{\infty}$ function $\overline{\psi}$ on $\Omega$ such that $\overline{\psi}|_{S\cap U}=\psi$. When $S$ is analytic, analyticity of $\psi$ is defined similarly).

We now investigate under what condition this generalised Cauchy problem can be reduced, by a change of coordinates, to a Cauchy problem of the standard type which we considered previously; namely:
$$
\left(\frac{\partial}{\partial t}\right)^{m}u=H(t,z,D^{j}_{t}D^{\alpha}_{z}u), \ |j|<m, \ |\alpha |+j\leq m
$$
and\pageoriginale $S$ is the hyper plane $t=0$. (Actually $H$ will be a linear or ``{\em affine}'' function).

We first prove a proposition (this does not involve $P$).

\begin{prop*}
Let $s_{0}\in S$. We can introduce coordinates $(y_{1},\ldots,y_{n})$ in a neighbourhood $V$ of so such that $S\cap V=\{(y_{1},\ldots,y_{n})|y_{1}=0\}$ and such that in this coordinate system the vector field $Z$ is represented in $V$ by the vector field $\dfrac{\partial}{\partial y_{1}}$. More precisely, there exists a neighbourhood $V$ of $s_{0}$ and a diffeomorphism $\phi$ (or an analytic diffeomorphism in the analytic case) onto a neighbourhood $V'$ of $0$ in $\mathbb{R}^{n}$ such that $\phi(S)=\{y\in V|y_{1}=0\}$, $[y=(y_{1},\ldots,y_{n})]$ and $\phi_{*}(Z)=\dfrac{\partial}{\partial y_{1}}$, where $\phi_{*}$ denotes the differential of $\phi$.
\end{prop*}

\begin{proof}
The proof is essentially same as the usual proof for the fact that given a nowhere zero vector field we can locally find a coordinate system in which the vector field has the form $\dfrac{\partial}{\partial y_{1}}$ (see e.g. N.J. Hicks, Notes on Differential Geometry, p. 124 Lemma). We can assume, by the implicit function theorem, that there is a coordinate system $(x_{1},\ldots,x_{n})$ in which $s_{0}$ is represented by $(0,\ldots,0)$ and $S$ is given by $x_{1}=0$. If $Z=\sum a_{i}(x)\dfrac{\partial}{\partial x_{i}}$, the transversality condition becomes $a_{1}(x)\neq 0$. By the existence theorem for ordinary differential equations, we can find a smooth map $F:I\times U'\to U$ such that $F(0,x_{1},\ldots,x_{n})=(x_{1},\ldots,x_{n})$ and $F_{*}\left(\dfrac{\partial}{\partial t}\right)=Z$, where $I$ (resp. $U$, $U'$) is a nhd of 0 in $\mathbb{R}$ (resp. in $\mathbb{R}^{n}$). Consider the map 
$$
F_{0}:(t,x_{2},\ldots,x_{n})\to F(t,0,x_{2},\ldots,x_{n})
$$
Note that $F_{0}(0,x_{2},\ldots,x_{n})=(0,x_{2},\ldots,x_{n})$. Since $F_{0^{*}}\left(\dfrac{\partial}{\partial x_{i}}\right)^{*}\frac{partial}{\partial x_{i}}$ for $i\geq 2$ and $F_{0^{*}}\left(\dfrac{\partial}{\partial t}\right)=Z$, $F_{0}$ is a local diffeo. Take $\phi=F^{-1}_{*}$ and $y_{1}=t$.
\end{proof}

\begin{remark*}
The integral curves through points in $x_{1}=0$ are transversal to the hyperplane $x_{1}=0$. We take the time parameter along the integral curves as $y_{1}$ coordinate.
\end{remark*}

Under\pageoriginale the diffeomorphism, the differential operator
$$
P=\sum\limits_{|\alpha|\leq m}a_{\alpha}(x)D^{\alpha}_{x}
$$
goes over into a differential operator
$$
\overline{P}=\sum\limits_{|\alpha|\leq m}\overline{a}_{\alpha}(y)D^{\alpha}_{y}
$$
If $\overline{a}_{(1,1,\ldots 1)}(y_{0})\neq 0[\alpha = (1,1,\ldots,1)]$ we get, in a neighbourhood of $y_{0}$, a differential operator of the Cauchy type by dividing out by $\overline{a}_{(1,1,\ldots,1)}(y_{0})$ and in the analytic case, we can solve the Cauchy problem for this operator which would solve the original Cauchy problem. We wish to translate the condition $a_{(1,1,\ldots,1)}(y_{0})\neq 0$ in the original set up i.e., in the intrinsic formulation.

\begin{defi*}
Let $P=\sum\limits_{|\alpha|\leq m}a_{\alpha}(x)D^{\alpha}$ be a linear differential operator in $\Omega$ and let $x\in \Omega$. The homogeneous polynomial of degree of $m$ defined on $\mathbb{R}^{n}$ by
$$
P_{m}(x,\xi)=\sum\limits_{|\alpha|=m}a_{\alpha}(x)\xi^{\alpha},\quad \xi=(\xi_{1},\ldots,\xi_{n})\in \mathbb{R}^{n},
$$
is called the characteristic form (or symbol or principal symbol) of $P$ at $x$ (We have formally replaced $\dfrac{\partial}{\partial x_{i}}$ by $\xi_{i}$ in the highest order terms of the operator)
\end{defi*}

\begin{exer*}
Let $P=\sum\limits_{|\alpha|\leq m}a_{\alpha}D^{\alpha}$ and $Q=\sum\limits_{|\beta|\leq n}b_{\beta}D^{\beta}$ be linear differential operators.
\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item If $R$ is the differential operator $PQ$ prove that
$$
R_{m+n}(x,\xi)=P_{m}(x,\xi)Q_{n}(x,\xi)
$$

\item If $m=n$, prove that
$$
(P+Q)_{m}(x,\xi)=P_{m}(x,\xi)+Q_{m}(x,\xi)
$$
\end{enumerate}
\end{exer*}

The next proposition gives information on how the symbol changes under coordinate transformations.

\begin{prop*}
Let\pageoriginale $\phi$ be a diffomorphism of an open set $\Omega$ in $\mathbb{R}^{n}$ onto an open set $\Omega'$ in $\mathbb{R}^{n}$. Let
$$
P=\sum\limits_{|\alpha|\leq m}a_{\alpha}(x)D^{\alpha}_{x}\quad\text{be a}
$$
linear differential operator on $\Omega$ and
$$
\overline{P}=\sum\limits_{|\alpha|\leq m}\overline{a}_{\alpha}(y)D^{\alpha}_{y}
$$
transformed differential operator on $\Omega'$. Let $J_{x}$ denote the Jacobian matrix of $\Phi$ for $x\in \Omega$. We have then, for $\xi \in \mathbb{R}^{n}$
$$
\overline{P}_{m}(y,\xi)=P_{m}(x,t_{J_{x}}\xi),\qquad\text{where}
$$
$x=\phi^{-1}(y)$ and ${}^{t}J_{x}$ denotes the transpose of the matrix $J_{x}$.
\end{prop*}

\begin{proof}
Write $\phi(x_{1},\ldots,x_{n})=(y_{1},\ldots,y_{n})$. Then
\begin{align*}
& J_{x}=\left(\dfrac{\partial y_{i}}{\partial x_{j}}(x)\right).\qquad\text{We have the chain rule}\\[3pt]
& \frac{\partial}{\partial x_{j}}=\sum\limits^{n}_{t=1}\frac{\partial y_{i}}{\partial x_{j}}\frac{\beta}{\partial y_{i}}\quad\text{i.e., \ if } u(y_{1},\ldots,y_{n})\quad \text{is a function on } \Omega',\\[3pt]
& \frac{\partial (u0\Phi)}{\partial x_{j}}(x)=\frac{\partial y_{i}}{\partial x_{j}}(x)\frac{\partial u}{\partial y_{i}}(y)\quad\text{where}\quad \phi(x)=y.
\end{align*}
This relation may be written as
$$
\begin{pmatrix}
\frac{\partial (u 0\Phi)}{\partial x_{1}}\\[3pt]
\vdots\\[3pt]
\frac{\partial(u 0 \Phi)}{\partial x_{n}}
\end{pmatrix}
=
{}^{t}J_{x}
\begin{pmatrix}
\frac{\partial u}{\partial y_{1}}(y)\\[3pt]
\vdots\\[3pt]
\frac{\partial u}{\partial y_{n}}(x)
\end{pmatrix}
$$
(Note:\pageoriginale
$$
\begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\[3pt]
a_{21} & a_{22} & \ldots & a_{2n}\\[3pt]
         &         &      &\\
a_{n1} & & & a_{nn}
\end{pmatrix}
\begin{pmatrix}
\alpha_{1}\\[3pt]
\vdots\\[3pt]
\alpha_{n}
\end{pmatrix}
=
\begin{pmatrix}
\beta_{1}\\[3pt]
\vdots\\[3pt]
\beta_{n}
\end{pmatrix}
$$
gives
$$
\sum^{n}_{j=1}a_{ij}\alpha_{j}=\beta_{i}\quad\text{or}\quad \sum^{n}_{i=1}a_{ji}\alpha_{i}=\beta_{j}.
$$
Thus we may write symbolically
$$
\begin{pmatrix}
\frac{\partial}{\partial x_{1}}\\[3pt]
\vdots\\[3pt]
\frac{\partial}{\partial x_{n}}
\end{pmatrix}
=
t_{J}
\begin{pmatrix}
\frac{\partial}{\partial y_{1}}\\[3pt]
\vdots\\[3pt]
\frac{\partial}{\partial y_{n}}
\end{pmatrix}
=
t_{J}\partial_{y},\quad\text{where}\quad \partial_{y}=
\begin{pmatrix}
\frac{\partial}{\partial y_{1}}\\[3pt]
\vdots\\[3pt]
\frac{\partial}{\partial y_{n}}
\end{pmatrix}
$$
The differential $\overline{P}$ is obtained from $P$ by replacing $\partial/\partial x_{j}$ by
$$
\sum \frac{\partial y_{i}}{\partial x_{j}}\frac{\partial}{\partial y_{i}}=(t_{J} \ \ \partial_{y})_{j}.
$$
Thus
$$
\overline{P}=\sum\limits_{|\alpha|\leq m}a_{\alpha}(x)({}^{t}J_{x}\partial_{y})^{\alpha}, \ \ x=\phi^{-1}(y).
$$
It follows from the last exercise (concerning the symbols of the product and sum of operators) that the symbol of $\overline{P}$ is
$$
\sum\limits_{|\alpha|\leq m}a_{\alpha}(x)({}^{t}J_{x}\xi)^{\alpha}=P_{m}(x,{}^{t}J \ \xi)
$$
\end{proof}

\begin{remark*}
The proposition shows that the symbol of a linear differential operator order $m$ on a manifold is intrinsically defined as a function on the cotangent bundle which is a homogeneous polynomial of degree $m$ on each fibre.
\end{remark*}

Now\pageoriginale $a_{(1,1,\ldots 1)}(y)=P_{m}(y,\xi)$ where $\xi$ is the vector $(1,0\ldots 0)$. We have, for this $\xi$,
$$
t_{J}\xi=
\left(\frac{\partial y_{1}}{\partial x_{1}},\ldots \frac{\partial y_{n}}{\partial x_{n}}\right).
$$
Thus if
$$
\sum\limits_{|\alpha|=m}a_{\alpha}(x)\left(\frac{\partial y_{1}}{\partial x_{1}}\right)^{\alpha_{1}}\ldots \left(\frac{\partial y_{1}}{\partial x_{n}}\right)^{\alpha_{n}}(x_{0})\neq 0
$$
the Cauchy problem is solvable in the analytic case in a neighbourhood of $x_{0}$. But it can be checked (see the next remark) that this condition is equivalent to
$$
\sum\limits_{|\alpha|=m}a_{\alpha}(x)\left(\frac{\partial \phi}{\partial x_{1}}\right)^{\alpha_{1}}\ldots \left(\frac{\partial \phi}{\partial x_{n}}\right)^{\alpha_{n}}{\displaystyle{\mathop{=}_{\text{def}}}}\;P_{m}(x,d\phi)\neq 0.
$$

\begin{remark*}
The covector $d\phi(x)$ generates the 1 dimensional space in the cotangent space $x$ orthogonal to the tangent space to $S$ at $x$.
\end{remark*}

\begin{defi*}
Let $S$ be defined by $\phi$ with $d\phi\neq 0$ on $S$. Let $x\in S$. We say that the point $x\in S$ is characteristic (resp. non-characteristic) with respect to the operator $P$ is
$$
P_{m}(x,d\phi)\,{\displaystyle{\mathop{=}_{\text{\rm def}}}}\;\sum\limits_{|\alpha|=m}a_{\alpha}(x)\left(\dfrac{\partial \phi}{\partial x_{1}}\right)^{\alpha_{1}}\ldots \left(\dfrac{\partial \phi}{\partial x_{n}}\right)^{\alpha}n=0(\text{resp.}~ \neq 0)
$$
We say that $S$ is characteristic (resp. non-characteristic) if every point of $S$ (resp. no point of $S$) is characteristic.
\end{defi*}

\begin{remark*}
The set of zeros of the symbol defines a cone (characteristic cone) in every cotangent space. A point $s\in S$ is characteristic if the orthogonal of the tangent space to $S$ at $s$ lies in the characteristic cone at $s$.
\end{remark*}

The above considerations show that if $S$ is non-characteristic the Cauchy problem can be solved in the analytic case.

\section*{Compatibility Conditions for Initial Data on Characteristics:}\pageoriginale

If a smooth hyper surface $S$ is non-characteristic, all partial derivatives (with respect to all variables) of a solution are determined uniquely on $S$ by the initial data, as is seen by writing the differential equation in the Cauchy-Kowalewski form in a neighbourhood of a point of $S$ Essentially this is what we used to write down the coefficients of the power series expansion in the analytic case).

We shall now see that the initial data cannot be prescribed arbitrarily on smooth characteristic hypersurfaces. For simplicity, let us consider a second order linear differential operator
$$
L=\sum\limits_{i,j}a_{ij}(x_{1}\ldots x_{n})\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}+b_{i}\frac{\partial}{\partial x_{i}}+C
$$
in $\mathbb{R}^{n}$, $n\geq 2$, with smooth coefficients and consider the equation $Lu=0$. Assume that $x_{1}=0$ is the hypersurface $S$ under consideration. If $x_{1}=0$ is characteristic, we have $a_{11}(0,x_{2},\ldots,x_{n})\equiv 0$. Since $Lu=0$, we have using $a_{11}=0$ on $x_{1}=0$,
\begin{align*}
& \sum\limits^{n}_{j=2}a_{1j}(0,x_{2},\ldots,x_{n})\frac{\partial}{\partial x_{j}}\left(\frac{\partial u}{\partial x_{1}}\big|_{x_{1}}=0\right)+b_{1}(0,x_{2},\ldots,x_{n})\left(\frac{\partial u}{\partial x_{1}}\big|_{x_{1}=0}\right)\\[3pt]
& = G(x_{2},\ldots,x_{n})
\end{align*}
where $G$ is a function determined by $u|x_{1}=0$ and the partial derivatives of $u|_{x_{1}=0}$ upto order 2. Thus $\left.\dfrac{\partial u}{\partial x_{1}}\right|_{x_{1}=0}$ satisfies a first order linear partial differential equation on $S$ and thus the initial data have to satisfy a certain compatibility condition. Even if $\left.\dfrac{\partial u}{\partial x_{1}}\right|_{x_{1}=0}$ is chosen to satisfy this condition, $\left.\dfrac{\partial^{2}u}{\partial x_{1}}\right|_{x_{1}=0}$ is not uniquely determined. In fact from the equation $\dfrac{\partial}{\partial x_{1}}Lu=0$, we see that
\begin{gather*}
\sum\limits_{j=2}^{n}a_{ij}(0,x_{2},\ldots,x_{n})\frac{\partial}{\partial x_{j}}\left(\left.\frac{\partial^{2}u}{\partial x^{2}_{1}}\right|_{x_{1}=0}\right)+\frac{\partial a_{11}}{\partial x_{1}}(0,x_{2},\ldots,x_{n})\left(\left.\frac{\partial^{2}u}{\partial x^{2}_{1}}\right|_{x_{1}=0}\right)\\[3pt]
=G'(x_{2},\ldots,x_{n})
\end{gather*}\pageoriginale
where $G'$ is determined by $u\big|_{x_{1}=0}$ and $\dfrac{\partial u}{\partial x_{1}}\big|_{x_{1}=0}$.

The linear differential operator
$$
\sum\limits^{n}_{j=2}a_{ij}(0,x_{2},\ldots,x_{n})\dfrac{\partial}{\partial x_{j}}
$$
or what is the same the vector field
$$
\sum\limits^{n}_{j=2}a_{ij}(0,x_{2},\ldots,x_{n})\dfrac{\partial}{\partial x_{j}}
$$
on $S$ thus plays an important role. The integral curves of this vector field on $S$ are called the {\em bicharacteristics}.

\section*{Discontinuities of Derivatives of Solutions and Characteristics:}

\begin{prop*}
Let $L$ be a second order linear differential operator on $\mathbb{R}^{n}(n\geq 2)$ with smooth coefficients. Suppose $u$ is a smooth function in a neighbourhood of 0 in $\mathbb{R}^{n}$ such that
\begin{enumerate}
\renewcommand{\labelenumi}{\rm(\theenumi)}
\item $u$ is of class $C^{1}$ in $U$

\item is of class $C^{2}$ in $U\cap \{(x_{1},\ldots,x_{n}|x_{1}\geq 0\}$ and in $U\cap \{(x_{1},\ldots,x_{n})|x_{1}\leq 0\}$; further $u$ satisfies $Lu=0$ in $U]cap \{x_{1}>0\}$ and in $U\cap \{x_{1}<0\}$)
\end{enumerate}
If $x_{1}=0$ is non-characteristic for $L$, then $u$ is of class $C^{2}$ in $U$.
\end{prop*}

\begin{remark*}
A function $u$ satisfying (1) and (2) is a distribution solution of $Lu=0$ as seen by applying the generalised Green's formula. Before proving the proposition, let us make a
\end{remark*}

\begin{defi*}
If\pageoriginale a function $v$ defined in $\Omega$ is continuous in $\Omega\cap [x_{1}\geq 0]$ and in $\Omega\cap [x_{1}\leq 0]$ we defined the jump of $v$ on $\Omega\cap [x_{1}=0]$ to be the function
\begin{gather*}
v_{J}(0,x_{2},\ldots,x_{n})=\lim\limits_{x_{1}\to +0}v(x_{1},\ldots,x_{n})-\lim\limits_{x_{1}\to -0}v(x_{1},\ldots,x_{n})\\[3pt]
\text{for}\quad (0,x_{2},\ldots,x_{n})\in \Omega \cap [x_{1}=0].
\end{gather*}
\end{defi*}

\section*{Proof of the Proposition:}

The jump of the function $Lu$ is clearly zero. Writing out $Lu$ explicitly as
\begin{align*}
Lu = a_{11}\frac{\partial^{2}u}{\partial x^{2}_{1}} &+ \sum\limits_{j\geq 2}a_{1j}\frac{\partial}{\partial x_{j}}\left(\frac{\partial u}{\partial x_{1}}\right)+\sum\limits^{n}_{i=2}a_{ii}\frac{\partial^{2}}{\partial x^{2}_{i}}+\\[3pt]
&+ \sum\limits^{n}_{i=1}b_{i}\frac{\partial u}{\partial x_{i}}+Cu
\end{align*}
we see by our assumptions that the jump of $Lu$ at 
$$
(0,x_{2},\ldots,x_{n})\quad\text{is}\quad a_{11}(0,x_{2},\ldots,x_{n})\times
$$
jump of $\dfrac{\partial^{2}u}{\partial x^{2}_{1}}$ at $(0,x_{2},\ldots,x_{n})$. Since $x_{1}=0$ is non-characteristic 
$$
a_{11}(0,x_{2},\ldots,x_{n})\neq 0,
$$
so that the jump of $\dfrac{\partial^{2}u}{\partial x^{2}_{1}}$ at $(0,x_{2},\ldots,x_{n})$ is zero. Since the other second partial derivatives clearly exist and are continuous, we see that $u$ is of class $C^{2}$.

\begin{remark*}
The proof shows that if $\dfrac{\partial^{2}u}{\partial x^{2}_{1}}$ has non-zero jump at a point of $x_{1}=0$, that point must be characteristics. We thus $x$ see that if discontinuities of second derivatives of a solution occur along a smooth hypersurface $S$, then $S$ must be characteristic.
\end{remark*}

\begin{exer*}
Let $u$ be a continuous function in $x_{1}\geq 0$ satisfying
\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item $u(0,x_{2},\ldots,x_{n})=0$ \ for all \ $(x_{2},\ldots,x_{n})$

\item $\sum\limits^{n}_{i=1}\dfrac{\partial^{2}u}{\partial x^{2}_{1}}=0$ \ in \ $x_{1}>0$.
\end{enumerate}
\end{exer*}

Let $\overline{u}$ be the function in $\mathbb{R}^{n}$ defined by
$$
\overline{u}(x_{1},\ldots,x_{n})=
\left(
\begin{array}{ccc}
u(x_{1},\ldots,x_{n}) & \text{if} & x_{1}\geq 0\\[3pt]
-u(-x_{1},\ldots,x_{n}) & \text{if} & x_{1}\leq 0
\end{array}
\right).
$$
Prove that $\overline{u}$ is a $C^{2}$ function in $\mathbb{R}^{n}$ and satisfies 
$$
\sum \frac{\partial^{2}u}{\partial x^{2}_{i}}=0\quad\text{in}\quad \mathbb{R}^{2}
$$
(Reflection principle).

\markboth{Bibliography}{Bibliography}

\begin{thebibliography}{99}
\bibitem{chap1-key1} E. Goursat. A course in Mathematical Analysis, Differential Equations.

\bibitem{chap1-key2} J. Hadamard: Lectures on the Cauchy Problem.

\bibitem{chap1-key3} G.B. Folland: Introduction to Partial Differential Equations.
\end{thebibliography}
