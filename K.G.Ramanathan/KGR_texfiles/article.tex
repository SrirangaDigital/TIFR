\section{Quadratic forms}\label{s1}



Let $K$ be a field of characteristic $\neq 2$. A \textit{quadratic
  form} $f$ over $K$ is a homogeneous polynomial of a degree $2$ in
$n$ variables $x_1,\ldots,x_n$ with coefficients in $K$;
$$
f=f(x_1,\ldots,x_n)=\sum\limits_{i, j=1}^{n}
a_{ij}x_ix_j,\quad a_{ij}\in K.
$$

We shall write $\underline{x}$ for the vector $(x_1,\ldots,x_n)$ so that 
$$
f(x_1,\ldots,x_n)=f(\underline{x}).
$$

We may, without loss in generality, assume that $a_{ij}=a_{ji}$ for
all $i,j$. The $n$ rowed square matrix
$$
s=(a_{ij})
$$
is symmetric and is called the symmetric matrix associated to $f$. If
$\underline{x'}$ denotes the column $\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix}$
  then
$$
f(\underline{x})=\underline{x}\, S \underline{x'}.
$$

The quadratic form is \textit{non-degenerate} if the determinant $|S|$
of $S$ is not zero.

If $\underline{x}= (x_1,\ldots,x_n)$ and
$\underline{y}=(y_1,\ldots,y_n)$ so that
$\underline{x}+\underline{y}=(x_1+y_1,\ldots, x_n+y_n)$ then
$$
f(\underline{x}+\underline{y})=f(\underline{x})+f(\underline{y})+2\underline{x}
S \underline{y}'.
$$

We put
$$
B(\underline{x},\underline{y})= \underline{x}\, S \underline{y}'
$$
and call it the \textit{associated bilinear form} to $f$. We have
\begin{equation}\label{eqn1}
2B(\underline{x},\underline{y})=f(\underline{x}+\underline{y})-f(\underline{x})-f(\underline{y}).
\end{equation}

The \textit{direct sum} $f_n \oplus \varphi_m$ of two quadratic
forms $f_n = f_n(x_1,\ldots, x_n)$ and
$\varphi_m=\varphi_m(y_1,\ldots,y_m)$ is a quadratic form in $n+m$
variables whose associated matrix is 
\begin{equation*}
\begin{pmatrix}
S & \ho\\
\ho & T
\end{pmatrix}
\end{equation*}
$S$ and $T$ being the associated matrices for $f_n$ and $\varphi_m$
respectively.

The \textit{tensor product} $f_n \otimes \varphi_m$ of two quadratic
forms is the quadratic form in $nm$ variables whose associated matrix
is the tensor product of $S$ and $T$. We shall be considering tensor
products only in the special case of diagonal forms. A quadratic form
$f_n(x_1,\ldots,x_n)$ is a \textit{diagonal form} if 
$$
f(\underline{x})=a_1x^{2}_1+\cdots + a_n x^{2}_n
$$
so that its associated matrix is the diagonal matrix
\begin{equation*}
\begin{pmatrix}
a_1 & & \ho\\
& \ddots &\\
\ho & & a_n 
\end{pmatrix}
\end{equation*}

We also write, using direct sum
\begin{equation}\label{eqn2}
f_n(\underline{x}) = <a_1> \oplus <a_2> \oplus\cdots \oplus < a_n
\end{equation}
as a direct sum of  $n$ quadratic forms each in a single variable.

If now 
$$
f_n(\underline{x})= \sum\limits_{i=1}^{n} a_ix^{2}_i, \varphi_m
(\underline{y})=\sum\limits _{j=1}^{m} b_jy^{2}_j
$$
then
\begin{equation}\label{eqn3}
f\otimes \varphi= \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_i b_j z^{2}_{ij}.
\end{equation}

It is easy to verify that for three quadratic forms $f, \varphi$ and
$\psi$
\begin{equation}\label{eqn4}
(f\oplus \varphi)\otimes \psi = (f\otimes \psi) \oplus (\varphi
  \otimes \psi).
\end{equation}

A quadratic form $f$ is said to \textit{represent} $a\in K$ if
there exist $\alpha_1,\ldots,\alpha_n$ in $K$ \textit{not all zero}
such that
$$
f(\underline{\alpha})=f(\alpha_1,\ldots,\alpha_n)=a.
$$

It is said to be \textit{universal} if it represents every element of
$K^{\ast}$. 

Two quadratic forms $f$ and $\varphi$ are said to be
\textit{equivalent} (in symbols $f\sim\varphi$) if there exists a
linear transformation
$$
x_i=\sum\limits_{j=1}^{n} c_{ij}y_j, \quad i=1,\ldots,n
$$
with $c_{ij}\in K$ and the matrix $(c_{ij})=C$ nonsingular, such
that 
$$
f(x_1,\ldots,x_n)= \varphi(y_1,\ldots,y_n).
$$

Since $C$ is nonsingular, this relation is an equivalence relation. It
is clear that if $S$ and $T$ are respectively the associated
symmetric matrices of $f$ and $\varphi$, then
\begin{equation}\label{eqn5}
T=C'SC
\end{equation}
$C'$ denoting the transpose of the matrix $C$. We can prove

\begin{prop}\label{prop0}
Every quadratic form is equivalent to a diagonal form.
\end{prop}

\begin{Proof}
It is enough to prove that for the symmetric matrix $S$ there exists a
non-singular $C$ such that $C' S C$ is a diagonal matrix. We may
assume that $S$ is not the identically zero matrix. It is then clear
that there exists a non-singular matrix $C_1$ such that 
$$
C'_1SC_1=(s_{ij}), s_{11} \neq \ho.
$$

We may therefore, without loss in generality, assume that this is
already true for $S$. Now 
\begin{equation*}
f(x_1,\ldots,x_n)=s_{11}x^{2}_1+2x_1(s_{12}x_2+\cdots+ s_{1n}x_n)+\varphi
\end{equation*}
where $\varphi$ is a quadratic form in the variable
$x_2,\ldots,x_n$. Since $s_{11}\neq 0$, we have
\begin{align*}
f&= s_{11}\left( x_1+\dfrac{s_{12}}{s_{11}} x_2+\cdots+
\dfrac{s_{1n}}{s_{11}} x_n
\right)^{2}-\dfrac{1}{s_{11}}(s_{12}x_{2}+\cdots+
s_{1n}x_n)^{2}\\
&\hspace{7cm}+\varphi(x_2,\ldots,x_n)\\
&= s_{11}y^{2}_1+\psi(y_2,\ldots,y_n)
\end{align*}
where
\begin{equation*}
\left.
\begin{aligned}
& y_1=x_1+\dfrac{s_{12}}{s_{11}} x_2+\cdots+ \dfrac{s_{1n}}{s_{11}}
  y_n\\
& y_i=x_j, \qquad\qquad i>1
\end{aligned}
\right\}
\end{equation*}
and $\psi(y_2,\ldots,y_n)$ is a quadratic form in the $n-1$ variables
$y_2,\ldots,y_n$.

If $C$ is the matrix
\begin{equation*}
C=
\begin{pmatrix}
1, & \dfrac{s_{12}}{s_{11}},\ldots,\dfrac{s_{1n}}{s_{11}}\\
\ho & \qquad  E_{n-1}
\end{pmatrix}
\end{equation*}
where $E_{n-1}$ is the unit matrix of order $n-1$, then
\begin{equation*}
c'^{-1}SC^{-1}=
\begin{pmatrix}
s_{11} & \ho\\
\ho & s_1
\end{pmatrix}
\end{equation*}
where $S_1$ is symmetric of order $n-1$. We can now complete the proof
by induction on $n$. 

We have the simple
\enprf
\end{Proof}


\begin{prop}\label{prop1}
If $S\sim T$ then $S$ and $T$ represent the same set of elements.
\end{prop}


\begin{Proof}
Let $S\sim T$ so that $T=C'SC.$, $|C|\neq \ho$. If $a\in K$ such
that 
\begin{equation*}
\alpha' S \underline{\alpha}=a, \quad
\alpha=\begin{pmatrix}\alpha_1\\\vdots\\\alpha_n\end{pmatrix},
\alpha_i\in K 
\end{equation*}
then
\begin{equation*}
a=\underline{\alpha'} S\underline{\alpha} = \underline{\alpha'}
C'^{-1} (C' SC)c^{-1}\underline{\alpha}=\underline{\alpha'} C'^{-1} TC\underline{\alpha}.
\end{equation*}

If $C\underline{\alpha}=\underline{\beta}$ then $C$ being non-singular
$\underline{\beta}$ is not identically zero since $\underline{\alpha}
\neq \underline{\ho}$.

Propositions~\ref{prop0} and \ref{prop1} tell us that in order to study representation
of zero by a quadratic form, it is enough to consider the case of
diagonal forms. We prove
\enprf
\end{Proof}


\begin{prop}\label{prop2}
If $f(x_1,\ldots,X_n)$ is a zero form (i.e. a quadratic form which
represents zero) then it is universal.
\end{prop}

\begin{Proof}
We may clearly take $f$ in the diagonal form 
\begin{equation*}
f(x_1,\ldots,x_n)=\sum\limits_{i=1}^{n} a_ix^{2}_i,\quad a_i\in K.
\end{equation*}

Let
\begin{equation}\label{eqn6}
\ho=\sum\limits_{i=1}^{n}a_i\alpha_i^{2}, \quad \alpha_i\in K
\end{equation}
and let without loss in generality $a_1\alpha_1\neq \ho$. Then
\begin{equation}\label{eqn7}
-1=\dfrac{a_2}{a_1}\left(\dfrac{\alpha_2}{\alpha_1}\right)^{2}+\cdots+
\dfrac{a_n}{a_1}\left(\dfrac{\alpha_n}{\alpha_1}\right)^{2}.
\end{equation}

Let $\beta\neq \ho$ be an element of $K$. Then
\begin{equation*}
\dfrac{\beta}{a_1}=\left(\dfrac{a+\dfrac{\beta}{a_1}}{2}\right)^{2}-\left(\dfrac{1-\dfrac{\beta}{a_1}}{2}\right)^{2}.
\end{equation*}

Inserting the value of $-1$, we get 
\begin{equation*}
\beta=a_1\left(\dfrac{1+\dfrac{\beta}{a_1}}{2}\right)^{2}+\sum\limits_{i=1}^{n}
a_i\left(\dfrac{\alpha_I}{\alpha_1}\left(\dfrac{1-\dfrac{\beta}{a_1}}{2}\right)\right)^{2}.
\end{equation*}
\enprf
\end{Proof}

\section{Formally real fields.}\label{s2}

Let $R$ be the field of real numbers and $P_{\ho}$ the set of positive
real numbers. Let $-P_{\ho}$ denote the set of elements $-x$ for
$x\in P_{\ho}$. Then every $a\in R$ is either zero are in $P_{\ho}$
or in $-P_{\ho}$.

Further if we put
$$
P=P_{\ho}\cup (\ho)
$$
then $P$ satisfies


\begin{equation}\label{eqn8}
\left.
\begin{aligned}
\mbox{i) } P&+P\subset P &\qquad  \mbox{ii) } P&\cdot P \subset P\\
\mbox{iii) } P &\cap(-p) = (\ho) &\qquad  \mbox{iv) } p&\cup (-p)=R
\end{aligned}
\right\}
\end{equation}
where $P+P= \{x+y | x, y \in P\}$ and similarly for P.P.

We shall now define a field $K$ to be \textit{ordered} if there exists
a subset $P$ of $K$ satisfying (\ref{eqn8}) with $R$ replaced by $K$ in
iv). The existence of such a subset in $K$ shows that one can
introduce an order $>$ in $K$. For, define
\begin{equation}\label{eqn9}
a>b\quad \text{if}\quad a\neq b\quad \text{and}\quad a-b\in P.
\end{equation}

Then (\ref{eqn8}) show that $>$ defines an order in
$K$. In an ordered field $x^{2}\in P$ for every $x$ and by $1)P$
contains all elements which are sums of squares. Furthermore iv)
implies that 
$$
\sum\limits_{i=1}^{n} x^{2}_i=\ho,\quad x_i\in K
$$
only if $x_i$ are zero. For if $x_1\neq \ho$ then 
\begin{equation*}
-1=\sum\limits_{i=2}^{n}\left(\dfrac{x_i}{x_1}\right)^{2}\in P
\end{equation*}
and $-1=-(1)^{2}\in -P$ so that 
\begin{equation*}
-1\in P\cap(-p)
\end{equation*}
which contradicts iii). Thus in an ordered field $-1$ cannot be
expressed as a sum of squares.

Let us call a field \textit{formally real} if $-1$ is not a sum of
squares in the field. From above we see that every ordered field is
formally real. We can even prove the converse. As a matter of fact we
have (see [\ref{eqn9}]).


\begin{thm}[Artin]\label{thm1}
A field is ordered if and only if it is formally real.
\end{thm}

\begin{Proof}
We have only to prove that a formally real field is ordered.

Let $K$ be formally real and $S$ the subset of $K$ containing zero and
all elements of $K$ which are sums of squares of elements of $K$. Let
$F$ be the family of subsets $V_\alpha$ of $K$ satisfying
\begin{equation}\label{eqn10}
\left.
\begin{aligned}
\mbox{i) } V_\alpha &\supset S &\qquad  \mbox{ii) } V_\alpha& +
V_\alpha \subset V_\alpha\\
\mbox{iii) } V_\alpha & \cap (-V_\alpha) = (\ho) &\qquad  \mbox{iv) }
V_\alpha&\cdot V_\alpha \subset V_\alpha.
\end{aligned}
\right\}
\end{equation}

Clearly $S$ is a member of $F$. To see this we have only to check
iii). This follows from the fact that $K$ is formally real.

Let us define in $F$ a partial ordering $>$ by
$$
V_\alpha > V_\beta\quad \text{if}\quad V_\alpha \supset V_\beta.
$$

It is easy to see that this partial order is inductive and we can
apply Zorn's lemma. Let $V$ be a maximal element of $F$. We assert
that $V$ in addition to satisfying (\ref{eqn10}) also satisfies
\begin{equation}\label{eqn11}
V\cup (-V)=K.
\end{equation}

This would mean, by definition, that $K$ is an ordered field.

Suppose now that (\ref{eqn11}) is \textit{not} satisfied. There is
then $x\in K$ ($x$ clearly $\neq 0$) such that
\begin{equation}\label{eqn12}
x\not\in V,\quad x\not\in-V
\end{equation}

Put now 
\begin{equation*}
V_{\ho}-V-xV=\{a-xb \mid a, b \in V\}.
\end{equation*}

Clearly since $V$ contains $\ho$, $V_{\ho}\supset V$. Now $V_{\ho}$
satisfies (\ref{eqn10}) for, if $a, b, c d\in V$ then
\begin{align*}
(a-xb)+ (c-xd)&= (a+c) - x(b+d)\in V_{\ho}\\
(a-xb)(c-xd)&= (ac+ x^{2}bd)-x(ad+bc)\in V_{\ho}
\end{align*}
since $V$ satisfies (\ref{eqn10}). Let
$$
a-xb= -(c-xd).
$$

Then $a+c=(b+d)x$. If $b+d \neq \ho$, then
$$
x=\dfrac{a+c}{b+d}=\dfrac{(a+c)(b+d)}{(b+d)^{2}}\in V
$$
which is not true by hypothesis and so $b+d=\ho$. By iii) this means
$b=d=\ho$ and so $a+c=\ho$ or $a=c=\ho$. Thus $V_{\ho}$ satisfies
iii). This means $V_{\ho}$ is in $F$ and since $V_{\ho}\supset V$
this gives a contradiction to maximality of $V$. Thus $V$ satisfies
(\ref{eqn11}) and $K$ is ordered.

Examples of formally real fields are $Q,R,Q(\sqrt{2}),\ldots$ One can
easily see that a field of characteristic $p>\ho$ is not formally real
since
$$
\underbrace{1+1+\cdots+1}_{\text{ptimes}}=p=\ho
$$
and so $-1=1+1+\cdots+1, p-1$ times. On the other hand there are
fields of characteristic zero which are not formally real. For
instance if $m>\ho$ is a rational integer, $K=Q(\sqrt{-m})$ is not
formally real, since
\begin{align*}
-1&=\dfrac{(\sqrt{-m})^{2}}{m}=m\left(\dfrac{\sqrt{-m}}{m}\right)^{2}\\
&= \left(\dfrac{\sqrt{-m}}{m}\right)^{2}+\cdots+
\left(\dfrac{\sqrt{-m}}{m}\right)^{2}, \quad \text{m times.}
\end{align*}

The field $Q$ has only one order namely the usual order as a subfield
of the real numbers; for in any ordering the set $P$ will consist of
zero and all positive rational numbers only. On the other hand a field
might have more than one order. For instance in $k=Q(\sqrt{2})$ define


\begin{enumerate}[i)]
\item $a+b\sqrt{2} >_1 \ho$ if $a+b\sqrt{2}>\ho$ in the usual sense as
  a real number.

\item $a+b\sqrt{2}>_2 \ho$ if $a-b\sqrt{2}>\ho$ in the usual sense.
\end{enumerate}

Here $a$ and $b$ are rational numbers. One can verify that these
define orders. It is also an easy exercise to show that $K$ has no
other orders.

An element $\alpha$ of an ordered field is said to be \textit{totally
  positive} if $\alpha$ is positive in \textit{every} ordering of the
field. In the above example of $Q(\sqrt{2})$, numbers $a+b\sqrt{2}$
with 
$$
a+b\sqrt{2}>\ho,\quad a-b\sqrt{2}>\ho
$$
are totally positive; for example $3+\sqrt{2}$ is totally positive
whereas $1+\sqrt{2}$ through positive in $>_1$ is not positive in
$>_2$. The totally positive elements of a field have a very nice
property (theorem~\ref{thm2} below).

If $K$ is an extension field of $k$ and $K$ is ordered say by means of
the subset $P$ then $K$ is ordered by the subset
$$
P_{\ho}=P\cap K.
$$

This order is said to be the induced order and the order in $K$ an
extended order. As the example $Q(\sqrt{2})$ shows, it might be
possible to extend an order in $k$ to several orders in $K$. We also
remark that if $\alpha \in k$ is positive as an element of $P$ in
$K$, then $\alpha$ is positive as an element of $P_\ho$ in the induced order.
\enprf
\end{Proof}

We now prove


\begin{thm}\label{thm2}
An element of an ordered field is a sum of squares if and only if it
is totally positive.
\end{thm}

\begin{Proof}
Let us assume, if possible, that $\alpha$ is totally positive and that
it is not a sum of squares. We will arrive at a
contradiction. Consider now the field $L=K(\sqrt{-\alpha})$ we assert
that it is formally real; if not 
\begin{equation}\label{eqn13}
-1=\sum\limits_{i}\beta^{2}_i,\quad \beta_i\in L
\end{equation}

Let us note that if (\ref{eqn13}) holds, then $\sqrt{-\alpha} \notin
K$. For then $L=K$ and 
$$
\alpha=
\left(\dfrac{\alpha+1}{2}\right)^{2}-\left(\dfrac{\alpha-1}{2}\right)^{2}=
\left(\dfrac{\alpha+1}{2}\right)^{2} + \left(\dfrac{\alpha-1}{2}\right)^{2}\sum\limits_{j}\beta^{2}_j
$$
which by hypothesis on $\alpha$ is false.

Since $\sqrt{-x} \notin K$, $L$ is an extension of $K$ of degree $2$
and since each $\beta_i\in L$ is of the form $a_i+\sqrt{-\alpha} b_i,
a_i, b_i\in K$, we get
\begin{equation*}
\left.
\begin{aligned}
-1&=\sum\limits_{i} (a_i+\sqrt{-\alpha} b_i)^{2}= \sum\limits_i
a^{2}_i-\alpha\sum\limits_ib^{2}_i\\
&+2\sqrt{-\alpha} \sum\limits_i a_ib_i.
\end{aligned}
\right\}
\end{equation*}

Since $1, \sqrt{-\alpha}$ are linearly independent over $K$, 
$$
1+\sum\limits_i a^{2}_i=\alpha \sum\limits_ib^{2}_i.
$$

Therefore
$$
\alpha=\dfrac{1+\sum\limits_ia^{2}_i}{\sum\limits_i b^{2}_i}=
\sum\limits_i c^{2}_i,\quad c_i\in K
$$
which is contrary to hypothesis. This contradiction came from
(\ref{eqn13}). Thus $L$ is formally real. Now
$$
-\alpha=\left(\sqrt{-\alpha}\right)^{2}>\ho
$$
in $L$ and therefore, by our remarks, $-\alpha>\ho$ in the order in
$K$ obtained by restricting the order in $L$. Therefore $\alpha$ is
not totally positive - a contradiction. This theorem is completely
proved.
\enprf
\end{Proof}


If the field is not formally real, then
$$
-1=\sum\limits_{i=1}^{n}a^{2}_i,\quad a_i\in K
$$
which means that
$$
f=\sum\limits_{i=1}^{n+1}x^{2}_i
$$
is a zero form in $K$. Therefore by proposition~\ref{prop2}, every element of
$K$ is a sum of $n+1$ squares.

Thus if a field is not formally real, then every element is sum of a
bounded number of squares. Is this true in formally real fields? We
shall first consider the case of the rational number field. We have


\begin{thm}\label{thm3}
Every positive rational number is the sum of $4$ squares of rational numbers.
\end{thm}


\begin{Proof}
Consider the identity (due to Euler)
\begin{equation}\label{eqn14}
\left.\begin{aligned}
\left(\sum\limits_{i=1}^{4} a^{2}_i\right) \left(\sum\limits_{i=1}^{4}
b^{2}_i\right) &= (a_1b_1-a_2b_2-a_3b_3+a_4b_4)^{2}\\
                 &+(-a_1b_2+a_2b_1+a_3b_4-a_4b_3)^{2}\\
                 &+(a_1b_3-a_3b_1-a_2b_4+a_2b_4)^{2}\\
                 &+(a_1b_4+a_4b_1+a_2b_3+a_3b_2).^{2}
\end{aligned}\right\}
\end{equation}

If $a_1, a_2,\ldots, b_1,\ldots,b_4$ are integers, so are the summands
on the right of (\ref{eqn14}) integers. If $r=\dfrac{a}{b}>\ho$ is a
positive rational number, then $r=\dfrac{ab}{b^{2}}$ and since $ab$ is
a product of primes, it is enough to show, because of (\ref{eqn14})
that every prime number is sum of $4$ squares of rational numbers.

If $p=2$ there is nothing to prove; $p=1+1+\ho+\ho$. Let $p$ be an odd
prime. Consider the $p+1$ numbers
$$
-\left(1+x^{2}\right), y^{2}, \ho \leq x\leq \dfrac{p-1}{2}, \quad \ho \leq y\leq \dfrac{p-1}{2}.
$$

These cannot all be distinct $\pmod p$. So for some $x,y$ in this range
$$
1+x^{2}+y^{2}\equiv \ho \pmod p.
$$

But $1+x^{2}+y^{2}\leq
1+\left(\dfrac{p-1}{2}\right)^{2}+\left(\dfrac{p-1}{2}\right)^{2} <
1+\dfrac{p^{2}}{4}+\dfrac{p^{2}}{4}=\dfrac{p^{2}}{2}+1<p^{2}$ since
$p>2$. Therefore
$$
1+x^{2}+y^{2}=mp, \quad 1\leq m<p.
$$ 

Therefore
$$
mp=x^{2}+y^{2}+z^{2}+t^{2}, \quad x,y,z,t\quad \text{integers.}
$$

Let us assume that we have proved that every prime $<p$ is a sum of
$4$ squares of rational numbers. Then by Euler's identity, since
$1\leq m<p$, $m$ is a sum of $4$ squares and therefore $p$ is a sum of
$4$ squares. This proves theorem $3$.
\enprf
\end{Proof}


Actually one can prove a much stronger theorem (see \cite{key5}).

\begin{thm}[Lagrange]\label{thm4}
Every positive integer is a sum of $4$ integral squares.
\end{thm}

\begin{Proof}
After what is said above, it is enough to prove that every odd prime is
a sum of four squares of integers. Let $m$ be the smallest integer
$1\leq m <p$ for which 
$$
mp=x^{2}_1+x^{2}_2+x^{2}_3+x^{2}_4.
$$

Put
\begin{equation}\label{eqn15}
x_i\equiv a_i\pmod m, |a_i|\leq \dfrac{m}{2}.
\end{equation}

In the first place $m$ cannot be even for then either
$x_1,x_2,x_3,x_4$ are all even or all odd or two are even and two are
odd. In any case by a proper rearrangement one has
$$
\dfrac{mp}{2}=\left(\dfrac{x_1+x_2}{2}\right)^{2}+\left(\dfrac{x_1-x_2}{2}\right)^{2}+\left(\dfrac{x_3+x_4}{2}\right)^{2} +\left(\dfrac{x_3-x_4}{2}\right)^{2}
$$
and the summands on the right are integers. This will contradict
minimality of $m$. So let $m$ be odd. We assert that if $m$ is minimal
then $m=1$. If not from (\ref{eqn15}) we get
$$
a^{2}_1+a^{2}_2+a^{2}_3+a^{2}_4\equiv x^{2}_1+\cdots + x^{2}_4\equiv
  \ho \pmod m\\
$$
and
$$
a^{2}_1+\cdots+
a^{2}_4<\dfrac{m^{2}}{4}+\dfrac{m^{2}}{4}+\dfrac{m^{2}}{4}+\dfrac{m^{2}}{4}=m^{2}
$$
(since $m$ is odd). Therefore
$$
a^{2}_1+\cdots+a^{2}_4=\text{km},\quad 1\leq k<m.
$$
Using Euler's identity
\begin{equation}\label{eqn16}
\begin{aligned}
&kpm^{2}=\left(x^{2}_1+\cdots+x^{2}_4\right)\left(a^{2}_1+\cdots+a^{2}_4\right)\\
&=\left(a_1x_1+a_2x_2+a_3x_3+a_4x_4\right)^{2}+\cdots
\end{aligned}
\end{equation}

From (\ref{eqn15})
$$
a_1x_1+\cdots+a_4x_4\equiv a^{2}_1+\cdots + a^{2}_4\equiv \ho \pmod m.
$$

In a similar manner for the other summands. Thus 
$$
kp=\left(\dfrac{X_1}{m}\right)^{2}+\left(\dfrac{X_2}{m}\right)^{2}+\left(\dfrac{X_3}{m}\right)^{2}+\left(\dfrac{X_4}{m}\right)^{2}   
$$
where $X_1,\cdots,X_4$ are divisible by $m$. But then $1\leq k < m$
which contradicts minimality of $m$. So $m=1$ and the theorem is
proved.
\enprf
\end{Proof}

It will be seen that Landau's solution of Hilbert's problem to be given
later is based exactly on this idea of Lagrange's.

It is easy to see that in both theorems~\ref{thm3} and \ref{thm4} the
number \ref{thm4} is critical. There are infinitely many numbers which
are not sums of three squares. We have 

\begin{prop}\label{prop3}
$7$ is not the sum of three squares of rational numbers.
\end{prop}

\begin{Proof}
For let $7=x^{2}+y^{2}+z^{2}, x, y, z$ rational, then clearing off
denominators, we have 
$$
7t^{2}=u^{2}+v^{2}+w^{2}
$$
for integers $u,v,w$ and $t$. If $t$ is even then either all of
$u,v,w$ are even or one is even and two are odd. In the first case we
can divide $t,u,v,w$ by $2$ and go on doing it until $t$ is odd.

In the second case $u^{2}+v^{2}+w^{2}\equiv 2\pmod 4$ whereas
$7t^{2}\equiv \ho \pmod 4$ which is not possible. Thus $t$ is odd. In
this case $7t^{2}\equiv -1 \pmod 8$ whereas for no combination can the
left side be $-1 \pmod 8$. The proposition is proved.

It is to be noted that we could have had $8m+7$ in the proposition
instead of $7$. The argument would have been the same. 

If we take an algebraic number field $K$ (that is a field which is of
finite degree over $Q$), then $K$ has only a finite number of
imbeddings into $R$ and so only a finite number of distinct
orders. Indeed if $K=Q(\alpha)$ and $f(x)$ is the irreducible
polynomial satisfied by $\alpha$, then the number of orders of $K$ is
precisely equal to the number of real roots of $f(x)$. (see,
\cite{key9}). Hilbert conjectured \cite{key7} what was later proved by
Siegel \cite{key16} that every totally positive number of $K$ is a sum
of $4$ squares. In particular if $K$ is totally complex, that is all
roots of $f(x)$ are complex, then in $K$
$$
-1=\alpha^{2}_1+\alpha^{2}_2+\alpha^{2}_3+\alpha^{2}_4,\quad \alpha_i
\in K.
$$

This can be seen in the case of imaginary quadratic fields
$Q(\sqrt{-m})$ rather easily since
$$
-1=m\left(\dfrac{\sqrt{-m}}{m}\right)^{2}=\left(\dfrac{\sqrt{-m}}{m}\right)^{2}\left(a^{2}_1+\cdots+a^{2}_4\right)
$$
because of Lagrange's theorem \ref{thm4}. It can also be seen in the
case of a few other fields. However the general statement is not easy
to prove.
\enprf
\end{Proof}


\section{Theorems of Pfister and Cassels.}\label{s3}

Suppose $K$ is a field of characteristic $\neq 2$. The following
identities are well-known
\begin{align*}
&\left(x^{2}_1+x^{2}_2\right)\left(y^{2}_1+y^{2}_2\right)=(x_1x_2-y_1y_2)^{2}+(x_1y_2+x_2y_1)^{2}\\
&\left(x^{2}_1+x^{2}_2+x^{2}_3+x^{2}_4\right)\left(y^{2}_1+y^{2}_2+y^{2}_3+y^{2}_4\right)=z^{2}_1+z^{2}_2+z^{2}_3+z^{2}_4
\end{align*}
where by Euler's identity $z_1=x_1y_1+x_2y_2+x_3y_3+x_4y_4$ and so
on. A similar identity for products of $8$ squares
$$
\left(\sum\limits_{i=1}^{8} x^{2}_i\right) \left(\sum\limits_{i=1}^{8} y^{2}_i\right)=\sum\limits_{i=1}^{8}z^{2}_i
$$
is also known where $z_1,\ldots, z_8$ are bilinear functions of the
$x'$s and $y'$s. It is clear that, for example, elements of $K^{\ast}$
which are sums of two squares form a group. Thus 
$$
\alpha=x^{2}_1+x^{2}_2, \beta=y^{2}_1+y^{2}_2
$$
gives
\begin{align*}
\alpha\beta&=(x_1y_1-x_2y_2)^{2}+(x_1y_2-x_2y_1)^{2}\\
\alpha^{-1}&=
\left(\dfrac{x_1}{\alpha}\right)^{2}+\left(\dfrac{x_2}{\alpha}\right)^{2}.
\end{align*}

Similarly for four and eight squares.

In general identities such as above for sums of squares in which the
$z'$s are bilinear functions of the $x'$s and $y'$s do not exist for
$n>8$. Hurwitz has shown this for the real number field $R$. Pfister
\cite{key15} has proved the following very interesting 

\begin{thm}\label{thm5}
Let $K$ be a field of characteristic $\neq 2$ and $n=2^{m}.$ Let
$x_1;\ldots,x_n, y_1,\ldots,y_m$ be algebraically independent elements
over $K$. \break There exists then an identity
$$
\left(\sum\limits_{i=1}^{n}x^{2}_i\right)\left(\sum\limits_{i=1}^{n}y^{2}_i\right)=\sum\limits_{i=1}^{n}z^{2}_i
$$
where
$$
z_i=\sum\limits_{j=1}^{n}t_{ij}y_j,\quad i=1,\ldots,n
$$
with $t_{ij}$ in $k(x_1,\ldots,x_n)$.
\end{thm}

\begin{Proof}
We use induction on $m$. If $m=1,2$ the above identities prove the
result; so let us assume that the result is proved for $m\geq 2$
instead of $m+1$. Let in case $m, T=(t_{ij})$ be then rowed matrix
such that
$$
\begin{pmatrix}z_1\\\vdots\\z_n\end{pmatrix}=T\begin{pmatrix}y_1\\\vdots\\y_n\end{pmatrix}.
$$

Because of the identity (which exists by induction hypothesis)
\begin{equation}\label{eqn17}
(z_1,\ldots,z_n) \begin{pmatrix}z_1\\\vdots\\z_n\end{pmatrix} = (y_1,\ldots,y_n)T'T\begin{pmatrix}y_1\\\vdots\\y_n\end{pmatrix}
\end{equation}

This also equals
\begin{equation}\label{eqn18}
(y_1,\ldots,y_n)\left(\sum\limits_{i=1}^{n}x^{2}_iE_n\right) \begin{pmatrix}y_1\\\vdots\\y_n\end{pmatrix}.
\end{equation}
(\ref{eqn17}) and (\ref{eqn18}) show, since $y_1,\ldots,y_n$ are
algebraically independent over $K(x_1,\ldots,x_n)$
\begin{equation}\label{eqn19}
T'T=TT'\left(x^{2}_1+\ldots x^{2}_n\right)E_n.
\end{equation}
(\ref{eqn19}) again gives to the identity in the theorem.

Consider now the case $m+1$. Let
\begin{align*}
(x_1,\ldots,x_{2n})&=(x_1,\ldots,x_n. x_{n+1},\ldots,x_{2n})\\
&=(x_1, x_2)
\end{align*}
so that $X_1=(x_1,\ldots,x_n), X_2=(x_{n+1},\ldots,x_{2n})$, By
induction hypothesis there exist $T_1$ and $T_2$ such that 
\begin{align*}
T'_1T_1&=T_1T'_1=\left(x^{2}_1+\cdots x^{2}_n\right)E_n.\\
T'_2T_2&=T_2T'_2=\left(x^{2}_{n+1}+\cdots+x^{2}_{2n}\right)E_n
\end{align*}

Put now
$$
T=
\begin{pmatrix}
T_1 & T_2\\
T_2 & C
\end{pmatrix}
$$
where $C$ is a matrix to be determined such that
\begin{equation}\label{eqn20}
T'T=TT'=\left(x^{2}_1+\cdots+x^{2}_{2n}\right)E_{2n}.
\end{equation}

Writing down the condition (\ref{eqn20}) we get
$$
T'T=
\begin{pmatrix}
T'_1T_1+T'_2T_2 & T'_1T_2+T'_2C\\
T'_2T_1+C'T_2 & T'_2T_2+C'C
\end{pmatrix}
$$
and so 
$$
T'_1T_2+T'_2C=\ho,\quad C=-{T'^{-1}}_2T'_1T_2.
$$

Substituting now in (\ref{eqn21}) we find that (\ref{eqn20}) is
satisfied. The theorem is thus proved.

It should be observed that the $t_{ij}$, in general are rational
functions of $x_1,\ldots,x_n$ so that if we specialize
$x_1,\ldots,x_n$, $y_1,\ldots,y_n$ in $K$, the $z_i$ may not be
defined at all so that the group property which we observed in the
case of $1,2,4,8$ squares, though true in case of $2^{m}$ for any $m$
cannot be deduced from the above theorem without some labour.

In order to prove this group property, we shall consider a slightly
more general situation which will actually be more useful in our later
work.

Let $a_1,\ldots,a_m$ be any elements of $K$ and put 
\begin{equation}\label{eqn21}
f_m=\bigotimes\limits^{m}_{i=1}<1, a_i>
\end{equation}
where $<1,a_i>$ is the quadratic form $x^{2}+a_iy^{2}$.

This is a quadratic form in $n=2^{m}$ variables. Because of the
property
\begin{equation}\label{eqn22}
f_m=f_{m-1}\otimes <1,a_m> = f_{m-1}\oplus a_mf_{m-1}
\end{equation}
we have 
\begin{equation}\label{eqn23}
\left.\begin{aligned}
f_m&=x^{2}_1+a_1x^{2}_2+a_2\left(x^{2}_3+a_1x^{2}_4\right)\\
                 &+a_3\left(x^{2}_5+a_1x^{2}_6+a_2x^{2}_7+a_1a_2x^{2}_8\right)\\
                 &+\cdots+
a_m\left(x^{2}_{2^{m-1}+1}+a_1x^{2}_{2^{m-1}+2}+\cdots+a_1\cdots a_{m-1}x^{2}_{2^{m}}\right).
\end{aligned}\right\}
\end{equation}
We shall now prove
\enprf
\end{Proof}


\begin{thm}\label{thm6}
If $b\neq \ho$ is an element of $K$ represented by $f_m$, then
$f_m\sim bf_m$.
\end{thm}

\begin{Proof}
We prove the theorem by induction on $m$. Take the case $m=1$ so that
$f_1=x^{2}_1+a_1x^{2}_2$. Then
$$
b=u^{2}_1+a_1u^{2}_2.
$$
\begin{align*}
\text{Therefore}\quad bf_1&=
\left(u^{2}_1+a_1u^{2}_2\right)\left(x^{2}_1+a_1x^{2}_2\right)\\
&=(u_1x_1+a_1u_2x_2)^{2}+a_1(u_1x_2-u_2x_1)^{2}.
\end{align*}

If we consider the non-singular transformation
$$
\begin{pmatrix}
x_1\\
x_2
\end{pmatrix}\longrightarrow \begin{pmatrix}
u_1 & a_1u_2\\
-u_2 & u_1
\end{pmatrix}\begin{pmatrix}
x_1\\
x_2
\end{pmatrix}
$$
then $bf_1\sim f_1$. Let us therefore assume theorem proved for
$m-1\geq1$ instead of $m$. By (\ref{eqn22}) $b=c+a_md$ where $c$ and
$d$ are in $K$  represented by $f_{m-1}$. Let us first consider the
case $cd\neq \ho$. Then, by induction hypothesis
\begin{equation}\label{eqn24}
cf_{m-1}\sim f_{m-1}\sim df_{m-1}\sim cd f_{m-1}.
\end{equation}

Now
\begin{align*}
bf_{m}&=(c+a_{m}d)f_m=(c+a_md)(f_{m-1}\oplus a_mf_{m-1})\\
&\sim(c+a_md)(f_{m-1}\oplus cd a_mf_{m-1}), \quad \text{by}
(\ref{eqn24})\\
&= (c+a_md)(f_{m-1}\otimes <1, cd a_m>)\\
&=f_{m-1}\otimes (c+a_md)<1, cd a_m>.
\end{align*}

Since $cx^{2}_1+a_mdx_2^{2}$ represents $c+a_md$, it is equivalent to
$<c+a_md, ca_md(c+a_md)>=(c+a_md)<1, ca_md>$. Therefore 
\begin{align*}
bf_m&\sim f_{m-1}\otimes <c, a_md>=cf_{m-1}\oplus a_mdf_{m-1}\\
&\sim f_{m-1}\oplus a_m f_{m-1}=f_m, \quad \text{by} (\ref{eqn24}).
\end{align*}

Let now $cd=\ho$ say $c=\ho$. Then $b=a_md$ where $f_{m-1}$ represents
$d$. Hence
\begin{equation*}
\begin{aligned}
bf_m&=a_md(f_{m-1}\oplus a_mf_{m-1})=a_mdf_{m-1}+df_{m-1}\\
&{} \sim a_mf_{m-1}\oplus f_{m-1}=f_m.
\end{aligned}
\end{equation*}

In a similar way the case $d=\ho$ can be dealt with.

If $b\neq \ho$ is such that $f_m\sim bf_m$ then since $f_m$ has the
diagonal form with $1$ as one of the diagonal elements, $bf_m$
represents $b$ and so $f_m$ represents $b$. The foregoing theorem
shows that the set of elements $b\in K^{\ast}$ such that $f_m\sim
bf_m$ is precisely the same as the set of elements in $K^{\ast}$
representable by $f_m$. But if $b, c$ in $K^{\ast}$ such that
$f_m\sim bf_m$, $f_m\sim cf_m$, then 
$$
f_m\sim b^{-1}f_m,\quad f_m\sim bc f_m.
$$

Thus the set of elements in $K^{\ast}$ representable by $f_m$ form a
group. We have therefore 
\enprf
\end{Proof}

\begin{cro}\label{cro1}
The set $M(f_m)$ of elements of $K^{\ast}$ representable by $f_m$
forms a subgroup of $K^{\ast}$.
$$
\text{In particular if}\quad f_m=\sum\limits^{2^{m}}_{i=1}x^{2}_i,
\quad \text{then}
$$
\end{cro}

\begin{cro}\label{cro2}
If $n=2^{m}$, the non-zero elements of $K$ which are representable by
sums of $2^{m}$ squares form a group.
\end{cro}

Let now $K$ be a field of characteristic $\neq 2$ and $K(x)$ the field
of rational functions of $x$ over $K$. Let $\mathcal{V}=K[x]$ be the
ring of polynomials in $x$. We now prove the following important
theorem due to Cassels \cite{key3}.

\begin{thm}[Cassels]\label{thm7}
Let $f(x)\in \mathcal{V}$ be a sum of $n$ squares of
  elements of $K(x)$. Then it is even a sum of $n$ squares in $K$.
\end{thm}

\begin{remnonum}
What is important to observe is that the same $n$ will suffice for
representability in $\mathcal{V}$. Without this condition the theorem
is due to Artin \cite{key1}.
\end{remnonum}

\begin{Proof}
The proof is based on a method analogous to the method of descent in
the proof of Lagrange's theorem.
\begin{align*}
&\text{We first dispose off two trivial cases.}\\
&\text{First let}\, n=1. \quad \text{Then}
f(x)=\left(\dfrac{p(x)}{q(x)}\right)^{2} \text{implies that}
\end{align*}
$q(x)$ divides $p(x)$ since $\mathcal{V}$ is a principal ideal domain.

Next let $-1$ be a sum of $n-1$ squares in $K$. Then 
$$
-1=\sum\limits_{i=1}^{n-1}b^{2}_i, \quad b_i\in K.
$$

Therefore
$$
f(x)=\left(\dfrac{1+f(x)}{2}\right)^{2}+\left(\dfrac{1-f(x)}{2}\right)^{2}\left(\sum\limits^{n-1}_{i=1} b^{2}_i\right).
$$

Let now
$$
f(x)=\left(\dfrac{p_1(x)}{q_1(x)}\right)^{2}+\cdots+ \left(\dfrac{p_n(x)}{q_n(x)}\right)^{2}.
$$

Clearing denominators we get a decomposition
\begin{equation}\label{eqn25}
f(x)\cdot(h(x))^{2}=\sum\limits_{i=1}^{n}(g_i(x))^{2}
\end{equation}
where $h(x),g_i(x)$ are polynomials in $\mathcal{V}$. We now take a
decomposition in which $h(x)$ has the smallest degree. The idea is to
construct another solution of (\ref{eqn25}) with degree $h_1(x)$
smaller than degree $h(x)$ if degree $h(x)$ is $>\ho$. We proceed as
in the proof of theorem~\ref{thm4}. Since Euclidean division holds in
$\mathcal{V}$ we have
$$
g_i(x)=t_i(x)h(x)+u_i(x),u_i(x)=\ho\,\text{or}\, \deg u_i(x)<\deg
h(x)i=1,\ldots,n .
$$

Clearly at least one $u_i(x)\neq \ho$ since otherwise $h(x)$ divides
all the $g_i(x)$ and our theorem would be proved.

Consider the quadric cone
\begin{equation}\label{eqn26}
fH^{2}=G^{2}_1+\cdots+ G^{2}_n
\end{equation}
where $H, G_1,\ldots,G_n$ are the coordinates. Let $X$ and $Y$ be the
two points
$X=(h,g_1(x),\ldots,g_n(x)),Y=(1,t_1(x),\ldots,t_n(x))$. $X$ is a
point on the quadric cone because of (\ref{eqn25}). Any point on the
line joining these two points has coordinates
$$
(\lambda(1-h)+h,\lambda(t_1-g_1)+g_1,\ldots,\lambda(t_n-g_n)+g_n)
$$
where $\lambda$ is a parameter. If we substitute this in the equation
of the quadric cone, we get a quadratic for $\lambda$ which has one
root $\lambda=\ho$ corresponding to the point $X$. The other root
gives precisely the point
$$
(h^{\ast}(x),g^{\ast}_1(x),\ldots,g^{\ast}_n(x))
$$
where
\begin{align*}
h^{\ast}(x)&=h\left(\sum\limits_{i}t^{2}_i-f\right)-2\left(\sum\limits_{i} g_it_i-hf\right)\\
g^{\ast}(x)&=g_i\left(\sum\limits_{i}
t^{2}_i-f\right)-2\left(\sum\limits_{i}g_it_i-hf\right)t_i, i-1,\ldots,n.
\end{align*}
Substituting for $t_i$ we get

\begin{equation*}
\left.\begin{aligned}
h^{\ast}(x)&=h(x)\left\{\sum\limits_{i}\left(\dfrac{g^{2}_i}{h}+\dfrac{u^{2}_i}{h}-\dfrac{2g_iu_i}{h}\right)-f\right\}\\
                 &-2\left\{\sum\limits_{i}\left(g_i\dfrac{g_i}{h}-u_i\right)-hf\right\}.\\
\end{aligned}\right\}
\end{equation*}

This gives
$$
h^{\ast}(x)\cdot h(x)=\sum\limits_{i}u^{2}_i(x).
$$

Since $u_i(x)$ are not all zero and their sum
$\sum\limits_{i}u^{2}_i(x)$ is not zero (since then $-1$ would be a sum
of $n-1$ squares), we see that $h^{\ast}(x)\neq \ho$. Furthermore
$$
\deg h^{\ast}(x)+\deg h(x)\leq 2\max \deg u_i(x)<2 \deg h(x).
$$

Therefore $\deg h^{\ast}(x)<\deg h(x)$. This therefore gives a
contradiction. Therefore if $h(x)$ has minimum degree, then this
degree is zero. The theorem is completely proved.
\enprf
\end{Proof}

The interest in the theorem is due also to the fact that a similar
theorem is \textit{not true} for rings of polynomials in more than one
variable, in general.

Let us take the ring of polynomial $R[x,y]$ in two variables over the
real field $R$. Consider the polynomial
\begin{equation}\label{eqn27}
f(x,y)=1+x^{2}y^{2}\left(x^{2}-3\right)+x^{2}y^{4}.
\end{equation}

It is easily seen that 
$$
f(x,y)=\dfrac{1}{1+x^{2}}\left\{\left(1-x^{2}y^{2}\right)^{2}+x^{2}\left(1-y^{2}\right)^{2}+x^{2}y^{2}\left(1-x^{2}\right)^{2}\right\}
$$
so that it is a sum of squares in the field $R(x,y)$. However, it is
\textit{not} a sum of squares in the ring $R[x,y]$. Let, if possible 
\begin{equation}\label{eqn28}
f(x,y)=\sum\limits^{m}_{i=1}(g_i(x,y))^{2}
\end{equation}
where $g_i(x,y)\in R[x,y]$, and $m$ is \textit{any} positive
integer. By consideration of degrees in (\ref{eqn27}), it follows that
$g_i(x,y)$ are all of degree $\leq 3$. Further for $x=\ho$ and all
$y,f(\ho,y)=1$. Similarly for all $x$ and $y=\ho, f(x,\ho)=1$. This
means that there exist real numbers $\alpha_1,\ldots,\alpha_m$ such
that $g_i(x,y)-\alpha_i$ vanishes for $x=\ho$ and any $y$ and
similarly for $y=\ho$ and any $x$.

Therefore
$$
g_i(x,y)=\alpha_i+xy h_i(x,y)
$$
where $h_i(x,y)$ is a linear polynomial in $x$ and $y$. The
coefficient of $x^{2}y^{2}$ in $\sum\limits_{i}(g_i(x,y))^{2}\geq \ho$
being the sum of squares of the constant term in $h_i(x,y)$, a linear
polynomial with real coefficients. On the other hand the coefficient
of $x^{2}y^{2}$ is $-3<\ho$. Thus (\ref{eqn28}) is impossible.

The example is due to Motzkin \cite{key4}.


\section{Sums of squares in fields.}\label{s4}

Let $K$ be a field of characteristic $\neq 2$. We consider the problem
of representing elements of $K$ as sums of squares. If $K$ is not
formally real then 
$$
-1=\sum\limits_{i=1}^{s}a^{2}_i.
$$

The smallest value of $s=s(K)$ so that $-1$ is a sum of $s$ squares is
called the \textit{stufe} of the field $K$. If $K$ is a field of
characteristic $p\neq 2 (\text{so} >2)$ , $K$ contains the prime field
$F_p$ of $p$ elements. We have

\begin{prop}\label{prop4}
If $F_p$ if the finite field of $p$ elements, $p$ odd, then 
\end{prop}

\begin{equation*}
s(F_p) =
\begin{cases}
1 & \text{if} p\equiv 1\pmod 4\\
2 & \text{if} p\equiv-1\pmod 4.
\end{cases}
\end{equation*}


\begin{Proof}
We can identify $F_p$ with the ring of residues of $Z \pmod p$. If
$p\equiv 1 \pmod 4$, then 
$$
x^{2}\equiv -1\pmod p
$$
has a solution. If $p\equiv-1 \pmod 4$ then we have seen in the proof
of theorem~\ref{thm3} that there exist integers $x,y$ with 
$$
-1 \equiv x^{2}+y^{2}\pmod p.
$$

$xy\neq \ho \pmod p$.

If $-1$ is a sum of $s(K)$ squares, then every element of $K$ is a sum
$s(K)+1$ squares. Thus 
\enprf
\end{Proof}


\begin{prop}\label{prop5}
If $K$ is a field of characteristic $p>2$, then every element of $K$
is a sum of at most three squares.
\end{prop}

Let us therefore take $K$ to have characteristic zero. Let it also be
\textit{not} formally real. We shall prove the important theorem due
to \textit{Pfister} \cite{key14}.

\begin{thnm}\label{thnm7}
If $K$ is not formally real then $s(K)=2^{m}$ for some integer $m\geq \ho$.
\end{thnm}


\begin{Proof}
We may consider only the case $s(K)>2$. Let if possible $s(K)$ be not
a power $2$. There is an integer $m\geq 1$ such that 
$$
2^{m}<s(K)<2^{m+1}.
$$

By definition
\begin{equation}\label{eqn29}
-1=\sum\limits_{i=1}^{s}a^{2}_i,\quad s=s(K).
\end{equation}

Clearly $\sum\limits_{i=1}^{2}a^{2^{m}}_i$ and
$\sum\limits_{i=2^{n}+1}^{s} a^{2}_i+1$ are both different from
zero. From (\ref{eqn29}) we get
\begin{equation}\label{eqn30}
-1=\dfrac{\sum\limits_{i=1}^{2^{m}}a^{2}_i}{1+\sum\limits_{i=2^{m}+1}^{s}a^{2}_i.}
\end{equation}

Now $s-2^{m}+1<2^{m+1}-2^{n}+1<2^{m}+1$ and so the numerator and
denominator in (\ref{eqn30}) are sum of most $2^{m}$ squares. By
corollary~\ref{cro2}  to theorem~\ref{thm6} such elements of $K^{\ast}$ form
a group. Therefore
$$
-1=\sum\limits_{i=1}^{2^{m}}b^{2}_1,\quad b_i\in K.
$$

But this contradicts definition of $s$ since $s>2^{m}$.

We have incidentally proved 
\enprf
\end{Proof}


\begin{thm}\label{thm8}
In a field which is not formally real, every element is a sum of
$2^{m}+1$ squares, for suitable integer $m$.
\end{thm}

One might ask whether there exist fields with $2^{m}$ as the stufe,
$m$ being \textit{any} positive integer. That this is indeed true
follows from the lemmas below.

\begin{lem}\label{lem1}
Let $K$ be a formally real field and $K(x)$ the field of rational
functions of $x$. Then $x^{2}+d, d\neq \ho$ in $K$ is a sum of $n$
squares in $K(x)$ if and only if $d$ is a sum of $n-1$  squares in $K$.
\end{lem}

\begin{Proof}
If $d$ is a sum of $n-1$ squares in $K$ then clearly $x^{2}+d$ is a
sum of $n$ squares. Conversely let $x^{2}+d$ be a sum of $n$ squares
in $K(x)$. By Cassels' theorem it is a sum of $n$ squares in $K[x]$.
$$
x^{2}+d=\sum\limits_{i=1}^{n}(g_i(x))^{2},\quad g_i(x)\in K[x].
$$

The polynomials $g_i(x)$ have all to be of degree $\leq 1$ in $x$. For
otherwise equating the coefficients of the highest degree term on both
sides, we see that zero will be a sum of squares in $K$ which because
$K$ is formally real, would have to be trivial
representation. Therefore
\begin{equation}\label{eqn31}
x^{2}+d=\sum\limits_{i=1}^{n}(a_ix+b_i)^{2},\quad a_i,b_i\in K.
\end{equation}

Choose now $c$ in $K$ such that 
$$
c=\pm(a_nc+b_n)
$$
for some choice of the sign. This is possible. Put $x=c$ on both sides
in (\ref{eqn31}). Then
$$
c^{2}+d=\sum\limits_{i=1}^{n-1}(a_ic+b_i)^{2}+c^{2}
$$
which shows that $d$ is a sum of $n-1$ squares in $K$.
\enprf
\end{Proof}


\begin{lem}\label{lem2}
Let $K$ be formally real and $K(x_1,\ldots,x_n)$ the field of rational
functions in $n$ variables $x_1,\ldots,x_n$. Then
$x^{2}_1+x^{2}_2+\cdots +x^{2}_n$ is not the sum of $n-1$ squares in $K(x_1,\ldots,x_n)$.
\end{lem}


\begin{Proof}
We use induction on $n$. The case $n=1$ is trivial. So assume that the
result is proved for $n-1$ instead of $n$. Put now $x^{2}_1+\cdots+
x^{2}_{n-1}=d$ and $x_n=x$. Then $x^{2}+d$ is a sum of $n-1$ squares
in $K(x_1,\ldots,x_{n-1})(x)$. By the lemma~\ref{lem1} this would mean that
$d=x^{2}_1+\cdots+x^{2}_{n-1}$ is sum of $n-2$ squares in
$K(x_1,\ldots,x_{n-1})$. By induction hypothesis this is impossible. 
\enprf
\end{Proof}

We are now ready to prove

\begin{thm}\label{thm9}
For every integer $m\geq \ho$ there is a field with stufe $2^{m}$.
\end{thm}


\begin{Proof}
Let $n=2^{m}$ and $K=R(x_1,\ldots,x_{n+1})$ where $R$ is the field of
real numbers. Clearly $K$ is formally real. Let $y$ be an algebraic
element over $K$ satisfying 
\begin{equation}\label{eqn32}
y^{2}=-\left(x^{2}_1+\cdots+ x^{2}_{n+1}\right),
\end{equation}
and $L=K(y)$ the extension field of $K$. We assert that $s(L)=2^{m}$.

From (\ref{eqn32}) if follows that $s(L)\leq n+1$. But since $s(L)$ is
a power of $2$
$$
s=s(L)\leq n=2^{m}.
$$

Suppose if possible that $s(L)<n$. Then
$$
-1=\sum\limits_{i=1}^{s}a^{2}_i,\quad a_i\in L.
$$ 

Since every element of $L$ is of the form $cy+d$, we get 
\begin{equation*}
%\left.\begin{aligned}
-1= \sum\limits_{i=1}^{s}(c_iy+d_i)^{2}=y^{2}\sum\limits_{i=1}^{s}c^{2}_i+\sum\limits_{i=1}^{s}d^{2}_i+2y\sum\limits_{i=1}^{s}c_id_i.\\
%\end{aligned}\right\}
\end{equation*}

But since $1,y$ are linearly independent over $K$, we get
$$
-1=y^{2}\sum\limits_{i=1}^{s}c^{2}_i+\sum\limits_{i=1}^{s}d^{2}_i.
$$

Therefore
$$
-1=\sum\limits_{i=1}^{s}d^{2}_i=y^{2}\sum\limits_{i=1}^{s}c^{2}_i.
$$

Now $\sum\limits_{i=1}^{s} c^{2}_i\neq \ho$ since otherwise $-1$ would
be a sum of $<s$ squares.

Hence
$$
-y^{2}=\dfrac{1+\sum\limits_{i=1}^{s}d^{2}_i}{\sum\limits_{i=1}^{s} c^{2}_i.}
$$

Since $s=s(L)<2^{m}$, it is $\leq 2^{m-1}$ and $1+s\leq
1+2^{m-1}<2^{m}$ unless $m=1$. We take $m>1$. By theorem~\ref{thm6}
corollary~\ref{cro2}, we have 
$$
x^{2}_1+\cdots +x^{2}_{n+1}=-y^{2}=\sum\limits_{i=1}^{2^{m}}f^{2}_i.
$$

But $2^{m}<n+1$ and by lemma~\ref{lem2} this is impossible. Thus
$s(L)=2^{m}$ and our theorem is proved.
\enprf
\end{Proof}


One can ask whether there exist formally real fields in which there is
no bound $N$ so that every totally positive element is a sum of at
most $N$ squares.

We consider the field $K$ of rational functions in infinite number of
variables $x_1,\ldots,x_n,\ldots$. If we denote by $K_n$ the
field $K_n=R(x_1,\ldots,x_n)$, then 
$$
K=\bigcup\limits_{n=\ho}^{\infty} K_n, \quad K_{\ho}=R.
$$

Consider the polynomial $x^{2}_1+\cdots+x^{2}_n$. Suppose it is sum of
$n-1$ squares in $K$. Then if in the sum a variable $x_m m>n$ appears,
then 
\begin{equation}\label{eqn33}
x^{2}_1+\cdots+x^{2}_n=\sum\limits_{i=1}^{n-1}a^{2}_i
\end{equation}
where $a_i$ are polynomials in $x_m$ with coefficients which are
rational functions in other variables. This shows that in a sum like
(\ref{eqn33}), cannot contain any variables other than
$x_1,\ldots,x_n$. Lemma~\ref{lem2} then shows that (\ref{eqn33}) is
impossible. $k$ can for example be a subfield of the field of real
numbers whose transcendence degree over $Q$ is infinite and countable.

If we consider function fields in a finite number of variables, then
under some conditions on the coefficient field, it is possible to
prove that every element representable as a sum of squares is actually
a sum of a bounded number of squares, this bound depending only on the
coefficient field and the transcendence degree of the function
field. We shall now consider the simplest case of such a function
field $K(x)$ in one variable over a field $K$ which is either real
closed $R_{\ho}$ or is an algebraic number field $P$ of finite degree
over $Q$. These have the property that every finite algebraic
extension of $K$ which is not formally real has stufe $g=1$ or $\leq
4$ according as $K=R_{\ho}$ or $P$. Moreover in $K$ every element
which is a sum of squares is a sum of $g$ squares. This is due to the
theorem of Hilbert-Siegel \cite{key16}.

We shall first prove the following lemma.

\begin{lem}\label{lem3}
Every irreducible polynomial in $K[x]$ which is a sum of squares, is
actually a sum of $2g$ squares in $K[x]$.
\end{lem}

\begin{Proof}
Let $f(x)$ be an irreducible polynomial which is a sum of squares in
$K(x)$. Then by Cassels' theorem, it is a sum of squares in $K[x]$.

\begin{equation}\label{eqn34}
f(x)=\sum\limits_{i}(\varphi_i(x))^{2}.
\end{equation}

If $\alpha$ is a root of $f(x)$, then all $\varphi_i(\alpha)$ cannot
be zero. Therefore we have a non-trivial sum 
$$
\ho=\sum\limits_{i}(\varphi_i(\alpha))^{2}
$$
in $K(\alpha)$. Hence the field $K(\alpha)$ is \textit{not} formally
real. By remarks above, in $K(\alpha)$
$$
-1=\sum\limits_{i=1}^{g}\beta_i^{2},\quad \beta_i\in K(\alpha).
$$

If $f(x)$ is of degree $n$, then each $\beta_i$ in $K(\alpha)$ can be
chosen to be a polynomial $g_i(\alpha)$ of degree $\leq n-1$ in
$\alpha$ with coefficients in $K$. Consider the polynomial
$$
h(x)=1+\sum\limits_{i=1}^{g}(g_i(x))^{2}.
$$

It is of degree $\leq 2n-2$ and vanishes for $x=\alpha$, Thus 
\begin{equation}\label{eqn35}
h(x)=f(x)\cdot\varphi(x).
\end{equation}

We may, without loss in generality, assume that $f(x)$ is monic.

From (\ref{eqn35}) we see that $\deg \varphi(x)\leq n-2<\deg
f(x)$. Further since $K[x]$ is a principal ideal domain
$$
\varphi(x)=a\cdot(t(x))^{2}\cdot u_1(x)\ldots u_r(x)
$$
where $u_1(x),\ldots,u_r(x)$ are mutually coprime, irreducible monic
polynomials of degree $< \deg f(x)$, $t(x)$ is a monic polynomial and
$a\in K$ is an element representable as a sum of squares in $K$ and
therefore as a sum of $g$ squares in $K$. Hence 
$$
a\cdot f(x).(t(x))^{2}\cdot u_1(x)\ldots
u_r(x)=1+\sum\limits_{i=1}^{g} (g_i(x))^{2}.
$$

It is clear that 
\begin{equation}\label{eqn36}
a\cdot f(x)\cdot u_1(x)\ldots u_r(x)=\sum\limits_{i=1}^{g+1}(h_i(x))^{2}
\end{equation}
by Cassels' theorem. Now $u_1(x),\ldots,u_r(x)$ have the same property
as $f(x)$ namely that every root $\alpha$ of any of them generates
over $K$ a non-formally real field.

We can therefore use induction. Suppose we assume that every monic
irreducible polynomial over $K$ of degree $<\deg f(x)$ is such that
any root of it generates over $K$ a non-formally real field, then it
is a sum of $2g$ squares in $K[x]$. From (\ref{eqn36}), it follows
that since $g+1\leq 2g$ and $2g$ is a power of $2$, by Corollary~\ref{cro2} of theorem~\ref{thm6}, a $f(x)$ is a sum of $2g$ squares in
$K(x)$ and by Cassels' theorem, also in $K[x]$. Since a is a sum of
squares in $K$, it is a sum of $g$ squares and therefore $f(x)$ is a
sum of $2g$ squares in $K[x]$. 

In order to complete the induction we must prove it for the smallest
degree $n$ of $f(x)$. $n$ is clearly $\geq 2$. In this case if we
proceed as in the above proof, we find $\varphi(x)=a$ is a constant.
\enprf
\end{Proof}

We now prove the following important theorem

\begin{thm}\label{thm10}
If $K=P$ or $R_{\ho}$, then every polynomial in $K[x]$ which is a sum
of squares, is a sum of at most $2g$ squares,
\end{thm}

\begin{Proof}
Let $f(x)=\sum\limits_{i}(\varphi_i(x))^{2}$. Then $f(x)=a\varphi(x)$
where $\varphi(x)$ is monic and a is a sum of squares in $K$ and
therefore sum of $g$ squares. Now 
$$
a\varphi(x)=a\cdot(g(x))^{2}\cdot h_1(x)\cdots h_t(x)=\sum\limits_{i}(\varphi_i(x))^{2}
$$
where $g(x), h_1(x),\ldots,h_t(x)$ are monic polynomials and
$h_1,h_2,\ldots,h_t$\break mutually coprime and irreducible. Therefore by
Cassels' theorem 
$$
ah_1(x)\ldots h_t(x)=\sum\limits_{i}(\psi_i(x))^{2},\psi_i(x)\in K[x].
$$

If $\alpha$ is a root of $h_i(x)$, then $K(\alpha)$ is not formally
real and so by lemma~\ref{lem3}, $h_i(x)$ is a sum of $2g$ squares of
polynomials in $K[x]$. Using the fact that $2g$ is a power of $2$ and
sums of $2g$ squares form a group, we obtain the theorem.

If $f(x)\in K[x]$ is a sum of squares, then clearly for every $\alpha
\in Q$, $f(\alpha)\geq \ho$. If $f(x)$ is monic irreducible, then any
root $\alpha$ of $f(x)$, generates a non-formally real filed
$K(\alpha)$ over $K$. To see this observe that if
$f(x)=\prod\limits_{i=1}^{t}(x-\alpha_i)\prod\limits_{j=1}^{u}\left\{(x-v_j)(x+\overline{v}_{j})\right\}$
where $\alpha_i$ are distinct and real and $v_j$ are distinct and
complex, one can choose rational $x$ such that $f(x)<\ho$ if
$t>\ho$. Using now the previous theorem we have
\enprf
\end{Proof}

\begin{thm}\label{thm11}
If $f(x)\in K[x]$, $K=R_{\ho}$ or $P$ and $f(\alpha)\geq \ho$ for
every rational $\alpha$, then $f(x)$ is a sum of at most $2g$ squares
in $K[x]$.
\end{thm}

In case $K=R$, this is due to Hilbert \cite{key8} and in case $K=Q$,
this is due to Landau \cite{key10}.


If $K=R$, it is trivial to see that not every positive definite
polynomial in $K[x]$ is a square. In case $K=Q, x^{2}+7$ is not a sum
of $4$ squares, since then $7$ would be a sum of $3$ squares, which it
is not by proposition~\ref{prop3}. Thus the smallest integer $g(K[x])$
for which \textit{every} positive definite polynomial is sum of
$g(K[x])$ squares is $\geq 5$ and $\leq 8$. It would be interesting to
prove that $g(K[x])=5$ for $K=P$. 

\section{The Tsen-Lang theorem.}\label{s5}

The main theorem proved in this section is 

\begin{thm}\label{thm12}
Let $\mathbb{C}$ be the field of complex numbers and $K$ a field of
algebraic functions in $n$ variables over $\mathbb{C}$. Let
$f_i(x_1,\ldots,x_m)$, $i=1,\ldots,r$ be $r$ quadratic forms in $m$
variables over $K$. If $m>r\cdot 2^{n}$ these quadratic forms have a
common zero in $K$.
\end{thm}

We recall that an algebraic function field over a field $L$ in $n$
variables is a finite algebraic extension of a field of rational
functions over $L$ in $n$ variables. As in section~\ref{s1} we mean by
a zero a set of values of $x_1,\ldots,x_m$ \textit{not} all zero.

The proof of the theorem depends on the following lemmas.

\begin{lem}\label{lem4}
Let $f_1,\ldots,f_r$ be $r$ homogeneous polynomials in $m>r$ variables
with complex coefficients. They have a common zero in $\mathbb{C}$. 

This follows from the fact that $\mathbb{C}$ is algebraically closed
and since $m>r$, Hilbert mallstellensatz assures us the existence of a
zero in $\mathbb{C}$.
\end{lem}

\begin{lem}\label{lem5}
Let $L=\mathbb{C}(z_1,\ldots,z_n)$ be the field of rational functions
in $n$ variables over $\mathbb{C}$. Let $f_1,\ldots,f_r$ be $r$
quadratic forms in $m$ variables over $L$. If $m>n>2^{n}$ these forms
have a common nontrivial zero in $L$.
\end{lem}

\begin{Proof}
We use induction on $m$. In case $n=\ho$, the lemma follows from lemma~\ref{lem4}. Let us therefore assume the lemma proved for $n-1$ instead
of $n$. We can without loss in generality assume that the coefficients
of $f_1,\ldots,f_r$ are all in the polynomial ring
$\mathbb{C}[z_1,\ldots,z_n]$. Let us assume that $t$ is the largest of
the degrees in
$z_n$ of the coefficients in $f_1,\ldots,f_r$ considered as
polynomials in $z_n$ with coefficients in $C(z_1,\ldots,z_{n-1})$. Put
now 
\begin{equation}\label{eqn37}
x_k=\sum\limits_{p=\ho}^{s}a_{pk} z^{p}_n,\quad 1\leq k \leq m
\end{equation}
where $a_{pk}$ are to be determined in $C(z_1,\ldots,z_{n-1})$. By
inserting these values in $f_i(x_1,\ldots,x_n)$, we see that 
$$
f_i(x_1,\ldots,x_n)=\sum\limits_{p=\ho}^{2s+t}A_p^{(i)} z^{p}_n,\quad
1\leq i\leq r
$$
where $A^{(i)}_p$ are quadratic forms in the $m(s+1)$ variables
$a_{pk}$ in (\ref{eqn37}) with coefficients in
$C(z_1,\ldots,z_{n-1})$. The number of quadratic forms is
$(2s+t+1)r$. By induction hypothesis these have a zero in
$C(z_1,\ldots,z_{n-1})$ if 
$$
m(s+1)\gg(2s+t+1)r\cdot 2^{n-1}.
$$

This gives
$$
(m-r.\cdot 2^{n})s>(t+1)r.2^{n-1}-m.
$$

By choosing $s$ sufficiently large, this inequality may be satisfied. 

If $K$ is any algebraic function field of dimension $n$ over
$\mathbb{C}$ there exist $z_1,\ldots,z_n$ algebraically independent
over $\mathbb{C}$ such that $K$ is finite algebraic over
$\mathbb{C}(z_1,\ldots,z_n)$. We have 
\enprf
\end{Proof}

\begin{lem}\label{lem6}
Let $K$ be finite algebraic over $L=\mathbb{C}(z_1,\ldots z_n)$. Any
$r$ quadratic forms $f_1,\ldots,f_r$ in $m$ variables over $K$ have a
non-trivial zero in $K$ if $m>r\cdot 2^{n}$.
\end{lem}

\begin{Proof}
Let $K$ be of degree $t$ over $L$ and $\omega_1,\ldots,\omega_t$ a
basis of $K$ over $L$. Every element of $K$ is of the form 
$$
\alpha=\sum\limits_{i=1}^{t}a_i\omega_i,\quad a_i\in L.
$$

Put now
$$
x_k=\sum\limits_{p=1}^{t}u_{kp}\omega_p,\quad u_{kp}\in L, 1\leq k\leq m
$$

Then
$$
f_i(x_1,\ldots,x_m)=\sum\limits_{p=1}^{t}B_p^{(i)}\omega_p,\quad 1\leq
i\leq r
$$
where the $B^{(i)}_p$ are quadratic forms in the $mt$ variables
$u_{k}p$ with coefficients in $L$. The number of quadratic forms is
$rt$. By lemma~\ref{lem5}, these forms have a non-trivial zero in $L$
if 
$$
mt > rt\cdot 2^{n}
$$
which is precisely the hypothesis.
\enprf
\end{Proof}


\begin{prf}
Klar.


This is a special case of a general theorem proved by Lang
\cite{key11}. Actually much stronger results can be proved (see
\cite{key6}).

We have given only the theorem which is useful for us in the next
section.
\enprf
\end{prf}

\section{Pfister's theorem.}\label{s6}

Let  $R$ be the field of real numbers and $K=K_n=R(x_1,\ldots,x_n)$
the field of rational functions in $n$ variables $x_1,\ldots,x_n$ over
$R$. A rational function $f(x_1,\ldots,x_n)$ is said to be \textit{positive
definite} if for every $\alpha_1,\ldots,\alpha_n$ in $p$ such that
$f(\alpha_1,\ldots,\alpha_n)$ is defined,
$f(\alpha_1,\ldots,\alpha_n)\geq \ho$, It is clear that if 
$$
f(x_1,\ldots,x_n)=\sum\limits_{i}(\varphi_i(x_1,\ldots,x_n))^{2}
$$
where $\varphi_i(x_1,\ldots,x_n)$ are in $K_n$, then $f$ is positive
definite. Hilbert in his famous address at the International
Mathematical Congress in 1900, proposed (as the 17th) the problem If
$f(x_1,\ldots,x_n)\in K_n$ is positive definite, them at is as sum of
squares in $K_n$. In view of the theorem~\ref{thm2} in section~\ref{s2} at with the only necessary to prove that $K_n$ is formally
real (easy) and that a positive definite function is totally positive
(non-trivial). The problem was completely solved bu Artin \cite{key1} in 1927
not only for $R(x_1,\ldots,x_n)$ but also for
$Q(x_1,\ldots,x_n)$. Hilbert \cite{key8} himself had solved the
problem for $n=1$ (Landau \cite{key10} in the case $Q$) and
$m=2$. Indeed he showed that in case $n=1$ every such $f$ is a sum of
$2$ squares (see solution below) and that in case $n=2$, four squares
would suffice.

It is clear that $2$ squares is the best possible in case $n=1$. It
was recently shown by Cassels, Ellison and Pfister \cite{key4} that
in case $n=2$, four is the best possible. (It would be desirable to
give a simpler proof of this fact. What we had shown in Motzkin's
example is that it is not a sum of any number of squares in
$R[x_1,x_2].$). Artin's general solution did not specify whether every
positive definite function could be represented as a sum of a fixed
number (depending on $n$ and $R$) of squares. Recently James $Ax$
(unpublished) showed that in case $n=3$, every positive definite
function in $R(x_1,x_2,x_3)$ is a sum of at most $8$ squares. If, as
in section~\ref{s4}, we denote by $g(n)$ the smallest number of
squares required to represent every positive definite function in
$R(x_1,\ldots,x_n)$ as a sum of squares, then 
$$
g(\ho)-1, g(1)-2,\quad g(2)=4, \quad g(3)\leq 8.
$$

$Ax$ conjectured that $g(m)\leq 2^{n}$. It is clear from lemma~\ref{lem2} that $g(n)\geq n+1$. Our object now is to prove the
following theorem due to Pfister \cite{key13}.

\begin{thm}[Pfister]\label{thm13}
Let $L$ be an algebraic function field of $n$ variables over $R$. Then
every element of $L$ which is a sum of squares in $L$ is actually a
sum of at most $2^{n}$ squares in $L$. 
\end{thm}


In case the ground field is $Q$. Landau showed for $Q(x)$ that
$g(1)\leq 8$. An analogue of Pfister's theorem is not known for
$Q(x_1,\ldots,x_n)$, $n>1$. 

If one considers instead of the field $R(x_1,\ldots,x_n)$ the ring
$R[x_1,\ldots,\break x_n]$, we had seen through Motzkin's example that a
positive definite polynomial in case $n=2$ need not be sum of squares
of polynomials. One can however ask whether any polynomial in
$R[x_1,\ldots,x_n]$ which is a sum of squares, is a sum of a bounded
numbers of squares the bound depending only on $n$. All that is now
known is that it is a sum of at most $2^{n}$ squares of elements of
$R(x_1,\ldots,x_n)$ which are polynomials in $x_n$ with coefficients in
$R(x_1,\ldots,x_{n-1})$. Indeed such a statement was proved by Landau
\cite{key10} for $n=2$ and in general by Artin \cite{key1} however
without any reference to $2^{n}$.

We shall give now Hilbert's proof for the case $n=1$. Let $f(x)$ be a
positive definite polynomial with real coefficients.  Then 
$$
f(x)=\lambda
\prod\limits_{i=1}^{t}(x-\alpha_i)^{m_i}\prod\limits_{j=1}^{u}\left\{(x-\beta_i)(x-\overline{\beta}_i)\right\}^{n_i}
$$
where $\alpha_i$ are real, $\beta_i$ complex and $\overline{\beta}_i$
its complex conjugate $\lambda$ is a
real number and the $m_i's$ and $n_i's$ are non-negative rational
integers. Since $f(a)\geq \ho$ for all real $a$, we see that $\lambda
> \ho$ and the $m_i's$ are positive even integers. Put 
$$
\varphi(x)=\sqrt{\lambda}\prod\limits_{i=1}^{t}(x-\alpha_i)^{m_{i/2}}\prod\limits_{j=1}^{u}(x-\beta_i)^{n_i}.
$$

Then $\varphi(x)$ is a polynomial with complex coefficients and so 
$$
\varphi(x)=A(x)+\sqrt{-1} B(x)
$$
where $A(x)$ and $B(x)$ are polynomials with real coefficients. Now 
\begin{align*}
f(x)&=\varphi(x)\cdot
\overline{\varphi(x)}=(A(x)+\sqrt{-1}B(x))(A(x)-\sqrt{-1} B(x))\\
&=(A(x))^{2}+(B(x))^{2}.
\end{align*}

In order to prove the theorem of pfister, we proceed in the following
way. Let us denote by $M(f)$ the set of all values which a quadratic
form takes, the field being obvious from the context and by $M(f)$ the
non-zero elements represented by $f$. 

As at the beginning $L$ is an algebraic function field in $n$
variables over $R$. Then $F=L(\sqrt{-1})$ is a function field of $n$
variables over $\mathbb{C}$. The Tsen-Lang theorem applies to $F$, and
every quadratic form in more that $2^{n}$ variables represents
zero. By proposition~\ref{prop2} every quadratic form in $2^{n}$ variables
over $F$ is universal. We first prove 


\begin{lem}\label{lem7}
If $f$ is the quadratic form $\bigotimes\limits_{i=1}^{m}<1, a_i>$ in
$2^{m},m\geq n$ variables, $a_1,\ldots,a_m$ being in $L$, then $f$
represents every elements of $L^{\ast}$ which is a sum of two squares
of elements of $L$. 
\end{lem}

\begin{Proof}
We may assume that $f$ is not a zero form in $L$, else by proposition
\ref{prop2}, the lemma is trivial. Since $m\geq n$, $f$ is universal
in $L\left(\sqrt{-1}\right)$. Let $\alpha\neq \ho$ be an element which is a sum
of squares in $L$. Then 
$$
\alpha=\left(u+\sqrt{-1} v\right)\left(u-\sqrt{-1}v\right)
$$
where $u, v\in L$. There exists therefore a vector
$\underline{z}=\underline{x}+\sqrt{-1}\underline{y}$ in $L\left(\sqrt{-1}\right)$
such that 
$$
f\left(\underline{x}+\sqrt{-1}\underline{y}\right)=u+iv.
$$ 

From equation~(\ref{eqn1}) in section~\ref{s1}, we have 
$$
u+\sqrt{-1}v=f\left(\underline{x}+\sqrt{-1}\underline{y}\right)=f(\underline{x})-f(\underline{y})+2\sqrt{-1}
B(\underline{x}, \underline{y}).
$$

Therefore
$$
u^{2}+v^{2}=(f(\underline{x})-f(\underline{y}))^{2}+4B^{2}(\underline{x}, \underline{y}).
$$

We can take $v\neq \ho$ so that $f(\underline{y})\neq \ho$. Then 
\begin{equation*}
\begin{aligned}
(f(\underline{x})-f(\underline{y}))^{2}+4B^{2}(\underline{x},
  \underline{y})&=f(\underline{y})\cdot
  f\left(\left(\dfrac{f(\underline{x})}{f(\underline{y})}-1-4\left(\dfrac{B(\underline{x},\underline{y})}{f(\underline{y})}\right)^{2}\right)\underline{y}\right.\\
&\hspace{4cm}\left.+\dfrac{2B(\underline{x},\underline{y})}{f(\underline{y})}\underline{x}\right)
\end{aligned}
\end{equation*}

as a simple checking shows. But by corollary~\ref{cro1} to theorem~\ref{thm6}, the elements of $M(f)$ form a group. This there exists a
vector $\underline{W}$ such that 
$$
u^{2}+v^{2}=f(\underline{w}).
$$

Let us now denote by $f_m$ the quadratic form 
$$
f_m(\underline{x})=\sum\limits_{i=1}^{m}x^{2}_i.
$$

We then have the following simple
\end{Proof}

\begin{lem}\label{lem8}
Let $m\geq n$. Let $f'=f_m\oplus zf_m$ where $z\neq \ho$ in $L$. Then 
$$
y\in M_L(f')\Rightarrow 1+y \in M_2(f')
$$
\end{lem}

\begin{Proof}
Let $y=a+bz$, $a,b\in f_m$ so that 
\begin{equation}\label{eqn38}
y=e^{2}_1+e^{2}_2+a_1+\cdots+a_{m-1}+a_{m^{z}}
\end{equation}
where $1\leq i\leq m$, $a_i$ is a sum of $2^{i}$ squares in $L$. Let
now 
$$
\varphi=\bigotimes\limits_{i=1}^{m}<1,b_i>,\quad b_i\in L
$$
where
\begin{equation}\label{eqn39}
\begin{aligned}
b_m &=
\begin{cases}
za_m & \text{if } a_m\neq \ho\\
z & \text{if } a_m=\ho
\end{cases}\\
b_i&=
\begin{cases}
a_i & \text{if } a_i\neq \ho \\
& \qquad\qquad\qquad 1\leq i \leq m-1.\\
1 & \text{if } a_i=\ho 
\end{cases}
\end{aligned}
\end{equation}

By lemma~\ref{lem7} there exist $x_1,\ldots,x_{2^{m}}$ in $L$ such
that 
\begin{equation*}
\left.\begin{aligned}
e^{2}_1+e^{2}_2&=x^{2}_1+b_1x^{2}_2+b_2\left(x^{2}_3+b_1x^{2}_4\right)+\cdots+\\
                 &
b_m\left(x^{2}_{2^{m-1}+1}+b_1x^{2}_{2^{m-1}+2}+\cdots+ b_1\ldots b_{m-1}x^{2}_{2^{m}}\right)\\
\end{aligned}\right\}
\end{equation*}

Inserting this value in (\ref{eqn38}) we have, using (\ref{eqn39})
\begin{equation}\label{eqn40}
\left.\begin{aligned}
1+y&=
1+x^{2}_1+a_1\left(1+x^{2}_2\right)+a_2\left(1+x^{2}_3+a_1x^{2}_4\right)+\cdots+\\
&a_mz\left(1+x^{2}_{2^{m-1}+1}+\cdots+a_1\ldots a_{m-1} x^{2}_{2^{m}}\right)
\end{aligned}\right\}
\end{equation}

It is to be observed in (\ref{eqn40}) that the sums in brackets are
respectively sums of $2^{1},2^{2},2^{3},\ldots,2^{m}$ squares since
  $a_i$ and there fore $a_1\ldots a_i$ is a sum of $2^{i}$
  squares. Therefore the right of (\ref{eqn40}) is a sum of 
$$
\left(1+1+2+2^{2}+\cdots+2^{m-1}\right)\text{squares}
+z\left(1+1+2+2^{2}+\cdots+2^{m-1}\right)
$$
squares $=u+vz, u,v\in f_m$. Therefore $1+y\in f'=f_m\oplus zf_{m}$. 

We now come to the proof of Pfister's theorem.

Let $m\geq n$, $f_m=\sum\limits_{i=1}^{2^{m}}x^{2}_i$ and $b$ an
element in $M(f_m)$, $b\neq -1$.

Consider the form
$$
f'=f_m\oplus -(1+b)f_m,\quad 1+b\neq \ho.
$$

Since $b\in M(f_m)$, we have $b-(1+b)=-1$ is in $M(f')$. Using lemma~\ref{lem8} with $-1$ in place of $y$ we see that $f'$ is a zero form
and hence 
$$
\ho=u-(*)v, \quad u,v\in M(f_m).
$$
$uv\neq \ho$. This gives 
$$
M+b=\dfrac{u}{v}
$$
and by theorem~\ref{thm6} corollary~\ref{cro2}, $\dfrac{u}{v}\in
M(f_m)$. Thus if $b\neq-1$ is in $M(f_m)$, then $1+b\in M(f_n)$. 

If therefore $u$ and $v$ are in $M(f_m),uv\neq \ho$, then for $u+v\neq
\ho, u\break\left(u+\dfrac{u}{v}\in M(f_m)\right)$ and so $u+v\in M(f_m)$. Since
$f_m=f_{m-1}\oplus f_{m-1}$ then for any $\lambda\in
M(f_m)\lambda=\mu+\nu, \mu,\nu \in M(f_{m-1})$ and if $m-1\geq n$, then 
$$
\lambda=\mu* \in M(f_{n-1}).
$$

Therefore
$$
M(f_m)\subset M(f_m),\quad m\geq n.
$$

Therefore any element of $L$ which is a sum of squares is in $M(f_n)$
and our theorem is proved. 
\end{Proof}



\begin{thebibliography}{99}
\bibitem{key1}
\textbf{E. Artin}, Collected papers, Addison-Wesley 1965 p.253--295.

\bibitem{key2}
\textbf{Z.I. Borevich and I.R. Shafarevic}, Number Theory, Academic Press 1966,
p.390--396.

\bibitem{key3}
\textbf{J.W.S. Cassels}, On the representation of rational functions as sums of
squares, Acta Arithmetica, 9(1964), p.79--82.

\bibitem{key4}
\textbf{W.J. Ellison and A. Pfister}, On sums of squares and on
Elliptic curves over function fields, J. Number Theory, 3(1971)
p.125--149.

\bibitem{key5}
\textbf{H.Davenport}, The Higher Arithmetic, Hutchinson U.Library
1962, p.114--127.

\bibitem{key6}
\textbf{M.Greenberg}, Lectures on forms in many variables,
W.A.Benjamin Inc., 1969, p.15--23.

\bibitem{key7}
\textbf{D.Hilbert}, The foundations of Geometry, chicago (The open
Court Publishing Co.) 1902, p.116--121.

\bibitem{key8}
Gesammelte Abhandlungen Bd.2, Springar, chelsea Reprint 1965,
p.154--161 and p.345--365.

\bibitem{key9}
\textbf{N.Jacobson}, Lectures on Abstract algebra, Vol.III, Van
Nostrand, East West, Press, 1966, Chapter VI.

\bibitem{key10}
\textbf{E.Landau}, \"{U}ner die Darstelland von definiter Funktionen
durch Quadrate, Math, Amalen, 62(1906), p.272--285.

\bibitem{key11}
\textbf{S.Lang}, On Quasi Algebraic closure, Arm. of Math. 55(1952),
p.373--390.

\bibitem{key12}
\textbf{F.Lorenz}, Quadratische Formen thier K\"{o}rpenm lecture
Notes, Springer Verlag, 1970.

\bibitem{key13}
\textbf{A.Pfister}, Zur Darstellung definiter Funktionen als summe von
Quadraten, Inventiones Math. 4(1967), p.229--237.

\bibitem{key14}
\textbf{A.Pfister}, Darstellung von-1 als summe von Quadraten in einem
K\"{o}rper, J. Lond. Math. Soc. 40 (1965), p.159--65.

\bibitem{key15}
Multiplikative quadratische Formen, Archiv. Math. 16(1965), p.363--70.

\bibitem{key16}
\textbf{C.L.Siegel}, Darstellung total positiver zahlen  durch
Quadrate, Math. Zeit, 11(1921), p.246--275.
\end{thebibliography}
