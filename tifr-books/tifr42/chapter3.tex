\chapter{The three-body problem: general collision}\label{chap3}

\section{Asymptotic estimates}\label{chap3:sec1}

In this\pageoriginale chapter we shall be concerned with the problem
of determining the nature of the first singularity of the three-body
problem when there is a general collision, that is when all the
mass-points collide at $t = t_1$. We shall show that in this case, $t
= t_1$ is in general an essential singularity for at least one of the
coordinate functions of the masses. 

Let us denote as before the coordinates of the three mass-points $P_k$
by $(x_k, y_k,z_k)$, $k =1,2,3$, and their masses by $m_k$. Also let
$q$ denote any one of the nice coordinate functions $x_1, \ldots,
z_3$. We recall that if $U$ denotes the potential function 
\begin{equation*}
U= \frac{m_1m_2}{r_{12}} + \frac{m_2m_3}{r_{23}} +
\frac{m_1m_3}{r_{13}}, \tag{3.1.1}\label{chap3:eq3.1.1} 
\end{equation*}
then the equations of motion are given by
\begin{equation*}
m \dot{q} = U_q.\tag{3.1.2}\label{chap3:eq3.1.2}
\end{equation*}
We have the ten algebraic integrals associated with the system namely,
the six integrals of the centre of gravity, the three integrals of
angular momenta and the energy integral these are given by 
\begin{align*}
& \sum\limits^3_{k=1} m_k x_k = \alpha t+ \alpha', \sum\limits^3_{k=1}
  m_k y_k = \beta t + \beta', \sum\limits^3_{k=1} m_k z_k = \gamma t +
  \gamma';\\ 
& \sum\limits^3_{k=1} m_k (x_k \dot{y}_k - y_k \dot{x}_k) = \lambda,
  \sum\limits^3_{k=1} m_k (y_k \dot{z}_k - z_k \dot{y}_k) = \mu, 
\end{align*}\pageoriginale
\begin{align*}
& \sum\limits^3_{k=1} m_k (z_k \dot{x}_k - x_k \dot{z}_k) = \nu;
  \tag{3.1.3}\label{chap3:eq3.1.3}\\ 
& T - U = h. \tag{3.1.4}\label{chap3:eq3.1.4}
\end{align*}
We may assume, by changing the coordinates by linear functions of the
variable $t$, that the centre of gravity remains fixed at the origin
throughout the motion, so that 
\begin{equation*}
\sum\limits^3_{k=1} m_k x_k = \sum\limits^3_{k=1} m_k y_k = \sum\limits^3_{k=1} m_k z_k = 0. 
\tag{3.1.5}\label{chap3:eq3.1.5}
\end{equation*}
Let $t = t_1$ be the first singularity. So all the coordinates $q(t)$
are regular analytic functions in the interval $t_o \leq t < t_1$, and
at least one coordinates ceases to be regular at $t_1$. Let $\rho_k$
denote the distance of the point $P_k$ from the centre of gravity 0: 
\begin{equation*}
\rho^2_k = x^2_k + y^2_k + z^2_k, \; k  =1,2,3.
\tag{3.1.6}\label{chap3:eq3.1.6} 
\end{equation*}
We introduced in Chapter \ref{chap2} the moment of inertie
\begin{equation*}
\sigma \equiv \sum_q mq^2 = \sum\limits^3_{k=1} m_k
\rho^2_k. \tag{3.1.7}\label{chap3:eq3.1.7} 
\end{equation*}
Differentiating this twice in succession with respect to we have  
\begin{equation*}
\frac{1}{2} \dot{\sigma} = \sum\limits_q mq \dot{q}, \frac{1}{2}
\ddot{\sigma} = \sum\limits^q m\dot{q}^2  + \sum\limits_q mq \;
\ddot{q}.\tag{3.1.8}\label{chap3:eq3.1.8} 
\end{equation*}\pageoriginale
Then we obtain the Lagrange formula
\begin{equation*}
\frac{1}{2} \ddot{\sigma} =  2T - U, \tag{3.1.9}\label{chap3:eq3.1.9} 
\end{equation*}
which can be written, by means of (\ref{chap3:eq3.1.4}), also as 
\begin{equation*}
\frac{1}{2} \ddot{\sigma} = T + h = U +
2h\tag{3.1.10}\label{chap3:eq3.1.10} 
\end{equation*}
By Theorem \ref{chap2:thm2.2.1} we know that $U(t) \to \infty$ as $t
\to t_1$, so that for $t$ sufficiently close to $t_1$, $U + 2h >0$ and
so $\ddot{\sigma} > 0$ and hence $\sigma$ is a convex function of $t$
and has a limit (non-negative) as $t \to t_1$. We may assume that
$\sigma$ is convex in the interval $\tau \leq t < t_1$ itself. We
observe that $\sigma = 0$ if and only if all the coordinates $q$
vanish, that is, $P_1, P_2, ,P_3$ are all at 0. Since by assumption
$t_1$ is the first singularity, there is no collision in the interval
$\tau \leq t < t_1$ and so $\sigma (t) >0$ in $\tau \leq t  <
t_1$. Further $\sigma (t)$ is a regular analytic function  in this
interval. We have seen in Chapter \ref{chap2} that $\sigma (t)$  has a
limit $\sigma_1$ as $t \to t_1 \cdot \sigma_1>0$ or $ \geq 0$
according as $\sigma$ is monotone in monotone increasing or monotone
decreasing in a small interval to the left of $t_1$, that is,
according as $\dot{\sigma} (t) > 0$ or $\dot{\sigma} (t) <  0$ in this
interval. If $\sigma_1>0$ there is only a simple collision at $t =
t_1$. We have studied this cased in Chapter \ref{chap2}. So we shall
consider only the case $\sigma_1 =0$ (which can happen only when
$\dot{\sigma} (t) < 0$) and study the nature of the singularity more
closely in this case.  

We shall\pageoriginale now change our notation and introduce the
variable $t_1 - t$ instead of $t$. It is clear that the equations of
motion (\ref{chap3:eq3.1.2}) remain invariant under this change of
variable. Then as $t$ varies in the interval $\tau \leq t < t_1$, the
variabel $t_1 - t$ varies in the interval $0 < t_1 - t\leq t_1 - \tau$
and $t_1 - t$ tends to 0 through decreasing values as $t \to t_1$
through increasing values. From now on we shall write $t$ in place of
the variable $t_1 -t$ and $\tau$ in place of $t_1 -\tau$. Thus in the
new notation $0 < t\leq \tau$ and $t \to 0$ through decreasing real
values. We consider the coordinates $q$ as functions of the new
variable $t$ and write again $q = q(t)$. Now $U(t) \to \infty$ as $t
\to 0$ and hence in a sufficiently small interval $0 < t \leq t_o$
with $0 < t_o \leq \tau$, we have $U(t) + 2h >0$ so that
$\ddot{\sigma} (t) >0$ in $0 < t< t_o$. Therefore $\sigma(t)$ is again
a convex function of $t$ in $0 <t\leq t_o$. Moreover, $\ddot{\sigma}
(t) >0$ implies that $\dot{\sigma} (t)$ is monotone increasing as $t$
decreases to 0 and is positive in $0 < t \leq t_o$. 

We shall now study the asymototic behaviour of $\sigma(t)$ and
$\dot{\sigma}(t)$ as $t \to 0$. First of all we have the following
inequality: 

\setcounter{subtheorem}{0}
\begin{subtheorem}\label{chap3:thm3.1.1}
$\dot{\sigma}^2 \leq 8 \sigma T$ for $0 < t\leq t_o$. 
\end{subtheorem}

\begin{proof}
By the definition of $\sigma$ and $T$ we have $2\sigma T =
\sum\limits_q mq^2 \sum_q m\dot{q}^2$. For arbitrary real numbers
$\alpha_k, \beta_k, k = 1, \ldots, r$, we have the following identity
of Lagrange (see Chapter \ref{chap2}, \S\ \ref{chap2:sec2}): 
$$
\sum\limits^r_{k=1} \alpha^2_k \sum\limits^r_{k=1} \beta^2_k =
\left(\sum\limits^r_{k=1} \alpha_k \beta_k \right)^2 + \sum\limits_{1
  \leq k < l \leq r} \left(\alpha_k \beta_1 - \alpha_1 \beta_k
\right)^2. 
$$
Taking $r = 9$ and $\alpha_k = q_k \sqrt{m_k}$, $\beta_k =
\dot{q}_k\sqrt{m_k}$, we obtain 
\begin{align*}
2 \sigma T = \left(\frac{1}{2} \dot{\sigma} \right)^2 + \sum\limits_{1
  \leq k < l  \leq 9} m_k m_l (q_k \dot{q}_l -q_l \dot{q}_k)^2
\tag{3.1.11}\label{chap3:eq3.1.11} 
\end{align*}\pageoriginale 
But 
$$
\sum\limits_{1 \leq k < l \leq 9} m_k m_l (q_k \dot{q}_l - q_l
\dot{q}_k)^2 \geq 0.  
$$
Hence from (\ref{chap3:eq3.1.11}) we get $2 \sigma T \geq
(\dfrac{1}{2} \dot{\sigma})^2$, i.e. $\dot{\sigma}^2 \leq 8 \sigma T$,
which completes the proof. 
\end{proof}

\begin{subtheorem}\label{chap3:thm3.1.2}
There is a positive constant $\kappa$ such that
\begin{align*}
& \sigma (t) \sim \kappa t^{4/3} \text{~ as ~} t \to 0;
  \tag{3.1.12}\label{chap3:eq3.1.12}\\ 
& \dot{\sigma} (t) \sim \frac{4}{3} \kappa t^{1/3} \text{~ as ~} t \to
  0. \tag{3.1.13}\label{chap3:eq3.1.13} 
\end{align*}
\end{subtheorem}

\begin{proof}
By Theorem \ref{chap3:thm3.1.1}, $8 \sigma T - \dot{\sigma}^2 \geq
0$. By the Lagrange formula (\ref{chap3:eq3.1.10}), $\ddot{\sigma} = 2
T + 2h$ and this can be written 
$$
\ddot{\sigma} = \frac{1}{4} (8\sigma T - \dot{\sigma}^2+
\dot{\sigma}^2) \sigma^{-1} + 2h,  
$$
and hence,
\begin{equation*}
\ddot{\sigma} - \frac{1}{4}  \dot{\sigma}^2 \sigma^{-1} = \frac{1}{4}
(8\sigma T - \dot{\sigma}^2) \sigma^{-1} +
2h. \tag{3.1.14}\label{chap3:eq3.1.14} 
\end{equation*}
Multiplying both sides by $\sigma^{-1/4}$ we find that 
$$
\left(\dot{\sigma} \sigma^{-1/4} \right) = \ddot{\sigma} \sigma^{-1/4}
- \frac{1}{4} \dot{\sigma}^2 \sigma^{-5/4} = \frac{1}{4} (8 \sigma T -
\dot{\sigma}^2) \sigma^{-5/4} + 2h \sigma^{-1/4}.  
$$
If we denote by $\sigma_o, \dot{\sigma}_o$ the values of $\sigma (t)$
and $\dot{\sigma}(t)$, at $t = t_o$, we obtain, on integration from
$t$ to $t_o$, 
\begin{equation*}
\dot{\sigma}_o \sigma_o^{-1/4} - \dot{\sigma} \sigma^{-1/4} =
\frac{1}{4} \int\limits^{t_o}_{t} (8 \sigma T - \dot{\sigma}^2)
\sigma^{-5/4}dt  + 2h \int\limits^{t_o}_{t} \sigma^{-1/4}
dt. \tag{3.1.15}\label{chap3:eq3.1.15} 
\end{equation*}
Here\pageoriginale $\sigma(t)$ and $\dot{\sigma}(t)$ are positive and
hence $\dot{\sigma} \sigma^{-1/4} \geq 0$ as $t \to 0$, so it can only
become $+ \infty$ if it has no finite limit as $t \to 0$. Consequently
the left side can at worst become $-\infty$ as $t \to 0$. On the right
side, $(8 \sigma T - \dot{\sigma}^2) \sigma^{-5/4} \geq 0$ by Theorem
\ref{chap3:thm3.1.1} and so $\int\limits^{t_o}_t (8\sigma T - \dot{\sigma}^2)
\sigma^{-5/4} dt \geq 0$. This has either a finite positive limit as
$t \to 0$, or it tends to $+ \infty$ as $t \to 0$. We shall, however,
show that this integral converges and that $\dot{\sigma}
\sigma^{-1/4}$ tends to a finite limit. For this it is sufficient to
prove that the integral $\int\limits^{t_o}_t  \sigma^{-1/4} dt$
converges as $t \to 0$. 

We shall get a lower estimate for $\sigma$. In the following $\mu_1,
\mu_2, \ldots$ denote positive constants which depend only on the
masses $m_1, m_2, m_3$. Let $\mu = \min\limits_{k=1,2,3} m_k$, so that
$\sigma \geq \mu (\rho^2_1 + \rho^2_2 + \rho^2_3)$ and by the Schwarz
inequality this gives $\sigma \geq \dfrac{\mu}{9}(\rho_1 + \rho_2 +
\rho_3)^2 $. By the triangle inequality we have $r_{12} \leq \rho_1 +
\rho_2$ and hence
\begin{equation*}
\dot{\sigma} \geq \frac{\mu}{9} r^2_{12} , \text{~ or ~}
\frac{1}{r_{12}} > \mu_1 \sigma^{-1/2}. \tag{3.1.16}\label{chap3:eq3.1.16}
\end{equation*}
This gives a lower estimate for the potential function: $U(t) >\mu_2
\sigma^{-1/2}$. Moreover, $\dfrac{1}{2} \ddot{\sigma} = U + 2h$ so
that, for $t$ sufficiently near 0,
\begin{equation*}
\ddot{\sigma}(t) > \mu_3 \sigma^{-1/2}, \tag{3.1.17}\label{chap3:eq3.1.17}
\end{equation*}
and on multiplication throughout by $2 \dot{\sigma}$,
${(\dot{\sigma}^2)}^\bigdot \leq 4 \mu_3
{(\sigma^{1/2})}^\bigdot$. Integrating this 
from $0$ to $t$  we have 
$$
\dot{\sigma}^2 - \dot{\sigma}(0)^2 \geq \mu_4 (\sigma^{1/2}  - \sigma
(0)^{1/2}). 
$$\pageoriginale
Since $\sigma \to 0$ as $t\to 0$, $\sigma(0) = 0$ and further,
$\dot{\sigma} (0)^2 \geq 0$, so that $\dot{\sigma}^2 \geq \mu_4
\sigma^{1/2}$, and hence
\begin{equation*}
\dot{\sigma} \geq \mu_5 \sigma^{1/4}. \tag{3.1.18}\label{chap3:eq3.1.18}
\end{equation*}
Then ${(\sigma^3/4)}^\bigdot = \dfrac{3}{4} \dot{\sigma} \sigma^{-1/4} \geq
\dfrac{3}{4} \mu_5$, which on integration from 0 to $t$, with
$\sigma(0) = 0$, gives $\sigma^{3/4} \geq \mu_6 t$, or
\begin{equation*}
\sigma (t) \geq \mu_7 t^{4/3}, \; 0 <t \leq
t_o. \tag{3.1.19}\label{chap3:eq3.1.19} 
\end{equation*}
Thus we obtain an upper estimate for the integral $\int\limits^{t_o}_t
\sigma^{-1/4} dt$: 
$$
\int\limits^{t_o}_t \sigma^{-1/4} dt \leq \mu_8 \int\limits^{t_o}_t
t^{-1/3} dt, 
$$
and the last integral converging as $t \to 0 $, the convergence of
$\int\limits^{t_o}_t \sigma^{-1/4}dt $ follows.

Now we shall show that $\sigma (t)$ actually behaves like $t^{4/3}$
asymptotically as $t \to 0$. Consider the indentity (\ref{chap3:eq3.1.15}). Since
the second integral on the right converges, it follows that
$\dot{\sigma} \sigma^{-1/4}$ tends to a finite limit $a \geq 0$ as $t
\to 0$. So $(\sigma^{3/4})^. = \dfrac{3}{4} \dot{\sigma} \sigma^{-1/4}
\to \dfrac{3}{4} a$ as $t \to 0$. In other words. $(\sigma^{3/4})^. =
\dfrac{3}{4} a + o (1)$ as $t \to 0$. Integrating this from 0 to $t$
we see that 
$$
\sigma^{3/4} = \dfrac{3}{4} \text{ at } + o (t),
$$\pageoriginale
which implies that, as $t\to 0$,
\begin{equation*}
\sigma (t) \sim (\frac{3}{4} a)^{4/3}
t^{4/3}. \tag{3.1.20}\label{chap3:eq3.1.20} 
\end{equation*}
The constant $a$ is necessarily positive as $a = 0$ would imply that
$\sigma (t) \to 0$ more rapidly than $t^{4/3}$ as $t \to 0$, which is
not possible because of (\ref{chap3:eq3.1.19}). So $a > 0$; let $\kappa =
(\dfrac{3}{4} a)^{4/3}$. Then we have $\sigma \sim \kappa t^{4/3}$ as
$t \to 0$, which proves the first assertion. Further, from
$\dfrac{3}{4} \dot{\sigma} \sigma^{-1/4} = \dfrac{3}{4} a + o (1)$, we
have
$$
\dot{\sigma} = a\sigma^{1/4} + o (\sigma^{1/4}) \sim \frac{4}{3}
\kappa^{3/4} \kappa^{1/4} t^{1/3}, 
$$
that is, $\dot{\sigma} \sim \dfrac{4}{3} \kappa t^{1/3}$ as $t \to
0$. This completes the proof of the theorem.
\end{proof}

We remark that the asymptotic estimate for $\dot{\sigma}$ proved
directly in Theorem \ref{chap3:thm3.1.2} is an improvement over the
asymptotic 
estimate $\sigma \sim \kappa t^{4/3}$, in the following sense. If we 
could ``differentiate'' the asymptotic relation (\ref{chap3:eq3.1.12})
for $\sigma$ 
with respect to $t$, we would have obtained the asymptotic relation
(\ref{chap3:eq3.1.13}) for $\dot{\sigma}$. But in general such a
differentiation is 
not permissible and so the direct proof above is an improvement.

We conjecture that we can again ``differentiate'' the relation
(\ref{chap3:eq3.1.13}) formally and obtain an asymptotic estimate for
$\ddot{\sigma}$ in the form
\begin{equation*}
\ddot{\sigma} = \frac{4}{9} \kappa t^{-2/3} \text{~ as ~} t \to
0. \tag{3.1.21}\label{chap3:eq3.1.21} 
\end{equation*}
We shall\pageoriginale see later that (\ref{chap3:eq3.1.21}) in fact holds. For
proving this we shall again use the Lagrange formula $\dfrac{1}{2}
\ddot{\sigma} = U + 2h $. So we proceed first to determine the
asymptotic behaviour of $U(t)$ itself as $t \to 0$.

Consider the function
\begin{equation*}
g(t) = (8\sigma T - \dot{\sigma}^2)
t^{-2/3}. \tag{3.1.22}\label{chap3:eq3.1.22} 
\end{equation*}
In the course of the proof of Theorem \ref{chap3:thm3.1.2} we have
shown that the 
integral
$$
\int\limits^{t_o}_t (8\sigma T - \dot{\sigma}^2) \sigma^{-5/4} dt
$$
converges as $t \to 0$. This means that the integral
$\int\limits^{t_o}_t g (t)t^{2/3} \sigma^{-5/4}dt$ converges as $t \to
0$. But  we have proved above that $\sigma \sim\kappa t^{4/5}$ and so
$\sigma^{-5/4} \sim \kappa^{-5/4} t^{-5/3}$. Hence
$$
\int\limits^{t_o}_t g(t) t^{2/3} \sigma^{-5/4} dt =
\int\limits^{t_o}_t g(t) (\kappa^{-5/4} t^{-1} + o (t^{-1})) dt
$$
converges as $t \to 0$. In particular, the integral
\begin{equation*}
\int\limits^{t_o}_t g(t) \frac{dt}{t}\tag{3.1.23}\label{chap3:eq3.1.23}
\end{equation*}
converges as $t \to 0$. But by Theorem \ref{chap3:thm3.1.1}, $g(t) \geq 0$ and
therefore it follows that
$$
\underset{t\to 0}{\underline{\lim\limits}} \; g(t) = 0,
$$
for $\underset{t\to 0}{\underline{\lim\limits}} \; g(t)
>0$\pageoriginale would imply that the 
integral $\int\limits^{t_o}_t g(t) \dfrac{dt}{t}$ diverges. We shall
next prove that 
$$
\overline{\lim\limits_{t \to 0}} g(t) = 0. 
$$
Suppose if possible that $\overline{\lim\limits_{t \to 0}} g(t) > 3
\epsilon$, $0< \epsilon < 1$. We shall prove that this leads to a
contradiction. By the continuity of $g(t)$ in the interval $0 < t\leq
\tau$, we can find a decreasing sequence of numbers $\tau \geq t_1 >
t_2 > \ldots > 0$ such that 
\begin{align*}
& \epsilon \leq g(t) \leq 3 \epsilon, \; t_{2k} \leq t \leq t_{2k-1},
  \tag{3.1.24} \label{chap3:eq3.1.24}\\ 
&  g(t_{2k}) = \epsilon, \; g(t_{2k -1}) =
  3\epsilon. \tag{3.1.25}\label{chap3:eq3.1.25} 
\end{align*}
By Theorem \ref{chap3:thm3.1.2} we know that there exists a positive
number $\kappa$ 
depending only on the three masses such that $\sigma(t) \sim\kappa
t^{4/3}$ and $\dot{\sigma}(t) \sim \dfrac{4}{3} \kappa t^{1/3}$ as $t
\to 0$. Hence
\begin{equation*}
\sigma(t) = \kappa t^{4/3} (1+\delta_o(t)), \; \dot{\sigma} (t) =
\frac{4}{3} \kappa t^{1/3} (1+ \delta_1(t)),
\tag{3.1.26}\label{chap3:eq3.1.26} 
\end{equation*}
where $\delta_o(t), \delta_1(t) \to 0$ as $t \to 0$. If an asymptotic
estimate for $T$ were known we could then get an estimate for $8
\sigma T t^{-2/3}$ and hence also for $g(t)$. But we do not have such
an estimate for $T$ as yet. However, we can get an upper estimate for
$T$ as follows. From the definition of $g$ we have
$$
T = \frac{1}{8}(g(t) t^{2/3} + \dot{\sigma}^2) \sigma^{-1}. 
$$
By (\ref{chap3:eq3.1.24}),\pageoriginale $g(t) \leq 3 \epsilon$,
$t_{2k} \leq t \leq 
t_{2k-1}$. On the other hand, by (\ref{chap3:eq3.1.26}), 
$$
\sigma^{-1} =\kappa^{-1} t^{-4/3} (1+ \delta_o (t))^{-1}, \;
\dot{\sigma}^2= (\frac{4}{3}\kappa)^2 t^{2/3} (1+ \delta_1 (t))^2. 
$$
Hence we obtain
$$
T \leq \frac{1}{8} (3 \epsilon t^{2/3} + (\frac{4}{3} \kappa)^2
t^{2/3} (1+ \delta_1 (t))^2) \kappa^{-1} t^{-4/3} (1+ \delta_o
(t))^{-1}. 
$$
It follows that $T \leq $ constant. $t^{-2/3}$, that is 
\begin{equation*}
T = 0 (t^{-2/3}) t_{2k} \leq t \leq t_{2k-1}, k \to
\infty. \tag{3.1.27}\label{chap3:eq3.1.27} 
\end{equation*}
Since $T = \dfrac{1}{2} \sum\limits_q m \dot{q}^2$,
(\ref{chap3:eq3.1.27}) implies, 
in particular, that $\dot{q}^2 = 0 (t^{-2/3})$ as $t\to 0$, $t_{2k }
\leq t \leq t_{2k-1}$, so that we have an estimate for the velocity
components of $P_1$, $P_2$, $P_3$:
\begin{equation*}
\dot{q} = 0 (t^{-1/3}) \text{ as } t \to
0. \tag{3.1.28}\label{chap3:eq3.1.28} 
\end{equation*}
In view of the energy integral $T - U =h$ then we have
\begin{equation*}
U  = T - h =0 (t^{-2/3}) \text{ as } t \to
0. \tag{3.1.29}\label{chap3:eq3.1.29} 
\end{equation*}
By the definition of $U$, (\ref{chap3:eq3.1.29}) implies that
\begin{equation*}
r^{-1}_{kl} = 0 (t^{-2/3}) \text{ as } t \to
0. \tag{3.1.30}\label{chap3:eq3.1.30} 
\end{equation*}
From these estimate we can get upper estimates for the derivatives
$\dot{U}$ and $\dot{T}$  in the following way. We have
$$
\dot{U} = - \sum\limits_{1 \leq k < l \leq 3} \frac{m_k m_l}{r^3_{kl}}
(x_k - x_l) (\dot{x}_k - \dot{x}_l).
$$\pageoriginale
But $\dfrac{|x_k - x_l|}{r_{kl}} \leq 1$ and so we have
$$
|\dot{U}| \leq \sum\limits_{1 \leq k < l \leq 3}
\frac{m_km_l}{r^2_{kl}} (|\dot{x}_k| + |\dot{x}_l|).
$$
Then the estimates (\ref{chap3:eq3.1.28}) and (\ref{chap3:eq3.1.30})
show that 
\begin{equation*}
\dot{U} = 0 (t^{-5/3}) \text{ as } t \to
0. \tag{3.1.31}\label{chap3:eq3.1.31} 
\end{equation*}
Once again, by the energy integral, $\dot{T} = \dot{U}$, so that
\begin{equation*}
\dot{T} = 0 (t^{-5/3}) \text{ as } t \to
0. \tag{3.1.32}\label{chap3:eq3.1.32} 
\end{equation*}
The asymptotic formula (\ref{chap3:eq3.1.26}) together with the
estimate (\ref{chap3:eq3.1.27}) 
and (\ref{chap3:eq3.1.32}) enables us to calculate the total variation
of $g(t)$ in 
the interval $t_{2k} \leq t \leq t_{2k-1}$. In fact, on
differentiation with respect to $t$ we have
\begin{gather*}
(T \sigma t^{-2/3})^. = \dot{T} \sigma t^{-2/3} + T \dot{\sigma}
  t^{-2/3} - \frac{2}{3} T \sigma t^{-5/3} = 0 (t^{-1}), \text{ as } t
  \to  0\\
\text{ and } \quad t_{2k} \leq t \leq t_{2k -1}.
\end{gather*}
Hence we have the inequality
$$
8 (T \sigma t^{-2/3})^. \leq \mu t^{-1} \text{ as } t \to 0,
$$
and on integration from $t_{2k}$ to $t_{2k-1}$ we have 
\begin{equation*}
8 \int\limits^{t_{2k-1}}_{t_{2k}}  (T \sigma t^{-2/3})^. dt \leq \mu
\int\limits^{t_{2k-1}}_{t_{2k}} \frac{dt}{t} \text{ as } k \to
\infty.  \tag{3.1.33}\label{chap3:eq3.1.33}
\end{equation*}
On the\pageoriginale other hand, using the asymptotic estimate
(\ref{chap3:eq3.1.26}) for $\dot{\sigma} (t)$, we find that the total
variation of $\dot{\sigma}^2 t^{-2/3}$ in the interval $t_{2k} \leq t
\leq t_{2k-1}$ is $o(1)$ as $k \to \infty$. Thus there exists a
positive integer $k_o$ such that for $k \geq k_o$, the variation of
$\dot{\sigma}^2 t^{-2/3}$ in the interval $t_{2k} \leq t \leq t_{2k
  -1}$ is smaller than $\epsilon$ and also the estimate
(\ref{chap3:eq3.1.33}) for the variation of $T \sigma t^{-2/3}$
holds. So the  variation of $g(t)$ in the interval $t_{2k} \leq t \leq
t_{2k-1}$ is estimated by 
$$
2 \epsilon = g(t_{2k-1}) -g (t_{2k}) \leq \epsilon + \mu
\int\limits^{t_{2k-1}}_{t_{2k}} \frac{dt}{t}, \; k \geq k_o.  
$$
Hence $\int\limits^{t_{2k -1}}_{t_{2k}} \dfrac{dt}{t} \geq
\dfrac{\epsilon}{\mu}$, from which it follows that for $k \geq k_o$ 
$$
\int\limits^{t_{2k-1}}_{t_{2k}} g(t) \dfrac{dt}{t} \geq \frac{\epsilon^2}{\mu}. 
$$
Since corresponding to $k \geq k_o$ there are infinitely many disjoint
intervals $t_{2k} \leq t \leq t_{2k -1}$, each of which gives a
contribution exceeding $\epsilon^2 / \mu$ to the integral, it follows
that $\int\limits^\tau_0 g(t) \dfrac{dt}{t}$ diverges, which is a
contradiction. This means that necessarily $\overline{\lim\limits_{t
    \to 0}} g(t) = 0$. and hence $g(t) \to 0$ as $t \to 0$, which
proves the required assertion.  

As a consequence of (\ref{chap3:eq3.1.22}) and the fact that $g(t) \to
0$ as $t \to 0$, we have 
\begin{equation*}
8 \sigma T - \dot{\sigma}^2 = 0 (t^{2/3}) \text{ as } t \to
0,\tag{3.1.34}\label{chap3:eq3.1.34} 
\end{equation*}
and hence\pageoriginale
\begin{equation*}
T \sim\frac{1}{8} (\frac{4}{3} \kappa t^{1/3})^2 (\kappa t^{4/3})^{-1}
= \frac{2}{9} \kappa t^{-2/3}, \text{ as } t \to
0. \tag{3.1.35}\label{chap3:eq3.1.35} 
\end{equation*}
By the energy integral $U = T -h$ it follows now that
\begin{equation*}
U \sim \frac{2}{9} \kappa t^{-2/3} \text{ as } t\to
0. \tag{3.1.36}\label{chap3:eq3.1.36} 
\end{equation*}

We have already proved in Chapter \ref{chap2} the theorem of Sundman
that if there is a general collision at $t=0$, then the constants of
angular momenta $\lambda,\mu, \nu$ all vanish. This can also be proved
with the help of the estimates we have obtained, in the following
way. 

\begin{subtheorem}[Sundman]\label{chap3:thm3.1.3}
If there is a general collision at $t = 0$, then $\lambda = \mu = \nu
= 0$. 
\end{subtheorem}

\begin{proof}
We denote by $q_1, \ldots, q_9$ the nine coordinates $x_1, \ldots,
z_3$, and by $\mu_1, \ldots, \mu_9$ the corresponding masses. Taking
$\alpha_k = q_k \sqrt{\mu_k}$ and $\beta_k = \dot{q}_k \sqrt{\mu_k}$
in the Lagrange identity 
$$
\sum\limits^9_{k=1} \alpha^2_k \sum\limits^9_{k=1} \beta^2_k =
\left(\sum\limits^9_{k=1} \alpha_k \beta_k \right)^2 +
\sum\limits_{1\leq k < l \leq q} \left(\alpha_k \beta_l - \alpha_l
\beta_k \right)^2,  
$$
we obtain
$$
8 \sigma T - \dot{\sigma}^2 = 4 \sum\limits_{1 \leq k < l \leq 9}
\mu_k \mu_1 (q_k \dot{q}_l - q_l\dot{q}_k)^2.  
$$
Since by (\ref{chap3:eq3.1.34}), $8 \sigma T - \dot{\sigma}^2 = o
(t^{2/3})$ as $t \to 0$, it follows that  
$$
\sum\limits_{1\leq k < l \leq 9} \mu_k \mu_l (q_k \dot{q}_l - q_l
\dot{q}_k)^2 = o (t^{2/3}) \text{ as } t \to 0.  
$$
Since all\pageoriginale the quantities $\mu_k, q_k, \dot{q}_k$ are
real, we see immediately that 
$$
q_k \dot{q}_l - q_l \dot{q}_k  = o (t^{1/3}) \text{ as } t \to 0, \;
k, \; l =1, \ldots, 9 , \; k  \neq l.  
$$
(This estimate would naturally not be valid in the complex case). If
we change our notation and denote by $p$, $q$ two distinct coordinates
$x_1, \ldots, z_3$, then we can write 
\begin{equation*}
p\dot{q} - q\dot{p} = o (t^{1/3}) \text{ as } t \to
0. \tag{3.1.37}\label{chap3:eq3.1.37} 
\end{equation*}
We recall that the integrals of angular momenta are given by 
\begin{gather*}
\sum\limits^3_{k=1} m_k (x_k \dot{y}_k - y_k \dot{x}_k) = \lambda,
\sum\limits^3_{k=1} m_k (y_k \dot{z}_k - z_k \dot{y}_k) = \mu, \\ 
\sum\limits^3_{k=1} m_k (z_k \dot{x}_k - x_k \dot{z}_k) = \nu. 
\end{gather*}
If there is a general collision at $t = 0$, then $\sigma (t) \to 0$ as
$t \to 0$ and the estimates (\ref{chap3:eq3.1.27}) hold. Consequently,
taking for $p,q$ the coordinates $x_k, y_k; y_k, z_k ; z_k, x_k$ in
turn, we see that $\lambda = \mu = \nu = 0$, and this completes the
proof. 
\end{proof}

We remark that the converse of Theorem \ref{chap3:thm3.1.3} is not in
general true, that is to say, $\lambda = \mu = \nu = 0$ does not
necessarily imply that there is a general collision at the singularity
$t = 0$.  

\section{The limiting configuration at a general collision}\label{chap3:sec2}
We shall now prove a result due to Sundman that in the case of a
general collision the three mass-points always lie in a fixed plane
through the centre of gravity (which is assumed to be
fixed\pageoriginale fixed at the origin). For this purpose we shall
proceed as follows.  

It may happen that at the initial time $t = \tau$ the three point
$P_1, P_2 , P_3$ lie on the same straight line. We assume for the
moment that this is not the case. Hence at time $t = \tau$ the area of
the triangle $P_1 P_2 P_3$ is different from zero. This plane can be
taken to be the $(x,y)$-plane, by means of an orthogonal
transformation, if necessary,  applied to the plane determined by
$P_1, P_2, P_3$. We verify first that the differential equations of
motion remain invariant under a fixed orthogonal transformation. Let
$A = (a_{kl})$ denote the three-rowed matrix of the orthogonal
transformation. Then we have  
\begin{equation*}
\sum\limits^3_{j=1}  a_{kj} a_{lj} = \delta_{kl}, \; k,l = 1,2,3,
\tag{3.2.1}\label{chap3:eq3.2.1} 
\end{equation*}
where $\delta_{kk} = 1$ and $\delta_{kl} = 0$ if $k \neq l$. Let
$(x_k, y_k , z_k)$ denote the original coordinates of the point $P_k$
and $(X_k, Y_k, Z_k)$ its coordinates after the orthogonal
transformation. Then,  
\begin{align*}
X_k &= a_{11} x_k + a_{12} y_k + a_{13} z_k,\\ 
Y_k &= a_{21}x_k + a_{22} y_k + a_{23}z_k, Z_k = a_{31} x_k + a_{32}
y_k + a_{33}z_k.  
\end{align*}
Differentiating $X_k$ twice with respect to $t$ and using the
equations of motion $m_k \ddot{q}_k = U_{q_k}, q_k = x_k, y_k, z_k $,
$k = 1,2,3$, we have  
\begin{align*}
m_k \ddot{X}_k & = a_{11} m_k \ddot{x}_k + a_{12} m_k \ddot{y}_k +
a_{13} m_k \ddot{z}_k \\ 
& = a_{11} U_{x_k} + a_{12} U_{y_k} + a_{13}
U_{z_k}.\tag{3.2.2}\label{chap3:eq3.2.2} 
\end{align*}
On the other hand, we have, by the chain-rule,
$$
U_{x_k} = U_{X_k} (X_{k})_{x_k} + U_{Y_k} (Y_k)_{x_k} + U_{Z_k}
(Z_k)_{x_k} = a_{11} U_{X_k} + a_{21} U_{Y_k} + a_{31} U_{Z_k}, 
$$\pageoriginale
and we have similar relations for $U_{y_k}$ and $U_{z_k}$. Therefore,
using (\ref{chap3:eq3.2.1}) and (\ref{chap3:eq3.2.2}) we get $m_k
\ddot{X}_k = U_{X_k}$, $k = 1,2,3$, and similarly, $m_k \ddot{Y}_k =
U_{Y_k}$, $m_k \ddot{Z}_k = U_{Z_k}$, $k = 1,2,3$. Thus the
differential equations remain unchanged by the orthogonal
transformation. Moreover, if the integrals of angular momenta
$\lambda, \mu, \nu$, vanish in the original coordinate system, they
vanish also in the new coordinate system. This follows by direct
computation. For instance,  
\begin{align*}
\sum\limits^3_{k=1} m_k (X_k \dot{Y}_k - Y_k \dot{X}_k) & = (a_{11}
a_{22} - a_{12} a_{21}) \sum\limits^3_{k=1} m_k (x_k \dot{y}_k - y_k
\dot{z}_k) + \\ 
& + (a_{12} a_{23} - a_{13} a_{22}) \sum\limits^3_{k=1} m_k (y_k
\dot{z}_k - z_k \dot{y}_k) + \\ 
& + (a_{13} a_{21} - a_{11} a_{23}) \sum\limits^3_{k=1} m_k (z_k
\dot{x}_k - x_k \dot{z}_k) = 0. 
\end{align*}
Then we have the following

\setcounter{subtheorem}{0}
\begin{subtheorem}[Sundman]\label{chap3:thm3.2.1}
If the centre of gravity remains fixed at the origin and there is a
general collision at $t = 0$, then the three mass-points $P_1, P_2,
P_3$ remain in a fixed plane throughout the  motion. 
\end{subtheorem}

\begin{proof}
Suppose that $P_1, P_2, P_3$ are not in the same straight line at $t =
\tau$. We perform an orthogonal transformation and assume that $P_1,
P_2, P_3$ lie in the $(x,y)$-plane at $t = \tau$. Then $z_k(\tau) =
0$, $k = 1,2,3$. Since the area of the triangle formed by $P_1, P_2,
P_3$ at $t = \tau$ is not zero, we have 
\begin{equation*}
\begin{vmatrix}
x_1 & x_2 & x_3\\
y_1 & y_2 & y_3\\
1 & 1 & 1 
\end{vmatrix} \neq 0 \text{ at } t =
\tau. \tag{3.2.3}\label{chap3:eq3.2.3} 
\end{equation*}\pageoriginale
Since the centre of gravity remains fixed at the origin, we have 
\begin{equation*}
\sum\limits^3_{k=1} m_k \dot{z}_k = 0 \text{ at } t =
\tau. \tag{3.2.4}\label{chap3:eq3.2.4} 
\end{equation*}
Moreover, since $z_k (\tau) = 0$, $k = 1,2,3,$ and there is a general
collision at $t = 0$, we have from the integrals of angular momentum
$(\lambda = \mu = \nu = 0)$, 
\begin{equation*}
\sum\limits^3_{k=1} m_k x_k \dot{z}_k = 0, \; \sum\limits^3_{k=1} m_k
y_k \dot{z}_k = 0, \text{ at } t =
\tau. \tag{3.2.5}\label{chap3:eq3.2.5} 
\end{equation*}
Equations (\ref{chap3:eq3.2.4}) and (\ref{chap3:eq3.2.5}) form a
system of three linear equations satisfied by $\dot{z}_k, k = 1,2,3,$
at $t = \tau$. Since the matrix of this system of linear equations
has, by (\ref{chap3:eq3.2.3}), a determinant $\neq 0$, it follows that
$\dot{z}_k (\tau) = 0$, $k = 1,2,3$. From the equations of  motion we
have  
\begin{align*}
m_1 \ddot{z}_1 & = U_{z_1} = \frac{m_1 m_2}{r^3_{12}} (z_2 - z_1) +
\frac{m_1 m_3}{r^3_{13}}  (z_3 - z_1), \\ 
\text{or } \qquad \ddot{z}_1 & = \frac{m_2}{r^3_{12}} (z_2 - z_1) +
\frac{m_3}{r^3_{13}} (z_3 - z_1), 
\end{align*}
and similarly, 
$$
\ddot{z}_2 = \frac{m_1}{r^3_{12}} (z_1 - z_2) + \frac{m_3}{r^3_{23}}
(z_3 - z_2) , \ddot{z}_3 = \frac{m_1}{r^3_{13}} (z_1 - z_3) +
\frac{m_2}{r^3_{23}} (z_2 - z_3).  
$$
At the initial time $t = \tau$, $r_{12} (\tau)$, $r_{23} (\tau)$,
$r_{13}(\tau) \neq 0$. But since\pageoriginale $z_k(\tau) = 0$, it
follows from the equations above that $\ddot{z}_k(\tau) = 0$, $k =
1,2,3$. Differentiating the equations successively and using the fact
that $z_k, \dot{z}_k, \ddot{z}_k$ vanish at $t = \tau$, we find that
all the derivatives of $z_k$ vanish at $t = \tau$, $k= 1,2,3$. Since
there is no collision in the interval $0 < t \leq \tau$, we know that
all the coordinate functions $z_k(t)$ are regular analytic functions
in $0 <t \leq \tau$. It then follows that $z_k (t) \equiv 0$ for $0 <
t <\tau$, $k =1 ,2,3$. We could also prove this fact directly without
making use of the analyticity of $z_k$ in $0 <t\leq \tau$. In fact,
consider the system of differential equation $m_k \ddot{q}_k =
U_{q_k}$, $q_k = x_k, y_k, z_k, \; k = 1,2,3$. We prove as before that
$\dot{z}_k(\tau) = 0$. Then we use the fact that if we fix
$x_k(\tau)$, $y_k (\tau)$, $z_k(\tau)$ and $\dot{x}_k(\tau),
\dot{y}_k(\tau), \dot{z}_k (\tau)$, then this system of differential
equations has a unique solution. If we now fix $x_k(t)$ and $y_k(t)$,
the differential equations for $z_k$ with initial conditions $z_k
(\tau) = 0 = \dot{z}_k(\tau)$ is identically satisfied by $z_k(t) =
0$, $0 < t \leq \tau$, because the differential equations contain the
differences $z_k - z_l$ in the numerator. Then by uniqueness the two
solutions coincide. 

Next we consider the case in which $P_1, P_2, P_3$ lie on a straight
line at the initial time $t = \tau$. Choose this line as the $x$-axis
and choose as $(x,y)$-plane the plane determined by this line and the
direction of the velocity vector of $P_3$ at the initial time, that
is, $\dot{z}_3 = 0$ at $t = \tau$. Thus we have $y_1(\tau) = y_2(\tau)
= y_3(\tau) = 0$. Since $\lambda = \mu = \nu = 0$, the condition
$\dot{z}_3(\tau) = 0$ implies that $m_1 \dot{z}_1(\tau)$ and $m_2
\dot{z}_2 (\tau)$ satisfy the homogeneous linear equations $m_1
\dot{z}_1(\tau) + m_2 \dot{z}_2 (\tau) = 0$ and\pageoriginale
$m_1x_1(\tau) \dot{z}_1(\tau) + m_2 x_2(\tau) \dot{z}_2(\tau) =
0$. The matrix of this system of homogeneous linear equations is 
$$
\begin{pmatrix}
x_1(\tau) & x_2 (\tau)\\
1 & 1
\end{pmatrix}.
$$
Since there is no collision at $t = \tau$, it follows that $x_1 (\tau)
\neq x_2(\tau)$ and hence this matrix has a non-vanishing
determinant. One then obtains $\dot{z}_k(\tau) = 0$, $k =
1,2,3$. Repeating the argument used in the earlier case one has $z_1 =
z_2 = z_3 = 0$ for all $t$ in $0 < t \leq \tau$. This completes the
proof of the theorem. 
\end{proof}

In view of Theorem \ref{chap3:thm3.2.1} we may assume that $P_1, P_2,
P_3$ remain in the fixed plane $z = 0 $ throughout the motion, so that
$z_k(t) = 0$, $k =1,2,3$, for all $t, 0 < t \leq \tau$. We wish to
determine the behaviour of the six coordinates $x_k , y_k, k = 1,2,3$,
near $t = 0$. Let $q$ denote any of these six coordinates. By Theorem
\ref{chap3:thm3.1.2} we have $\sigma = \sum\limits_q mq^2 \sim \kappa
t^{4/3}$ as $t \to 0$, which implies that $q = 0 (t^{2/3})$ as $t \to
0$. One would conjecture that every one of the six coordinates $q$ can
be expanded as a power-series in the variable $t^{1/3}$, starting with
the term $t^{2/3}$, in a neighbourhood of $t = 0$. This was the case
when there was a simple collision, as show in Chapter \ref{chap2}. It
is no longer so in the case of a general collision. However, one can
get an expansion for $q$ in the variable $t^{1/3}$, this time with
irrational exponents. 

If $q$ denotes any of $x_k, y_k, k = 1,2,3$, we set
\begin{equation*}
q = q^{*} t^{2/3}.\tag{3.2.6}\label{chap3:eq3.2.6}
\end{equation*}\pageoriginale
Since $q = 0 (t^{2/3})$, we have $q^* =0 (1)$ as $t \to
0$. Differentiating (\ref{chap3:eq3.2.6}) with respect to $t$ one
obtains  
\begin{equation*}
\dot{q} = \dot{q}^* t^{2/3} + \frac{2}{3}  q^*
t^{-1/3}. \tag{3.2.7}\label{chap3:eq3.2.7} 
\end{equation*}
Similarly, if $p$ denotes a coordinate distinct from $q$, let $p=p^*
t^{2/3}$, then $p^* = 0 (1)$ and  
\begin{equation*}
\dot{p} = \dot{p}^* t^{2/3} + \frac{2}{3} p^*
t^{-1/3}. \tag{3.2.8}\label{chap3:eq3.2.8} 
\end{equation*}
From (\ref{chap3:eq3.2.7}) and (\ref{chap3:eq3.2.8}) we obtain 
$$
p\dot{q} - q \dot{p} = (p^* \dot{q}^* - q^* \dot{p}^*) t^{4/3}. 
$$
By (\ref{chap3:eq3.1.31}), $p\dot{q} - q \dot{p} = 0 (t^{1/3})$ as $t
\to 0$ and hence we have 
\begin{equation*}
p^* \dot{q}^* - q^* \dot{p}^* = o (t^{-1}) \text{ as } t \to
0. \tag{3.2.9}\label{chap3:eq3.2.9} 
\end{equation*}
We introduce the following notation. If $f$ is a homogeneous function
of degree $m$ in the variables $q_1, \ldots, q_6$, let $f^*$ denote
the function of the variables  $q^*_1, \ldots, q^*_6$ defined by the
relation $f^* (q^*) = f(q^*)$. Then $f$ and $f^*$ are related by the
equation $f = f^* t^{2m/3}$. Since $\sigma $ is a homogeneous function
of degree 2 in $q_1, \ldots, q_6$, we have $\sigma = \sigma^*
t^{4/3}$. On the other hand, $\sigma \sim\kappa t^{4/3}$ and so we
have $\sigma^* \sim \kappa$ as $t \to 0$, that is  
\begin{equation*}
\sigma^* (t) = \kappa + o (1) \text{ as } t \to 0. 
\tag{3.2.10}\label{chap3:eq3.2.10}
\end{equation*}
From the\pageoriginale relation $\sigma = \sigma^* t^{4/3}$ we have,
by differentiation,  
\begin{equation*}
\dot{\sigma}(t) = \dot{\sigma}^* t^{4/3} + \frac{4}{3} \sigma^*
t^{1/3} , \tag{3.2.11}\label{chap3:eq3.2.11} 
\end{equation*}
Again by Theorem \ref{chap3:thm3.1.2}, $\dot{\sigma} \sim \dfrac{4}{3}
\kappa t^{1/3}$ and (\ref{chap3:eq3.2.10}) and (\ref{chap3:eq3.2.11})
imply that $\dot{\sigma}^* t^{4/3} = o (t^{1/3})$ as $t \to 0$, or 
\begin{equation*}
\dot{\sigma}^* (t) = o (t^{-1}) \text{ as } t \to
0. \tag{3.2.12}\label{chap3:eq3.2.12} 
\end{equation*}
Since $q$ is regular analytic in $0 < t \leq \tau$, $q^*$ is also
regular analytic in this interval and on differentiating $\sigma^*$,
as we may, we have 
\begin{equation*}
\frac{1}{2} \dot{\sigma}^* = \sum\limits_q mq^*
\dot{q}^*. \tag{3.2.13}\label{chap3:eq3.2.13} 
\end{equation*}
The estimate $q^* = 0(1)$ together with (\ref{chap3:eq3.2.9}) and
(\ref{chap3:eq3.2.13}) implies  
\begin{align*}
\frac{1}{2} \dot{\sigma}^* p^* - \dot{p}^* \sigma^* & =  \sum_q m (q^*
\dot{q}^* p^* - \dot{p}^* q^{*^2})\\ 
& =  \sum\limits_q m q^* (\dot{q}^* p^* - \dot{p}^* q^*) = o (t^{-1}) \text{ as } t \to 0. 
\end{align*}
Using once again $p^* = 0(1)$, $\dot{\sigma}^* = 0 (t^{-1})$, the last
formula gives $\dot{p}^* \sigma^* = o(t^{-1})$ as $t \to 0$, from
which it follows, by (\ref{chap3:eq3.2.10}), that $$ 
\dot{p}^* = o (t^{-1}) \text{ as } t \to 0. 
$$

We have seen that $q^* = qt^{-2/3} = 0 (1)$ as $t \to 0$ and we want
to determine the exact behaviour of $q^* (t)$ as $t \to 0$. For this
purpose we consider the triangle determined by the points $(x^*_k,
y^*_k)$, $k = 1,2,3$, which\pageoriginale will be referred to
hereafter as the ``big triangle''. We observe that the centre of
gravity of the system with respect to the $*$-coordinates also remains
fixed at the origin: in fact,  
$$
\sum\limits^3_{k=1} m_k x^*_k = t^{-2/3} \sum\limits^{3}_{k=1} m_k x_k
= 0, \; \sum\limits^3_{k=1} m_k y_k^* = t^{-2/3} \sum\limits^{3}_{k=1}
m_k y_k = 0. 
 $$
All the coordinates $q^*$ are bounded as $t \to 0$ and we expect that
$q^*$ will have finite limit values as $t \to 0$, so that the big
triangle has a limiting position as $t \to 0$. This will be proved
only at the end. At present we have the following 

\begin{subtheorem}\label{chap3:thm3.2.2}
Let the  centre of gravity remain fixed at the origin and let there be
a general collision at $t = 0$. Then the figure of the big triangle
has a limiting configuration as $t \to 0$, and this limiting
configuration is either an equilateral triangle or a set of three
collinear points. 
\end{subtheorem}

\begin{proof}
  We shall, first of all, write down the equations of motion
  $m\ddot{q} = U_q$, $q = x_k$, $y_k$, $k = 1,2,3$, in terms of the
  variables  $q^*$. By definition we have $U = \sum\limits_{1 \leq k <
    l \leq 3} m_k m_l r^{-1}_{kl}$, and $r_{kl}$ being a homogeneous
  function of degree 1 in $q_k$, $U$ is a homogeneous function of
  degree $-1$ in the six variables $x_k, y_k, k = 1,2,3$. Then $U_q$
  is a homogeneous function of degree $-2$. Using the notation
  introduced earliear, $U_q = U^*_{q^*} (q^*)
  t^{-4/3}$. Differentiating $q = q^* t^{2/3}$ with respect to $t$, we
  obtain 
\begin{align*}
\ddot{q} & = \ddot{q}^* t^{2/3} + \frac{4}{3} \dot{q}^* t^{-1/3} -
\frac{2}{9} q^* t^{-4/3} \\ 
& = (\dot{q} t^{4/3})^. t^{-2/3} - \frac{2}{9} q^* t^{-4/3}
\tag{3.2.14}\label{chap3:eq3.2.14} 
\end{align*}\pageoriginale
The equations of motion now become
$$
(\dot{q}^* t^{4/3})^. t^{-2/3} - \frac{2}{9} q^* t^{-4/3} = \ddot{q} =
\frac{1}{m} U_q = \frac{1}{m} U^*_{q^*} t^{-4/3}, 
$$
or
\begin{equation*}
-\frac{2}{9} q^* + (\dot{q}^* t^{4/3})^. t^{2/3} = \frac{1}{m}
U^{*}_{q^*}. \tag{3.2.15}\label{chap3:eq3.2.15} 
\end{equation*}
We shall replace each term in (\ref{chap3:eq3.2.15}) by its average
over the  interval $(t, 2t)$, $0 < t < 2t \leq \tau$. We shall first
prove that  
\begin{equation*}
\frac{1}{t} \int\limits^{2t}_t (\dot{q}^* t^{4/3})^. t^{2/3} dt = o(1)
\text{ as } t \to 0.  
\tag{3.2.16}\label{chap3:eq3.2.16}
\end{equation*}
In fact, integrating the left side by parts, we obtain 
\begin{align*}
\frac{1}{t} \int\limits^{2t}_t (\dot{q}^* t^{4/3})^. t^{2/3}  dt & =
\frac{1}{t} \left(\left[\dot{q}^* t^{4/3} \cdot t^{2/3} \right]^{2t}_t
- \frac{2}{3} \int\limits^{2t}_t \dot{q}^* t^{4/3} \cdot t^{-1/3} dt
\right)\\ 
& = \frac{1}{t} (o(t^{-1}) t^2 - o (t)),
\end{align*}
as $t \to 0$, since $\dot{q}^* = o (t^{-1})$ as $t \to 0$, and so the
right side is $0(1)$ as $t \to 0$, which proves
(\ref{chap3:eq3.2.16}). Next, we consider the two terms in
(\ref{chap3:eq3.2.15}). If $t_1$ and $t_2$ are real numbers such that
$t \leq t_1 < t_2 \leq 2t$, then we have  
$$
q^* (t_2) - q^* (t_1) = \int\limits^{t_2}_{t_1} (q^*)^. dt. 
$$
Since $\dot{q}^* = o (t^{-1})$ as $t \to 0$ and $0 < t_2 - t_1 \leq
t$, it follows that the right\pageoriginale side is $0(1)$   and
therefore we see that  
\begin{equation*}
q^* (t_2) = q^* (t_1) + o(t) \text{~ as ~} t \to 0
\tag{3.2.17}\label{chap3:eq3.2.17} 
\end{equation*}
We also have 
$$
U^*_{q_*} (t_2) - U^*_{q^*} (t_1) = \int\limits^{t_2}_{t_1} (U^*_{q^*})^. dt.  
$$
But $(U^*_{q^*})^. = \sum\limits_{p^*} U^*_{q^* p^*} \dot{p}^*$, where
$p^*$ denotes any one of $x^*_k$, $y^*_k$, $k = 1,2,3$. Once again,
since $U$ is homogeneous of degree $-1$ in $q$, $U = U^* t^{-2/3}$. By
(\ref{chap3:eq3.1.36}), $U \sim \dfrac{2}{9} \kappa t^{-2/3}$ as $t
\to 0$ and so we have  
$$
U = \frac{2}{9} \kappa t^{-2/3} (1+ \delta (t)), \delta (t) \to 0
\text{ as } t \to 0.  
$$
From this it follows that $U^* = \dfrac{2}{9} \kappa (1+ \delta (t))$,
that is, 
\begin{equation*}
U^* = 0(1) \text{ as } t \to 0. \tag{3.2.18}\label{chap3:eq3.2.18}
\end{equation*}
If we denote by $r^*_{kl} (t)$ the sides of the big triangle, then 
$$
r^*_{kl} (t)^2 = (x^*_k - x^*_l)^2 + (y^*_k - y^*_l)^2,
$$
and we deduce from (\ref{chap3:eq3.2.18}) and the definition of $U^*$
the estimate 
\begin{equation*}
(r^*_{kl})^{-1} = 0 (1) \text{ as } t \to
  0. \tag{3.2.19}\label{chap3:eq3.2.19} 
\end{equation*}
Since $U^* = \sum\limits_{1 \leq k < l \leq 3} m_k m_l r^{*-1}_{kl}$,
on differentiation with respect to $p^*$ and $q^*$ in succession, and
then using (\ref{chap3:eq3.2.19}), we get 
$$
U^*_{p^*q^*} = 0 (1) \text{ as } t \to 0. 
$$
Since\pageoriginale $(p^*)^. = o (t^{-1})$ as $t \to 0$, we conclude
that $(U^*_{q^*})^. = o (t^{-1})$, so that we have as before  
$$
U^*_{q^*} (t_2) = U^*_{q^*} (t_1) = \int\limits^{t_2}_{t_1} (U^*_{q^*})^. dt. 
$$
This implies that 
\begin{equation*}
U^*_{q^*} (t_2) = U^*_{q^*} (t_1) + o(1) \text{ as } t \to
0. \tag{3.2.20} \label{chap3:eq3.2.20} 
\end{equation*}
We take the average of (\ref{chap3:eq3.2.15}) over the interval $(t,
2t)$, and using\break (\ref{chap3:eq3.2.16}), (\ref{chap3:eq3.2.17})
and (\ref{chap3:eq3.2.20}) we obtain, for every fixed $t_1$ in $t \leq
t _1 \leq 2t$, 
$$
-\frac{2}{9} q^*(t_1) + o(1) = \frac{1}{m} U^*_{q^*} (t_1) + o(1)
\text{~ as ~} t \to 0. 
$$
Hence, for $t$ sufficiently near 0 we have
\begin{equation*}
-\frac{2}{9} q^* (t) = \frac{1}{m} U^*_{q^*} (t) + o(1) \text{ as } t
\to 0. \tag{3.2.21}\label{chap3:eq3.2.21} 
\end{equation*}
This is no longer a system of differential equations, but a system of
six algebraic equations satisfied asymptotically by the six
coordinates $q^*(t) = x^*_k(t)$, $y^*_k(t)$, $k = 1,2,3$. The system
(\ref{chap3:eq3.2.21}) can now be used to determine the behaviour of
$q^*$ more closely as $t \to 0$.  

We observe that the system (\ref{chap3:eq3.2.21}) is left invariant by
an orthogonal transformation of the variables $x^*_y, y^*_k, k =
1,2,3$. As in the case of the proof of Theorem \ref{chap3:thm3.2.1}
this can be verified by a direct computation of $U^*_{q^*}$, in terms
of the new variables. An orthogonal transformation corresponds to a
rotaion of the axes (in the plane of the $*$-coodinates). We apply an
orthogonal transformation (depending on $t$) in the\pageoriginale
plane of motion and assume that the new $x^*$-axis is parallel to the
direction of the vector $P_3 P_1$  (at time $t$). Let $X_k = X_k (t)$,
$Y_k = Y_k (t)$ be the new coordinates of the points $P_k, k = 1,2,3$,
at time $t$. Then $Y_1 =  Y_3$ by assumption. Writing down the
equations (\ref{chap3:eq3.2.21}) for the coordinates $Y_1, Y_2, Y_3$,
we have  
\begin{align*}
- \frac{2}{9} Y_1 & = \frac{m_2}{R^3_{12}} (Y_2 - Y_1) +
- \frac{m_3}{R^3_{13}} (Y_3 - Y_1) + o(1),\\ 
- \frac{2}{9} Y_2 & = \frac{m_1}{R^3_{12}} (Y_1 - Y_2) +
- \frac{m_3}{R^3_{23}} (Y_3 - Y_2) + o(1),\\ 
- \frac{2}{9} Y_3 & = \frac{m_1}{R^3_{13}} (Y_1 - Y_3) +
- \frac{m_2}{R^3_{23}} (Y_2 - Y_3) + o(1), 
\end{align*}
as  $t \to 0$, where $R_{kl}$ denotes $r^*_{kl}(t)$, which is clearly
left invariant by the orthogonal transformation. One can also write
down similar algebraic equations for $X_1, X_2, X_3 $. Since $Y_1 =
Y_3$ the preceding equations become 
\begin{align*}
- \frac{2}{9} Y_1 & = \frac{m_2 (Y_2 - Y_1)}{R^3_{12}} + o(1), \; -
\frac{2}{9} Y_2 = \left(\frac{m_1}{R^3_{12}} + \frac{m_3}{R^3_{23}}
\right) (Y_1 - Y_2) + o(1), \\ 
& \qquad - \frac{2}{9} Y_3  = \frac{m_2 (Y_2 - Y_1)}{R^3_{23}} + o
(1), \tag{3.2.22}\label{chap3:eq3.2.22} 
\end{align*}
as $t \to 0$. It is not immediate that $Y_1, Y_2, Y_3$ have limit
values as $t \to 0$. Since $|Y_k - Y_l|R^{-1}_{kl} \leq 1$ and
$R^{-2}_{kl} = (r^*_{kl})^{-2} = 0(1)$ as $t \to 0$ by
(\ref{chap3:eq3.2.19}), it follows from (\ref{chap3:eq3.2.22}) that
$Y_1(t)$, $Y_2(t)$, $Y_3(t)$ are all $0(1)$  as $t \to 0$. Hence, by
the Weierstrass theorem, we can find a sequence of values of $t$ which
tends to zero such that the corresponding\pageoriginale sequence of
values of $Y_k(t)$ converges to a finite limit value as $t \to 0$
through this sequence. We shall see later on that these limit values
of $Y_k(t)$ are independent of the sequence of values of $t$
chosen. We denote also the limit values of the coordinates $X_1(t),
\ldots, Y_3(t)$ by $X_1, \ldots, Y_3$ respectively, the limit values
of $R_{kl}(t)$ by $R_{kl}$, $k \neq l$, and we can then omit the error
term $o(1)$ in (\ref{chap3:eq3.2.22}). Once again, since $Y_1 = Y_3$,
we get from (\ref{chap3:eq3.2.22}), 
$$
(Y_2 - Y_1) \; (R^{-3}_{12} - R^{-3}_{23}) = 0, 
$$
which means that either $Y_2 - Y_1 = 0$ or $R^{-3}_{12} - R^{-3}_{23}
=0$. 

Suppose for the moment that $R^{-3}_{12} - R^{-3}_{23} \neq 0$. Then
$Y_1 = Y_2$ and hence $Y_1 = Y_2 = Y_3$. Since the centre of gravity
remains fixed at the origin, $m_1 Y_1  + m_2 Y_2 + m_3 Y_3 = 0 $ and
so it follows that $Y_1 = Y_2 = Y_3 = 0$, which means that the three
points represented by $(X_k (t), Y_k(t))$, $k = 1,2,3$, tend to points
situated on a straight line, as $t \to 0$. 

Suppose on the other hand that $Y_1 \neq Y_2$; then necesarily $R_{12}
= R_{23}$. If the three points are not collinear in the limiting
position, then one can interchange $P_1, P_2, P_3$ (which means on
orthogonal transformation with matrix independent of $t$) and repeat
this argument and get $R_{23} = R_{13}$.  

Hence, only two possibilities can occur, namely, either the three
points represented by $(X_k, Y_k)$, $k = 1,2,3$, are collinear, or
they lie at the vertices of an equilateral triangle, as $t \to 0$. We
shall\pageoriginale refer to these alternatives as the {\em collinear
  case} and the {\em equilateral case} respectively. This is
equivalent to saying that either all the angles at the vertices tend
to $\dfrac{\pi}{3}$, or two of the angles tend to 0 and the third to
$\pi$, as $t \to 0$. This argument involves the choice of a sequence
of values of $t$ such that the corresponding $Y_k(t)$, $k = 1,2,3$,
tend to finite limits. If we consider another sequence of values of
$t$ tending to 0 such that the corresponding sequences of values of
$Y_k(t)$ also converge to finite limits, then it may happen that the
above alternatives get interchanged. That is to say, the points
represented by $(X_k(t), Y_k (t))$ may tend to the vertices of an
equilateral triangle as $t \to 0$ through one sequence of values,
while they way tend to collinear points through another sequence. But,
since the angles of the triangle are determined by the sides
$r^*_{kl}(t)$ which are continuous functions of $t$ in $0 < t \leq
\tau$, and are bounded away from zero as $t \to 0$, we see that the
angles are also continuous function of $t$ in $0 <t\leq \tau$. Hence
the above possibility canont happen and we conclude that the limiting
positions of the points in the plane determined by $(x^*_k, y^*_k)$,
$k = 1,2,3$, are either at the vertices of an equilateral triangle or
in three collinear points.  This completes the proof of Theorem
\ref{chap3:thm3.2.2}. 
\end{proof}

\begin{subtheorem}\label{chap3:thm3.2.3}
If the limiting configuration of the big triangle is an equilateral
triangle, then the side of the triangle is given by the positive cube
root of  
\begin{equation*}
r^3 = \frac{9}{2} (m_1 + m_2 + m_3). \tag{3.2.23}\label{chap3:eq3.2.23}
\end{equation*}
\end{subtheorem}

\begin{proof}
If the\pageoriginale sides of the big triangle are $R_{kl} (t)$, $k
\neq l$, then $R_{kl}(t) \to R_{kl} = r$, $k \neq l$, $k,l =
1,2,3$. Once again denoting by $X_k$ the limiting values of $X_k(t)$
as $t \to 0$, we have from (\ref{chap3:eq3.2.21}) the following
algebraic equation satisfied by $X_1, X_2, X_3$: 
\begin{align*}
-\frac{2}{9} X_1 &= \frac{m_2}{r^3} (X_2 - X_1) + \frac{m_3}{r^3} (X_3 
 - - X_1),\\
- \frac{2}{9} X_2 &= \frac{m_1}{r^3} (X_1 - X_2) + \frac{m_3}{r^3}
(x_3 - X_2), \\ 
- \frac{2}{9} X_3 &= \frac{m_1}{r^3} (X_1 - X_3) + \frac{m_2}{r^3}
(X_2 - X_3).  
\end{align*}
Since $m_1 X_1 + m_2 X_2 + m_3 X_3 = 0$, we obtain from these equations
$$
-\frac{2}{9} X_k =  -(m_1 + m_2 + m_3) X_k r^{-3},  \; k = 1,2,3.
$$
Similarly we have for $Y_k$,
$$
-\frac{2}{9} Y_k = - (m_1 + m_2 + m_3) Y_k r^{-3}, \; k = 1,2,3. 
$$
Since by (\ref{chap3:eq3.2.19}), $(R_{kl} (t))^{-1} = (r^*_{kl}
(t))^{-1} = 0(1)$ as $t \to 0$, it follows that $R_{kl} (t)$ is
bounded away from zero. Thus at least one of the $X_k, Y_k$,
$k=1,2,3$, is different from zero and we obtain 
$$
r^3 = \frac{9}{2} (m_1 + m_2 + m_3), 
$$
which proves the assertion.

Theorem \ref{chap3:thm3.2.3} can also be used to determine explicitly
the constant $\kappa > 0$ in the asymptotic estimates for $q^* (t)$,
$\dot{q}^*(t), \ldots $ in the equilateral case. Since $U \sim
\dfrac{2}{9} \kappa t^{-2/3}$ as $t \to 0$ and $U^* (q^*) t^{-2/3} =
U(q)$, we have  
\begin{equation*}
U^* (q^*) \sim\frac{2}{9} \kappa \text{ as } t \to 0.\tag{3.2.24}\label{chap3:eq3.2.24}
\end{equation*}\pageoriginale
The distances $R_{kl} (t)$ are invariant under orthogonal
transformations an since $R_{12} = R_{23} = R_{13} = r$, it follows
that as $t \to 0$, 
\begin{equation*}
U^* \to \frac{m_1 m_2 + m_2 m_3 + m_1 m_3}{r}\tag{3.2.25}\label{chap3:eq3.2.25}
\end{equation*}
(\ref{chap3:eq3.2.24}) and (\ref{chap3:eq3.2.25}) together imply that
$$
\frac{2}{9} \kappa = (m_1 m_2 + m_2 m_3 + m_1 m_3)r^{-1}, 
$$
which determines $\kappa$ explicitly in terms of the masses.

Next we consider the collinear case. Then the limiting distances
$R_{12}, R_{23},R_{31}$ are no longer equal. Let $\rho = \max (R_{12},
R_{23}, R_{31})$. Suppose that $P_1$ and $P_3$ are at the distance
$\rho$ at $t = 0$: $\rho^2 = (X_3 - X_1)^2 + (Y_3 - Y_1)^2$. $P_2$
lies between $P_1$ and $P_3$; let $R_{23} = \omega \rho$ where $0 <
\omega <1$. Then $R_{12} = (1-\omega)\rho$. So 
\begin{equation*}
R_{31} = \rho, R_{23} = \omega \rho, \; R_{12} = (1-\omega)
\rho. \tag{3.2.26}\label{chap3:eq3.2.26} 
\end{equation*}
Once again we make use of the equations (\ref{chap3:eq3.2.21}). Since
the centre of gravity remains fixed at the origin, it follows that the
equations satisfied by $X_1, X_2, X_3$ are not linearly
independent. We obtain as in Theorem \ref{chap3:thm3.2.3}, 
\begin{align*}
& \frac{2}{9} (X_1- X_3) = \frac{2}{9}\rho = \frac{m_1}{\rho^2} +
  \frac{m_2}{\omega^2 \rho^2} + \frac{m_2}{(1-\omega)^2 \rho^2} +
  \frac{m_3}{\rho^2}, \\ 
& \frac{2}{9} (X_2 - X_3) = \frac{2}{9} \omega \rho =
  \frac{m_1}{\rho^2} - \frac{m_1}{(1-\omega)^2 \rho^2} +
  \frac{m_2}{\omega^2 \rho^2} + \frac{m_3}{\omega^2 \rho^2}
  \tag{3.2.27}\label{chap3:eq3.2.27}\\ 
\text{or } \qquad & 
\begin{cases}
\dfrac{2}{9} \rho^3 = m_1 + m_2 (\omega^{-2} + (1-\omega)^2 ) + m_3\\[5pt]
\dfrac{2}{9} \omega \rho^3 = m_1 (1-(1-\omega)^{-2}) + m_2 \omega^{-2}
+ m_3 \omega^{-2}. \tag{3.2.28}\label{chap3:eq3.2.28} 
\end{cases}
\end{align*}\pageoriginale
Eliminating $\rho$ between (\ref{chap3:eq3.2.27}) and (\ref{chap3:eq3.2.28}) we get
$$
m_1 (1 -\omega - (1-\omega)^{-2}) + m_2 ((1-\omega)\omega^{-2} -
(1-\omega)^{-2} \omega) + m_2 (\omega^{-2} - \omega) = 0.  
$$
Hence $\omega$ satisfies the equation
\begin{equation*}
m_1 ((1-\omega)^3 - 1) \omega^2 + m_2 ((1-\omega)^3 - \omega^3) + m_2
((1-\omega)^2 - \omega^3 (1-\omega)^2) = 0,
\tag{3.2.29}\label{chap3:eq3.2.29} 
\end{equation*}
which is an algebraic equation of the fifth degree. This equation has
only one root $\omega$ in $0 < \omega <1$. This can be seen as
follows: we can write (\ref{chap3:eq3.2.29}) in the form 
\begin{equation*}
\frac{m_1 + m_2 \omega}{m_1 + m_2 \omega^{-2}} = \frac{m_3 + m_2
  (1-\omega)}{m_3 + m_2
  (1-\omega)^{-2}}. \tag{3.2.30}\label{chap3:eq3.2.30} 
\end{equation*}
Both sides of (\ref{chap3:eq3.2.30}) are continuous functions of
$\omega$ in $0 < \omega <1$. As $\omega$ increases from 0 to 1, the
left side of (\ref{chap3:eq3.2.30}) increases from 0 to 1, while the
right side decreases from 1 to 0. Hence there exists just one real
number $\omega$  in $0 < \omega <1$ satisfying
(\ref{chap3:eq3.2.30}). This unique root is completely determined by
the masses $m_1, m_2, m_3$. Substituting this value of $\omega$ in
(\ref{chap3:eq3.2.27}) we obtain $\rho$ as the positive cube root and
hence we obtain also the constant $\kappa$ explicitly in the
asymptotic relation $U^* \sim \dfrac{2}{9} \kappa$ as $t \to 0$, since  
$$
U^* \to \frac{m_1 m_2}{(1-\omega) \rho} + \frac{m_2 m_3}{\omega \rho}
+ \frac{m_1 m_3}{\rho}.  
$$\pageoriginale
This corresponds to the case in which $P_2$ lies between $P_1$ and
$P_3$. The other two possibilities are obtained by cyclic permutation
of (1,2,3). 

Thus, if there is a general collision, we get as the limiting
configuration either an equilateral triangle or three collinear
points. It is a remarkable fact that the fifth degree equation
(\ref{chap3:eq3.2.29}) for $\omega$ already appears in the work of
Euler (1767). Euler considered the one-dimensional problem in which
all the points are collinear and studied the particular solution
giving a general collision in this case. 
\end{proof}

\section{A particular solution}\label{chap3:sec3}

While we have proved that the limiting configuration of the big
triangle is either an equilateral triangle or a set of three collinear
points, we have not as yet proved that the triangle itself has a {\em
  limiting position} relative to the old fixed coordinate system. We
do not also know yet whether the two limiting possibilities can
actually be realized while remaining in a fixed coordinate system. We
shall now show that the two cases, the equilateral case and the
collinear case, do in fact occur. This will be done by giving an
explicit particular solution of the three-body problem near $t = 0$,
the time of a general collision. 

We consider\pageoriginale  the case in which
\begin{equation*}
q(t) = q^*  \cdot g(t) , \; q = x_k , y_k, \; k = 1,2,3,
\tag{3.3.1}\label{chap3:eq3.3.1} 
\end{equation*}
where the $q^*$ are unknown constants, not all zero, and $g(t)$ is an
unknown twice continuously differentiable function of $t$ in the
interval $0 < t \leq \tau$, and since $q(t)$ should tend to zero as $t
\to 0$, we assume that $g(t) \to 0$ as $t \to 0$. Then the
differential equations of motion take the form  
\begin{equation*}
m q^* \ddot{g} = U^*_{q^*}  g^{-2}, \text{ or, } m q^* \ddot{g} g^2 =
U^*_{q^*}, \tag{3.3.2}\label{chap3:eq3.3.2} 
\end{equation*}
where $U^* (q^*) = U(q^*)$, so that by the homogeneity of $U$, we have
$U_q = U^*_{q^*} g^{-2}$. Since $q^*$ are constants, not all zero, the
right sides in (\ref{chap3:eq3.3.2}) are constants and hence 
$$
\ddot{g} g^2 = \frac{1}{mq^*} U^*_{q^*}
$$
is a constant (we take a $q^* \neq 0$). This constant cannot be zero
as otherwise $U^*_{q^*} = 0$ for all coordinates $q$ and from the
relation $\sum\limits_{q^*} U^*_{q^*} q^* = - U^*$, it would follow
that $U^*$ is the constant 0 and this is not the case, by definition
of $U^*$. In view of the considerations of the last section, we take
this constant to be $-\dfrac{2}{9}$ so that we have 
\begin{equation*}
\ddot{g} g^2 = - \frac{2}{9}, \tag{3.3.3}\label{chap3:eq3.3.3}
\end{equation*}
and hence the system of equations (\ref{chap3:eq3.3.2}) becomes
$$
-\frac{2}{9} q^* = \frac{1}{m} U^*_{q^*},
$$\pageoriginale
which is of the same form as the system of algebraic equations
(\ref{chap3:eq3.2.21}) satisfied by the variables $q^*$ in
\S\ \ref{chap3:sec2}. Since the equations of motion are invariant
under an orthogonal transformation in the plane of the motion, we can
take the $x$-axis to be parallel to the direction of the vector
determined by the points $(x^*_3, y^*_3)$ and $(x^*_1, y^*_1)$ and
passing through the centre of gravity, which is assumed fixed at the
origin.  Hence in the new coordinate system $Y_1 = Y_3$ and one can
show, using the argument given earlier, that the points defined by
$(x^*_k, y^*_k)$, $k =1,2,3$, are either collinear or at the vertices
of an equilateral triangle. Hence in order to prove that the two
limiting possibilities occur, we shall determine the function $g(t)$
explicitly. 

First of all, we observe that the function $g$ cannot vanish anywhere
in the interval $0< t \leq \tau$ by (\ref{chap3:eq3.3.3}). Integrating
the equation $2 \dot{g} \ddot{g} = - \dfrac{4}{9} \dot{g} g^{-2}$ we
obtain 
$$
\dot{g}^2 = \frac{4}{9} \left(\frac{1}{g} + C \right), 
$$
where $C$ is a constant of integration. From this we get
$$
\frac{3}{2} \frac{\dot{g}}{\sqrt{C+ g^{-1}}}  = 1, \text{ or, }
\frac{3}{2} \frac{\dot{g} \sqrt{g}}{\sqrt{1+ Cg}} = 1,  
$$
and this, on integration from 0 to $g$ using the fact that $g(t) \to
0$ as $t \to 0$, gives 
$$
\frac{3}{2} \int\limits^g_0 \frac{\sqrt{g}}{\sqrt{1+ Cg}} dg = t.
$$
Once\pageoriginale again, as $g(t) \to 0$, it follows that $1+ Cg \to
1$ and hence, for $t$ sufficiently small in $0 < t \leq \tau$, $Cg(t)$
is also small. Then we can expand $(1+ Cg)^{\frac{1}{2}}$ as a
power-series in $g(t)$ (with parameter $C$) which converges for
sufficiently small $t$ and we have  

$\dfrac{3}{2} \int\limits^g_0 \sqrt{g}$ (1 + power-series in $g$
without constant term) $dg = t$, that is, since $g(t) \to 0$ as $t \to
0 $, 

$g^{3/2} $ (1 + power-series in $g$ without constant term) $ = t$.  
Hence by inversion we have
\begin{equation*}
g = t^{2/3} +  \text{  power-series in  } t^{2/3} \text{   with term
  of degree } \geq 2, \tag{3.3.4}\label{chap3:eq3.3.4} 
\end{equation*}
and the power-series converges for $t$ sufficiently small. The
integration above could also be carried out by using trigonometric or
hyperbolic functions (according as $C$ is negative or positive) and we
could obtain $g(t)$. 

The simplest solution for $g(t)$ is the one corresponding to $C=0$. In
this case $\dfrac{3}{2} \sqrt{g} \dot{g} =1$ and on integration, $g(t)
= t^{2/3}$ and hence we have $q = q^* t^{2/3}$. This proves that both
the alternatives can occur in this special case. We have already
mentioned that the one-dimensional problem in which the three points
situated on a straight line have a general collision on the line was
treated by Euler in 1767. 

We now proceed to determine the constants $q^*$ of
(\ref{chap3:eq3.3.1}) in some cases. We have assumed throughout that
the centre of gravity remains fixed at the origin and so
$\sum\limits^3_{k=1} m_k x^*_k = 0 = \sum\limits^3_{k=1} m_k
y^*_k$. We apply an\pageoriginale orthogonal transformation in the
plane of motion and choose the $X$-axis to be parallel to the
direction of the vector defined by $(x^*_3, y^*_3)$ and $(x^*_1,
y^*_1)$ and passing through the centre of gravity. Hence in the new
coordinates $X,Y$, we have $Y_1 = Y_3$ and $X_1 \geq X_3$, where
$(X_k, Y_k)$ are the new coordinate of the points $(x^*_k, y^*_k)$, $k
=1,2,3$. First consider the equilateral case. Then we have proved that
the side of the equilateral triangle is given by $\dfrac{2}{9} r^3 =
m_1 + m_2 + m_3$. Let $m = m_1 + m_2 + m_3$. Since the centre of
gravity is at the origin, 
\begin{equation*}
\sum\limits^3_{k=1} m_k X_k  = 0, \; \sum\limits^3_{k=1} m_k Y_k =
0. \tag{3.3.5}\label{chap3:eq3.3.5} 
\end{equation*}
Since the triangle is equilateral, we have
\begin{equation*}
X_1 - X_3 = r, \; X_2 - X_3 = \frac{1}{2} r, \; Y_1 - Y_3 = 0, \; Y_2
- Y_3 = \frac{r}{2} \sqrt{3}. \tag{3.3.6}\label{chap3:eq3.3.6} 
\end{equation*}
Using (\ref{chap3:eq3.3.5}) we get
{\fontsize{10}{12}\selectfont
\begin{align*}
& \sum\limits^3_{k=1} m_k X_k = m_1 (X_1 - X_3) + m_2 (X_2 - X_3) + m
  X_3 = m_1 r + \frac{1}{2} m_2 r + m X_3 = 0,\\ 
& \sum\limits^3_{k=1} m_k Y_k = mY_1 - (m_2+ m_3) Y_1 + m_2 Y_2 + m_3
  Y_3 = m Y_1 + \frac{1}{2} m_2 r \sqrt{3} = 0.  
\end{align*}}
From these and (\ref{chap3:eq3.3.6}) we obtain
\begin{align*}
X_3 & = - \frac{m_1 + \frac{1}{2} m_2}{m} r, \; X_1 =
\frac{\frac{1}{2} m_2 + m_3}{m_1} r, \; X_2 = \frac{1}{2} \frac{m_3 -
  m_1}{m} r, \tag{3.3.7}\label{chap3:eq3.3.7}\\ 
Y_1 & = Y_3 = - \frac{1}{2} \frac{m_2}{m} r \sqrt{3}, \; Y_2 =
\frac{1}{2} \frac{m_1 + m_3}{m}
r\sqrt{3}. \tag{3.3.8}\label{chap3:eq3.3.8} 
\end{align*}
Thus,\pageoriginale if $g(t) = t^{2/3}$, the original coordinates are
given by 
\begin{equation*}
X_k t^{2/3} , \; Y_k t^{2/3}, \; k =
1,2,3. \tag{3.3.9}\label{chap3:eq3.3.9} 
\end{equation*}
Differentiating with respect to $t$ we get
\begin{equation*}
(X_2 t^{2/3})^. = \frac{1}{3} \frac{m_3 - m_1}{m} rt^{-1/3}, (Y_2
  t^{2/3})^. =\frac{1}{\sqrt{3}}  \frac{m_1 + m_3}{3} rt^{-1/3}
  . \tag{3.3.10}\label{chap3:eq3.3.10} 
\end{equation*}

In the collinear case, we have $Y_1 = Y_2 = Y_3$ and since
$\sum\limits^3_{k=1} m_k X_k =0$ and $X_1 - X_3 = \rho$, $X_2 - X_3 =
\omega \rho$, we get $m_1 \rho + m_2 \omega \rho + m X_3 = 0$, so that  
\begin{equation*}
X_3 = -\frac{m_1 + m_2 \omega}{m} \rho, \; X_1 = \frac{m_2 (1-\omega)
  + m_3}{m} \rho, \; X_2 = \frac{m_3 \omega - m_1 (1-\omega)}{m} \rho
. \tag{3.3.11}\label{chap3:eq3.3.11} 
\end{equation*}
The originale coordinates in this case are 
$$
X_k t^{2/3}, \; Y_k t^{2/3} = 0, \; k = 1,2,3.
$$

\section{Reduction to a rotating coordinate system}\label{chap3:sec4}

We shall now go back to the general problem of collision. We have
already exhibited particular solutions which involve one parameter
(name\-ly, the constant $C$) to show that both the alternatives can be
realized for the limiting configuration of the big triangle. The
solutions 
$$
q = q^* g = q^* t^{2/3} + \ldots, q^* = X_k, Y_k, K = 1,2,3,
$$
suggests  that in the general case we may expect to get for $q = q(t)$
power-series in the variable $t^{1/3}$ starting with the term
$t^{2/3}$. However,\pageoriginale this is not true; the general
solution involves many more parameters. The difficulty of the problem
consists in the fact that we cannot yet prove (this will be proved
only at the end) that the big triangle referred to a {\em fixed}
coordinate system has a limiting position as $t \to 0$; all that we
have proved so far is the existence of a limiting configuration
relative to a rotating coordinate system. The triangle itself may go
on rotating above its centre of gravity, assumed fixed at the origin,
because in our proof we have made use of an orthogonal transformation
in the plane of motion dependeing on the time variable $t$. We cannot
yet determine the limiting position of the big triangle. In order to
study this problem more closely we proceed as follows. 

We use a fixed coordinate system relative to the {\em initial}
position of the big triangle.  We shall first reduce the system of
differential equations of motion to one containing a smaller number of
equations. The idea is to introduce relative coordinates of $P_1$ and
$P_2$ with respect to $P_3$ as we did in the case of simple collisions
in Chapter \ref{chap2} and to make use of the general theory of
transformations. 

Let $(x_k , y_k )$, $k =1,2,3,$ be the coordinates of $P_k$ at time
$t$ with respect to the fixed coordinate system through the
origin. Let the relative coordinates of $P_1$ and $P_2$ with respect
to $P_3$ be $(\xi_1, \xi_2)$ and $(\xi_3, \xi_4)$ respectively; that
is,  
\begin{equation*}
\xi_1 = x_1 - x_3, \; \xi_2  = y_1 - y_3, \; \xi_3 = x_2 - x_3, \; \xi_4 = y_2 - y_3. 
\tag{3.4.1}\label{chap3:eq3.4.1}
\end{equation*}
Since\pageoriginale the centre of gravity remains fixed at the origin,
we have $m_1 \xi_1 + m_2 \xi_3 + mX_3 = 0$, $m_1 \xi_2 + m_2 \xi_4 +
mY_3 = 0$ where $m = m_1 + m_2 + m_3$. Then we have 
\begin{align*}
x_3 &= - \frac{m_1}{m} \xi_1 -\frac{m_2}{m} \xi_3,\\ 
y_3 &= - \frac{m_1}{m} \xi_2 -\frac{m_2}{m} \xi_4,\\
x_1 &= \xi_1 + x_3 = \frac{m_2+m_3}{m} \xi_1 - \frac{m_2}{m}
\xi_3,\\ 
x_2 &= \xi_3 + x_3  =- \frac{m_1}{m} \xi_1 + \frac{m_1 + m_3}{m} \xi_3,\\
y_1 &= \xi_2 + y_3  = \frac{m_2 + m_3}{m} \xi_2 - \frac{m_2}{m}
\xi_4,\\ 
y_2 &= \xi_4 + y_3 = - \frac{m_1}{m} \xi_2 + \frac{m_1+ m_3}{m}
\xi_4. \tag{3.4.2}\label{chap3:eq3.4.2} 
\end{align*}
If we set
\begin{equation*}
\eta_1 = m_1 \dot{x}_1, \; \eta_2 = m_1 \dot{y}_1 , \; \eta_3 = m_2
\dot{x}_2, \; \eta_4 = m_2 \dot{y}_2, \tag{3.4.3}\label{chap3:eq3.4.3} 
\end{equation*}
then, since the centre of gravity remains fixed at the origin,
\begin{align*}
m_3 \dot{x}_3 & = - m_1 \dot{x}_1 - m_2 \dot{x}_2 = - (\eta_1 + \eta_3), \\
m_3 \dot{y}_3 & = - m_1 \dot{y}_1 - m_2 \dot{y}_2 = - (\eta_2 +
\eta_4). \tag{3.4.4}\label{chap3:eq3.4.4} 
\end{align*}
Since $r^2_{13} = \xi^2_1 + \xi^2_2$, $r^2_{23} = \xi^2_3  + \xi^2_4$,
$r^2_{12} = (\xi_1 - \xi_3)^2 + (\xi_2 - \xi_4)^2$, it follows that
the potential function $U$ is now a function of the variables $\xi_1,
\ldots, \xi_4$ alone. On the other hand,  
\begin{align*}
T & = \frac{1}{2} \sum\limits^3_{k=1} m_k(\dot{x}^2_k + \dot{y}^2_k)\\
& = \frac{1}{2} \left(\frac{1}{m_1} + \frac{1}{m_3} \right) \;
\left(\eta^2_1 + \eta^2_2 \right) + \frac{1}{2} \left(\frac{1}{m_2} +
\frac{1}{m_3} \right) \left(\eta^2_3 + \eta^2_4 \right)\\ 
&\qquad + \frac{1}{m_3} \left(\eta_1 \eta_3 + \eta_2 + \eta_4 \right). 
\end{align*}
If $E$ denotes the total energy $T - U$, then the equations of motion
can be\pageoriginale written as a Hamiltonian system of eight
equations: 
\begin{equation*}
\dot{\xi}_k = E_{\eta_k}, \; \dot{\eta}_{k} = - E_{\xi_k}, k = 1,
\ldots, 4. \tag{3.4.5}\label{chap3:eq3.4.5} 
\end{equation*}
We observe that $E$ does not depend on the variable $t$ explicitly. If
there is a general collision at $t = 0$, we have already seen that
$x_k, y_k = 0(t^{2/3})$ and $\dot{x}_k , \dot{y}_k = 0 (t^{-1/3})$, $k
= 1, \ldots, 4$, as $t \to 0$. Since the $\xi_k$ are linear functions
of $x_k$ and $y_k$, and the $\eta_k$ linear functions of $\dot{x}_k$
and $\dot{y}_k$, it follows that, as $t \to 0 $,  
\begin{equation*}
\xi_k = 0 (t^{2/3}), \; \eta_k = 0 (t^{-1/3}), \; k = 1, \ldots,
4. \tag{3.4.6}\label{chap3:eq3.4.6} 
\end{equation*}
If we set $x_k = x^*_kt^{2/3}$, $y_k = y^*_k t^{2/3}$, $\xi_k =
\xi^*_k t^{2/3}$, $\eta_k = \eta^*_k t^{-1/3}$, then we see that
$x^*_k, y^*_k, \xi^*_k, \eta^*_k$ are $0(1)$ as $t \to 0$. 

Now we introduce a rotating coordinate system with origin at $P_3$ and
$x$-axis along the direction of the vector $P_3 P_1$, i.e. at any
instant $t$ we translate the origin to $P_3$ with the direction of the
$x,y$-axes in the plane of the triangle preserved. We want to consider
the limiting {\em position} of the big triangle with respect to the
fixed coordinate system. For this purpose, suppose that at time $t, 0
< t \leq \tau$, the vector $P_3 P_1$ makes an angle $p_4 = p_4 (t)$
(positively oriented) with the direction 
 of the $x$-axis in the old fixed coordinate system. The main
 difficulty is to obtain the behaviour of $p_4$ as $t \to 0$. The
 introduction of the new rotating coordinate system means a
 transformation of the variables $(\xi, \eta)$ into new variables
 which can be described as follows. Let us set  
\begin{equation*}
c = \cos p_4, \; s = \sin p_4, \tag{3.4.7}\label{chap3:eq3.4.7}
\end{equation*}\pageoriginale
and let $(p_1, 0)$, $(p_2, p_3)$ denote the coordinates of $P_1, P_2$
respectively in the new coordinate system; $P_3$ is $(0,0)$. Here $p_k
= p_k (t)$, $k=1, 2,3$. Then the relative coordinates $(\xi_1, \xi_2)$
and $(\xi_3, \xi_4)$ of $P_1$ and $P_2$ in the old system are given by 
\begin{equation*}
\xi_1 = p_1 c - o \cdot s, \; \xi_2 = p_1  s+ oc, \; \xi_3 = p_2 c -
p_3 s, \; \xi_4 = p_2 s + p_3 c.\tag{3.4.8}\label{chap3:eq3.4.8} 
\end{equation*}
The equations (\ref{chap3:eq3.4.8}) define a transformation of the
variables $\xi_1, \ldots, \xi_4$ to $p_1, \ldots, p_4$, and we claim
that this can be extended to a canonical transformation of the eight
independent variables $\xi_1, \ldots, \xi_4$, $\eta_1 , \ldots,
\eta_4$. The extension can be done by means of the generating function 
\begin{equation*}
W = \eta_1 p_1 c + \eta_2 p_1 s + \eta_3 (p_2 c - p_3 s) + \eta_4 (p_2
s + p_3 c). \tag{3.4.9}\label{chap3:eq3.4.9} 
\end{equation*}
It is clear that $W$ is linear in $\eta_k$ and $|W_{p_k \eta_l}| = p_1
\neq 0$. By our general theory (Chapter \ref{chap1},
\S\ \ref{chap1:sec2}), the full transformation is obtained by setting
$W_{\eta_k} = \xi_k$, $W_{p_k} = q_k$, $k =1, \ldots, 4$. It is clear
from the definition of $W$ that the first set of conditions is
satisfied and hence the canonical transformation thus obtained extends
(\ref{chap3:eq3.4.8}). Then we have 
\begin{align*}
q_1 & = \eta_1 c + \eta_2 s, \; q_2 = \eta_3 c + \eta_4 s, \; q_3 = -
\eta_3 s + \eta_4 c, \\ 
q_4 & = p_1 (-\eta_1 s + \eta_2 c) + p_2 (-\eta_2 s + \eta_4 c) -
p_3(\eta_3 c + \eta_4 s). \tag{3.4.10}\label{chap3:eq3.4.10} 
\end{align*}
We shall\pageoriginale introduce an auxiliary variable $q_o$ defined
by 
\begin{equation*}
q_o = -\eta_1 s + \eta_2 c, \tag{3.4.11}\label{chap3:eq3.4.11}
\end{equation*}
and then we can write 
\begin{equation*}
q_4 = p_1 q_o + p_2 q_3 - p_3 q_2. \tag{3.4.12}\label{chap3:eq3.4.12}
\end{equation*}
The last equation can be solved for $q_o$ since $p_1 \neq 0$, and we
can write $q_o = (q_4 - p_2 q_3 + p_3 q_2) p^{-1}_1$. We could also
take this as the definition of $q_o$. We can express $\eta_1, \ldots,
\eta_4$ in terms of $q_o, \ldots, q_3$, using (\ref{chap3:eq3.4.10}),
(\ref{chap3:eq3.4.11})  and (\ref{chap3:eq3.4.12}): 
\begin{equation*}
\eta_1 = q_1 c - q_o s, \; \eta_2 = q_1 s + q_o c, \; \eta_3 = q_2 c -
q_3 s, \; \eta_4 = q_2 s + q_3 c. \tag{3.4.13}\label{chap3:eq3.4.13} 
\end{equation*}
Then we have $r_{13} = (\xi^2_1 + \xi^2_2)^{1/2} = p_1$, $r_{23} =
(\xi^2_3 + \xi^2_4)^{1/2} = (p^2_2 + p^2_3)^{1/2}$, and $r_{12} =
((\xi_1 - \xi_3)^2 + (\xi_2 - \xi_4)^2)^{1/2} = ((p_1 - p_2)^2 +
p^2_3)^{1/2}$. So we can express $U$ as a function of $p_1, p_2, p_3$
alone. Also 
{\fontsize{10}{12}\selectfont
$$
T = \frac{1}{2} \left(\frac{1}{m_1} + \frac{1}{m_3} \right) \;
\left(q^2_0 + q^2_1 \right) + \frac{1}{2} \left(\frac{1}{m_2} +
\frac{1}{m_3} \right) \left(q^2_2 + q^2_3 \right) + \frac{1}{m_3}
\left(q_1 q_2 + q_o q_3 \right).  
$$}
This shows that the total energy $T - U$ expressed in terms of the new
variables $(p,q)$ does not contain $p_4$. We already know the
asymptotic behaviour of $p_1, p_2, p_3$; also $q_o , \ldots, q_3$
behave nicely. We do not know the behaviour of $p_4$, but this bad
coordinate disappears from the function $E$. 

Since $W$ does not contain the variable $t$ explicitly, we have
$E(\xi, \eta) = \mathbb{E} (p,q)$ and we denote $\mathbb{E}(p,q)$ by
$E(p,q)$ itself. 

The Hamiltonian\pageoriginale system (\ref{chap3:eq3.3.5}) now becomes 
\begin{equation*}
\dot{p}_k = E_{q_k}, \dot{q}_k = - E_{p_k}, k = 1,2,3, \dot{p}_4 =
E_{q_4} , \dot{q}_4 = 0. \tag{3.4.14}\label{chap3:eq3.4.14} 
\end{equation*}
Hence $q_4$ is a constant. We shall now prove that when there is a
general collision, this constant has necessarily to be zero. 

\setcounter{subtheorem}{0}
\begin{subtheorem}\label{chap3:thm3.4.1}
If there is a general collision at $t =0$, then $q_4(t) \equiv 0$.  
\end{subtheorem}

\begin{proof}
By Sundman's theorem (Theorem \ref{chap2:thm2.2.2}), the constants of
angular momenta $\lambda, \mu, \nu$, all vanish. In particular,
$\lambda = \sum\limits^3_{k=1} m_k (x_k \dot{y}_k - y_k \dot{x}_k) =
0$. $q_4$ is given by (\ref{chap3:eq3.4.12}). But from
(\ref{chap3:eq3.4.8}), and (\ref{chap3:eq3.4.13}) $\xi_1 \eta_2 -
\xi_2 \eta_1 = p_1 q_o$, $\xi_3 \eta_4 - \xi_4 \eta_3 = p_2 q_3 - p_3
q_2$, so that we can write 
$$
q_4 = (\xi_1 \eta_2 - \xi_2 \eta_1) + (\xi_3 \eta_4 - \xi_4 \eta_3). 
$$
Moreover, from the definitions (\ref{chap3:eq3.4.1}) and
(\ref{chap3:eq3.4.3}) of $\xi_k$, $\eta_k$, we have 
\begin{align*}
\xi_1 \eta_2 - \xi_2 \eta_1 & = m_1 (x_1 - x_3) \dot{y}_1 - m_1 (y_1 - y_3) \dot{x}_1 \\
& = m_1 (x_1 \dot{y}_1 - y_1 \dot{x}_1) - m_1 (x_3 \dot{y}_1 - y_3 \dot{x}_1),\\[5pt]
\xi_3 \eta_4 - \xi_4 \eta_3 & = m_2 (x_2 -x_3) \dot{y}_2 - m_2 (y_2 - y_3) \dot{x}_2\\
& = m_2 (x_2 \dot{y}_2 - y_2 \dot{x}_2) - m_2 (x_3 \dot{y}_2 - y_3 \dot{x}_2). 
\end{align*}
Hence we have 
$$
q_4 = \sum\limits^2_{k=1} m_k (x_k \dot{y}_k - y_k \dot{x}_k) - (m_1
\dot{y}_1 + m_2 \dot{y}_2) x_3 + (m_1 \dot{x}_1 + m_2 \dot{x}_2) y_3.  
$$\pageoriginale
Since the centre of gravity remains fixed at the origin of the
coordinate system $(x,y)$, we have $m_1 \dot{x}_1 + m_2 \dot{x}_2 = -
m_3 \dot{x}_3$, $m_1 \dot{y}_1+ m_2 \dot{y}_2 = - m_3 \dot{y}_3$ and
so,  
$$
q_4 = \sum\limits^3_{k=1} m_k (x_k \dot{y}_k - y_k \dot{x}_k) = \lambda, 
$$
and we know that $\lambda=0$. This completes the proof of the theorem.

Thus the hamiltonian system (\ref{chap3:eq3.4.14}) takes the form  
\begin{equation*}
\dot{p}_k = (E_{qk})_{q_4 =0}, \dot{q}_k = - (E_{p_k})_{q_4 = 0}, \; k
= 1,2,3, \; \dot{p}_4 = (E_{q_4})_{q_4 =
  0}.\tag{3.4.15}\label{chap3:eq3.4.15} 
\end{equation*}
We shall now introduce the variables $p^*_k$ by setting
\begin{equation*}
p_k = p^*_k t^{2/3}, \; k = 1,2,3. \tag{3.4.16}\label{chap3:eq3.4.16}
\end{equation*}
Since we know by (\ref{chap3:eq3.4.1}) and (\ref{chap3:eq3.4.8}) that
\begin{gather*}
p_1 = \xi_1 c + \xi_2 s = (x_1 - x_3 ) c+ (y_1 - y_3) s,\\ 
p_2 = \xi_3 c + \xi_4 s = (x_2 - x_3) c + (y_2 - y_3) s,\\
p_3 = - \xi_3 s + \xi_4 c = - (x_2 - x_3) s+ (y_2 - y_3) c,
\end{gather*}
and $(x^*_k, y^*_k)$, $k =1,2,3$, are the vertices of the big triangle
with respect to the original system of coordinates with origin at the
centre of gravity, we can take the coordinates of these vertices in
the new coordinate system with origin at $P_3$ and $x$-axis parallel
to $P_3 P_1$, to be $(p^*_1, 0)$, $(p^*_2, p^*_3)$, $(0,0)$. We know
by Theorem \ref{chap3:thm3.2.2}, that the\pageoriginale big triangle
has a limiting configuration which is either an equilateral triangle
or a set of three collinear points. (In the latter case, of course,
there are three possibilities, but we restrict ourselves to one of
them, namely the case in which $P_2$ is between $P_1 $ and $P_3$; the
two other cases are similar). In other words, we have proved that
relative to the rotating coordinate system, $p^*_k, k =1,2,3,$ tends
to a finite limit as $ t\to 0$. We shall denote these limits by
$\bar{p}_1, \bar{p}_2$ and $\bar{p}_3$.  We have also determined the
limiting mutual distances, in fact, in the equilateral case we have  
\begin{equation*}
\bar{p}_1 = r, \; \bar{p}_2 = \frac{1}{2} r, \; \bar{p}_3 =
\frac{1}{2} \sqrt{3} r, \tag{3.4.17}\label{chap3:eq3.4.17} 
\end{equation*}
and in the collinear case,
\begin{equation*}
\bar{p}_1 = \rho, \; \bar{p}_2 = \omega \rho, \bar{p}_3 = 0. 
\tag{3.4.18}\label{chap3:eq3.4.18}
\end{equation*}
Here $r, \rho, \omega$ are given by (\ref{chap3:eq3.2.23}),
(\ref{chap3:eq3.2.27}) and (\ref{chap3:eq3.2.30}); they are unique\-ly
determined by the masses. On the other hand, we know that since
$\dot{x}_k, \dot{y}_k, (k = 1,2,3)$, are $0(t^{-1/3})$ as $t \to 0$,
also $\dot{\eta}_k = 0 (t^{-1/3})$ as $t \to 0$, $k = 1,2,3,4$. Hence
$q_k, k = 0, \ldots, 3$, given by (\ref{chap3:eq3.4.10}), being linear
in $\eta_1, \ldots, \eta_4$, are also $0(t^{-1/3})$ as $t \to 0$, and
moreover $q_4 \equiv 0$. If we introduce the new variables $q^*_k, k =
0, \ldots, 3$, by setting 
\begin{equation*}
q_k = q^*_k t^{-1/3}, \tag{3.4.19}\label{chap3:eq3.4.19}
\end{equation*}
then $q^*_k = 0(1)$ as $t \to 0$. We would expect that $q^*_k$ tend to
finite limits as $t \to 0$. We show that this is in fact the case. 
\end{proof}

\begin{subtheorem}\label{chap3:thm3.4.2}
If there\pageoriginale is a genral collision at $t =0$, then $q^*_k$
tend to finite limits $\bar{q}_k$ as $t \to 0$. 
\end{subtheorem}

\begin{proof}
We consider the function $\sigma = \sum\limits^3_{k=1} m_k (x^2_k +
y^2_k)$ which, on differentiation with respect to $t$, gives
$\dfrac{1}{2} \dot{\sigma} = \sum\limits^3_{k=1} m_k( x_k \dot{x}_k +
y_k \dot{y}_k)$. This gives, in view of the fact that the centre of
gravity remains fixed at the origin and so $m_3 \dot{x}_3 = - (m_1
\dot{x}_1 + m_2 \dot{x}_2)$ and $m_3 \dot{y}_3 = - (m_1 \dot{y}_1 +
m_2 \dot{y}_2)$, 
\begin{align*}
\frac{1}{2} \dot{\sigma} & = \sum\limits^2_{k=1} m_k ((x_k \dot{x}_k +
y_k \dot{y}_k) - x_3 (m_1 \dot{x}_1 + m_2 \dot{x}_2 ) - y_3 (m_1
\dot{y}_1 + m_2 \dot{y}_2))\\ 
& = \sum\limits^2_{k=1} m_k ((x_k - x_3) \dot{x}_k + (y_k - y_3) \dot{y}_k).
\end{align*}
Using (\ref{chap3:eq3.4.1}), (\ref{chap3:eq3.4.3}),
(\ref{chap3:eq3.4.8}) and (\ref{chap3:eq3.4.13}) we obtain from this  
$$
\frac{1}{2} \dot{\sigma} = \xi_1 \eta_1 + \xi_2 \eta_2 + \xi_3 \eta_3
+ \xi_4 \eta_4 = p_1 q_1 + p_2 q_2 + p_3 q_3.   
$$
By Theorem \ref{chap3:thm3.1.2}, we have $\dot{\sigma} \sim
\dfrac{4}{3} \kappa t^{1/3}$. If we write $\dot{\sigma}$ in terms of
$p^*_k, q^*_k$, $k=1,2,3 $, we get, as $t \to 0$, 
$$
\frac{1}{2} \dot{\sigma} = (p^*_1 q^*_1  + p^*_2 q^*_2 + p^*_3 q^*_3)
t^{1/3} \sim\frac{2}{3} \kappa t^{1/3}.  
$$
Hence it follows that, as $t \to 0$,
\begin{equation*}
p^*_1 q^*_1 + p^*_2 q^*_2 + p^*_3 q^*_3 \to \frac{2}{3}
\kappa. \tag{3.4.20}\label{chap3:eq3.4.20} 
\end{equation*}
(The constant $\kappa$ has already been explicitly determined in terms
of the masses in both the equilateral and collinear cases). Since $q_4
= 0$, we also have from (\ref{chap3:eq3.4.12}), $p_1 q_o + p_2 q_3 -
p_3 q_2 = q_4 = 0$ and hence 
\begin{equation*}
p^*_1 q^*_o + p^*_2 q^*_3  - p^*_3 q^*_2 =
0. \tag{3.4.21}\label{chap3:eq3.4.21} 
\end{equation*}\pageoriginale

We consider first the equilateral case. By (\ref{chap3:eq3.1.3}), if
there is a general collision at $t = 0$ and $u,v$ are two distinct
coordinates among the $x_k, y_k$, $k = 1,2,3,$ then as $t \to 0$,
$u\dot{v} - v \dot{u} = o (t^{1/3})$, and so in particular, 
\begin{equation*}
m_1 (x_1 \dot{y}_1 - y_1 \dot{x}_1) = o (t^{1/3}), \; m_2 (x_2
\dot{y}_2 - y_2 \dot{x}_2) = o(t^{1/3})
. \tag{3.4.22}\label{chap3:eq3.4.22} 
\end{equation*}
But
\begin{align*}
m_1 (x_1 \dot{y}_1 - y_1 \dot{x}_1) & = \left(\frac{m_2 + m_3}{m}
\xi_1 - \frac{m_2}{m} \xi_3 \right) \eta_2 - \left( \frac{m_2 +
  m_3}{m} \xi_2 - \frac{m_2}{m} \xi_4\right) \eta_1 \\ 
& = \frac{m_2 + m_3}{m} (\xi_1 \eta_2 - \xi_2 \eta_1) - \frac{m_2}{m}
(\xi_3 \eta_2 - \xi_4 \eta_1);\\ 
m_2 (x_2 \dot{y}_2 - y_2 \dot{x}_2) & = \left(-\frac{m_1}{m} \xi_1 +
\frac{m_1 m_3}{m} \xi_3 \right) \eta_4 - \left(-\frac{m_1}{m} \xi_2 +
\frac{m_1 + m_3}{m}  \xi_4\right) \eta_3\\ 
& = - \frac{m_1}{m} (\xi_1 \eta_4 - \xi_2 \eta_3) + \frac{m_1 +
  m_3}{m} (\xi_3 \eta_4 - \xi_4 \eta_3).  
\end{align*}
Now passing to the variables $p_k, q_k$ using the transformation given
by (\ref{chap3:eq3.4.8}) and (\ref{chap3:eq3.4.13}), we find from
(\ref{chap3:eq3.4.22}) that, as $t \to 0$, 
\begin{align*}
& \frac{m_2 + m_3}{m} p_1 q_o - \frac{m_2}{m} (p_2 q_o - p_3 q_1) =
  m_1 (x_1 \dot{y}_1 - y_1 \dot{x}_1)  = o (t^{1/3}),\\ 
& - \frac{m_1}{m} p_1 q_3 + \frac{m_1 + m_3}{m} (p_2 q_3 - p_3 q_2) =
  m_2 (x_2 \dot{y}_2 - y_2 \dot{x}_2) = o (t^{1/3}).  
\end{align*}
Hence we have the following two additional relations for $p^*_k,
k=1,2,3$, and $q^*_k$, $k = 0, \ldots, 3$: as $t \to 0$, 
\begin{align*}
& (m_2 + m_3) p^*_1 q^*_o - m_2 (p^*_2 q^*_o - p^*_3 q^*_1) = o(1),
  \tag{3.4.23}\label{chap3:eq3.4.23}\\ 
& - m_1 p^*_1 q^*_3 + (m_1 + m_3) \; (p^*_2 q^*_3 - p^*_3 q^*_2) = o
  (1). \tag{3.4.24}\label{chap3:eq3.4.24} 
\end{align*}\pageoriginale
The four equations (\ref{chap3:eq3.4.20}), (\ref{chap3:eq3.4.21}),
(\ref{chap3:eq3.4.23}) and (\ref{chap3:eq3.4.24}) remain valid as $t
\to 0$; since $q^* = 0(1)$ as $t \to 0$ we can therefore write
$\bar{p}_k$ in place of $p^*_k$ and obtain the equations in the
unknowns $q^*_o, \ldots, q^*_3$: as $t \to 0$,  
\begin{gather*}
\bar{p}_1 q^*_o - \bar{p}_3 q^*_2 + \bar{p}_2 q^*_3 = 0, \\
\bar{p}_1 q^*_1  + \bar{p}_2 q^*_2  + \bar{p}_3 q^*_3 = \frac{2}{3} \kappa + o(1),\\
((m_2 + m_3) \bar{p}_1 - m_2 \bar{p}_2 ) q^*_o + m_2 \bar{p}_3 q^*_1 = o(1),\\
-(m_1 + m_3) \bar{p}_3 q^*_2 + ((m_1 + m_3) \bar{p}_2 - m_1 \bar{p}_1) q^*_3 = o(1).
\end{gather*}
Instead of the above asymptotic relations for $q^*$ we shall consider
for the moment the associated system of linear equations in $q^*$ with
the error terms omitted; we write $\bar{q}$ in place of $q^*$ and we
have  
\begin{align*}
&  \qquad \bar{p}_1 \bar{q}_o - \bar{p}_3 \bar{q}_2 + \bar{p}_2
  \bar{q}_3 = 0,\\ 
& \qquad \bar{p}_1 \bar{q}_1 + \bar{p}_2 \bar{q}_2 + \bar{p}_3
  \bar{q}_3 = \frac{2}{3} \kappa, \\ 
& ((m_2 + m_3) \bar{p}_1 - m_2 \bar{p}_2) \bar{q}_o + m_2 \bar{p}_3
  \bar{q}_1 = 0,\\  
& - (m_1 + m_3) \bar{p}_3 \bar{q}_2 + ((m_1 + m_3) \bar{p}_2 - m_1
  \bar{p}_1 ) \bar{q}_3 = 0. \tag{3.4.25}\label{chap3:eq3.4.25} 
\end{align*}
A solution\pageoriginale of this system of linear equations provides a
solution of the problem in the special case in which $p_k = \bar{p}_k
t^{2/3}$, $q_k = \bar{q}_k t^{-1/3}$. The special solution is given
by: 
\begin{gather*}
p_k = \bar{p}_k t^{2/3}, k = 1,2,3; p_4 = 0; \xi_1 = p_1 , \; \xi_2 =
0, \; \xi_3 = p_2, \xi_4 = p_3; \\ 
x_1 = \frac{m_2 + m_3}{m} p_1 - \frac{m_2}{m} p_2, \\ 
x_2 = - \frac{m_1}{m} p_1 + \frac{m_1 + m_3}{m} p_2, \\ 
y_1 = - \frac{m_2}{m} p_3, y_2 = \frac{m_1 + m_3}{m} p_3;\\
q_k =\bar{q}_k  t^{-1/3}, k = 0, 1,2,3;\\
q_1 = m_1 \dot{x}_1 = \frac{m_1}{m} (m_2 + m_3) \dot{p}_1 -
\frac{m_1m_2}{m} \dot{p}_2,\\  
q_o = m_1 \dot{y}_1 = - \frac{m_1 m_2}{m} \dot{p}_3, \\
q_2  = m_2 \dot{x}_2 = - \frac{m_1 m_2}{m} \dot{p}_1 + \frac{m_2}{m}
(m_1+ m_3) \dot{p}_2,\\  
q_3 = m_2 \dot{y}_2 = \frac{m_2}{m} (m_1 + m_3) \dot{p}_3. 
\end{gather*}

The determinant of the system (\ref{chap3:eq3.4.25}) of linear
equations in $\bar{q}_o, \bar{q}_1,\break \bar{q}_2, \bar{q}_3$ is
seen to be 
\begin{equation*}
-\bar{p}_1 \bar{p}_3 (m_1 m_3 p_1^{-2} + m_1 m_2 (\bar{p}_1 -
\bar{p}_2)^2 + m_2 m_3 \bar{p}^2_2 + (m_1 + m_3) m_2
\bar{p}^2_3). \tag{3.4.26}\label{chap3:eq3.4.26} 
\end{equation*}
In the equilateral case, using the values for $\bar{p}_k, k = 1,2,3,$
given by (\ref{chap3:eq3.4.17}), this is seen to be $-\dfrac{1}{2}
\sqrt{3} (m_1 m_2 + m_2  m_3 + m_3 m_1) r^4 \neq 0$, and so we obtain
the solution of (\ref{chap3:eq3.4.25}): 
\begin{align*}
\bar{q}_o &= - \frac{m_1 m_2}{\sqrt{3m}} r,\\ 
\bar{q}_1 &= \frac{m_1(m_2 + 2m_3)}{3m} r,\\ 
\bar{q}_2 &= \frac{m_2(m_3 - m_1)r}{3m},\\ 
\bar{q}_3 &= \frac{m_2 (m_1+ m_3)r}{\sqrt{3}
  m}. \tag{3.4.27}\label{chap3:eq3.4.27} 
\end{align*}
In the collinear case, since $\bar{p}_1 = \rho$, $\bar{p_2} = \omega
\rho$, $\bar{p}_3 = 0$, it turns out that the\pageoriginale
determinant (\ref{chap3:eq3.4.26}) is zero. However, we may, in this
case, use instead of the asymptotic relations (\ref{chap3:eq3.4.22}),
the relation $x_1 \dot{x}_2 - x_2 \dot{x}_1 = o (t^{1/3})$, $y_1
\dot{y}_2 - y_2 \dot{y}_1 = o (t^{1/3})$ leading to  
\begin{equation*}
m_1 m_2 (x_1 \dot{x}_2  - x_2 \dot{x}_1 + y_1 \dot{y}_2 - y_2
\dot{y}_1) = o (t^{1/3}) \text{ as } t \to
0. \tag{3.4.28}\label{chap3:eq3.4.28} 
\end{equation*}
Using the values given above for $x_1 , \ldots, \dot{y}_2$, this gives 
\begin{align*}
&m_1 \frac{(m_2 + m_3)}{m} p_1 q_2 - \frac{m_1 m_2}{m} (p_2 q_2 + p_3
  q_3 - p_1 q_1)\\ 
&\qquad\qquad - m_2 \frac{(m_1 + m_3)}{m} (p_2 q_1 + p_3 q_o) = o (t^{1/3})
\end{align*}
or
\begin{align*}
& m_2(m_1 + m_3) \bar{p}_3 \bar{q}_o - m_2 (m_1 \bar{p}_1 - (m_1 + m_3)
\bar{p}_2) \bar{q}_1+ m_1 (m_2 \bar{p}_2\\ 
&\qquad - (m_2 + m_3) \bar{p}_1) \bar{q}_2  
+ m_1 m_2 \bar{p}_3 \bar{q}_3 = o(1).
\end{align*}
Now we consider the system (\ref{chap3:eq3.4.25}) with the fourth
equation replaced by this, without the error term on the right. The
system is now of rank 4, the determinant is seen to be 
$$
-\omega (m_2 (1-\omega) + m_3) \; (m_1m_2 (1-\omega)^2 + m_1 m_3 + m_2
m_3 \omega^2)\rho^4 \neq 0, 
$$
and we can solve the system as we did earlier and we obtain the
solutions 
\begin{align*}
\bar{q_o} &= 0, \bar{q}_1 = \frac{2}{3} m_1 \frac{m_2(1-\omega ) +
  m_3}{m} \rho,\\ 
\bar{q}_2 &= \frac{2}{3} m_2 \frac{-m_1 (1-\omega) + m_3
  \omega}{m}\rho,\\ 
\bar{q}_3 &= 0. \tag{3.4.29}\label{chap3:eq3.4.29}
\end{align*}

We have thus proved the theorem in the special case of the particular
solution $p_k = p^*_k t^{2/3}$, $q_k = \bar{q}_k t^{-4/3}$, where
$p^*_k$ are constants. We now take up the general case. From the
relations (\ref{chap3:eq3.4.20}), (\ref{chap3:eq3.4.21}),
(\ref{chap3:eq3.4.23}) and (\ref{chap3:eq3.4.24}) we have the
asymptotic equations satisfied by $q^*_o, \ldots, q^*_3$: as $t \to
0$, 
\begin{gather*}
p^*_1 q^*_o + p^*_2 q^*_3 - p^*_3 q^*_2 = 0,\\
p^*_1 q^*_1 + p^*_2 q^*_2 + p^*_3 q^*_3 = \frac{2}{3} \kappa +
o(1),\\ 
((m_2 + m_3) p^*_1 - m_2 p^*_2) q^*_o + m_2 p^*_3 q^*_1 = o(1), \\ 
- m_1 p^*_1 q^*_3 + (m_1 + m_3) (p^*_2 q^*_3 - p^*_3 q^*_2) =
o(1). \tag{3.4.30}\label{chap3:eq3.4.30} 
\end{gather*}\pageoriginale
Since $p^*_k(t) \to \bar{p}_k$, $k = 1,2,3,$ as $t \to 0$, we can
write, for $t$ sufficiently near 0, $p^*_k (t)  = \bar{p}_k +
\epsilon_k(t)$, $\epsilon_k(t) = o(1)$ as $t \to 0$. In the
equilateral case, then, recalling the values of $\bar{p}_k$ given by
(\ref{chap3:eq3.4.17}), we can replace $p^*_k$ in the system
(\ref{chap3:eq3.4.30}) by 
$$
p^*_1 = r + \epsilon_1(t), \; p^*_2 = \frac{1}{2}  r + \epsilon_2 (t),
\; p^*_3 =\frac{1}{2} r \sqrt{3} + \epsilon_3(t). 
$$
The determinant of the system of linear equations
(\ref{chap3:eq3.4.30}) is a quartic polynomial in $p^*_k$ and hence a
continuous function of the variables $p^*_k$, $k=1,2,3$. Hence, as $t
\to 0$, this determinant tends to the determinant of the system
(\ref{chap3:eq3.4.30}) with $p^*_k$ replaced by $\bar{p}_k$. We have
seen that the latter determinant is $\dfrac{\sqrt{3}}{2} (m_1 m_2 +
m_2 m_3 + m_3 m_1) r^4 \neq 0 $. Hence, for $t$ sufficiently close to
0, the determinant of (\ref{chap3:eq3.4.30}) is also different from
zero, by continuity. Let $t$ be small enough for this condition to
hold. Then we consider the system of linear equations 
\begin{gather*}
p^*_1 q^*_o - p^*_3 q^*_2 + p^*_2 q^*_3 = 0\\
p^*_1 q^*_1 + p^*_2 q^*_2 + p^*_3 q^*_3 = \frac{2}{3} \kappa + \delta_1 (t),\\
((m_2 + m_3)p^*_1 - m_2 p^*_2) q^*_o + m_2 p^*_3 q^*_1 = \delta_2(t)\\
(-m_1 p^*_1 + (m_1+ m_3)p^*_2)q^*_3 - (m_1 + m_3) p^*_3 q^*_2 =
\delta_3 (t), \tag{3.4.31}\label{chap3:eq3.4.31} 
\end{gather*}
where\pageoriginale $\delta_k(t) \to 0$ as $t \to 0$. The determinant
of the system (\ref{chap3:eq3.4.31}) is  
$$
-\frac{1}{2 } \sqrt{3} (m_1 m_2 + m_2 m_3 + m_3 m_1) r^4 + \delta_4(t)
\neq 0, \; \delta_4 (t)  \to 0 \text{ as } t \to 0. 
$$
Hence we can solve the system (\ref{chap3:eq3.4.31}) and obtain
$q^*_k$ as rational functions of $p^*_k, \delta_k$, with the
non-vanishing determinant in the  denominator. Hence $q^*_k$ tend to
finite limits $\bar{q}_k$ as $t \to 0$, and these limits are the same
as the solutions of the system (\ref{chap3:eq3.4.31}) with $p^*_k$
replaced by their limits $\bar{p}_k$ and $\delta_k$ replaced by their
limit 0. We have already obtained these particular solutions. Hence,
in the equilateral case, we find that as $t \to 0$, 
\begin{gather*}
p_1 \sim rt^{2/3}, \; p_2 \sim \frac{r}{2} t^{2/3}, p_3 \sim
\frac{1}{2} \sqrt{3} r t^{2/3},\\ 
q_1 \sim\frac{m_1}{3m} (m_2 + 2m_2) rt^{-1/3}, q_2 \sim \frac{m_2}{3m}
(m_3 - m_1) rt^{-1/3},\\  
q_3 \sim \frac{m_2}{\sqrt{3m}} (m_1 + m_3) rt^{-1/3} , q_4 = 0. 
\end{gather*}
 
Next, in the collinear case, $\bar{p}_1 = \rho$, $\bar{p}_2 = \omega
\rho$, $\bar{p}_3 = 0$, where $\omega$, $\rho$ and $\kappa$ are
uniquely determined by the masses. It turns out, as we have seen
earlier, that the determinant of the system (\ref{chap3:eq3.4.30})
tends to zero as $t \to 0$, but we can replace the last of the
equations suitably, as we did for the particular solution (namely, by
using the relation $m_1m_2(x_1 \dot{x}_2 - x_2 \dot{x}_1 + y_1
\dot{y}_2 - y_2 \dot{y}_1) = o(t^{1/3})$ as $t \to 0$), and obtain a
linear system in $q^*_o, \ldots, q^*_3$ with determinant $\neq 0$. An
argument on the same lines as in the equilateral case applied to the
new system now proves that $q^*_k(t)$ tend to finite limits
$\bar{q}_k$ as $t \to 0$, $k = 0, \ldots, 3$, and we have 
$$
\bar{q}_1 = \frac{2m_1}{3m} (m_2 (1-\omega) + m_3) \rho, \bar{q}_2 =
\frac{2m_2}{3m} (m_3 \omega - (1-\omega) m_1) \rho, \bar{q}_3 = 0. 
$$\pageoriginale
Hence we have, in the collinear case, as $t \to 0$,
\begin{gather*}
q_1 \sim \frac{2m_1}{3m} (m_2 (1 - \omega) + m_3) \rho t^{-1/3},\\ 
q_2 \sim\frac{2m_2}{3m} (m_3 \omega - (1-\omega) m_1) \rho t^{-1/3}, q_3 = 0. 
\end{gather*}
This completes the proof of the theorem.
\end{proof}

We have proved that $q_4 \equiv 0$ for a collision orbit. If  we now
substitute $q_4 = 0$ in $E_{p_k}$ and $E_{q_k}$, $k =1,2,3,$ then we
get a Hamiltonian system 
$$
\dot{p}_k = (E_{q_k})_{q_4 = 0}, \; \dot{q}_k = - (E_{p_k})_{q_4 = 0}, k = 1,2,3,
$$
with six degrees of freedom. If we solve this system for $p_k, q_k$,
$k=1,2,3$, and substitute in the remaining equation $\dot{p}_4 =
(E_{q_4})_{q_4 = 0}$, then we get a differential equation for
$p_4$. We can integrate this to obtain $p_4$. But we cannot prove that
$p_4(t)$ has a limit as $t \to 0$ until we have proved that the
integral of the function $(E_{q_4})_{q_4 = 0}$ converges as $t \to
0$. Hence we cannot determine the behaviour of $p_4$ as yet. However,
in the case of the particular solution $x_k = x^*_k t^{2/3}$, $y_k =
y^*_k t^{2/3}$, $k =1,2,3,$ where $x^*_k, y^*_k$ are unknown
constants, it is clear that $p_4$ is a constant. By a rotation of the
coordinate system we may then assume that $p_4(t) \equiv 0$. For the
particular solution with this choice of coordinates, we have
$\dot{p}_4 = 0$, i.e. $(E_{q_4})_{q_4 = 0} = 0$. 

We had\pageoriginale introduced the variables $p^*_k, q^*_k$, $k
=1,2,3,$ by setting $p_k = p^*_k t^{2/3}$, $q_k = q^*_k t^{-1/3}$ and
we now set formally $p^*_4 = p_4$, $q^*_4 t^{1/3} = q_4$. With this
definition, 
$$
q^*_o = (q^*_4 - p^*_2 q^*_3 + p^*_3 q^*_2) / p^*_1, \; q_o = q^*_o t^{-1/3}. 
$$
We shall now express the total energy $E$ in terms of the variables
$p^*_k, q^*_k$. If $f$ is a function of the variables $p_k, q_k$, we
shall denote by $f^*$ the function of the variables $p^*_k, q^*_k$
defined by $f^*(p^*, q^*) =  f (p^*, q^*)$. Since $p_k = p^*_k
t^{2/3}$, $q_k = q^*_k t^{-1/3}$, $k = 1,2,3$, and $q_o = q^*o
t^{-1/3}$, we see that 
\begin{align*}
T(q_o, \ldots, q_3) & = \frac{1}{2} (\frac{1}{m_1} + \frac{1}{m_3})
(q^2_1 + q^2_0)\\ 
&\qquad + \frac{1}{2}  (\frac{1}{m_2} + \frac{1}{m_3}) (q^2_2 + q^2_3)
+ \frac{1}{m_3} (q_1 q_2+ q_o q_3)\\ 
& = T^* (q^*_o, \ldots, q^*_3) t^{-2/3};\\
U (p_1, p_2, p_3) & = \frac{m_1 m_2}{\sqrt{(p_1- p_2)^2 + p^2_3 }} +
\frac{m_2 m_3}{\sqrt{p^2_2+ p^2_3}} + \frac{m_1 m_3}{p_1}\\ 
&= U^* (p^*_1, p^*_2, p^*_3) t^{-2/3}.
\end{align*}
So we have
\begin{equation*}
E (p_1, p_2, p_3, q_o , \ldots, q_3) = E^* (p^*_1, p^*_2, p^*_3,q^*_o,
\ldots , q^*_3) t^{-2/3} , \tag{3.4.32}\label{chap3:eq3.4.32} 
\end{equation*}
and the differential equations (\ref{chap3:eq3.4.15}) can be written
down in terms of the variables $p^*_k, q^*_k$: 
\begin{align*}
\dot{p}_k & = \dot{p}^*_k t^{2/3} + \frac{2}{3} p^*_k t^{-1/3} =
E_{q_k} = E^*_{q^*_k} \frac{dq^*_k}{dq_k} t^{-2/3} = E^*_{q^*_k}
t^{-1/3}, k = 1,2,3;\\ 
\dot{q}_k & = \dot{q}^*_k t^{-1/3} -\frac{1}{3} q^*_k t^{-4/3}\\ 
&= - E_{p_k} = - E^*_{p^*_k} \frac{dp^*_k}{dp_t} t^{-2/3} = -
E^*_{p^*_k} t^{-4/3}, k = 1,2,3;\\ 
& \qquad \dot{p}_4 = \dot{p}^*_4 = E^*_{q^*_4} \frac{dq^*_4}{dq_4}
t^{-2/3} = E^*_{q^*_4} t^{-1},  \\ 
& \qquad \dot{q}_4 = \dot{q}^*_4 t^{1/3} + \frac{1}{3} q^*_4 t^{-2/3}
= - E^*_{p^*_4} t^{-2/3} (\equiv 0). 
\end{align*}
Finally,\pageoriginale then, the equations of motion take the form
\begin{align*}
t\dot{p}^*_k & = E^*_{q^*_k} - \frac{2}{3} p^*_k ; t \dot{q}^*_k = -
E^*_{p^*_k} + \frac{1}{3} q^*k, k = 1,2,3; \\ 
& \quad t \dot{p}^*_4 = E^*_{q^*_4}; t \dot{q}^*_4 = - \frac{1}{3}
q^*_4. \tag{3.4.33}\label{chap3:eq3.4.33} 
\end{align*}
Thus we have a system of differential equations of the first order in
which the right sides are explicitly determined functions of $p^*_k, k
= 1,2,3$, and $g^*_k,k=1,\ldots, 4$. Now we claim that the right sides
of (\ref{chap3:eq3.4.33}) can be expanded into power-series in the
seven independent variables $p^*_k, q^*_k$ in some neighbourhood of
$\bar{p}_k, k =1,2,3,$ and $\bar{q}_k, k = 1,2,3,4$. In order to see
this, we observe that $T$ is a homogeneous polynomial of the second
degree in the variables $q_k$, $k = 0, \ldots, 3$ alone and that $U$
is a homogeneous function of degree $-1$ in $p_k, k = 1,2,3,$
alone. Then $T^*$ is a homogeneous polynomial of degree 2 in $q^*_k$
and $U^*$ is a homogeneous function of degree $-1$ in $p^*_k$. Since
$r^*_{12} = ((p^*_1 - p^*_2)^2 + p^{*2}_3)^{1/2} $, $r^*_{23} =
(p^{*2}_2 + p^{*2}_3)^{1/2}$, $r^*_{13} = p^*_1$ are the sides of the
big triangle and hence do not vanish as $t \to 0$, the denominators in
$U^* = m_1 m_2/ r^*_{12} + m_2 m_3 / r^*_{23} + m_1 m_3 / r^*_{13}$ do
not vanish as $t \to 0$. Moreover, $p^*_k (t) \to \bar{p}_k$, $k
=1,2,3,$, and $q^*_k(t) \to \bar{q}_k$, $k =1, \ldots, 4$, as $t \to
0$. Hence we can expand $U^*$ as a power-series in $p^*_k$ in a
suitable neighbourhood of $\bar{p}_k, k =1,2,3$. Thus $E^* = T^* -
U^*$, and hence the right sides in (\ref{chap3:eq3.4.33}) can be
expanded as power-series in all the seven variables in a neighbourhood
of $\bar{p}_1, \bar{p}_2, \bar{p}_3, \bar{q}_1, \ldots \bar{q}_4$
which proves our assertion. 

Since\pageoriginale $p^*_k \to \bar{p}_k, q^*_k \to \bar{q}_k$, $k =
1,2,3,$ and $q^*_4 \to \bar{q}_4$, we can write $p^*_k = \bar{p}_k +
\delta_k + \delta_k$, $k =1,2,3$; $q^*_k = \bar{q}_k + \delta_{k+3}$,
$k=1, \ldots 4$ where $\delta_k(t) \to 0$ as $t \to 0$. Then it
follows that the right sides of (\ref{chap3:eq3.4.33}) can be expanded
as power-series in the seven independent variables $\delta_1, \ldots,
\delta_7$ for $|\delta_k|$ sufficiently small for $k = 1,2,3,$ and for
arbitrary $\delta_{k+3}$, $k =1,\ldots, 4$. We may also write $p^*_4 =
\delta_4$, but this variable does not actually appear on the right
sides of (\ref{chap3:eq3.4.33}). 

We now assert that the power-series expansion for the right sides in
(\ref{chap3:eq3.4.33}) in the variables $\delta_k$ do not contain
constant terms. This can be proved in the following way. We observe
that the particular solution $p^*_k = \bar{p}_k t^{2/3}$, $q^*_k =
\bar{q}_k t^{-1/3}$, $k = 1,2,3,$ of the three-body problem satisfies
the system of differential equations $\dot{p}_k = E_{q_k}$, $\dot{q}_k
= - E_{p_k}$, $k =1,2,3$, and in addition, $p_4 = 0$ by assumption and
$q_4 = 0$ for a collision orbit. Then $p^*_k, q^*_k$ also satisfy the
differential equations (\ref{chap3:eq3.4.33}); $p^*_k, q^*_k$ being
constants for the particular solution, the left sides of
(\ref{chap3:eq3.4.33}) are zero for this solution and hence the right
sides vanish. Thus  
\begin{gather*}
E^*_{q^*_k} (\bar{p}_k, \bar{q}_k) - \frac{2}{3} \bar{p}_k  = 0, \; -
E^*_{p^*_k} (\bar{p}_k, \bar{q}_k) + \frac{1}{3} \bar{q}_k = 0,\\ 
E^*_{q^*_4} (\bar{p}_k, \bar{q}_k) = 0, \; - E^*_{p^*_4} (\bar{p}_k,
\bar{q}_k) - \frac{1}{3} \bar{q}_4 = 0. 
\end{gather*}
But these are precisely the constant terms in the expansions in
power-series of the right sides of (\ref{chap3:eq3.4.33}) and so our
assertion is proved. 

We observe\pageoriginale that the variable $t$ appears explicitly on
the left side in (\ref{chap3:eq3.4.33}) and so we shall transform the
system into one in a new variable $s$ so that $s$ does not appear
explicitly on the left side. For this we introduce the new variable
$s$ by means of the substitution 
\begin{equation*}
t = e^{-s}, \text{ or } s = \log
\frac{1}{t}. \tag{3.4.34}\label{chap3:eq3.4.34} 
\end{equation*}
Then $ds = - \dfrac{dt}{t}$. We shall denote the derivative of a
function $f(s)$ with respect to $s$ by $f'(s)$. Then 
$$
\dot{p}^*_k = p^{*'}_k \frac{ds}{dt} = - \frac{1}{t} p^{*'}_k, \text{
  or } t \dot{p}^*_k  = - p^{*'}_k, \text{ and } t \dot{q}^*_k = -
q^{*'}_k,  
$$
so that the system (\ref{chap3:eq3.4.33}) is transformed into the new
system 
\begin{gather*} 
p^{*'}_k = \frac{2}{3} p^*_k - E^*_{q^*_k}, \; q^{*'}_k = E^*_{p^*_k}
-\frac{1}{3} q^*_k, k = 1,2,3; \\ 
p^{*'}_4 = - E^*_{q^*_4} , q^{*'}_4  = \frac{1}{3} q^*_4.
\tag{3.4.35}\label{chap3:eq3.4.35} 
\end{gather*}
Now we introduce the variables $\bar{p}_k + \delta_k, k = 1,2,3$, and
$\bar{q}_k + \delta_{k+3}$, $k =1, \ldots, 4$. We have $p^{*'}_k =
\delta'_k$, $q^{*'}_k = \delta'_{k+3}$,  
$k = 1,2,3$, $q^{*'}_4 = \delta'_7$, and the right sides of
(\ref{chap3:eq3.4.35}) do not contain constant terms in their
power-series expansions and the variable $p^*_4$ does not occur. So
the system (\ref{chap3:eq3.4.35}) takes the form 
\begin{equation*}
\delta'_k = \sum\limits^8_{l=1} a_{kl} \delta_l + \varphi_k (\delta_1,
\ldots, \delta_7), \; k = 1,\ldots, 8,
\tag{3.4.36}\label{chap3:eq3.4.36} 
\end{equation*}
where $\varphi_k(\delta_1, \ldots, \delta_7)$ are power-series in the
seven variables $\delta_k, k = 1,\break \ldots, 7$
starting\pageoriginale with terms of degree $\geq 2$. For a collision
orbit we know that $q_4 = 0$, and hence $\delta_7 = 0$; and $p^*_k \to
\bar{p}_k$, $q^*_k \to \bar{q}_k$, $k = 1,2,3$, as $t \to 0$, so that
$\delta_k \to 0$, $k = 1, \ldots, 6$, as $t \to 0$ or, equivalently,
as $s \to \infty$. Moreover, the coefficients $a_{kl}$ of the linear
parts of the equations in the system (\ref{chap3:eq3.4.36}), being
functions of $\bar{p}_k, \bar{q}_k, k =1,2,3$, alone, are functions
determined uniquely by the three masses. Also $a_{k8} = 0$, $k =1,
\ldots 8$. 

The nature of the solutions of the system (3.3.36) is related to the
solution of the associated linear system 
$$
\delta'_k = \sum\limits^8_{l=1} a_{kl} \delta_l
$$
in view of the stability theory of solutions of ordinary differential
equations and so we shall first study this simpler system. In order to
study this system closely it is necessary to study the characterstic
equation of the 8-rowed square matrix $A = (a_{kl})$ of the
coefficients. Since $q^{*'}_4 = \dfrac{1}{3} q^*_4$, we have
$\delta'_7 = \dfrac{1}{3} \delta_7$. Thus 
$$
a_{71} = 0, \; l \neq 7; a_{77} = \frac{1}{3}; a_{k8} = 0, k =1, \ldots, 8.
$$
So the matrix $A$ of the coefficients of the linear part in (3.3.36)
is  
$$
A = \begin{pmatrix}
B & * & 0\\
0 & \frac{1}{3} & 0\\
* & * & 0
\end{pmatrix},
$$
where $B$ is the 6-rowed square matrix of the coefficients $a_{kl}, k,
l = 1,\break \ldots, 6$. If $E_m$ denotes the $m$-rowed unit matrix,
then the characterstic polynomial of $A$\pageoriginale is given by 
\begin{equation*}
|z E_8 - A| = z(z -\dfrac{1}{3}) |z E_6 - B|. \tag{3.4.37}\label{chap3:eq3.4.37}
\end{equation*}
In order to simplify the computation of the characterstic polynomial
of $A$, we make a transformation $\delta_k = \sum\limits^8_{l=1}
c_{kl} \epsilon_l + \ldots$, with $|C| = \det (c_{kl}) \neq 0$, so
that the system (3.3.36) is transformed into the system of
differential equations (in matrix form): 
$$
\epsilon' = C^{-1} A C \epsilon +\chi (\epsilon_1, \ldots, \epsilon_8),
$$
where $\chi_k$ are again power-series starting with quadratic
terms. We wish to choose the transformation in such a way that the
matrix $C^{-1} AC$ has a simple form. (Let us recall that the
characteristic polynomial of $A$ is the same as that of $C^{-1} AC$: 
$$
|zE - C^{-1} AC| = |C^{-1} (zE -A) C| = |zE - A|. 
$$
We shall now show that such a transformation can be obtained from the
canonical transformation of the variables $(\xi_k, \eta_k)$ to $(p_k,
q_k)$, in the following way. Let us recall that under this
transformation, the variable $p_4$ does not appear in the new
expression for $E$. 

We consider the inverse of the canonical transformation from the
variables $(\xi_k, \eta_k)$ to the variables $(p_k, q_k)$. This again
is a canonical transformation and its Jacobian  matrix is symplectic
and has, in particular, a non-zero determinant. The transformation is
given explicitly as follows: 
\begin{align*}
\xi_1 & = p_1 c, \xi_2 = p_1 s, \xi_3 = p_2 c - p_3 s, \xi_4 = p_2 s +
p_3 c;\\ 
\eta_1 & = q_1 c-q_o s, \eta_2 = q_1 s + q_o c, \; \eta_3 = q_2 c -q_3
s, \; \eta_4 = q_2 s + q_3 c, \tag{3.4.38}\label{chap3:eq3.4.38} 
\end{align*}\pageoriginale
where $q_o = (q_4 - p_2 q_3 + p_3 q_2) p^{-1}_1$, $c = \cos p_4$, $s =
\sin p_4$.  

Since the system of differential equations (\ref{chap3:eq3.4.35}) was
obtained by the substitution: $p_k = p^*_k t^{2/3}$, $q_k = q^*_k
t^{-1/3}$, $k = 1,2,3,$ $p_4 = p^*_4$, $q_4 = q^*_4 t^{1/3}$, we have
$q_o = q^*_o t^{-1/3}$. We introduce the variables $\xi^*_k, \eta^*_k$
by setting 
\begin{equation*}
\xi_k = \xi^*_k t^{2/3}, \eta_k = \eta^*_k t^{-1/3}, k = 1, \ldots,
4. \tag{3.4.39}\label{chap3:eq3.4.39} 
\end{equation*}
Under these substitutions we obtain from (3.3.38) the following
transformation of the variables $p^*_k, q^*_k$ to $\xi^*_k, \eta^*_k$: 
\begin{align*}
\xi^*_1 & = p^*_1 c, \; \xi^*_2  = p^*_1 s, \xi^*_3 = p^*_2 c - p^*_3 s, \xi^*_4 = p^*_2 s + p^*_3 c,\\
\eta^*_1 & = q^*_1  c - q^*_o s, \; \eta^*_2 = q^*_1 s+ q^*_o c, \;
\eta^*_3 = q^*_2 c - q^*_3 s, \eta^*_4 = q^*_2 s + q^*_3
c. \tag{3.4.40}\label{chap3:eq3.4.40} 
\end{align*}
Then the substitutions $p^*_k = \bar{p}_k + \delta_k, q^*_k =
\bar{q}_k + \delta_{k+3}$, $k =1,2,3$, $p^*_4 = \delta_8$, $q^*_4 =
\bar{q}_4 + \delta_7$, imply that $\xi^*_k, \eta^*_k$ are functions of
the variables $\delta_1, \ldots, \delta_8$ and $\bar{\xi}_k,
\bar{\eta}_k$ are obtained by setting $\delta_k = 0$, $k = 1, \ldots,
8$. So we have  
\begin{align*}
\bar{\xi}_1 & = \bar{p}_1, \; \bar{\xi}_2 = 0, \; \bar{\xi}_3 =
\bar{p}_2, \; \bar{\xi}_4 = \bar{p}_3; \\  
\bar{\eta}_1 & = \bar{q}_1, \; \bar{\eta}_2 =\bar{q}_o, \;
\bar{\eta}_3 = \bar{q}_2, \; \bar{\eta}_4 =
\bar{q}_3. \tag{3.4.41}\label{chap3:eq3.4.41} 
\end{align*}
These\pageoriginale values have been determined explicitly in both the
equilateral and the collinear cases. It is also clear that $\xi^*_k
\to \bar{\xi}_k$, $\eta^*_k \to \bar{\eta}_k$, $k = 1, \ldots, 4$, as
$\delta_1 \to 0, \ldots, \delta_8 \to 0$. Since to say that $p^*_k,
q^*_k$ satisfy the system of differential equations
(\ref{chap3:eq3.4.35}) is equivalent to saying that $p_k, q_k$ satisfy
the Hamiltonian system, it follows that $\xi_k, \eta_k$ satisfy the
Hamiltonian system 
$$
\dot{\xi}_k = E_{\eta_k}, \dot{\eta}_k = - E_{\xi_k}, k =1, \ldots, 4. 
$$
By means of the substitution (\ref{chap3:eq3.4.39}) and $t = e^{-s}$,
we obtain the following system of differential equations for $\xi^*_k,
\eta^*_k$, considered as functions of the variable $s$: 
\begin{equation*}
\xi^{*'}_k  =\frac{2}{3} \xi^*_k - E^*_{\eta^*_k}, \eta^{*'}_k =-
\frac{1}{3} \eta^*_k + E^*_{\xi^*_k}, k =1, \ldots,
4. \tag{3.4.42}\label{chap3:eq3.4.42} 
\end{equation*}
On differentiating $E^*$ with respect to $\eta^*_k$ we get
\begin{equation*}
E^* \eta^*_k = 
\begin{cases}
\left(\dfrac{1}{m_1} + \dfrac{1}{m_3} \right) \eta^*_k +
\dfrac{1}{m_3} \eta^*_{k+2} , k =1,2; \\[6pt] 
\left(\dfrac{1}{m_2} + \dfrac{1}{m_3} \right) \eta^*_{k} +
\dfrac{1}{m_3} \eta^*_{k-2} , k =3,4.  
\end{cases}
\tag{3.4.43}\label{chap3:eq3.4.43}
\end{equation*}
We take 
\begin{equation*}
\zeta^*_k = \xi^*_k , \; \zeta^*_{k+4} = - E^*_{\eta^*_k}, \; k =1, \ldots, 4. 
\tag{3.4.44}\label{chap3:eq3.4.44}
\end{equation*}
Then we can solve $\zeta^*_{k+4} = - E^*_{\eta^*_k}$, $k = 1, \ldots,
4$, and use (\ref{chap3:eq3.4.43}) to express $\eta^*_k$ in terms of
$\zeta^*_{k+4}$, $k =1, \ldots 4$. We obtain 
\begin{equation*}
\eta^*_k =
\begin{cases}
-\dfrac{m_1}{m} (m_2 + m_3) \zeta^*_{k+4} + \dfrac{m_1 m_2}{m}
\zeta^*_{k+6}, \; k = 1,2; \\[6pt] 
\dfrac{m_1 m_2}{m} \zeta^*_{k+2} -\frac{m_2}{m} (m_1 + m_3)
\zeta^*_{k+4} , \; k =
3,4, \end{cases}\tag{3.4.45}\label{chap3:eq3.4.45} 
\end{equation*}\pageoriginale
where $m= m_1 + m_2 + m_3$. The system (3.3.42) then becomes
\begin{equation*}
\zeta^{*'}_k = \frac{2}{3} \zeta^*_k + \zeta^*_{k+4} ,
\zeta^{*'}_{k+4} = - \frac{1}{3} \zeta^*_{k+4} - F^*_k, \;k = 1,
\ldots, 4,\tag{3.4.46}\label{chap3:eq3.4.46} 
\end{equation*}
where
\begin{equation*}
F^*_k = 
\begin{cases}
\dfrac{m_1 + m_3}{m_1 m_3} E^*_{\zeta^*_k} + \frac{1}{m_3}
E^*_{\zeta^*_{k+2}}, \; k =1,2; \\[6pt] 
\dfrac{m_2 + m_3}{m_2 m_3} E^*_{\zeta^*_k} + \frac{1}{m_3}
E^*_{\zeta^*_{k-2}}, \; k =3,4. 
\end{cases}
\tag{3.4.47}\label{chap3:eq3.4.47}
\end{equation*}
By the definition of $\zeta^*_k$ it is clear that the $\zeta^*_k$ tend
to finite limits $\bar{\zeta}_k (k= 1, \ldots, 8)$ and we have  
\begin{equation*}
\bar{\zeta}_1 = \bar{p}_1, \; \bar{\zeta}_2 = 0, \;\bar{\zeta}_3 =
\bar{p}_2 , \; \bar{\zeta}_4 = \bar{p}_3 , \ldots
\tag{3.4.48}\label{chap3:eq3.4.48} 
\end{equation*}
where the $\bar{p}_k$ have already been determined. We now introduce
the variables $\epsilon_k$ by setting 
\begin{equation*}
\zeta^*_k = \bar{\zeta}_k + \epsilon_k, \; k =1, \ldots,
8. \tag{3.4.49}\label{chap3:eq3.4.49} 
\end{equation*}
Then the system of differential equations (\ref{chap3:eq3.4.46}) for
$\zeta^*_k$ implies that the $\epsilon_k$, considered as functions of
the variable $s$, satisfy the system of differential equations 
\begin{equation*}
\epsilon'_k = \frac{2}{3} \epsilon_k + \epsilon_{k+4} ,
\epsilon'_{k+4} = - \frac{1}{3} \epsilon_{k+4} - \sum\limits^4_{l=1}
h_{kl} \epsilon_l + \ldots, k =1 ,
\ldots,4,\tag{3.4.50}\label{chap3:eq3.4.50} 
\end{equation*} \pageoriginale
where the coefficients $h_{kl}$ are calcultated from (3.3.47) using
the relations $E^*_{\zeta^*_k} = - U^*_{\xi^*_k}$, $k =1, \ldots, 4$,
and substituting $\xi^*_k = \zeta^*_k = \bar{\zeta}_k + \epsilon_k$,
$k =1, \ldots, 4$. Using the values of $\bar{\zeta}_k$ given by
(\ref{chap3:eq3.4.48}), we find that the matrix $H = (h_{kl})$, $k, l
=1, \ldots, 4$, is given, in the equilateral case, by 
$$
{\fontsize{7}{9}\selectfont
H = r^{-3}
\begin{pmatrix}
\dfrac{m_2}{4} - 2(m_1 + m_3) & \dfrac{3\sqrt{3}}{4} m_2 & 0 &
-\dfrac{3\sqrt{3}}{2} m_2\\ 
\dfrac{3\sqrt{3}}{4} m_2 & m_1 + m_3 - \dfrac{5}{4} m_2 & -
\dfrac{3\sqrt{3}}{2}  m_2 & 0\\ 
-\dfrac{9}{4} m_1 & - \dfrac{3\sqrt{3}}{4} m_1 & \dfrac{m_1 + m_2 +
  m_3}{4} & \dfrac{3\sqrt{3}}{4} (m_1 -m_2- m_3)\\ 
-\dfrac{3\sqrt{3}}{4} m_1 & \dfrac{9}{4} m_1 & \dfrac{3\sqrt{3}}{4}
(m_1 - m_2 - m_3) & - \dfrac{5}{4} (m_1 + m_2 + m_3) 
\end{pmatrix}}
$$
and in the collinear case, by
$$ 
{\fontsize{5}{7}\selectfont
H = \rho^{-3}
\begin{pmatrix}
-2(m_1 + m_3) - 2m_2 \omega^{-3} & 0 & 2m_2 (\omega^{-3} - (1-\omega)^{-3}) & 0\\
0 & m_1 + m_3+ m_2 \omega^{-3} & 0 & m_2 ((1-\omega)^{-3} - \omega^{-3}) \\
- 2m_1(1-\omega^{-3}) & 0 & -2m_1^{-3} -2(m_2 + m_3) (1-\omega)^{-3} &  0\\
0 & m_1 (1-\omega^{-3}) & 0 & m_1 \omega^{-3} + (m_2 + m_3) (1-\omega)^{-3} 
\end{pmatrix}}
$$
Then the matrix of the coefficients of the linear part of the
system\break (\ref{chap3:eq3.4.50}) is given by  
$$
\begin{pmatrix}
\dfrac{2}{3} E_4 & E_4\\
- H & -\dfrac{1}{3} E_4 
\end{pmatrix},
$$
and the\pageoriginale characteristic polynomial of this matrix is
$$
\left| \left(z - \frac{2}{3} \right) \; \left(z + \frac{1}{3} \right) E_4 + H \right|, 
$$
which  is also the characteristic polynomial of the matrix $A$. Hence
we have, recalling (\ref{chap3:eq3.4.37}), 
$$
\left| \left(z - \frac{2}{3} \right) \; \left(z + \frac{1}{3} \right)
E_4 + H\right|  = \left|z E_8 - A \right| = \left(z - \frac{1}{3}
\right) \left| z E_6 - B\right|.  
$$ 
Denoting $\left(z -\dfrac{2}{3} \right) \; \left(z+ \dfrac{1}{3}
\right)$ by $x$, this gives  
$$
\left| x E_4 + H\right|  = \left(x+ \frac{2}{9} \right) \left|z E_6 -
B \right|.  
$$
Explicit calculation of the left side shows that, in the equilateral
case,  
$$
|xE_4 + H| = \left(x + \frac{2}{9} \right) \; \left(x - \frac{4}{9}
\right) \; \left(x^2 - \frac{2}{9} x- \frac{8}{81} + \frac{1}{3}a
\right),  
$$
where 
\begin{equation*}
a = (m_1 m_2 + m_2 m_3 + m_1 m_3 ) \; (m_1 + m_2 + m_3)^{-2},
\tag{3.4.51}\label{chap3:eq3.4.51} 
\end{equation*}
and in the collinear case, 
$$
|xE_4 + H| = \left(x + \frac{2}{9} \right) \; \left(x - \frac{4}{9}
\right) \; \left(x+\frac{2}{9} + \frac{2}{9} b \right) \; \left(x -
\frac{4}{9} - \frac{4}{9} b \right),  
$$
where 
\begin{equation*}
b = \frac{m_1(1 + (1-\omega)^{-1} + (1-\omega)^2) + m_3 (1+
  \omega^{-1} + \omega^{-2})}{m_1 + m_2 (\omega^{-2} + (1-\omega)^{-2}
  ) + m_3}. \tag{3.4.52}\label{chap3:eq3.4.52} 
\end{equation*}
Thus we obtain
\begin{align*}
|zE_6 - B| & = \left(x-\frac{4}{9} \right) \left(x^2 - \frac{2}{9} x -
\frac{8}{81} + \frac{1}{3}a \right), \text{ in the equilateral
  case. }\\ 
& \quad \left(x-\frac{4}{9} \right) \left(x + \frac{2}{9} +
\frac{2}{9}b \right) \left(x-\frac{4}{9} -\frac{4}{9} b \right),
\text{ in the collinear case.} 
\end{align*}\pageoriginale 
It is clear from (\ref{chap3:eq3.4.51}) that $a$ is positive, and
since $0< \omega <1$, $b$ is also positive. The characteristic
polynomial of $B$ is a cubic in $x$ and we shall determine all the
roots. Consider first the equilateral case. Since $x= (z+\dfrac{1}{3})
\; (z -\dfrac{2}{3})$, we have 
$$
x - \frac{4}{9} = z^2  - \frac{1}{3} z - \frac{2}{3} = (z-1)
(z+\frac{2}{3}).  
$$
The roots of $x^2 - \dfrac{2}{9} x - \dfrac{8}{81} + \dfrac{1}{3} a =
0$ are $x =\dfrac{1}{9} \pm \dfrac{1}{3} \sqrt{1-3a}$, and hence we
get $z^2 - \dfrac{1}{3} z - \dfrac{1}{3} \mp \dfrac{1}{3} \sqrt{1-3a}
= 0$, of which the roots are $z =\dfrac{1}{6} \pm \dfrac{1}{6}
\sqrt{13 \pm 12\sqrt{1-3a}}$. Here $0 \leq 1-3a <1$ because we have
$2(1-3a) = ((m_1 - m_2)^2 + (m_2 - m_3)^2 + (m_3 -m_1)^2) (m_1 + m_2 +
m_3)^{-2} \geq 0$, and $1-3a = 0$ if and only if $m_1 = m_2 = m_3$. So
it follows that all the eigen values of the matrix $B$ are real and
are given by 
\begin{align*}
-a_o & = - \frac{2}{3}, -a_1 = + \frac{1}{6} - \frac{1}{6}
\sqrt{13+12\sqrt{1-3a}},\\ 
-a_2 &= \frac{1}{6} - \frac{1}{6} \sqrt{13-12 \sqrt{1-3a}}, \\
a_3 & = \frac{1}{6} + \frac{1}{6} \sqrt{13-12 \sqrt{1-3a}}, \\
a_4 &= \frac{1}{6} + \frac{1}{6} \sqrt{13+12 \sqrt{1-3a}}, a_5
=1. \tag{3.4.53}\label{chap3:eq3.4.53} 
\end{align*}
Since $0 \leq 1-3a <1$, we have $-a_2\geq -a_1 > - a_o$, and all these
are negative while $a_5 > a_4 \geq a_3 > 0$. All the six roots are
distinct except when $1-3a =0$, i.e. $m_1 = m_2 = m_3$, and then $a_1
=a_2$, $a_3 = a_4$. 

Next we consider the collinear case. As before we get the 3 equations 
$$
x - \frac{4}{9} = (z-1) (z+\frac{2}{3}) = 0, \; x + \frac{2}{9} +
\frac{2}{9} b = z^2 - \frac{1}{3} z + \frac{2}{9} b = 0, 
$$\pageoriginale
$x - \dfrac{4}{9} - \dfrac{4}{9} b = z^2 - \dfrac{1}{3} z
-\dfrac{2}{3} - \dfrac{4}{9} b = 0$. Hence the eigen values of the
matrix $B$ are $z = 1$, $z = -\dfrac{2}{3}$, $z \dfrac{1}{6} \pm
\dfrac{1}{6} \sqrt{1-8b}$, $z = \dfrac{1}{6} \pm \dfrac{1}{6} z =
\sqrt{25 + 16b}$. Here $1 - 8b < 1$ and $1-8b$ can be negative, and in
this case two of the eigen values are complex, complex conjugates of
each other. For instance, if $m_1 = m_3$, then $\omega = \dfrac{1}{2}$
and $1 - 80b \lesseqqgtr 0$ if $\dfrac{m_2}{m_1} \dfrac{55}{4}$. If
$8b$ 1, all the eigen values are real and distinct. If $8b =1$, all
the eigen values are real and there is a multiple root. Finally, then,
the eigen values of the matrix $B$ in the collinear case are given by: 
\begin{align*}
-b_0 & = -\frac{2}{3}, - b_1 = \frac{1}{6} -\frac{1}{6} \sqrt{25 +16
  b}, \; b_2 = \frac{1}{6} -\frac{1}{6} \sqrt{1-8b},\\ 
b_3 & = \frac{1}{6} + \frac{1}{+} \sqrt{1-8b}, \; b_4 = \frac{1}{6} +
\frac{1}{6} \sqrt{25+16b} , \; b_5
=1. \tag{3.4.54}\label{chap3:eq3.4.54} 
\end{align*}
It is clear that $-b_o$ and $-b_1$ are two negative roots and $-b_1 <
-b_o$. There are four distinct positive roots if $8b <1$, and four
positive roots, two of them equal $(b_2 = b_3 =\dfrac{1}{6})$ if $8b
=1$, and two positive roots and a pair of complex conjugate roots with
positive real parts if $8b > 1$. 

In order to see how to utilize the knowledge of the eigenvalues for a
study of the solutions of the system of equations
(\ref{chap3:eq3.4.36}), it is necessary to investigate in some detail
the theory of stability of solutions of systems of ordinary
differential equations. 

\section{Stability theory of solutions of differential
  equations}\label{chap3:sec5} 

We shall\pageoriginale now study the problem of the stability of the
solutions of a system of ordinary  differential equations of the first
order. Let $s$  be a real variable and $x_1, \ldots, x_m$ independent
real variables. If $f$ is a continuously differentiable function of
$s$, we denote the derivative of $f$ with respect to $s$ by $f'$. We
consider the system of $m$ ordinary differential equations of the
first order in $m$ unknown real functions $x_k = x_k (s)$, $k =1,
\ldots, m$, of the variable $s$: 
\begin{equation*}
x'_k = \sum\limits^m_{l=1} a_{kl} x_l + \varphi_k (x_1, \ldots, x_m),
k =1, \ldots, m, \tag{3.5.1}\label{chap3:eq3.5.1} 
\end{equation*}
where the $\varphi_k$ are power-series in the $m$ real variables $x_1,
\ldots, x_m$ with real coefficients and starting with quadratic
terms. The $a_{kl}$ are real constants and we assume that the
$\varphi_k$ converge for $|x_k|$ sufficiently small. Let $\xi_1,
\ldots \xi_m$ be given real numbers. We want to consider the problem
of finding all solutions $x_k = x_k(s)$, $k=1, \ldots, m$ of
(\ref{chap3:eq3.5.1}), taking the initial values $x_k(0) = \xi_k$ and
studying their behaviour as $s \to \infty$. This is nearly the same as
the equilibrium problem in mechanics, which is the following. Suppose
that we have a mechanical system whose motion is governed by the
system of equations (\ref{chap3:eq3.5.1}). Since the right side of
(\ref{chap3:eq3.5.1}) contains no constant terms, $x_k(s) \equiv 0$ is
a particular solution. The solution $x_k(s) \equiv 0$ is called an
{\em equilibrium solution} of (\ref{chap3:eq3.5.1}). If the right
sides of (\ref{chap3:eq3.5.1}) were power-series, possibly with
constant terms, then $x_k(s) = c_k$, where $c_k, k=1, \ldots,
m$,\pageoriginale are constants would be called an equilibrium
solution if the $c_k$ are a set of common zeros of the right
sides. However, one sees easily that by taking the variables $X_k =
x_k -c_k$ in place of $x_k$, one can reduce the system to one in the
new variables, which is of the same form as (\ref{chap3:eq3.5.1}),
such that $X_k \equiv 0$ is an equilibrium solution for the new
system. The solution $x_k = x_k (s)$, $k =1, \ldots, m$, of
(\ref{chap3:eq3.5.1}) with initial values $x_k(0) = \xi_k$ defines a
curve in $m$-dimensional Euclidean space, starting from the point
$(\xi_1, \ldots, \xi_m)$. The problem of equilibrium consists in
finding the behaviour of the solution when the initial values are
varied in a sufficiently small neighbourhood of $(\xi_1, \ldots,
\xi_m)$. Let $s_o$ be a large positive number such that the solutions
$x_k(s) $ of (\ref{chap3:eq3.5.1}) have the property that $|x_k(s)|$,
$k =1, \ldots, m$, are sufficiently small for $s \geq s_o$, so that
when these values are inserted in the power-series, the latter
converge. Since the right side of (\ref{chap3:eq3.5.1}) does not
contain the variable $s$ explicitly, the system remains unchanged if
$s$ is replaced by the variable  $s - s_o$. We may assume that $s$ has
been replaced by $s-s_o$ and we then consider solutions in the
half-line $s\geq 0$. 

We have the following definition of stability of the solutions of
(\ref{chap3:eq3.5.1}). If for a given neighbourhood $V$ of $0$ in
$m$-dimensional Euclidean space we can find a neighbourhood $W$ of 0
with $W \subset V$, such that for any point $(\xi_1, \ldots, \xi_m)$
in $W$, the solutions $x_k(s)$ of the system (\ref{chap3:eq3.5.1})
taking initial values $x_k(0) = \xi_k, k =1, \ldots, m$, exist for all
$s \geq 0$ and remain in the neighbourhood $V$ as $s \to \infty$, then
the equilibrium solution of (\ref{chap3:eq3.5.1}) is called {\em
  stable}. If the equilibrium is\pageoriginale not stable, we would
also like to find for what initial conditions the solutions tend to
zero as $s \to \infty$. The problem of stability of solutions of
ordinary differential equations was first discussed by Poincar\'e by
the method of power-series expansions, and independently by
Liapounoff. But neither gave a method for obtaining all stable
solutions. Bohl, and subsequently, Perron (Math. Zeit. (1928))
considered the problem of determining all stable solutions. Bohl
studied the problem of stability also for systems of differential
equations more general than the ones we consider, in the sense that
the $\varphi_k$ were assumed to be functions which satisfied certain
growth conditions. Perron's method was simpler. We shall, however,
give a treatment different from both these. 

Consider the ball $\sum\limits^m_{k=1} x^2_k < \epsilon$, where
$\epsilon$ is a sufficiently small positive number. Instead of finding
all solution of (\ref{chap3:eq3.5.1}) which are asymptotic to 0 as $s
\to \infty$, we consider the more general problem of finding all
solutions $x_k(s)$ of (\ref{chap3:eq3.5.1}) which, for all $s \geq 0$,
belong to the ball $\sum\limits^m_{k=1} x^2_k < \epsilon$. For this we
start by simplifying the linear terms on the right side of
(\ref{chap3:eq3.5.1}) by a suitably chosen linear substitution. Let $A
= (a_{kl})$ denote the $m$-rowed square matrix of the real
coefficients $a_{kl}, k, l = 1 , \ldots, m$, of the linear terms in
(\ref{chap3:eq3.5.1}) and we write (\ref{chap3:eq3.5.1}) in the vector
notation as 
\begin{equation*}
x' = A x + \varphi (x). \tag{3.5.2}\label{chap3:eq3.5.2}
\end{equation*}
We now apply the linear substitution
\begin{equation*}
x = Cy \tag{3.5.3}\label{chap3:eq3.5.3}
\end{equation*}\pageoriginale
to $x$, where $C$ is a real $m$-rowed square matrix with $|C| \neq
0$. Then the system (\ref{chap3:eq3.4.2}) is transformed into the
system 
\begin{equation*}
y' = C^{-1} A C y +\psi (y_1, \ldots, y_m),
\tag{3.5.4}\label{chap3:eq3.5.4} 
\end{equation*}
where $\psi$ is the column vector whose components $\psi_k$ are
power-series in the $m$ independent variables $y_1, \ldots, y_m$ with
real coefficients and starting with quadratic terms. Under the
substitution (\ref{chap3:eq3.5.3}), which is not necessarily
orthogonal, the ball $\sum\limits^m_{k=1} x^2_k < \epsilon$ is
transformed into a bounded domain in $m$-dimensional $y$-space. Since
$\epsilon > 0$ can be chosen sufficiently small, we may assume that
this transformed domain is contained in the $y$-sphere
$\sum\limits^m_{k=1} y^2_k < \epsilon$. We choose the substitution
(\ref{chap3:eq3.5.3}) in such a way that the matrix $C^{-1} A C$ is in
the normal form. (The reduction of a matrix to the normal form was
first done by Weierstrass in 1868; it is what has subsequently been
called the Jordan canonical form). Then the system of differential
equations is reduced to a simpler form. For the moment we consider
only the special case of (\ref{chap3:eq3.5.2}) containing only linear
terms on the right: 
\begin{equation*}
x' = Ax. \tag{3.5.5}\label{chap3:eq3.5.5}
\end{equation*}
If all the eigenvalues $\lambda_1, \ldots, \lambda_m$ of the matrix
$A$ are real and distinct, then the matrix $C^{-1} A C $ is a diagonal
matrix  
$$
\begin{pmatrix}
\lambda_1  & & 0\\
& \ddots & \\
0 & & \lambda_m
\end{pmatrix}
$$
and\pageoriginale (\ref{chap3:eq3.5.5}) is reduced to the simple form
\begin{equation*}
y'_k = \lambda_k y_k, \; k = 1, \ldots, m. \tag{3.5.6}\label{chap3:eq3.5.6}
\end{equation*}
This can be integrated immediatelyl to give the solution
\begin{equation*}
y_k = c_k e^{\lambda_k s}, \; k = 1, \ldots, m, \tag{3.5.7}\label{chap3:eq3.5.7}
\end{equation*}
where the $c_k$ are constants of integration. Since the substitution
(\ref{chap3:eq3.5.3}) is linear, it is clear that if $x_k (s) \to 0 (k
= 1, \ldots, m)$ as $s \to \infty$, then $y_k(s) \to 0 (k = 1, \ldots,
m)$ too as $s \to \infty$, and conversely. Since we seek solutions
which tend to zero as $s \to \infty$, we should have 
\begin{equation*}
c_k = 0 \text{ if } \lambda_k \geq 0 \text{ and } c_k \text{ arbitrary
  real if } \lambda_k < 0. \tag{3.5.8}\label{chap3:eq3.5.8} 
\end{equation*}
Hence we find that the general solution of (\ref{chap3:eq3.5.5}) which
goes to zero as $s \to \infty$ contains exactly the same number of
arbitrary independent real parameters as the number of negative
eigen-values of the matrix $A$. This motivates the conjecture that
this result can be generalized to the system (\ref{chap3:eq3.5.2}):
suppose that all the eigen-values of $A$ are real and distinct and
exactly $n$ of them are negative then there exist exactly $n$
independent arbitrary  real parameters in a general solution
asymptotic to zero. This, however, is not in general true. It is true
if zero is not an eigen-value of $A$. For the moment we shall consider
only the case in which $A$ has only real eigen-values. Later we shall
generalize the result to the case in which some eigen-values are even
complex. 

We proceed\pageoriginale to the following general theorem. 

\setcounter{subtheorem}{0}
\begin{subtheorem}\label{chap3:thm3.5.1}
Suppose that all the eigen-values of the real matrix $A$ area real,
distinct and different from zero. If there are exactly $n$, $0 \leq n
\leq m$, negative eigen-values, then a general solution of
(\ref{chap3:eq3.5.2}) which is such that $\sum\limits^m x^2_k(s) <
\epsilon$ for all $s \geq 0$ contains exactly $n$ independent real
parameters. 
\end{subtheorem}

\begin{proof}
Let $\lambda_1, \ldots, \lambda_m$ be the eigen-values of $A$ and
suppose that  
\begin{equation*}
\lambda_1 < 0, \ldots, \lambda_n < 0, \lambda_{n+1} > 0, \ldots, \lambda_m > 0. \tag{3.5.9}\label{chap3:eq3.5.9}
\end{equation*}
It may happen that $n=0$ or $n=m$. By a suitable choice of the
substitution (\ref{chap3:eq3.5.3}), we transform the system
(\ref{chap3:eq3.5.2}) into the system 
\begin{equation*}
y'_k = \lambda_k y_k + \psi_k (y_1 , \ldots, y_m), k = 1, \ldots, m,
\tag{3.5.10}\label{chap3:eq3.5.10} 
\end{equation*}
where the $\psi_k$ are power-series with real coefficients and
starting with quadratic terms in the variables $y_1, \ldots, y_m$ and
convergent for small values of $|y_k|$. We may assume that $\epsilon$
is so small that the $\psi_k$ converge in the $y$-sphere
$\sum\limits^m_{k=1} y^2_k < \epsilon$. In order to simplify the
system (\ref{chap3:eq3.5.10}) further, we introduce a non-linear
substitution of the form 
\begin{equation*}
u_k = y_k - F_k (y_1, \ldots, y_n), \; k = 1, \ldots, m, \tag{3.5.11}\label{chap3:eq3.5.11}
\end{equation*}
where the $F_k$ are power-series with real coefficients in the $n$
independent variables $y_1, \ldots, y_n$ only, starting with quadratic
terms and convergent\pageoriginale for small $|y_k|$. We remark that
it is essential for our method that the $F_k$ are functions of the
variables $y_1, \ldots, y_n$ alone. If $n=0$, then all $F_k \equiv 0$
and if $n=m$, then $F_k$ are power-series in all the variables $y_1,
\ldots, y_m$. It is immediately seen that the Jacobian matrix of the
transformation (\ref{chap3:eq3.5.11}) of the variables $y_1, \ldots,
y_m$ to the variables $u_1, \ldots, u_m$ at the origin is the identity
and hence the transformation is locally invertible at the origin. By
local inversion we obtain $y_k$ as power-series in the variables $u_1,
\ldots, u_m$. In fact, if one considers the substitution
(\ref{chap3:eq3.5.11}) only for $k = 1, \ldots,n$, it defines a
transformation of the variables $y_1, \ldots, y_n$ to $u_1, \ldots,
u_n$. We can express $y_k, k = n+ 1, \ldots, m$, in terms of $u_1,
\ldots, u_n$ by inserting in $F_k$ the values of $y_1, \ldots, y_n$ in
terms of $u_1, \ldots, u_n$ got by inversion from the first $n$
equations in (\ref{chap3:eq3.5.11}). Hence the inverse transformation
of (\ref{chap3:eq3.5.11}) has the same form. It is clear that if we
make two such substitutions in succession, then the composite of the
two is again such a substitution. So the substitutions of the form
(\ref{chap3:eq3.5.11}) form a group. 

Differentiating (\ref{chap3:eq3.5.11}) with respect to the variable
$s$, we get 
$$
u'_k = y'_k - \sum\limits^n_{l=1} F_{ky_l} y'_l.
$$
Substituting for $y'_k$ from (\ref{chap3:eq3.4.10}) we obtain
$$
u'_k = \lambda_k y_k + \psi_k(y_1, \ldots, y_m) - \sum\limits^n_{l=1}
F_{ky_l} (y_1, \ldots, y_n) (\lambda_l y_l + \psi_l (y)).  
$$
Once again it follows from (\ref{chap3:eq3.5.11}) that $y_k = u_k +
F_k, k =1, \ldots, m$, so that we have, for $k =1, \ldots, m$, 
\begin{equation*}
u'_k = \lambda_k u_k + \lambda_k F_k + \psi_k - \sum\limits^n_{l=1}
\lambda_l F_{ky_l} y_l - \sum\limits^m_{l=1} F_{ky_l} \psi_l,
\tag{3.5.12}\label{chap3:eq3.5.12} 
\end{equation*}\pageoriginale
where the terms on the right can all be considered as functions of
$u_1, \ldots, u_m$ after substituting for $y_1, \ldots, y_m$ the
values obtained by the inverstion of (\ref{chap3:eq3.5.11}). We set,
for $k = 1, \ldots,m$,  
\begin{align*}
& \chi_k (u_1, \ldots, u_m) = \lambda_k F_k (y_1, \ldots, y_n) +
  \psi_k (y_1,\ldots, y_m) - \\ 
& \quad - \sum\limits^n_{l=1} \lambda_l y_l F_{ky_l} (y_1, \ldots,
  y_n) - \sum\limits^{n}_{l=1} \psi_l (y_1, \ldots, y_m) F_{ky_l}
  (y_1, \ldots, y_n), \tag{3.5.13}\label{chap3:eq3.5.13} 
\end{align*}
so that we can write the system (\ref{chap3:eq3.5.12}) in the form 
\begin{equation*}
u'_k = \lambda_k u_k + \chi_k (u_1, \ldots, u_m) \; k =1 , \ldots,
m. \tag{3.5.14}\label{chap3:eq3.5.14} 
\end{equation*}
From the definitions (\ref{chap3:eq3.5.13}) of the functions $\chi_k$,
it is clear that they are power-series with real coefficients,
starting with quadratic terms in the variables $u_1, \ldots, u_m$, and
convergent for small $|u_k|$. The idea of the proof now is to try to
find the power-series $F_k (y_1, \ldots, y_n)$ in
(\ref{chap3:eq3.5.11}) in such a way that the power-series $\chi_k$
have simple forms. (If in (\ref{chap3:eq3.5.11}) we took $F_k$ as
power-series in all the variables $y_1, \ldots, y_m$, then we could
secure $\chi_k \equiv 0$, but it would be difficult to prove the
convergence of $F_k$. However, with our choice of $F_k$, the $\chi_k$
may not all vanish identically, but the proof of convergence would be
simpler). We take for $F_k$ power-series in $y_1, \ldots, y_n$ with
undetermined coefficients and try to find the coefficients in such a
way that every term in $\chi_k$ contains at least one of the variables
$u_{n+1}, \ldots, u_m$ as a factor. That is, for $u_{n+1} = \ldots =
u_m = 0$, we have  
\begin{equation*}
\chi_k (u_1, \ldots, u_n , \; 0, \ldots, 0) \equiv 0, k =1 , \ldots,
m. \tag{3.5.15}\label{chap3:eq3.5.15} 
\end{equation*}\pageoriginale
In other words, we seek power-series $F_k (y_1, \ldots, y_n)$ so that
the substitution (\ref{chap3:eq3.5.11}) leads to the identity
(\ref{chap3:eq3.5.15}) for all $u_1, \ldots, u_n$ when we put $u_{n+1}
= \ldots = u_m = 0$ in the power-series $\chi_k$. We shall show that,
under an additional condition, the power-series $F_k$ are uniquely
determined by the requirement (\ref{chap3:eq3.5.15}). Then the system
of differential equations (\ref{chap3:eq3.5.14}) takes a simpler form
in which it can be integrated directly. We shall later prove the
convergence of the power-series $F_k$ thus obtained. 

Since $\epsilon > 0$ can be chosen as small as we want, the
neighbourhood $\sum\limits^m_{k=1} y^2_k < \epsilon$ of $y_1 = 0,
\ldots, y_m = 0$ is transformed by the substitution
(\ref{chap3:eq3.5.11}) into a neighbourhood of $u_1=0, \ldots,
u_m=0$. Once again we may assume that this transformed neighbourhood
is contained in the ball $\sum\limits^m_{k=1} u^2_k < \epsilon$. 

The conditions $u_{n+1} = 0, \ldots, u_m=0$ mean that $y_{n+1},\ldots,
y_m$ satisfy the relations 
\begin{equation*}
y_k = F_k (y_1, \ldots, y_n), \;k = n +1 , \ldots,
m. \tag{3.5.16}\label{chap3:eq3.5.16} 
\end{equation*}
Substituting these power-series with undetermined coefficients in the
expression (\ref{chap3:eq3.5.13}) defining $\chi_k$ as a power-series
in the variables $y_1, \ldots, y_n$, we get the functions 
$$
\chi_k (u_1, \ldots, u_n, 0 , \ldots, 0) \; k = 1, \ldots, m,
$$
where $u_l$ denotes the power-series $u_l(y_1, \ldots, y_n, F_{n+1}
(y_1, \ldots, y_n),\ldots,\break F_m(y_1, \ldots, y_n)) $, $l = 1,
\ldots, n$, in the variables $y_1, \ldots, y_n$. Since all the
power-series\pageoriginale $F_k$ and $\chi_k$ start with quadratic
terms, it follows that\break $\chi_k(u_1, \ldots, u_n , 0, \ldots,  0)$ is a
power-series starting with quadratic terms in the variables $y_1,
\ldots, y_n$. We thus get power-series in $y_1, \ldots, y_n$ and these
can be replaced, by local inversion of the substitution
(\ref{chap3:eq3.5.11}), by series in $u_1, \ldots, u_m$. It is however
not necessary to use the inversion of (\ref{chap3:eq3.5.11}), and so
not necessary to use the variables $u_k$ at all. Instead, one can
direectly consider the condition (\ref{chap3:eq3.5.15}) to hold
identically as power-series in $y_1, \ldots, y_n$. This implies
certain polynomial relations for the coefficients of $F_k$. The
coefficients can then be determined by induction from these relations,
in the following way. 

Let $g$ be an integer $\geq 2 $. Suppose that the coefficients of all
terms of total degrees $2,3, \ldots, g-1$ in $F_k(k = 1, \ldots,m)$
have been determined. Then we show that the coefficients of the terms
of total degree $g$ can be determined. Consider a term of total degree
$g$  in $F_k$, of the form  
\begin{equation*}
c_k y^{g_1}_1 \ldots y^{g_n}_n, \tag{3.5.17}\label{chap3:eq3.5.17}
\end{equation*}
where $g_1 , \ldots, g_n$ are non-negative integers such that $g_1 +
\ldots + g_n = g \geq 2$. The (real) coefficients $c_k$ in
(\ref{chap3:eq3.5.17}) is determined by equating to zero the
coefficient of the term $y^{g_1}_1 \ldots y^{g_n}_n$ in the
power-series $\chi_k$ obtained on replacing $y_l, l = n + 1, \ldots,
m$, by the power-series $F_l$. Then $c_k g_l y^{g_1}_1 \ldots y^{g_l
  -1}_1 \ldots  y^{g_n}_n $ is a term of total degree $g-1$ in
$F_{ky_l}$, so that the coefficient of $y^{g_1}_1 \ldots y^{g_n}_n$ in
$y_l F_{ky_l}$ is $c_k g_l$. Then in view of (\ref{chap3:eq3.5.13})
the\pageoriginale condition (\ref{chap3:eq3.5.15}) implies that 
\begin{equation*}
c_k(\lambda_k - \sum\limits^n_{l=1} g_l \lambda_l) = \text{
  coefficient of } y^{g_1}_1 \ldots y^{g_n}_n \text{ in } (-\psi_k +
\sum\limits^n_{l=1} F_{ky_l}
\psi_l). \tag{3.5.18}\label{chap3:eq3.5.18} 
\end{equation*}
Since $\psi_k$ starts with quadratic terms in $y_1, \ldots, y_n$ and 
$$
\psi_k (y_1, \ldots, y_m) = \psi_k (y_1, \ldots, y_n, F_{n+1}, \ldots,
F_m) , \; k = 1, \ldots, m,  
$$
where again $F_l, l = n +1, \ldots, m$, are power-series starting with
quadratic terms in $y_1, \ldots, y_n$, the coefficient of $y^{g_1}_1
\ldots y^{g_n}_n$ in $\psi_k$ involves only the coefficients in $F_k$
of total degrees $2,3, \ldots, g-1$, which are known by the induction
assumption, and is actually a polynomial in these known coefficients
of total degrees $\leq g -1$. The same is the case with the
coefficient of $y^{g_1}_1 \ldots y^{g_n}_n$ in $\sum\limits^{n}_{l=1}
F_{ky_l} \psi_l$. Thus the right side of (\ref{chap3:eq3.5.18}) is
known by the induction assumption. 

In the particular case $g =2$, we have only to consider the
contribution from the quadratic terms in $-\psi_k$, because $F_{ky_l}
\psi_l$ is a power-series starting with cubic terms in the variables
$y_1, \ldots, y_n$. Moreover, the quadratic terms in $-\psi_k$ in the
variables $y_1, \ldots, y_n$ give contributions only from the terms
$y_p y_q$, $p,q = 1, \ldots, n$, and not from the terms involving $y_p
F_q$, $p,q = n+1, \ldots, m$. Hence in this case the right side of
(\ref{chap3:eq3.5.18}) is completely determined by the coefficients of
$\psi_k$ itself, and hence we can start the induction. 

The coefficients\pageoriginale $c_k$ can then be determined from
(\ref{chap3:eq3.5.18}) whenever 
\begin{equation*}
\lambda_k \neq \sum\limits^n_{l=1} g_l \lambda_1, \; k  = 1, \ldots,
m, \tag{3.5.19}\label{chap3:eq3.5.19} 
\end{equation*}
where $g_l$ are non-nagative integers with $g_1 + \ldots + g_n = g
\geq 2$. We see that (\ref{chap3:eq3.5.19}) is actually only a finite
set of conditions to be satisfied by $\lambda_1, \ldots,
\lambda_m$. In fact, $-\lambda_l > 0$, $1 = 1, \ldots, n$, implies
that $-\sum\limits^n_{l=1} g_l \lambda_l \to \infty$ as $g \to
\infty$. This means that for integers $g_1, \ldots, g_n \geq 0$ such
that $g_1 + \ldots + g_n = g\geq g_o$, where $g_o$ is sufficiently
large, the condition (\ref{chap3:eq3.5.19}) is automatically
satisfied, and so we need assume (\ref{chap3:eq3.5.19}) only for a
finite set of $n$-tuples of integers $g_1, \ldots, g_n$, all
non-negative, with $2 \leq g <g_o$, $k =1 , \ldots,n$. On the other
hand, if $k = n+1, \ldots, m$, we know that $\lambda_k > 0$ and
$-\sum\limits^n_{l=1} g_l \lambda_l > 0$, so that
(\ref{chap3:eq3.5.19}) is always satisfied. Then it follows that all
the coefficients in the power-series $F_k$ can be determined by
induction, provided that the finite set of conditions
(\ref{chap3:eq3.5.19}) holds. (If the condition (\ref{chap3:eq3.5.19})
does not hold, there are complications which, however, can be
overcome, as we shall show later). 

We remark that if we had taken $F_k$ to be power-series in all the
variables $y_1, \ldots, y_m$, then $\lambda_k - \sum\limits^m_{l=1}
g_l \lambda_l$ would become arbitrarily small if $0 < n <m$, and so we
would have arbitrarily small denominators for the coefficients $c_k$
to be determined from (\ref{chap3:eq3.5.18}), which would make the
proof of convergence more difficult. 


We shall\pageoriginale prove the convergence of the power-series $F_k$
obtained above later by the Cauchy method of majorants. We proceed
with the proof of the theorem assuming for the moment the convergence
of the power-series $F_k$ for sufficiently small $|y_l|, l = 1,
\ldots, n$. 

We shall now determine the general solution of the system of
differential equations (\ref{chap3:eq3.5.14}): 
$$
u'_k = \lambda_k u_k + \chi_k, \; k =1 , \ldots, m,
$$
where the power-series $\chi_k$ are determined by
(\ref{chap3:eq3.5.13}) after inserting the power-series $F_k = F_k
(y_1 (u), \ldots, y_n (n))$, $k = n +1 , \ldots, m$, obtained above;
we seek only solutions $u_k$ such that $\sum\limits^m_{k=1} u^2_k <
\epsilon$ for all $s \geq 0$. First we show that $u_k \equiv 0$ for $k
= n+ 1, \ldots, m$. For this we set 
\begin{equation*}
v =\sum\limits^{m}_{k=n+1} u^2_k. \tag{3.5.20}\label{chap3:eq3.5.20} 
\end{equation*}
If $n=m$, this sum is empty and there is nothing to prove. So let $n <
m$. Now, if $u_k = u_k(s)$ are solutions of the system $u'_k =
\lambda_k u_k + \chi_k$, $k = n+1, \ldots, m$, then we obtain for $v$
the differential equation 
$$
v' = 2 \sum\limits^m_{k=n+1} u_k u'_k = 2 \sum\limits^m_{k=n+1}
\lambda_k u^2_k + 2 \sum\limits^m_{k=n+1} u_k \chi_k.  
$$
We estimate the right side from below. Let  $\lambda = \min
(\lambda_{n+1}, \ldots, \lambda_m)$. Then $\lambda > 0$ and $2
\sum\limits^m_{k=n+1} \lambda_k u^2_k \geq 2 \lambda v$. Since $\chi_k
(u_1, \ldots, u_n, \; 0, \ldots,0) \equiv 0$ identically in $u_1,
\ldots, u_n$, and $\chi$ starts with quadratic terms, at least one
of\pageoriginale $u_{n+1}, \ldots, u_m$ occurs as a factor in each
term of $\chi_k(u_1, \ldots, u_m)$. Since $u_k$ is real, $u^2_k \leq
v$, $k = n+1, \ldots, m$, and so $|u_k| \leq \sqrt{v}$. Consequently,
each term of $u_k \chi_k$, $k = n+1, \ldots, m$, has a factor of the
form $u_p, u_q$, $p,q = n +1, \ldots, m$, and hence of absolute value
$\leq v$ whereas the remaining factor of the term is a product of
powers of $u_1,\ldots, u_m$, at least of total degree 1. As the
$\chi_k$ are uniformly convergent, one can choose $\epsilon > 0$ so
small that $\sum\limits^m_{k=1} u^2_k < \epsilon$ implies $|2
\sum\limits^m_{k=n+1} u_k  \chi_k| \leq \lambda v$. Hence, in
particular, we have $2 \sum\limits^m_{k=n+1} u_k \chi_k \geq - \lambda
v$, so that we obtain the differential inequality $v'\geq \lambda v$
and hence $(v e^{-\lambda s})' \geq 0$. So $v e^{-\lambda s}$ is a
non-decreasing function of $s$ in $s \geq 0$. As $s \to \infty$,
$v(s)$ remains bounded since $v(s) \leq \sum\limits^{m}_{k=1} u^2_k <
\epsilon$. But $\lambda > 0$, so $e^{-\lambda s}$ and hence
$ve^{-\lambda s} \to 0$ as $s \to \infty$. Since $ve^{-\lambda s}$ is
non-negative and non-decreasing, we should have $ve^{-\lambda s}
\equiv 0$ and hence $v \equiv 0$. This means that $u_k(s) = 0$, $k=
n+1, \ldots, m$, which proves our assertion. 

Now in view of (\ref{chap3:eq3.5.13}), then system
(\ref{chap3:eq3.5.14}) reduces to $u'_k = \lambda_k u_k$, $k=1,\ldots,
n$. On integration we obtain a general solution of
(\ref{chap3:eq3.5.14}) with $\sum\limits^m_{k=1} u^2_k < \epsilon$ and
this is given by 
$$
u_k = c_k e^{\lambda_k s}, \; k =1, \ldots, m; \; c_k = 0, \; k = n+1,
\ldots, m.  
$$
Since $c_k = u_k(0)$, it follows that $\sum\limits^n_{k=1} c^2_k
<\epsilon$. Conversely, given the initial conditions $u_k(0) = c_k$
with $\sum\limits^n_{k=1}c^2_k < \epsilon$, since $\lambda_k < 0$ for
$k = 1, \ldots, n$, any general solution of (\ref{chap3:eq3.5.14})
necessarily satisfies $\sum\limits^m_{k=1} u^2_k <
\epsilon$.\pageoriginale On the other hand, by the uniqueness of the
solutions of systems of differential equations with  prescribed
initial conditions, we see that, given $c_k = u_k(0)$ with
$\sum\limits^n_{k=1} c^2_k < \epsilon$, we have determined the unique
solution of (\ref{chap3:eq3.5.14}) with these initial values
asymptotic to zero. Thus we have determined all solutions of
(\ref{chap3:eq3.5.14}) with the property that $\sum\limits^m_{k=1}
u^2_k <\epsilon$ for all $s \geq 0$. Going back to the differential
equations satisfied by the unknown functions $y_k(s)$ by means of the
inverse of the substitution (\ref{chap3:eq3.5.13}) we can express
$y_k$ in the form $y_k = u_k + G_k (u_1, \ldots, u_n)$, $k =1 ,
\ldots, m$, where the $G_k$ are power-series in $u_1, \ldots, u_m$
without linear terms. In fact, let $b_k$ denote the initial values of
$y_k$, $k = 1, \ldots, m$. Then $u_k(0) = 0$, $k = n+1, \ldots,m$, is
equivalent to $b_k = F_k (b_1,\ldots, b_n)$, $k = n + 1, \ldots,
m$. We can choose $b_1, \ldots, b_n$ arbitrarily with the only
condition that if $c_k = b_k - F_k (b_1, \ldots, b_n)$, $k = 1,
\ldots, n$, then $\sum\limits^n_{k=1} c^2_k < \epsilon$. Thus the
initial values $b_1, \ldots, b_m$ have to satisfy $m-n$ conditions
$b_k = F_k(b_1, \ldots, b_n)$, $k =1, \ldots, m$. If we prove that the
$F_k$ are convergent power-series, then it follows that the initial
values for $y_k(s)$ satisfy $m-n$ analytical relations. So the
solutions $y_k(s)$ with $\sum\limits^m_{k=1} y^2_k < \epsilon$ for all
$s \geq 0$ lie on an $(m-n)$-dimensional analytic manifold defined by
the equations $y_k = F_k (y_1, \ldots,y_n)$, $k =n+1, \ldots, m$, and
we have a (local) parametric representation for this manifold. The
solutions are given explicitly by  
\begin{align*}
y_k & = c_k e^{\lambda_k s} + G_k (c_1 e^{\lambda_1 s}, \ldots, c_n
e^{\lambda_ns}),\; k=1, \ldots, n;\\ 
y_k & = G_k (c_1 e^{\lambda_1s} , \ldots, c_n e^{\lambda_ns}), \; k =
n+ 1, \ldots, m. 
\end{align*}\pageoriginale
Finally we go back to the variables $x_k$ by means of the inverse of
the linear substitution $x = C_y$, $|C| \neq 0$. We see therefore that
if $a_1, \ldots, a_m$ denote the initial values $x_1(0), \ldots,
x_m(0)$ of the solution asymptotic, as $s\to \infty$, to the
equilibrium solution of the system (\ref{chap3:eq3.5.1}), then $a_k$
also satisfy $m-n$ analytic relations. Thus the solutions $x_k(s)$, $k
=1, \ldots, m$, asymototic to the equilibrium solution fill an
$(m-n)$-dimensional analytic manifold in $m$-dimensional Euclidean
space. The general solution then involves $n$ real parameters and we
have proved the theorem, but for the convergence of the power-series,
subject to the condition (\ref{chap3:eq3.5.18}). 

Now we shall proceed to prove the convergence of the power-series
$F_k$ by Cauchy's method of majorants. We have used this method
earlier to prove Theorem \ref{chap1:eq1.3.1}. It is a little more
difficult in the present case, since the equations
(\ref{chap3:eq3.5.13}) defining $\chi_k$ involve the partial
derivatives $F_{ky_l}$ and hence the condition (\ref{chap3:eq3.5.15})
will give a system of partial differential equations for $F_k$, $k=1,
\ldots, m$. For determining the power-series $F_k$, we use the
condition (\ref{chap3:eq3.4.15}) leading to  
\begin{align*}
\lambda_k F_k & - \sum\limits^n_{l=1} \lambda_l y_l F_{ky_l} = -
\psi_k(y_1, \ldots, y_n, \; F_{n+1}, \ldots, F_m)  + \\ 
& + \sum\limits^n_{l=1} F_{ky_l} (y_1, \ldots, y_n) \psi_l (y_1,
\ldots, y_n, F_{n+1},\ldots, F_m); \tag{3.5.21}\label{chap3:eq3.5.21} 
\end{align*}\pageoriginale 
which implies recurrence relations for the coefficients in $F_k, k
=1,\break 
\ldots, m$. Comparing the coefficients of a typical term $c_k
y^{g_1}_1 \ldots y^{g_n}_n$ of total degree $g = g_1 + \ldots + g_n
\geq 2$, we obtained the relation (\ref{chap3:eq3.5.18}), which may be
re-written as  
\begin{equation*}
c_k (\lambda_k - \sum\limits^n_{l=1} g_l \lambda_l) = \left\{-\psi_k +
\sum\limits^n_{l=1} \psi_l F_{ky_l} \right\}_{g_1 \ldots g_n} (k=1,
\ldots, m), \tag{3.5.22}\label{chap3:eq3.5.22} 
\end{equation*}
where $\{f\}_{g_1, \ldots, g_n}$ stands for the coefficient of the
term $y^{g_1}_1 \ldots y^{g_n}_n$ in the power-series $f$ in $y_1,
\ldots, y_n$. Under the assumption (\ref{chap3:eq3.5.19}), the
coefficients in $F_k$ are determined recursively by
(\ref{chap3:eq3.5.22}). In order to obtain majorants for $F_k$, we
estimate $c_k$ in the following way. From (\ref{chap3:eq3.5.22}) we
have 
\begin{equation*}
|c_k|| \lambda_k - \sum\limits^n_{l=1} g_l \lambda_l | = \left|
\left\{-\psi_k + \sum\limits^n_{l=1} \psi_1 F_{ky_l} \right\}_{g_1
  \ldots g_n} \right|, k =1 , \ldots
m. \tag{3.5.23}\label{chap3:eq3.5.23} 
\end{equation*}
Let $\alpha = \min (-\lambda_1, \ldots, -\lambda_n)$. Then $\alpha >
0$ and we can write  
$$
\lambda_k - \sum\limits^n_{l=1} \lambda_l g_l \geq \lambda_k +
\alpha(g_1+ \ldots g_n) = \lambda_k + \frac{\alpha}{2} (g_1 + \ldots +
g_n) + \frac{\alpha}{2} (g_1 + \ldots + g_n). 
$$
For sufficiently large $g = g_1 + \ldots + g_n$, we have $\lambda_k +
\dfrac{\alpha}{2}g > 0$ and hence for such $g_1, \ldots, g_n$,
$\lambda_k - \sum\limits^n_{l=1} g_l \lambda_l > \dfrac{\alpha}{2}
(g_1 + \ldots + g_n)$. Since by (\ref{chap3:eq3.5.19}), $\lambda_k -
\sum\limits^n_{l=1} g_l \lambda_l \neq 0$ and $\lambda_k +
\dfrac{1}{2} \alpha g \leq 0$ for only finitely many $n$-tuples $(g_1,
\ldots, g_n)$, we can find a sufficiently large positive constant
$\gamma_1$ such that always 
$$
|\lambda_k - \sum\limits^n_{l=1} g_l \lambda_l| > \gamma^{-1}_1 g, \;
k =1, \ldots, m. 
$$\pageoriginale
We shall hereafter denote by $\gamma_2, \gamma_3, \ldots$ sufficiently
large positive constants. It follows now from (\ref{chap3:eq3.5.23})
that  
\begin{equation*}
|c_k | (g_1+ \ldots + g_n) \leq |\gamma_1 \left\{-\psi_k +
\sum\limits^n_{l=1} \psi_l F_{ky_l} \right\}_{g_1 \ldots g_n}|, \; k =
1, \ldots, m . 
\tag{3.5.24}\label{chap3:eq3.5.24}
\end{equation*}
We know that $\psi_k$ are power-series starting with quadratic terms
and converging in a complex neighbourhood of $y_1 =0, \ldots, y_m =
0$, say, $|y_1| \leq \rho_1, \ldots , |y_m| \leq \rho_m$. Suppose that
$|\psi_k| \leq \gamma_2$ in this region, $k=1, \ldots, m$. If $h_1,
\ldots, h_m$ are non-negative integers, then by Cauchy's formula 
$$
\left| \left\{ \psi_k\right\}_{h_1 \ldots h_m}\right| \leq \gamma_2
\rho^{-h_1}_1 \ldots\rho^{-h_m}_m.  
$$
So we can write
$$
\psi_k \prec \sum\limits_{h_1 + \ldots + h_m \geq 2} \gamma_2
\frac{y^{h_1}_1 \ldots y^{h_m}_m}{\rho^{h_1}_1 \ldots \rho^{h_m}_m}.  
$$
If $h = h_1 + \ldots h_m$, then 
\begin{equation*}
\psi_k \prec \sum\limits^\infty_{h=2} \gamma_2 \sum\limits_{h_1 +
  \ldots + h_m = h} \left(\frac{y_1}{\rho_1} \right)^{h_1} \ldots
\left(\frac{y_m}{\rho_m} \right)^{h_m} \prec \sum\limits^\infty_{h=2}
\gamma_2 \left(\frac{y_1}{\rho_1} + \ldots \frac{y_m}{\rho_m}
\right)^h. \tag{3.5.25}\label{chap3:eq3.5.25} 
\end{equation*}
Let $\gamma^{-1}_3 = \min (\rho_1, \ldots, \rho_m)$. Then $\gamma_3 >
0$ and 
$$
\frac{y_1}{\rho_1} + \ldots + \frac{y_m}{\rho_m} \prec \gamma_3 (y_1 +
\ldots + y_m), 
$$
so that the right side of (\ref{chap3:eq3.5.25}) is majorized by
$\gamma_2 \sum\limits^\infty_{h=2} \gamma^h_3 (y_1 + \ldots + y_m)^h$,
which\pageoriginale is the formal power-series  
$$
\gamma_2 \frac{\gamma^2_3 (y_1 + \ldots + y_m)^2}{1-\gamma_3(y_1 +
  \ldots y_m)} \equiv \Psi (y_1 , \ldots, y_m), \text{ say.} 
$$
Then $\psi_k \prec \Psi$, $k =1 , \ldots, m$. Let $F^*_k$ denote the
power-series obtained by replacing the coefficients in $F_k$ by their
absolute values. Now substituting $\Psi$ for each $\psi_k$ and $F^*_k$
for $F_k$ we obtain a majorant for the power-series on the right side
of (\ref{chap3:eq3.5.21}). In fact, since the coefficients in $F^*_k$
are non-negative, the coefficients in $F^*_{ky_l}$ are also
non-negative and hence the right side of (\ref{chap3:eq3.5.24}) is
majorized by 
$$
\left\{ \gamma_4(1+ \sum\limits^n_{l=1} F^*_{ky_l} (y_1, \ldots, y_n))
\Psi (y_1, \ldots, y_n, F^*_{n+1}, \ldots, F^*_{m})\right\}_{g_1
  \ldots g_n},  
$$
which implies that  
$$
|c_k| (g_1+ \ldots g_n) \leq \left\{ \gamma_4 \left(1+
\sum\limits^n_{l=1} F^*_{ky_l} \right) \Psi \left(y_1, \ldots, y_n ,
F^*_{n+1}, \ldots, F^*_m \right)\right\}_{g_1 \ldots g_n}. 
$$
Since $|c_k|$ is the coefficient of $y^{g_1}_1 \ldots y^{g_n}_n$ in
$F^*_k$, we see that $|c_k| g_l$ is the coefficient of $y^{g_1}_1
\ldots y^{g_1 -1}_l \ldots y^{g_n}_n$ in $F^*_{ky_l}$, and hence of
$y^{g_1}_1 \ldots y^{g_n}_n$ in $y_l F^*_{ky_l}$. In other words,  
$$
\left\{ \sum\limits^n_{l=1} y_l F^*_{ky_l} \right\}_{g_1 \ldots g_n} =
|c_k| (g_1 + \ldots + g_n), 
$$
and thus we get the majorization
\begin{equation*}
\sum\limits^n_{l=1} y_l F^*_{ky_l} \prec \gamma_4 \left(1+
\sum\limits^n_{l=1} F^*_{ky_l} \right) \Psi (y_1, \ldots, y_n ,
F^*_{n+1}, \ldots, F^*_m). \tag{3.5.26}\label{chap3:eq3.5.26} 
\end{equation*}\pageoriginale
Let $G_1, \ldots, G_m$ be $m$ power-series with non-negative
undetermined coefficients, starting with quadratic terms, in the
variables $y_1, \ldots, y_n$, satisfying the $m$ partial differential
equations 
\begin{align*}
\sum\limits^{n}_{l=1} y_l G_{ky_l} &= \gamma_4 \left(1+
\sum\limits^n_{l=1} G_{ky_l} \right) \Psi (y_1, \ldots, y_n, \;
G_{n+1} , \ldots, G_m),\\  
&\qquad\qquad k = 1, \ldots, m. \tag{3.5.27}\label{chap3:eq3.5.27}
\end{align*}
Let $d_k = \left\{G_k \right\}_{g_1 \ldots g_n}$. Then comparing the
coefficients of $y^{g_1}_1 \ldots y^{g_n}_n$ on both sides of
(\ref{chap3:eq3.5.27}), we have 
\begin{equation*}
d_k (g_1 + \ldots g_n) = \gamma_4  \left\{(1+ \sum\limits^n_{l=1}
G_{ky_l}) \Psi (y_1, \ldots, y_n , \; G_{n+1} , \ldots, G_m)
\right\}_{g_1 \ldots g_n}\tag{3.5.28}\label{chap3:eq3.5.28} 
\end{equation*}
As before, the relation (\ref{chap3:eq3.5.28}) is a recurrence
relation for determining the coefficients $d_k$ of $G_k$. For $g =2$,
the right side of (\ref{chap3:eq3.5.28}) contains only coefficients of
quadratic terms of the form $y_p y_q$, $p, q= 1, \ldots, n$, alone in
$\Psi$, since $\Psi$ starts with quadratic terms so that all the terms
of $G_{ky_l} \Psi$, $y_p G_q$, $p =1, \ldots, n$; $q = n +1, \ldots,
m$, or $G_p G_q$, $p,q=n+1, \ldots, m$, are of total degree $\geq
2$. Hence the right side of (\ref{chap3:eq3.5.28}) is known in this
case, and therefore also the coefficients of the quadratic terms in
$G_k$. For $g>2$, we see, as in the determination of the coefficients
of $F_k$, that the right side of (\ref{chap3:eq3.5.28}) involves
coefficients in $G_k$ of terms of total degrees $2, \ldots, g-1$,
which are already determined, and thus all the coefficients in $G_k$
are uniquely determined by induction. So the power-series $G_k$ are
uniquely determined by (\ref{chap3:eq3.5.27}). 

We next\pageoriginale prove by induction that $F^*_k \prec G_k$, $k=1,
\ldots, m$. If $g= g_1 + \ldots + g_n =2$, then we see immediately
from (\ref{chap3:eq3.5.26}) that  
\begin{align*}
2 \left\{F^*_k \right\}_{g_1 \ldots g_n} & \leq \left\{\gamma_4
\left(1+\sum\limits^n_{l=1} F^*_{ky_l} \right) \Psi (y_1 , \ldots,
y_n, \; F^*_{n+1}, \ldots, F^*_m) \right\}_{g_1 \ldots g_n}\\ 
& = \left\{ \gamma_4 \Psi (y_1 ,\ldots, y_n, F^*_{n+1}, \ldots,
F^*_m)\right\}_{g_1 \ldots g_n}. 
\end{align*}
But by the above construction, the right side is precisely the
coefficient $2\{G_k\}_{g_1 \ldots g_n}$. Suppose that for all
non-negative integers $g_1, \ldots, g_n$ with $2 \leq g_1 + \ldots +
g_n \leq g-1$ we have 
\begin{equation*}
\{F^*_k\}_{g_1 \ldots g_n} \leq \{G^*_k\}_{g_1 \ldots g_n}, \; (k=1,
\ldots, m). \tag{3.5.29}\label{chap3:eq3.5.29} 
\end{equation*}
Then we shall show that for all non-negative integers $h_1, \ldots,
h_n$ with $h_1 + \ldots + h_n = g$, we have  
$$
\{F^*_k\}_{h_1 \ldots h_n} \leq \{G^*_k\}_{h_1 \ldots h_n}. 
$$
By (\ref{chap3:eq3.5.36}) we have
\begin{align*}
&\{F^*_k\}_{h_1 \ldots h_n} (h_1 + \ldots + h_n) \leq\\ 
&\qquad\left\{\gamma_4 \left(1+ \sum\limits^n_{l=1} F^*_{ky_l} \right) 
  \Psi (y_1, \ldots, y_n, F^*_{n+1}, \ldots, F^*_m) \right\}_{h_1
    \ldots h_n}.  
\tag{3.5.30}\label{chap3:eq3.5.30}
\end{align*}
On the right side only coefficients of terms of total degree $\leq g
-1$ occur, and for these coefficients, (\ref{chap3:eq3.5.29})
holds. In other words, the right side of (\ref{chap3:eq3.5.50}) is
majorized by the coefficient 
$$ 
\left\{\gamma_4 \left(1+ \sum\limits^n_{l=1} G_{ky_l} \right) \Psi
(y_1, \ldots, y_n, \; G_{n+1}, \ldots, G_m) \right\}_{h_1 \ldots h_n}.  
$$
But by\pageoriginale the construction of the power-series $G_k$, this
is equal to $\{G_{k}\}_{h_1 \ldots h_n}\break (h_1 + \ldots + h_n)$ and
this, by induction, proves our assertion. So in order to prove the
convergence of $F^*_k$, and hence of $F_k$, it is enough to prove the
convergence of $G_k$. 

It is easy to see that $G_1 = \ldots = G_n$. In fact, consider the
power-series $G$ with non-negative undetermined coefficients starting
with qua\-dratic terms, satisfying the partial differential equation 
\begin{equation*}
\sum\limits^n_{l=1} y_l G_{y_l} = \gamma_4 \left( 1+
\sum\limits^n_{l=1} G_{y_l}\right) \Psi (y_1, \ldots, y_n, G, \ldots,
G). \tag{3.5.31}\label{chap3:eq3.5.31} 
\end{equation*}
The coefficients of $G$ can be determined by induction as in the case
of $G_k$. We have already remarked that the coefficients of the
quadratic terms are the same in all the $G_k$ and they are obtained by
the contributions from the quadratic terms of the type $y_p y_q$, $p,
q =1, \ldots, n$, in $\Psi$ alone, and that there is no contribution
either from terms of the type $y_pG_q, p = 1, \ldots, n; q = n+ 1,
\ldots,m$, or from terms of the type $G_p G_q$, $p, q = n+1,\ldots,
m$. But these are exactly the coefficients of the corresponding
quadratic terms of $G$. Then using the recurrence formula
(\ref{chap3:eq3.5.28}), we see by induction that all the corresponding
coefficients of $G_k$ are equal, and equal to those of $G$. Hence, $G
= G_1 = \ldots = G_m$ is uniquely determined by
(\ref{chap3:eq3.5.31}). 

If we set $y_1 = \ldots = y_n = y$ in the power-series $G(y_1, \ldots,
y_n)$, we obtain a power-series with non-negative coefficients in one
variable $y$, starting with quadratic term; we shall denote this by
$H(y)$. If $H(y)$  converges\pageoriginale for some positive value of
$y$, then it is clear that $G(y_1 , \ldots, y_n)$ converges for $|y_1|
< y, \ldots , |y_n| < y$. In fact, 
$$
\left|\left\{ G(y_1, \ldots, y_n)\right\}_{g_1 \ldots g_n}  \right|
\leq \{H(y)\}_{g_1+ \ldots + g_n}.  
$$
Since $H(y) =G(y, \ldots, y)$, it follows from (\ref{chap3:eq3.5.31})
and the definition of $\Psi$ that we have  
\begin{align*}
y H_y & = \gamma_4 (1+H_y) \Psi (y, \ldots, y, H, \ldots, H )\\
& = \gamma_5 (1+ H_y) \frac{(ny+ (m-n) H)^2}{1-\gamma_3 (ny + (m-n) H)}.
\end{align*}
The right side of this can be majorized further as follows. Since
$$
\frac{(ny + (m-n) H)^2}{1-\gamma_3 (ny + (m-n)H)} = \gamma^{-2}_3
\sum\limits^{\infty}_{l=2}  \gamma^l_3 (ny + (m-n) H)^l,  
$$
we have
$$
\frac{(ny+(m-n) H)^2}{1-\gamma_3 (ny + (m-n) H)} \prec \gamma^{-2}_3
\sum\limits^\infty_{l=2} (\gamma_3 m)^l (y+H)^l = \gamma^{-2}_3
\frac{(\gamma_3m)^2 (y+H)^2}{1-\gamma_3 m (y+H)}.  
$$
Putting $\gamma_3 m = \gamma_6$ and $\dfrac{(y+H)^2}{1-\gamma_6 (y+H)}
= \Phi (y,H)$, we get the majorization 
$$
y H_y \prec \gamma_7 (1+ H_y) \Phi (y,H).
$$
Let $J = J (y)$ be the power-series in $y$ starting with the second
degree term, satisfying the differential equation 
\begin{equation*}
y \; J_y = \gamma_7 (1+ J_y) \Phi (y,
J).\tag{3.5.32}\label{chap3:eq3.5.32} 
\end{equation*}
The coefficients\pageoriginale in the power-series $J$ can be
determined by induction on comparing coefficients on both sides. It is
easy to see, as in the case of $G$ and $H$, that $J \succ H$, and
hence it is enough to prove the convergence of $J$. One can integrate
(\ref{chap3:eq3.5.32}) and obtain $J$ directly. However, one can
majorize $J$ by the following simple method. Let 
\begin{equation*}
J(y) = \sum\limits^\infty_{k=2} a_k y^k;
\tag{3.5.33}\label{chap3:eq3.5.33} 
\end{equation*}
$J$ starts with quadratic terms since $\dfrac{\gamma^2_3 (y_1 + \ldots
  + y_m)^2}{1-\gamma_3(y_1 + \ldots + y_m)}$ does and hence also $H$
does. Then on comparing the coefficients of $y^k$ on both sides of
(\ref{chap3:eq3.5.32}), we obtain 
\begin{equation*}
ka_k = \left\{\gamma_7 \left(1+ \sum\limits^\infty_{l=2} la_l y^{l-1}
\right) \Phi (y,J) \right\}_k \;  (k =2,3,
\ldots). \tag{3.5.34}\label{chap3:eq3.5.34} 
\end{equation*}
The right side of (\ref{chap3:eq3.5.34}) has contributions only from
coefficients $a_l$ of terms of degree at most $k-1$. In other words,
it involves only the coefficients $a_2, \ldots, a_{k-1}$ and so $0 <
\dfrac{l}{k} < 1$. From (\ref{chap3:eq3.5.34}), 
$$
a_k = \left\{ \gamma_7 \left(\frac{1}{k} + \sum\limits^{k-1}_{l=2}
\frac{l}{k}a_l y^{l-1} \right) \Phi (y,J) \right\}_k.  
$$
If we take
\begin{equation*}
a^*_k = \left\{ \gamma_7 \left(1+ \sum\limits^{k-1}_{l=2} a^*_l
y^{l-1} \right) \Phi (y,J) \right\}_k, \; k = 2,3, \ldots,
\tag{3.5.35}\label{chap3:eq3.5.35} 
\end{equation*}
then again by induction $a_k \leq a^*_k$ for all $k \geq 2$. We get a
power-series in one variable $y$ with non-negative coefficients and
starting with the\pageoriginale second degree term by setting 
$$
K(y) = \sum\limits^\infty_{k=2} a^*_k \; y_k. 
$$
Then $J \prec K$. We see that the relations (\ref{chap3:eq3.5.35})
defining $a^*_k$ inductively imply that  
$$
K = \gamma_7 (1+ y^{-1} K) \frac{(y+K)^2}{1-\gamma_6 (y+K)}. 
$$
Since $K$ starts with quadratic terms, $K_1 = y^{-1} K$ is a
power-series starting with linear terms and moreover satisfies the
identity  
\begin{equation*}
K_1 = \frac{\gamma_7 y (1+ K_1)^3}{1-\gamma_6 y
  (1+K_1)}. \tag{3.5.36}\label{chap3:eq3.5.36} 
\end{equation*}
This is again obtained in a constructive way, and it is now enough to
prove the convergence of $K_1$, which implies the convergence of $K$,
and so that of $J$ and hence of $G_k, F_k$ also. 

In order to obtain a solution of the algebraic equation
(\ref{chap3:eq3.5.36}) for $K_1$ in a convergent power-series, one may
use the implicit function theorem. (It is easily verified that the
conditions of this theorem are satisfied). We shall, however, prove
the convergence of $K_1$ directly without determining a majorant of
$K_1$ explicitly. For this purpose we construct a simpler power-series
which majorizes $K_1$. We can write 
\begin{align*}
K_1 & = \gamma_7 y(1+K_1)^3 \sum\limits^\infty_{l=30} (\gamma_6y)^l
(l+K_1)^l = \gamma_7 y \sum\limits^\infty_{l=0} (\gamma_6 y)^l (1+
K_1)^{l+3}\\ 
& = \gamma_7 y\sum\limits^\infty_{l=0} (\gamma_6 y)^l
\sum\limits^{l+3}_{r=0} \binom{l+3}{r} K^r_1, 
\end{align*} 
by the\pageoriginale binomial theorem. Since the binomial coefficients
$\binom{l+3}{r}$ are sma\-ller than $2^{l+3}$ for $r =0$, $1, \ldots,
l+3$ and all $l = 0, 1, \ldots,$ we can write for each term on the
right 
$$
\sum\limits^{l+3}_{r=0} \binom{l+3}{r} (\gamma_6y)^l K^r_1 \prec
\sum\limits^{l+3}_{r=0} 2^{l+3} (\gamma_6 y)^l K^r_1 = 8
\sum\limits^{l+3}_{r=0} (2 \gamma_6 y)^l K^r_1.  
$$
Setting $l+r =h$, it follows that 
\begin{align*}
\sum\limits^\infty_{l=0} (\gamma_6 y)^l & \sum\limits^{l+3}_{r=0}
\binom{l+3}{r} K^r_1 \prec 8 \sum\limits^\infty_{h=0}
\sum\limits^h_{r=0} (2\gamma_6 y)^{h-r} K^r_1\\ 
& \prec 8 \sum\limits^\infty_{h=0} \; \sum\limits^h_{r=0} \binom{h}{r}
(2\gamma_6 y)^{h-r} K^r_1 = 8 \sum\limits^\infty_{h=0} (2\gamma_6 y +
K_1)^h,  
\end{align*}
again by the binomial theorem, so that we have the majorization 
\begin{equation*}
K_1 \prec8 \gamma_7 y \sum\limits^\infty_{h=0} (2\gamma_6 y + K_1)^h =
\frac{8\gamma_7 y}{1-(2\gamma_6 y + K_1)}
. \tag{3.5.37}\label{chap3:eq3.5.37} 
\end{equation*}
Now let $L$ be a power-series with indeterminate coefficients
satisfying the algebraci equation 
\begin{equation*}
L = \frac{8\gamma_7 y}{1 - (2\gamma_6 y +
  L)}. \tag{3.5.38}\label{chap3:eq3.5.38} 
\end{equation*}
Once again, as in the case of $K_1$, one can use the implicit function
theorem to obtain $L$. Since we are interested only in finding a
majorant for $K_1$, we shall first show that $L$ majorizes $K_1$ and
then find a majorant for $L$ itself. The coefficients of $K_1$ and $L$
can be determined inductively by comparing coefficients of $y^k$, $k =
0$, $1, \ldots, $ on both sides in (\ref{chap3:eq3.5.36}) and
(\ref{chap3:eq3.5.38}) respectively. It is\pageoriginale clear that
both $K_1$ and $L$ lack constant terms and also that the coefficient
of $y$ in $K_1$ is $\gamma_7$, while in $L$ it is $8
\gamma_7$. Suppose that the coefficients of $y, \ldots, y^{k-1}$ in
$K_1$ are majorized by those in $L$. Then by (\ref{chap3:eq3.5.37}) we
have  
$$
\{K_1\}_k \leq \left\{ \frac{8 \gamma_7 y}{1-(2y_6 y + K_1)}
\right\}_k  = \left\{8 y_7 y \sum\limits^\infty_{h=0} (2\gamma_6 y +
K_1)^h \right\}_k.  
$$
It is easy to see that the right side involves only the coefficients
of $y, \ldots, y^{k-1}$ in $K_1$ and hence is smaller than  
$$
\left\{ 8\gamma_7 y \sum\limits^\infty_{h=0} (2\gamma_6  y +
L)^h\right\}_k = \left\{ \frac{8\gamma_7 y}{1-(2\gamma_6 y + L)}
\right\}_k. 
$$
Then it follows by induction that $K_1 \prec L$. We majorize $L$
further in the following way. Since 
\begin{equation*}
2 \gamma_6 y \prec \frac{2\gamma_6 y}{1-(2\gamma_6 y + L)}, \tag{3.5.39}\label{chap3:eq3.5.39}
\end{equation*}
if we write $M = 2 \gamma_6 y + L $, then by (\ref{chap3:eq3.5.38})
and (\ref{chap3:eq3.5.39}), 
$$
M = 2 \gamma_6 y + L \prec \frac{(2\gamma_6 + 8 \gamma_7 )y}{1-M} =
\frac{\gamma_8 y}{1-M}.  
$$
Let us denote by $N$ the power series in $y$ with undetermined
coefficients satisfying the algebraic equation 
\begin{equation*}
N =\frac{\gamma_8 y}{1-N}. \tag{3.5.40}\label{chap3:eq3.5.40}
\end{equation*}
Then it is easily seen as before that $M \prec N$. From
(\ref{chap3:eq3.5.40}) it follows\pageoriginale that $N$ satisfies the
equation $N^2 - N + \gamma_8 y = 0$, which can also be written as 
$$
4 N^2 - 4 N+ 1 = 1 - 4 \gamma_8 y, \text{ or, }  (1-2N)^2 = 1-4 \gamma_8 y.
$$
From this we get
$$
1+4 N \prec 1 +4 N + \ldots = (1-2N)^2 = (1-4 \gamma_8 y)^{-1},
$$
and hence,
$$
4 N  \prec (1- 4 \gamma_8 y)^{-1} -1 = 4 \gamma_8 y (1-4 \gamma_8 y)^{-1}. 
$$
In other words, we have the majorization
$$
K_1 \prec N \prec (1-4 \gamma_8 y)^{-1} \gamma_8 y,
$$
and the last is a geometric series in $4 \gamma_8 y$ converging for
$|y| < \dfrac{1}{4 \gamma_8}$. Hence we conclude that the power-series
$F_k(y_1, \ldots, y_n)$, $k=1, \ldots, m$, converge for $|y_k| <
\dfrac{1}{4 \gamma_8}$, which completes the proof of the
convergence. We have thus proved Theorem \ref{chap3:thm3.5.1} under
the restriction (\ref{chap3:eq3.5.19}). 

We shall now remove the restriction (\ref{chap3:eq3.5.19}). The
eigenvalues of $A$ are again all real, distinct and non-zero but need
no longer to satisfy the restriction (\ref{chap3:eq3.5.19}). We shall
show that in this case, the solutions $y_k(s)$ of
(\ref{chap3:eq3.5.10}), and hence the solutions $x_k(s)$ of
(\ref{chap3:eq3.5.2}), will be now power-series in the variables $s$,
$e^{\lambda_1 s}, \ldots, e^{\lambda_ns}$, and not power series in the
$e^{\lambda_ks} , k =1, \ldots, n$, alone. We shall give only the
construction of the solution and the proof of
convergence\pageoriginale will be on exactly the same lines as in the
previous case. 

First of all we remark that we can no longer use the relation
(\ref{chap3:eq3.5.22}): 
$$
c_k \left(\lambda_k - \sum\limits^n_{l=1} g_l \lambda_l \right) =
\left\{ -\psi_l + \sum\limits^n_{l=1} F_{ky_l} \psi_l\right\}_{g_1
  \ldots g_n}, 
$$
to determine the coefficients in the power-series $F_k$. If for a
given $k=1, \ldots, n$, we have  
\begin{equation*}
\lambda_k = \sum\limits^n_{l=1} g_l \lambda_l,
\tag{3.5.41}\label{chap3:eq3.5.41} 
\end{equation*}
for a finite set of $n$-tuples $(g_1, \ldots, g_n)$ of non-negative
integers $g_1, \ldots, g_n$ with $g_1 + \ldots g_n = g \geq 2$, then
unless the right side of (\ref{chap3:eq3.5.22}) vanishes for each of
these $n$-tuples, we would get a contradiction. Since the right side
of (\ref{chap3:eq3.5.22}) may not necessarily vanish for all such
$n$-tuples $(g_1, \ldots, g_n)$ for which (\ref{chap3:eq3.5.41})
holds, we cannot use the argument above for determining the
coefficients $c_k$ of $y^{g_1}_1 \ldots y^{g_n}_n$ for this
exceptional set $(g_1, \ldots, g_n)$. Hence it is not possible to
determine the power-series $F_k$ from the requirement
(\ref{chap3:eq3.5.15}), namely that $\chi (u_1, \ldots, u_n, 0,
\ldots, 0) \equiv 0$ in $u_1, \ldots, u_n$. We therefore modify the
proof in the following way. 

We replace the requirement (\ref{chap3:eq3.5.15}) by a weaker
condition. We allow $\chi (u_1, \ldots, u_n, 0, \ldots, 0)$ to be a
polynomial in $u_1, \ldots, u_n$ for just those $k$ for which
(\ref{chap3:eq3.5.41}) holds. Let $V_k(u_1, \ldots, u_n)$ be
polynomials in the $n$ variables $u_1, \ldots, u_n$, with real
undetermined coefficients\pageoriginale such that  
\begin{equation*}
\chi_k (u_1, \ldots, u_n, 0, \ldots, 0) = V_k (u_1, \ldots, u_n), \; k
=1, \ldots, m  
\tag{3.5.42}\label{chap3:eq3.5.42}
\end{equation*}
Since $\lambda_k \neq \sum\limits^n_{l=1} g_l \lambda_l$ for $k = n
+1, \ldots, m$, we may assume that  
\begin{equation*}
V_k(u_1, \ldots, u_n) \equiv 0, \; k = n+1, \ldots, m,
\tag{3.5.43}\label{chap3:eq3.5.43} 
\end{equation*}
and consider (\ref{chap3:eq3.5.42}) only for $k =1, \ldots, n$. In
this case we assume that every term in $V_k$ is of the form $\alpha_k
u^{g_1}_1 \ldots u^{g_n}_n, g_1, \ldots, g_n$ non-negative integers
with $g_1 + \ldots + g_n \geq 2$ for which (\ref{chap3:eq3.5.41})
holds. There exist only finitely many such $n$-tuples $(g_1, \ldots,
g_n)$, and hence only finitely many $\alpha_k$, which determine the
polynomial $V_k$. In order to determine $\alpha_k$ we observe that,
since in (\ref{chap3:eq3.5.11}) the power-series $F_k$ start with
quadratic terms, the coefficients of $y^{g_1}_1 \ldots y^{g_n}_n$ in
$V_k$, considered as a function of $y_1, \ldots, y_n$ after
substituting (\ref{chap3:eq3.5.11}) for $u_1,\ldots, u_n$, is
precisely $\alpha_k$ and  
$$
\alpha_k u^{g_1}_1 \ldots u^{g_n}_n = \alpha_k y^{g_1}_1 \ldots
y^{g_n}_n + \ldots .  
$$
Hence we can determine $\alpha_k$ by equating the coefficients of
$y^{g_1}_1 \ldots y^{g_n}_n$ on both sides of (\ref{chap3:eq3.5.42}),
considering $V_k$ and $\chi_{k}$ as power-series in $y_1, \ldots,
y_n$. We define the coefficient $c_k$ of $y^{g_1}_1 \ldots y^{g_n}_n$
in $F_k$ for which $g_1, \ldots, g_n$ satisfy (\ref{chap3:eq3.5.41})
to be zero. Then we have  
\begin{align*}
& \alpha_k = \Big\{\psi_k (y_1 , \ldots, y_n, F_{n+1}, \ldots, 
  F_m)\\ 
& -  \sum\limits^n_{l=1} F_{ky_l} (y_1, \ldots, y_n) \psi_l (y_1,
  \ldots, 
  y_n, F_{n+1}, \ldots, F_m)\\ 
& \qquad + \alpha_k y^{g_1}_1 \ldots y^{g_n}_n - V_k (y_1 - F_1,
  \ldots, y_n - F_n)  
\Big\}_{g_1 \ldots g_n} \tag{3.5.44}\label{chap3:eq3.5.44}
\end{align*}
for all\pageoriginale $g_1, \ldots, g_n$ satisfying
(\ref{chap3:eq3.5.41}) for the $k$ in question. In
(\ref{chap3:eq3.5.44}) $\alpha_k$ can be determined explicitly
provided that the right side is known. The coefficients of $y^{g_1}_1
\ldots y^{g_n}_n$ where $g_1, \ldots, g_n$ satisfy
(\ref{chap3:eq3.5.41}) are by assumption zero, while for all other
$g_1, \ldots, g_n$ we have (\ref{chap3:eq3.5.19}), so that the
coefficients can be determined as before from (\ref{chap3:eq3.5.22})
where now the term $\{V_k(y_1-F_1, \ldots, y_n - F_n)\}_{g_1 \ldots
  g_n}$ has to be added on the right side. Then the polynomials $V_k$
are completely determined. The convergence of $V_k$ as power-series in
$y_1, \ldots, y_n$ can be proved without much difficulty. 

To obtain all solutions of (\ref{chap3:eq3.5.14}): $u'_k = \lambda_k
u_k + \chi_k$, $k = 1, \ldots, m$, we set as before, $v=
\sum\limits^m_{k=n+1} u^2_k$, and then using (\ref{chap3:eq3.5.14}),
we have 
$$
v' = 2 \sum\limits^m_{k=n+1} u_k u'_k = 2\sum\limits^m_{k=n+1}
(\lambda_k u^2_k + u_k \chi_k ). 
$$
Since $V_k = 0$ for $k = n+1, \ldots, m$, and in each term of $\chi_k$
for such $k$ we get one of $u_{n+1}, \ldots, u_m$ as a factor, our
previous argument goes through. We have, as before, $v' \geq \lambda
v$ and since $ve^{-\lambda s} \geq 0$ and nondecreasing, we have
$v=0$, so that $u_{n+1} = \ldots = u_m \equiv 0$. Substituting in
(\ref{chap3:eq3.5.14}) we obtain the system of differential equations 
$$
u'_k = \lambda_k u_k + V_k (u_1, \ldots, u_n), \; k = 1, \ldots, n,
$$
where each $V_k$ is a polynomial with real coefficients containing
only terms of the form $u^{g_1}_1 \ldots u^{g_n}_n$ where $g_1,
\ldots, g_n$ satisfy (\ref{chap3:eq3.5.41}). We arrange $\lambda_1,
\ldots, \lambda_n$ in decreasing order and assume that  
$$
0 > \lambda_1 > \ldots > \lambda_n. 
$$\pageoriginale
Then we claim that for every $k=1, \ldots, n$, $V_k$ is a polynomial
only in the variables $u_1, \ldots, u_{k-1}$. To see this, consider
the typical term $\alpha_k u^{g_1}_1 \ldots u^{g_n}_n$ in $V_k$. We
shall prove that $g_k = g_{k+1} = \ldots = g_n = 0$. Suppose, if
possible, that for some $l, k +1 \leq l \leq n$, we have $g_l \neq 0$,
so $g_l >0$. Since (\ref{chap3:eq3.5.41}) is satisfied, we have
$-\lambda_k = \sum\limits^n_{r=1} g_r(-\lambda_r)$. But $0 < -
\lambda_1 < \ldots < - \lambda_n$ by our ordering of the eigenvalues
and so each $g_r(-\lambda_r) \geq 0$. Since $g_l > 0$ and $-\lambda_l
> 0$, it follows that  
$$
-\lambda_k \geq g_l (-\lambda_l) \geq -\lambda_l, \text{ or }
\lambda_k \leq \lambda_l,  
$$
which is impossible since $l>k$. Hence $g_{k+1} = \ldots = g_n =0$
necessarily, and it only remains to prove that $g_k=0$. Suppose, if
possible, that $g_k > 0$. Since $g_1 + \ldots + g_k = g_1 + \ldots +
g_n \geq 2$, we have only two possibilities, either $g_k=1$ or $g_k
\geq 2$. If $g_k =1$, then at least one of $g_1, \ldots, g_{k-1}$ is
an integer $\geq 1$ and hence $-\lambda_k > g_k (-\lambda_k) = -
\lambda_k$, which is a contradiction. If $g_k \geq 2$, then
$g_k(-\lambda_k) > - \lambda_k$, so $-\lambda_k > -\lambda_k$, which
is again a contradiction. Hence $g_k = 0$. 

So finally we obtain the following system of differential equations
for $u_1, \ldots, u_m$: 
\begin{align*}
u'_k & = \lambda_k u_k + V_k (u_1, \ldots, u_{k-1}), \; k = 1, \ldots,
n, \\ 
u'_k & = \lambda_k u_k = 0, \; k = n+1, \ldots,
m. \tag{3.5.45}\label{chap3:eq3.5.45} 
\end{align*}
We determine\pageoriginale the general solution of
(\ref{chap3:eq3.5.45}) inductively. Since $V_1 \equiv 0$, $u'_1 =
\lambda_1 u_1$ and hence $u_1 = c_1 e^{\lambda_1 s}$, where $c_1$ is a
constant of integration. Next, $V_2$ contains only terms of the form
$\alpha_2 u^{g_1}_1$ where $g_1 \geq 2$ and $\lambda_2 = \lambda_1
g_1$. There is only one integral solution $g_1$ of $\lambda_2 =g_1
\lambda_1$, so that we have  
$$
V_2 (u_1) = \alpha_2 u^{g_1}_1. 
$$  
Inserting $u_1 = c_1 e^{\lambda_1 s }$ in $V_2 (u_1)$, we get the
differential equation 
$$
u'_2 = \lambda_2 u_2  + \alpha_2 c^{g_1}_1 e^{\lambda_1 g_1 s} =
\lambda_2 u_2 + \alpha_2 c^{g_1}_1 e^{\lambda_2 s}, 
$$
which is the same as $(u_2 e^{-\lambda_2 s})' = V_2 (c_1) =\alpha_2
c^{g_1}_1$, and on integration this gives 
$$
u_2 e^{-\lambda_2 s} = \alpha_2 c^{g_1}_1 s + c_2, \text{ or } u_2 =
(c_2 + V_2 (c_1) s)e^{\lambda_2s}. 
$$
If we denote the polynomial $V_2(c_1) s$ by $\mathscr{P}_2 (c_1, s)$,
then 
$$
u_2 = (c_2 + \mathscr{P}_2 (c_1, s))e^{\lambda_2 s}.
$$
Let us suppose that we have already proved that 
$$
u_{k-1} = (c_{k-1} + \mathscr{P}_{k-1} (c_1, c_2, \ldots, c_{k-2}, s)e^{\lambda_{k-1} s}, 
$$
where $\mathscr{P}_{k-1}$ is a polynomial in the $k-1$ real variables
$c_1, \ldots, c_{k-2}$ and $s$, and vanishes for $s=0:
\mathscr{P}_{k-1}$ is uniquely determined by $0 = V_1, V_2, \ldots,
V_{k-1}$. Since $\mathscr{P}_1 \equiv 0$, we have seen that this holds
for $k=2,3$. We now prove that  
$$
u_k = (c_k + \mathscr{P}_k(c_1, \ldots, c_{k-1}, s)e^{\lambda_k s}. 
$$\pageoriginale
We set
$$
c_l + \mathscr{P}_l  (c_1, \ldots, c_{l-1}, s) = Q_l (c_1, \ldots,
c_l, s), \; l = 1, \ldots, k-1.  
$$
Then from (\ref{chap3:eq3.5.45}), we have the differential equation 
$$
u'_k = \lambda_k u_k + V_k (Q_1 e^{\lambda_1s}, \ldots, Q_{k-1}
e^{\lambda_{k-1}} s). 
$$
We recall once again that all the terms of $V_k$ are of the form 
$\alpha_k u^{g_1}_1\break \ldots u^{g_{k-1}}_{k-1}$, where $g_1 , \ldots,
g_n$ satisfy the relations $\lambda_k = \sum\limits^{k-1}_{l=1} g_l
\lambda_1$. Hence  
$$
V_k (Q_1 e^{\lambda_1 s}, \ldots, Q_{k-1} e^{\lambda_{k-1} s}) = V_k
(Q_1, \ldots , Q_{k-1}) e^{\lambda_k s}, 
$$
so that we have the differential equation
\begin{align*}
u'_k & = \lambda_k u_k + V_k (Q_1, \ldots, Q_{k-1}) e^{\lambda_ks},\\
\text{or, } \qquad (u_k e^{-\lambda_ks})' & = V_k (Q_1, \ldots, Q_{k-1}), 
\end{align*}
which, on integration from 0 to $s$ gives,
$$
u_k = (c_k + \mathscr{P}_k(c_1, \ldots, c_{k-1}, s)e^{\lambda_ks}). 
$$
where $\mathscr{P}_k(c_1, \ldots, c_{k-1},s) = \int\limits^s_0
V_k(Q_1, \ldots, Q_{k-1}) ds$. 
This proves our assertion. Here $c_1, \ldots, c_k$ are constants of
integration and are uniquely determined by the initial values
$u_k(0)$. We observe that since $V_k(Q_1,\break \ldots, Q_{k-1})$ is a
polynomial in $c_1, \ldots, c_{k-1},s$,\pageoriginale and each
contains a positive power of $s$, it follows that $\mathscr{P}_k(c_1,
\ldots, c_{k-1}, 0) = 0$. Hence $u_k(0) = c_k$, so that if the
solutions $u_1, \ldots, u_n$ are to satisfy the relation
$\sum\limits^m_{k=1} u_k (s)^2 < \epsilon$, then we should necessarily
have $\sum\limits^m_{k=1} c^2_k < \epsilon$. However, if $u_k(0) =
c_k$ where $\sum\limits^m_{k=1} c^2_k < \epsilon$, then this may not
imply that $\sum\limits^m_{k=1} u_k(s)^2 < \epsilon$ for all $s \geq
0$. In the previous case, all the $\mathscr{P}_k$ were zero and
$\lambda_1, \ldots, \lambda_n < 0$ and we had $\sum\limits^m_{k=1}
u_k(s)^2 < \epsilon$. But in the present case, this is not in general
true for all $s \geq 0$. However, since $u_k(s) = Q_k (c_1, \ldots,
c_{k-1}, s) e^{\lambda_k s}$ and $Q_k$ is a polynomial while
$\lambda_k < 0$, it follows that for sufficiently large $s$, $u_k(s)$
are so small that $\sum\limits^m_{k=1} u_k(s)^2 < \epsilon$, and
moreover, $u_k(s) \to 0$ as $s \to \infty$. This again is a
constructive method of determining the solutions. 

In order to obtain the solution of the original system of equations in
the unknown functions $x_k$, $k = 1, \ldots, m$, we first solve for
$y_1, \ldots, y_m$ in terms of $u_1, \ldots, u_m$. We have, by
inversion of (\ref{chap3:eq3.5.11}), 
\begin{align*}
y_k & = u_k + G_k (u_1, \ldots, u_n), \; k = 1, \ldots, n,\\
y_k & = G_k (u_1, \ldots, u_m), \; k = n+1, \ldots, m,
\end{align*}
where $G_k$ are power-series with real coefficients, starting with
quadratic terms. Hence the $y_l(l=1, \ldots, m)$ are power-series in
$(c_k + \mathscr{P}_k(c_1, \ldots,\break c_{k-1}, s) e^{\lambda_k s}, k = 1,
\ldots, n$. Since the $x_l$ are linear functions of $y_l$, the same
assertion holds for $x_l$ also and thus we obtain all\pageoriginale
the asymptotic solutions of the original system $x' = Ax+
\varphi(x)$. They involve $n$ real parameters $c_1, \ldots, c_n$. This
completes the proof of Theorem \ref{chap3:thm3.5.1}. 
\end{proof}

We shall now consider the situation in which the eigenvalues of the
matrix $A = (a_{kl})$ are not necessarily real. We have 

\begin{subtheorem}\label{chap3:thm3.5.2}
Suppose that the eigen-values of the matrix $A = (a_{kl})$ are
distinct, some possibly complex, and that all eigen-values have
non-zero real parts. Then the general solutions $x_k(s)$ in $s \geq 0$
of the system 
$$
x'_k = \sum\limits^m_{l=1}  a_{kl} x_l +  \varphi_k  (x_1, \ldots, x_m), \; k =1 , \ldots, m. 
$$
which satisfy the condition $\sum\limits^m_{k=1} x_k(s)^2 < \epsilon$
for small $\epsilon > 0$, involve as many real parameters as the
number of eigen-values with negative real parts.  
\end{subtheorem}

\begin{proof}
Let $\lambda_1, \ldots,\lambda_m$ be the distinct eigen-values of the
real matrix $A = (a_{kl})$, and let $\lambda_k = \rho_k + i \tau_k$,
$k =1, \ldots, m$. Since $A$ is real, its characteristic polynomial
has real coefficients and so its complex roots occur in pairs of
complex conjugates. Hence if $\lambda_k$ is a complex eigen-value, the
conjugate complex $\bar{\lambda}_k$ is also an eigen-value
$\lambda_l$, where $l = l_k$; so $\lambda_l = \bar{\lambda}_k$ for $l
= l_k$. Then $\bar{\lambda}_l = \lambda_k$ and so $l_{l_k} = k$ for
all $k$. If $\lambda_k$ is a real eigen-value, then $\lambda_k =
\bar{\lambda}_k = \lambda_l$ and since the eigen-values are simple, $l
= l_k = k$. Hence $(l_1, \ldots, l_m)$ is a permutation of $(1,
\ldots, m)$ and since the $\lambda_k$ are simple, the permutation
consists entirely of transpositions. 
\end{proof}

We consider\pageoriginale a linear transformations $x = Cy$, $C$ being
a complex matrix with $|C| \neq 0$. Since all the eigen-values of $A$
are distinct and different from zero, we can find a $C$ such that
$C^{-1} AC = D$ is in the normal diagonal form: 
$$
D = \begin{pmatrix}
\lambda_1 & & 0\\
 & \ddots & \\
0 & & \lambda_m
\end{pmatrix}.
$$
Then $AC = CD$ and we can determine the matrix $C$ from this
condition. Let $C_k$, $k =1, \ldots, m$, denote the columns of
$C$. Then we have $AC_k = \lambda_k C_k$, $k = 1, \ldots, m$, and this
can be seen immediately by comparing the elements on both sides of $AC
= CD$. Hence $C_k$ is an eigen-vector of the matrix $A$ belonging to
the eigen-value $\lambda_k$. Since the $\lambda_k$ are distinct, the
eigen-vectors $C_k$ are all distinct. These eigen-vectors are uniquely
determined up to constant scalar factors, in general complex. Again
using our earlier notation, 
$$
A \bar{C}_k = \bar{A} \bar{C}_k = \bar{\lambda}_k \bar{C}_k =
\lambda_l \bar{C}_k, \; l = l_k,  
$$
so that $\bar{C}_k$ is an eigen-vector belonging to the eigen-value
$\lambda_l$ where $l = l_k$. Hence $\bar{C}_k$ is a scalar multiple of
$C_l$ and so by a suitable normalization we may assume that  
\begin{equation*}
\bar{C}_k = C_l, \; \bar{C}_l = C_k, \; l = l_k, \;k =1, \ldots,
m.\tag{3.5.46}\label{chap3:eq3.5.46} 
\end{equation*}
Since the matrix $C$ is complex, $y$ is a complex vector. Since $x$ is
real, $x = \bar{x}$ and so $Cy = \bar{C}\bar{y}$. It follows from
this, by (\ref{chap3:eq3.5.46}), that  
$$
C_k y_k + C_l y_l = \bar{C}_k \bar{y}_k + \bar{C}_l \bar{y}_l =
C_l\bar{y}_k + C_k \bar{y}_l.  
$$\pageoriginale
But $y = C^{-1}x$ is a uniquely determined vector and so we should
have  
$$
y_l = \bar{y}_k, \; y_k = \bar{y}_l, \; l = l_k, \; k =1, \ldots, m. 
$$
As we have to deal with formal power-series, we shall drop the
assumption that $x$ is real. The relation $x = Cy =\bar{C} \bar{y}$
can be given a sense even when $x$ is not real if we define formally
the indeterminates $\bar{y}_1, \ldots, \bar{y}_m$ by setting 
\begin{equation*}
y_l = \bar{y}_k, \; y_k = \bar{y}_l, \; l = l_k, \; k =1, \ldots,
m. \tag{3.5.47}\label{chap3:eq3.5.47} 
\end{equation*}
(Thus $\bar{y}_1,\ldots, \bar{y}_m$ is just a permutation of the
indeterminates $y_1, \ldots, y_m$, this permutation consisting
entirely of transpositions). 

By the substitution $x = Cy$, the given system of differential
equations $x' = Ax + \varphi (x)$ goes over into the system  
\begin{equation*}
y' = Dy + C^{-1} \varphi (x), \tag{3.5.48}\label{chap3:eq3.5.48} 
\end{equation*}
where $C^{-1}\varphi(x)$ is a column vector $\sigma(x)$ of
power-series $\sigma_k(x)$. The coefficients of $\sigma_k$ are complex
and they are obtained in the following way. If $\alpha_k$ denotes the
coefficient of $x^{g_1}_1 \ldots x^{g_m}_m$ of degree $g = g_1 +
\ldots + g_m \geq 2 $ in $\varphi_k(x)$, then the coefficient of
$x^{g_1}_1 \ldots x^{g_m}_m$ in $\sigma_k (x)$ is given by
$\sum\limits^m_{l=1} d_{kl} \alpha_l$, $C^{-1} = (d_{kl})$. 

If $f = f(x_1, \ldots, x_m)$\pageoriginale is a formal power-series
with complex coefficients in the $m$ indeterminates $x_1, \ldots,
x_m$, then we denote by $\bar{f} = \bar{f} (x_1, \ldots, x_m)$ the
power-series obtained by replacing the coefficients in $f$ by their
complex conjugates and by $f^*$ that obtained by replacing the
coefficients in $f$ by their absolute values: thus 
$$
\{\bar{f}\}_{g_1 \ldots g_m} = \overline{\{f\}_{g_1 \ldots g_m}}
\text{ and } \{f^*\}_{g_1 \ldots g_m} = |\{f\}_{g_1 \ldots g_m}|. 
$$
Because of $\bar{\varphi} (x) = \varphi (x) =  C \sigma (x)$ we have
$\bar{\sigma}_k(x) = \sigma_l (x)$. Since $Cy = x = \bar{C} \bar{y}$,
we have also $\bar{\sigma}_k (\bar{C} \bar{y}) =
\sigma_l(Cy)$. Denoting $\sigma_k(Cy)$ by $\psi_k(y)$, we have, by the
last formula, $\psi_l(y) = \bar{\psi}_k(\bar{y})$. Hence we can
rewrite the system of differential equations (\ref{chap3:eq3.5.48}) in
the form 
\begin{equation*}
y' = Dy + \psi (y), \tag{3.5.49}\label{chap3:eq3.5.49}
\end{equation*}
where $\psi(y)$ is a column-vector whose components are power-series
$\psi_k(y)$ with complex coefficients and starting with quadratic
terms. 

Let $\lambda_1, \ldots, \lambda_n$ denote the eigen-values whose real
parts are negative and $\lambda_{n+1}, \ldots, \lambda_m$ those whose
real parts are positive; so  
\begin{equation*}
\rho_1 < 0, \ldots, \rho_n < 0; \; \rho_{n+1} > 0, \ldots, \rho_m >
0. 
\tag{3.5.50}\label{chap3:eq3.5.50}
\end{equation*}
Since for $l = l_k$, $\lambda_l = \bar{\lambda}_k = \rho_k - i
\tau_k$, it follows that as the index $k$ runs through $1, \ldots, n$,
$l_k$ also runs through $1, \ldots, n$, and if $k$ runs through $n+1,
\ldots, m$, so does $l_k$.  

As in the proof of Theorem \ref{chap3:thm3.4.1}, we now make a
non-linear transformation of the variables $y_1, \ldots, y_m$ to the
variables $u_1,\ldots, u_m$ of the form\pageoriginale 
$$
u_k = y_k - F_k(y_1, \ldots, y_n), \; k =1, \ldots, m,
$$
where the $F_k$ are power-series in the variables $y_1, \ldots, y_n$
alone, with complex coefficients and starting with quadratic terms. As
in the case of real eigen-values, the differential equations
(\ref{chap3:eq3.5.49}) are transformed into 
\begin{equation*}
u'_k = \lambda_k u_k + \chi_k (u_1, \ldots, u_m), \; k =1, \ldots, m,
\tag{3.5.51}\label{chap3:eq3.5.51} 
\end{equation*}
where $\chi_k(u_1, \ldots,u_m)$ is defined by
\begin{equation*}
\chi_k = \lambda_k F_k + \psi_k - \sum\limits^n_{l=1} F_{ky_l}
(\lambda_l y_l + \psi_l). \tag*{$(\ast)$} 
\end{equation*}
Our argument for the construction of the power-series $F_k$ goes
through as in the real case. 

First suppose that $\lambda_k \neq \sum\limits^n_{l=1} g_l\lambda_l$
for all $n$-tuples of non-negative integers $g_1, \ldots, g_n$ with $g
=g_1 + \ldots + g_n \geq 2$. This is only a finite set of conditions
on the $\lambda_k$, as in the real case. We determine $F_k$ again by
requiring that $\chi_k(u_1, \ldots, u_n, 0, \ldots, 0) \equiv 0$,
i.e. 
\begin{align*}
&\lambda_k F_k + \psi_k (y_1, \ldots, y_n, \; F_{n+1}, \ldots, F_m)\\ 
&\qquad\qquad -\sum\limits^n_{l=1} F_{ky_l} (\lambda_l y_l + \psi_l (y_1, \ldots,\\
&\qquad\qquad\qquad y_n, \; F_{n+1}, \ldots, F_m)) =0 . 
\tag*{$(\ast\ast)$}
\end{align*}
$k=1, \ldots, m$, and we get as before a recurrence formula for
finding the coefficients in $F_k$. If $c_k y^{g_1}_1 \ldots
y^{g_n}_n$ is a term of total degree $g = g_1 + \ldots g_n \geq 2 $ in
$F_k$, then comparing the coefficients of $y^{g_1}_1 \ldots
y^{g_n}_n$, we have  
$$
c_k \left(\lambda_k - \sum\limits^n_{l=1} g_l \lambda_1 \right) =
\{-\psi_k(y_1, \ldots, y_n,\; F_{n+1}, \ldots, F_m)  +
\sum\limits^n_{l=1} F_{ky_1} \psi_l\} _{g_1 \ldots g_n}.  
$$\pageoriginale
We find $c_k$ by induction as in the real case. The convergence of the
power-series $F_k$ thus obtained is proved as in the real case. Let
$\alpha = \min (-\rho_1, \ldots, -\rho_n) > 0$. Then there exists a
positive number $\gamma_1$ depending only on $\alpha$, such that  
$$
|\lambda_k - \sum\limits^n_{l=1} g_l \lambda_l| \geq |\re (\lambda_k -
\sum\limits^n_{l=1} g_l \lambda_l) | > \gamma^{-1}_1 (g_1 + \ldots +
g_n).  
$$
On the other hand, the $\psi_k$ are power-series convergent in a
complex neighbourhood $|y_1| < \underline{\rho_1}, \ldots, |y_m|
<\underline{\rho_m}$ of $y_1 = 0, \ldots, y_m = 0$. We shall denote by
$\gamma_2, \gamma_3 \ldots$ large positive constants. If $|\psi_k|
\leq \gamma_2$ in this neighbourhood, then by Cauchy's theorem, 
$$
|\{\psi_k\}_{h_1 \ldots h_m}| \leq \gamma_2 \underline{\rho_1}^{-h_1}
\ldots  \underline{\rho_m}^{-h_m}, 
$$
for all $n$-tuples of non-negative integers $h_1, \ldots, h_m$ with
$h_1 + \ldots + h_m \geq 2$. So we have 
\begin{align*}
\psi_k, \prec \sum\limits_{h_1 + \ldots + h_m \geq 2} & \gamma_2
\frac{y^{h_1}_1 \ldots y^{h_m}_m}{\u{\rho}^{h_1}_1 \ldots
  \u{\rho}^{h_m}_m} \prec \sum\limits^\infty_{h=2} \gamma_2
\left(\frac{y_1}{\u{\rho}_1} + \ldots + \frac{y_m}{\u{\rho}_m}
\right)^h\\ 
& \prec \gamma_2 \gamma^2_3 \frac{ (y_1 + \ldots +
  y_m)^2}{1-\gamma_3(y_1+ \ldots + y_m)} \equiv \Psi (y_1, \ldots,
y_m),  
\end{align*}
where $\gamma^{-1}_3 = \min (\u{\rho_1}, \ldots, \u{\rho}_m) > 0$. Then we have 
$$
|c_k|(g_1+ \ldots + g_n) \leq \gamma_1 |\{\psi_k+ \sum\limits^n_{l=1}
F_{ky_1} \psi_l\}_{g_1 \ldots g_n}|. 
$$
We replace\pageoriginale $F_k$ by $F^*_k$ and $\psi_k, \psi_l$ by
$\Psi$ and we see that $F_k \prec F^*_k$ and  
$$
|c_k| (g_1 + \ldots + g_n) \leq | \gamma_4 \left\{ (1+
\sum\limits^n_{l=1} F^*_{ky_l}) \Psi\right\}_{g_1 \ldots g_n}|. 
$$
The rest of the proof is the same as before and we conclude that the
power-series $F_k, k =1, \ldots, m$, converge for complex values of
$y_1, \ldots, y_n$ in a complex neighbourhood of $y_1 = 0, \ldots, y_n
= 0$. (The $F_k$ can now be regarded as convergent power-series in the
complex variables $y_1, \ldots, y_m$).  

We can now obtain $y_k$ in terms of $u_k$ by locally inverting the
substitution $u_k = y_k - F_k (y_1, \ldots, y_n)$, $k = 1, \ldots, m$,
and we find that  
\begin{equation*}
y_k = u_k + G_k (u_1, \ldots, u_m), \; k = 1, \ldots,
m,\tag{3.5.52}\label{chap3:eq3.5.52} 
\end{equation*}
where the $G_k$ are uniquely determined power-series with complex
coefficients. Since the $F_k$ converge as functions of the complex
variables $y_1, \ldots, y_n$, we can now look upon $u_1, \ldots, u_m$
also as complex variables and $G_k$ are therefore convergent
power-series in the complex variables $u_1, \ldots, u_m$. Since we are
interested in real solutions $x_k(s)$ of the original system, we have
to find out under what conditions for $u_1, \ldots, u_m$ we shall have
$\bar{y}_k = y_l$, $l = l_k$, $k =1, \ldots, m$, so that $x_1, \ldots,
x_m$ all are real. We might conjecture that this condition is again
$\bar{u}_k = u_l$, $l = l_k$, $k=1, \ldots, m$, and conversely. This
is true and it is enough to prove this in the following formal
situation. Suppose we introduce the indeterminates $\bar{y}_k = y_l$,
$l = l_k$; we shall prove that if $u_1,\ldots, u_m$ and $\bar{u}_1,
\ldots, \bar{u}_m$ are defined by $u_k = y_k - F_k(y_1, \ldots, y_n)$,
$\bar{u}_k = \bar{y}_k - \bar{F}_k (\bar{y}_1, \ldots, \bar{y}_n)$
then $\bar{u}_k = u_l$, $l = l_k$, $k=1, \ldots, m$. And for this
purpose it is\pageoriginale enough to prove that $\bar{F}_k
(\bar{y}_1,\ldots, \bar{y})_n  = F_l (y_1, \ldots, y_n)$, or
equivalently, that  
$$
\bar{F}_k(y_{l_1}, \ldots, y_{l_n}) = F_{l_k} (y_1, \ldots, y_n), \; k
= 1, \ldots, m.  
$$
For this we observe that the condition $\chi_k (u_1, \ldots, u_n, \;
0, \ldots, 0) \equiv 0$, $k = 1, \ldots, m$, gives the following
identity in the formal power-series: 
\begin{align*}
& \lambda_{l_k} F_{l_k} (y_1, \ldots, y_n) - \sum\limits^n_{r=1}
  \lambda_{l_r} y_{l_r} F_{l_r} y_{l_r} (y_1, \ldots, y_n)
  \tag{3.5.53}\label{chap3:eq3.5.53} \\ 
& = - \psi_{l_k}  (y_1, \ldots, y_n, F_{n+1} , \ldots, F_m) +
  \sum\limits^n_{r=1} F_{l_k} y_{l_r} (y_1, \ldots, y_n)\\ 
&\qquad\qquad \psi_{l_r} (y_1, \ldots, y_n,\; F_{n+1}, \ldots, F_m).  
\end{align*}
By replacing the coefficients on both sides by their complex
conjugates we have the identity 
\begin{align*}
& \bar{\lambda}_{l_k} \bar{F}_{l_k} (y_1, \ldots, y_n) -
  \sum\limits^n_{r=1} \bar{\lambda}_{l_r} y_{l_r} \bar{F}_{l_k
    y_{l_r}}  (y_1, \ldots,
  y_n)\tag*{(3.5.53)$'$} \label{chap3:eq3.5.53'}\\ 
& = - \bar{\psi}_{l_k} (y_1, \ldots, y_n, \bar{F}_{n+1}, \ldots,
  \bar{F}_m)\\ 
&\qquad + \sum\limits^n_{r=1} \bar{F}_{l_k y_{l_r}} (y_1, \ldots, y_n)
  \bar{\psi}_{l_r} (y_1, \ldots, y_n, \bar{F}_{n+1}, \ldots,
  \bar{F}_m).  
\end{align*}
(Recall that $\bar{f}$ denotes the power-series whose coefficients are
the complex conjugates of those of $f$). The permutation $y_{l_1},
\ldots, y_{l_m}$ of the indeterminates $y_1,\ldots, y_m$ introduces
the indeterminates $\bar{y}_1,\ldots, \bar{y}_m$. We have seen that
since $l_{l_k} = k$, $k=1,\ldots, m $, $\psi_k(y_1, \ldots, y_m) =
\bar{\psi}_l (\bar{y}_1, \ldots, \bar{y}_m) = \bar{\psi}_l (y_{l_1},
\ldots, y_{l_m})$. Now using the fact that $\bar{\lambda}_{l_k} =
\lambda_k$ and $\bar{y}_{l_r} = y_r$, we get from
\ref{chap3:eq3.5.53'} the following identity: 
\begin{align*}
& \lambda_k \bar{F}_{l_k} (\bar{y}_1, \ldots, \bar{y}_n) -
  \sum\limits^n_{r=1} \lambda_r y_r \bar{F}_{l_k y_r} (\bar{y}_1,
  \ldots, \bar{y}_n)\\ 
& = - \psi_l (y_1, \ldots, y_n, \bar{F}_{l_{n+1}} (\bar{y}_1, \ldots,
  \bar{y}_n), \ldots, \bar{F}_{l_m} (\bar{y}_1, \ldots, \bar{y}_n))\\ 
&  + \sum\limits^n_{r=1} \bar{F}_{l_k y_r} (\bar{y}_1, \ldots,
  \bar{y}_n) \psi_r (y_1, \ldots, y_n,\bar{F}_{l_{n+1}}\\ 
&\qquad(\bar{y}_1,
  \ldots, \bar{y}_n),\ldots, \bar{F}_{l_m} 
(\bar{y}_1, \ldots,\bar{y}_n)) \tag{3.5.54}\label{chap3:eq3.5.54} 
\end{align*}\pageoriginale 
because the passage from $y_1, \ldots, y_m$ to $\bar{y}_1,\ldots,
\bar{y}_m$ is nothing but the permutation $y_{l_1}, \ldots, y_{l_m}$,
and moreover, when $k$ runs through $1, \ldots, n$, so does $l_k$ and
when $k$ runs through $n+1, \ldots, m$, so does $l_k$. Hence
$\bar{F}_l (\bar{y}_1, \ldots, \bar{y}_n)$, $l = l_k$, $k =1, \ldots,
m$ satisfy the functional equation  $(\ast\ast)$ for $F_k(y_1, \ldots,
y_n)$. On the other hand, $F_k(y_1, \ldots, y_n)$ is uniquely
determined by $(\ast\ast)$. Hence by the uniquenes of the solution of
$(\ast)$, we have, for $l =l_k$, $k=1, \ldots, m$,
$\bar{F}_l(\bar{y}_1,\ldots, \bar{y}_n) = F_k(y_1, \ldots, y_n)$. It
is also clear that if $F_k$ converges as a power series in the complex
variables $y_1, \ldots, y_n$ in a complex neighbourhood of $y_1 = 0,
\ldots, y_n = 0$ and we replace $y_1, \ldots, y_n$ in
$\bar{F}_k(y_1,\ldots, y_n)$ by the complex conjugates $\bar{y}_1,
\ldots, \bar{y}_n$, then the corresponding variable $u_k$ defined by
$u_k = y_k - F_k (y_1, \ldots, y_n)$  goes over into the conjugate
variable $\bar{u}_k$. This proves the assertion that $\bar{u}_k =
\bar{u}_l$, $l=l_k$, $k=1, \ldots, m$. 

Now it follows from the definition $(\ast)$ that also
$$
\bar{\chi}_k (\bar{u}_1, \ldots, \bar{u}_m) = \chi_l (u_1, \ldots, u_m).
$$

Thus we have the reduced system of differential equations
$$
u'_k = \lambda_k u_k + \chi_k, \; u_l = \bar{u}_k,  \; l = l_k, \; k =1, \ldots, m, 
$$\pageoriginale
under the assumption that $\sum\limits^m_{k=1}|u_k|^2 < \epsilon$ for
all $s \geq 0$ and sufficiently small $\epsilon$. In order to obtain
the explicit solutions we consider the function $v(s)$ defined by  
$$
v = \sum\limits^m_{k=n+1} |u_k|^2 = \sum\limits^m_{k=n+1} u_k \bar{u}_k. 
$$
Differentiating with respect to $s$ we have 
$$
v' = \sum\limits^m_{k=n+1} (u'_k \bar{u}_k + u_k \bar{u}'_k).
$$
It is clear that $v'$ is real; since $\lambda_l = \bar{\lambda}_k$,
$u_l = \bar{u}_k$, $\chi_l (u) = \bar{\chi}_k(\bar{u})$ for $l=l_k$,
we see that $\bar{u}'_k = u'_l = \lambda_l u_l + \chi_l (u) =
\bar{\lambda}_k \bar{u}_k + \bar{\chi}_k(\bar{u})$. Hence 
\begin{align*}
v' & = \sum\limits^m_{k=n+1} (\lambda_k + \bar{\lambda}_k) u_k
\bar{u}_k + \sum\limits^m_{k=n+1} (u_k \bar{\chi}_k(\bar{u}) +
\bar{u}_k \chi_k(u))\\ 
& = 2 \sum\limits^m_{k=n+1} \rho_k |u_k|^2 + 2 \sum\limits^m_{k=n+1} \re (\bar{u}_k \chi_k (u)). 
\end{align*}
Let $\rho = \min (\rho_{n+1}, \ldots, \rho_m) > 0$. Since $\chi_k$
starts with quadratic terms and each term of $\chi_k$ contains at
least one of $u_{n+1}, \ldots, u_m$ as a factor, and $\chi_k$ is
uniformly convergent, we show as in the real case that for
sufficiently small $\epsilon$, 
$$
2 \sum\limits^m_{k=n+1} \re (\bar{u}_k \chi_k (u)) \geq - \rho v.
$$
So we have $v' \geq \rho v $ and hence $(v e^{-\rho s})' \geq 0 $ and
$ve^{-\rho s}$ is non-decreasing.\pageoriginale Since $ve^{-\rho s}
\geq 0$ it follows that $v = 0$, or $u_k(s) = 0$ for all $s \geq 0$,
$k =n+1, \ldots, m$. Thus the system of differential equations is
further reduced to  
$$
u'_k = \lambda_k u_k, \;k =1, \ldots, n.
$$
Hence
$$
u'_l = \lambda_l u_l, \; l = l_k, \; k =1 , \ldots, n ,
\bar{\lambda}_k  = \lambda_l.  
$$
Integrating these we obtain
$$
u_k= c_k e^{\lambda_k s}, \; u_l = c_l e^{\lambda_ls}, l = l_k, k = 1,
\ldots,n. 
$$
Since $\lambda_l = \bar{\lambda}_k$ and $u_l = \bar{u}_k$ for $l
=l_k$, $k =1, \ldots, n$, and all $s \geq 0$, we have also $c_l =
\bar{c}_k$. Thus we get exactly $n$ real parameters in the real
solution $x_k = x_k(s)$ of the original system asymototic to the
equilibrium solution. This proves Theorem \ref{chap3:thm3.4.2} under
the restriction $\lambda_k \neq \sum\limits^n_{l=1} \lambda_l g_l$.  

It is easy to extend the argument to the case in which $\lambda_k =
\sum\limits^n_{l=1} \lambda_l g_l$ for some given $k$, by imposing the
same condition as in the real case, namely, $\chi_k (u_1, \ldots, u_n,
\; 0, \ldots, 0) = V_k (u_1, \ldots, u_n)$, a polynomial with complex
coefficients in $u_1, \ldots, u_n$ consisting entirely of terms of the
form $\alpha_k u^{g_1}_1 \ldots u^{g_n}_n$ where $g_1, \ldots, g_n$
are non-negative integers with $g_1 + \ldots + g_n\geq 2$, satisfying
$\lambda_k = \sum\limits^n_{l=1} \lambda_l g_l$. Once again, if $0
<\rho_1 < \ldots < \rho_n$, we can show that $V_k$ is actually a
polynomial in $u_1, \ldots, u_{k-1}$ only, $\sum\limits^m_{k=1}
|u_k|^2 < \epsilon$ for $\epsilon$ sufficiently small.\pageoriginale
We have now, in addition, $\bar{V}_l(\bar{u}_1, \ldots, \bar{u}_n) =
V_k (u_1, \ldots, u_n)$, $l=l_k$.  Then the solution are given by  
\begin{align*}
u_k & = Q_k (c_1, \ldots, c_k, s) e^{\lambda_k s}, k =1, \ldots, n;\\
u_k & = 0, \; k = n+1, \ldots, m,
\end{align*}
where $Q_k$ are polynomials in $s$ with complex coefficients,
$q_k(c_1, \ldots,\break c_k,0) = c_k$ and $c_l = \bar{c}_k$, $\bar{Q}_k
(\bar{c}_1, \ldots, \bar{c}_k, s) = Q_l (c_1, \ldots, c_k, s)$. Once
again, the general solution asymptotic to the equilibrium solution
contains $n$ real parameters, and this completes the proof of Theorem
\ref{chap3:thm3.5.2}. 

It remains now to consider the solutions in the case in which the
matrix $A = (a_{kl})$ may have multiple eigen-values. The problem is
more complicated in this case, the difficulty being that the matrix
$A$ cannot now be reduced to the diagonal form. We can nevertheless
extend our earlier results to this case. We have  

\begin{subtheorem}\label{chap3:thm3.5.3}
Suppose that the matrix $A$ has eigen-values which are, in general,
complex, all with non-zero real parts, some of them possibly
multiple. Then the general solution of the system $x' = Ax + \varphi
(x)$ such that $\sum\limits^m_{k=1} x^2_k < \epsilon$ for small
$\epsilon > 0$ contains exactly $n$ arbitrary real parameters, $n$
being the number of eigen-values with negative real parts. 
\end{subtheorem}

\begin{proof}
We transform the matrix $A$ in the following way. We can find a matrix
$C$ with $|C| \neq 0$ such that $C^{-1} AC = D$ breaks up into boxes
along the main diagonal. More precisely, the matrix $D = (d_{kl})$ has
the following\pageoriginale property: if $\lambda_1, \ldots,
\lambda_m$ are the eigen-values of $A$,  
\begin{equation*}
d_{kk} = \lambda_k, d_{kl} = 0 \text{ for } k \neq l \text{ and } k
\neq l +1 , \tag{3.5.55}\label{chap3:eq3.5.55} 
\end{equation*}
$d_{kl} = 0$ for $k = l +1$ if $\lambda_l \neq \lambda_{l+1}$, $d_{kl}
= 0$ or 1 for $k =l+1$ if $\lambda_l = \lambda_{l+1}$. 

Let $e_1 = 0$, $e_k = 0$ if $\lambda_k \neq \lambda_{k-1}$, $e_k = 0$
or 1 if $\lambda_k = \lambda_{k-1}$, $k = 2, \ldots, m$. Then the
transformed system $y' = Dy + \psi (y_1,\ldots, y_m)$ can be written
in the form 
\begin{equation*}
y'_k = \lambda_k y_k + e_k y_{k-1}  + \psi_k(y_1, \ldots, y_m), \;k =
1, \ldots, m, \tag{3.5.56}\label{chap3:eq3.5.56} 
\end{equation*}
where $\psi_k$ again are power-series with complex coefficients
starting with quadratic terms and converging in a neighbourhood of
$y_1 = 0, \ldots, y_m =0$. Let $\lambda_k = \rho_k + i \tau_k $ and
let us suppose that  
\begin{equation*}
0 < - \rho_1 < - \rho_2 < \ldots < - \rho_n ; \rho_k > 0, \; k =
n+1,\ldots,m,\tag{3.5.57}\label{chap3:eq3.5.57} 
\end{equation*}
so that $\lambda_1, \ldots, \lambda_n$ are all the eigen-values with
negative real parts. As before we perform the non-linear
transformation 
\begin{equation*}
u_k = y_k - F_k (y_1,\ldots, y_n) , \; k =1, \ldots, m,
\tag{3.5.58}\label{chap3:eq3.5.58} 
\end{equation*}
where $F_k$ are power-series in $y_1, \ldots, y_n$ with complex
coefficients to be determined, starting with quadratic terms. We
restrict ourselves to the case in which $\lambda_k \neq
\sum\limits^n_{r=1} g_r \lambda_r$, $k =1, \ldots, m$, for
non-negative integers $g_1,\ldots, g_n$ with $g_1 + \ldots g_n \geq
2$. When this is not the case the proof can be modified as in the
earlier situations. 

From (\ref{chap3:eq3.5.58}),\pageoriginale by differentiation, on
using (\ref{chap3:eq3.5.56}), we have 
$$
u'_k  = \lambda_k (u_k + F_k) + e_k y_{k-1} + \psi_k -
\sum\limits^n_{r=1} F_{ky_r} (\lambda_r y_r + e_r y_{r-1} + \psi_r),  
$$
which can be written in the form
$$
u'_k = \lambda_k u_k + e_k u_{k-1} + \chi_k (u_1, \ldots, u_m), k =1, \ldots, m,
$$
where 
\begin{equation*}
\chi_k = \lambda_k F_k - \sum\limits^n_{r=1} \lambda_r y_r F_{ky_r} +
e_k F_{k-1} - \sum\limits^n_{r=1} e_r y_{r-1} F_{ky_r} + \psi_k -
\sum\limits^n_{r=1}\psi_r F_{ky_r}.  
\tag{3.5.59}\label{chap3:eq3.5.59}
\end{equation*}
Once again we determine the power-series $F_k$ by requiring that 
$$
\chi_k (u_1, \ldots, u_n, 0, \ldots, 0 )  =0 , \; k =1,\ldots, m.
$$
Since $u_k=0$ implies that $y_k = F_k (y_1, \ldots, y_n)$, $k
=n+1,\ldots, m$, this condition implies the following identity: 
\begin{align*}
& \lambda_k F_k - \sum\limits^n_{r=1} \lambda_r y_r F_{ky_r} = - e_k
  F_{k-1} + \sum\limits^n_{r=1} e_r y_{r-1} F_{ky_r} \\ 
& - \psi_k (y_1, \ldots, y_n, F_{n+1}, \ldots, F_m) +
  \sum\limits^n_{r=1} \psi_r (y_1, \ldots, y_n, F_{n+1}, \ldots, F_m)
  F_{ky_r}. \tag{3.5.60}\label{chap3:eq3.5.60} 
\end{align*}
Let $c_{k, g_1,\ldots, g_n} y^{g_1}_1 \ldots y^{g_n}_n$ denote a term
of total degree $g = g_1+\ldots + g_n \geq 2 $ in the power-series
$F_k$. Comparing the coefficients of $y^{g_1}_1 \ldots y^{g_n}_n$ on
both sides of (\ref{chap3:eq3.5.60}), we have  
\begin{align*}
 {k, g_1, \ldots, g_n} &  \left(\lambda_k - \sum\limits^n_{r=1} g_r
 \lambda_r \right) =\{-\psi_k + \sum\limits^n_{r=1} F_{ky_r}
 \psi_r\}_{g_1, \ldots, g_n} - e_k c_{k-1, g_1, \ldots, g_n} + \\ 
& \quad + \sum\limits^n_{r=1} e_r (g_r+1) c_{k, g_1, \ldots, g_{r-1}
   -1, \; g_r + 1, \ldots, g_n}.  
\tag{3.5.61}\label{chap3:eq3.5.61}
\end{align*}\pageoriginale
Since coefficients of terms of total degree $g$, namely $c_{k-1, g_1,
  \ldots, g_n}$ and $c_{k, g_1, \ldots, g_{r-1} -1, g_{r} + 1, \ldots,
  g_n}$, $r =1,\ldots,n$, occur on the right side of
(\ref{chap3:eq3.5.60}), we cannot carry out the induction construction
for the coefficients as in the previous cases. However, we can argue
by induction on introducing a lexicographic ordering for the indices
in the subscripts. 

First of all, suppose that the coefficients $c_{k-1, g_1, \ldots,
  g_n}$ of total degree $g_1 + \ldots + g_n = g$ have already been
determined. Then the second term on the righ side of
(\ref{chap3:eq3.5.61}) is known. For $k=1$ the term corresponding to
this does not appear and so in order to determine $c_{1, g_1, \ldots,
  g_n}$ it is enough to consider only the terms 
$$
\left\{-\psi_1 + \sum\limits^n_{r=1} \psi_r F_{1y_r} \right\}_{g_1,
  \ldots, g_n} + \sum\limits^n_{r=1} e_r(g_r+1) c_{1,g_1, \ldots
  g_{r-1} -1, g_r + 1, \ldots, g_n} .  
$$
If all the coefficients of terms of total degrees $2, \ldots, g-1$
have been determined in $F_1, \ldots, F_n$, then since $\{-\psi_1 +
\sum\limits^n_{r=1} \psi_r F_{1y_r}\}_{g_1, \ldots, g_n}$ involves
only these coefficients, it follows that it is known. Thus we have to
deal only with the term  
$$
\sum\limits^n_{r=1} e_r (g_r +1) c_{1, g_1 , \ldots, g_{r-1}  -1 ,
  g_r+1, \ldots,g_n}. 
$$\pageoriginale
For this we introduce the lexicographic ordering for the
subscripts\break $(g_1, \ldots, g_n)$. If $(g_1, \ldots, g_n)$ and
$(h_1, \ldots, h_n)$ are $n$-tuples of non-negative integers, then we
say that $(g_1, \ldots, g_n)$ is lower than $(h_1, \ldots,h_n)$,
written $(g_1, \ldots, g_n) < (h_1, \ldots, h_n)$ if the first of the
non-vanishing differences $g_1 - h_1, \ldots, g_n - h_n$ is
negative. It is clear that this ordering is transitive: if $(g_1,
\ldots, g_n) < (h_1, \ldots, h_n)$ and $(h_1, \ldots, h_n) < (k_1,
\ldots, k_n)$, then $(g_1, \ldots, g_n) < (k_1, \ldots, k_n)$. In this
ordering we find that each set $(g_1, \ldots, g_{r-1} -1, g_r + 1,
\ldots, g_n)$ of the subscripts  of $c_{k, g_1, \ldots, g_{r-1} -1,
  g_r + 1,\ldots, g_n}$, $r = 2, \ldots, n$, is lower than $(g_1,
\ldots, g_n)$. (Since $e_1 = 0$ by definition, the case $r=1$ is taken
care of). We carry out the induction in the following manner. Given
$(g_1, \ldots, g_n)$, suppose that we have already determined  
\begin{itemize}
\item[{\rm (i)}] all coefficients in $F_1, \ldots, F_n$ of total
  degrees $2, \ldots, g-1$; 

\item[{\rm (ii)}] the coefficient $c_{k-1, g_1, \ldots, g_n}$ of
  $y^{g_1}_1 \ldots y^{g_n}_n$ in $F_{k-1}$; and  

\item[{\rm (iii)}] all the coefficients $c_{k, h_1, \ldots, h_n}$ of
  $y^{h_1}_1 \ldots y^{h_n}_n$ in $F_k$ of total degree $h_1 + \ldots
  + h_n = g$ where $(h_1, \ldots, h_n) < (g_1 , \ldots, g_n)$.  
\end{itemize}
Then we can determine the coefficient $c_{k,g_1, \ldots, g_n}$ of
$y^{g_1}_1 \ldots y^{g_n}_n$ in $F_k$ from the recurrence formula
(\ref{chap3:eq3.5.61}). 

If $k=1$ and $(g_1, \ldots, g_n) = (0, \ldots, 0,2)$, then the
coefficient $c_{1, 0,\ldots, 0,2}$ is determined by the coefficient of
$y^2_n$ in $\psi_1$ and so is known. Then we can determine all the
coefficients of the quadratic terms in $F_k, k =1, \ldots, n$,
successively from the recurrence relation (\ref{chap3:eq3.5.61}). 
Hence\pageoriginale we can begin the induction. Thus, whenever
$\lambda_k\neq \sum\limits^n_{r=1} g_r \lambda_r$, $k=1, \ldots, m$,
all the coefficients in $F_1, \ldots, F_n$ can be determined by
induction on the lexicographic ordering of the subscripts. If this
condition is not satisfied for some $k$, then we set, as before, 
$\chi_k(u,\ldots, u_n, 0, \ldots, 0) =V_k(u_1, \ldots,u_n)$, where
$V_k$ are polynomials consisting only of terms of the form $\alpha_k
u^{g_1}_1 \ldots u^{g_n}_n$ with $\lambda_k = \sum\limits^r_{r=1} g_r
\lambda_r$. The proof is easily modified to suit this case and we
shall not go into the details. 

Before discussing the condition in order that the solution $x_k(s)$ be
real, we shall prove the convergence of the power-series $F_k$
obtained above. This presents some difficulty; the estimates we
obtained in the case of simple eigen-values do not suffice. The
coefficients $c_{k, g_1, \ldots, g_n}$ are determined from the
recurrence formula (\ref{chap3:eq3.5.61}) and we have assumed that
$\lambda_k - \sum\limits^n_{r=1} g_r \lambda_r \neq 0$, $k=1, \ldots,
m$. We have 
$$
|\lambda_k - \sum\limits^n_{r=1} g_r \lambda_r | \geq \rho_k -
\sum\limits^n_{r=1} g_r \rho_r | \geq \rho_k - \sum\limits^n_{r=1} g_r
\rho_r, \; k =1, \ldots, m. 
$$
If $\alpha = \min (-\rho_1, \ldots, -\rho_n) > 0$, we can write  
{\fontsize{10}{12}\selectfont
$$
|\lambda_k - \sum\limits^n_{r=1} g_r \lambda_r| \geq \rho_k  + \alpha
(g_1+ \ldots + g_n) = \rho_k + \frac{\alpha}{2} (g_1 + \ldots + g_n
-1) + \frac{\alpha}{2} (g_1 + \ldots + g_n +1).  
$$}
There are at most finitely many $n$-tuples $(g_1, \ldots,g_n)$ for
which $\rho_k + \dfrac{\alpha}{2} (g_1+ \ldots + g_n -1) < 0$, $k=1,
\ldots, m$, $g_1 + \ldots + g_n \geq 2$. For all other $n$-tuples we
have $|\lambda_k - \sum\limits^n_{r=1} g_r \lambda_r| \geq
\dfrac{\alpha}{2} (g_1 + \ldots + g_n  +1)$. Since $|\lambda_k -
\sum\limits^n_{r=1} g_r \lambda_r| > 0$,\pageoriginale we can find a
constant $\gamma_1 \geq 1$ such that $|\lambda_k - \sum\limits^n_{r=1}
g_r \lambda_r| > \gamma^{-1}_1 (g_1+ \ldots + g_n +1)$, $k=1, \ldots,
m$ for all $n$-tuples $(g_1, \ldots, g_n)$. Hence, from
(\ref{chap3:eq3.5.61}), 
\begin{align*}
& \gamma^{-1}_1 (g_1 + \ldots + g_n +1) |c_{k, g_1, \ldots, g_n}| \leq
  e_k |c_{k-1, g_1, \ldots, g_n}| + \\ 
& + \sum\limits^n_{r=2} e_r (g_r+1) |c_{k, g_1, \ldots, g_{r-1} -1,
    g_r + 1, \ldots, g_n}| + |\{-\psi_k + \sum\limits^n_{r-1} F_{ky_r}
  \psi_r\}_{g_1, \ldots, g_n}|. 
\tag{3.5.62}\label{chap3:eq3.5.62}
\end{align*}
Denoting by $F^*_k$ the formal power-series obtained by replacing the
coefficients in $F_k$ by their absolute values, we have $F_k \prec
F^*_k$, $k=1, \ldots, m$. We denote by $\gamma_2, \gamma_3, \ldots$
sufficiently large positive constants. As in the case of real
eigen-values we have the majorization for $\psi_k$: 
$$
\psi_k \prec \sum\limits^\infty_{g=2} \gamma^g_2 (y_1 + \ldots y_m)^g
= \frac{\gamma^2_2 (y_1 + \ldots + y_m)^2}{1-\gamma_2 (y_1 + \ldots +
  y_m)}. 
$$
We define
{\fontsize{9}{11}\selectfont
$$
\Psi(y_1, \ldots, y_n) = \frac{\gamma^2_2 (y_1 + \ldots + y_n +
  F^*_{n+1} (y_1, \ldots, y_n) + \ldots + F^*_m (y_1, \ldots,
  y_n))^2}{1 - \gamma_2 (y_1 + \ldots + y_n + F^*_{n+1} (y_1, \ldots,
  y_n) + \ldots + F^*_m (y_1,\ldots, y_n))} 
$$}
and we have 
$$
\psi_k(y_1, \ldots, y_n, F_{n+1} (y_1,\ldots, y_n), \ldots, F_m (y_1,
\ldots, y_n)) \prec \Psi (y_1, \ldots, y_n). 
$$

We next observe that the coefficient of $y^{g_1}_1 \ldots y^{g_n}_n$
in $F^*_k$ is $|c_{k, g_1, \ldots, g_n}|$, while its coefficient in
$y_r F^*_{ky_r}$ is $g_r |c_{k, g_1, \ldots, g_n}|$, 
so that\pageoriginale
$$
(1+ g_1 + \ldots + g_n) |c_{k, g_1, \ldots, g_n}| = \left\{F^*_k +
\sum\limits^n_{r=1} y_r F^*_{ky_r} \right\}_{g_1, \ldots, g_n} 
$$
Now (3.4.62) implies the majorization
\begin{equation*}
\gamma^{-1}_1 \left(F^*_k +\sum\limits^n_{r=1} y_r F^*_{ky_r} \right)
\prec e_k F^*_{k-1} + \sum\limits^n_{r=2} e_r y_{r-1} F^*_{ky_r} +
\left(1+ \sum\limits^n_{r=1} F^*_{ky_r} \right)
\Psi. \tag{3.5.63}\label{chap3:eq3.5.63} 
\end{equation*}
From this we want to obtain a majorization similar to what we had in
the case of simple eigen-values. In order to achieve this we replace
all the $F^*_k$ by just one function $F$, which majorizes all the
$F^*_k$, independently of $k$. Let $\mu_1, \ldots, \mu_n$ be positive
numbers $\leq 1$ which we shall choose suitably later. Let  $F =
\sum\limits^m_{k=1} \mu_k F^*_k$, so that $F_k \prec F^*_k \prec
\mu^{-1}_k F$, $k=1, \ldots, m$. It would thus be sufficient to prove
the convergence of the power-series $F$, with non-negative
coefficients, starting with quadratic terms. Multiplying both sides of
(\ref{chap3:eq3.5.63}) by $\mu_k$ and summing over $k=1, \ldots,m$,
and making use of the facts that $\sum\limits^m_{k=1} \mu_k F^*_{ky_r}
= F_{y_r}$, $e_1 = 0$ and $e_k \leq 1$, $k=2,\ldots, n$, we have 
\begin{gather*}
\gamma^{-1}_1 (F + \sum\limits^n_{r=1} y_r F_{y_r}) \prec
\sum\limits^m_{k=2} \mu_k F^*_{k-1} + \sum\limits^m_{r=2} e_r y_{r-1}
F_{y_r} + \sum\limits^n_{k=1} \mu_k \Psi + \sum\limits^n_{r=1} F_{y_r}
\Psi\\ 
\prec \sum\limits^m_{k=2} \mu_k F^*_{k-1} + \sum\limits^n_{r=2}
y_{r-1} F_{y_r} + \left(m+ \sum\limits^n_{r=1} F_{y_r} \right) \Psi.  
\tag{3.5.64}\label{chap3:eq3.5.64}
\end{gather*}
We now choose $\mu_k$ in such a way that all the ratios $\mu_k /
\mu_{k-1}$ are independent of $k$: let $\mu_1=1$, $\mu_k/ \mu_{k-1} =
\gamma^{-1}_1$, $k=2, \ldots, m$, then all $\mu_k \leq
1$.\pageoriginale Since $\mu_k = \dfrac{\mu_k}{\mu_{k-1}} \cdot
\mu_{k-1} = \gamma^{-1}_1 \mu_{k-1}$, $k=2, \ldots, m$, we see that 
$$
\sum\limits^m_{k=2} \mu_k F^*_{k-1} = \gamma^{-1}_1
\sum\limits^{m}_{k=2} \mu_{k-1} F^*_{k-1} \prec \gamma^{-1}_1 F. 
$$
Substituting this in (\ref{chap3:eq3.5.64}) we have
$$
\gamma^{-1}_1 \sum\limits^n_{r=1} y_r F_{y_r} \prec
\sum\limits^n_{r=2} y_{r-1} F_{y_r} + (m+ \sum\limits^n_{r=1} F_{v_r})
\Psi.  
$$
Once again we use the fact that each $F^*_k \prec \mu^{-1}_k F$ to
obtain a majorant for $\Psi$. From the definition of $\Psi$ we have  
\begin{align*}
 \Psi (y_1,\ldots, y_m) & = \sum\limits^\infty_{g=2} \gamma^{g}_2
 (y_1+ \ldots + y_n + F^*_{n+1} + \ldots + F^*_m)^g\\ 
& \prec \sum\limits^\infty_{g=2} \gamma^g_2 (y_1 + \ldots+ y_n +
 (\mu^{-1}_{n+1}+ \ldots + \mu^{-1}_m) F)^g. 
\end{align*}
Taking $\gamma_3 = \mu^{-1}_{n+1} + \ldots + \mu^{-1}_m$, we have
$$
y_1+ \ldots +y_n + F^*_{n+1} + \ldots + F^*_m \prec \gamma_3 (y_1+
\ldots+ y_n + F), 
$$
and hence, with $\gamma_4 = \gamma_2 \gamma_3$, 
$$
m \Psi (y_1, \ldots, y_m) \prec m \sum\limits^\infty_{g=2} \gamma^g_4
(y_1 + \ldots y_n + F)^g \equiv \Phi , \text{ say}. 
$$
Hence we have
\begin{equation*}
\gamma^{-1}_1 \sum\limits^n_{r=1} y_r F_{y_r} \prec
\sum\limits^n_{r=2} y_{r-1} F_{y_r} + (1+ \sum\limits^n_{r=1} F_{y_r})
\Phi. \tag{3.5.65}\label{chap3:eq3.5.65} 
\end{equation*}
Let $\nu_1, \ldots, \nu_n$ be positive numbers, to be chosen suitably
later. Setting $y_k= \nu_k y$, $k =1, \ldots, n$, and $F(y_1, \ldots,
y_n) = F(\nu_1 y, \ldots, \nu_n y) = G(y)$, we have\pageoriginale 
$$
G_y = \sum\limits^n_{r=1} F_{y_r} \frac{dy_r}{dy} =
\sum\limits^n_{r=1} \nu_r F_{y_r}, 
$$
so that $yG_y = \sum\limits^n_{r=1} \nu_r y F_{y_r} =
\sum\limits^n_{r=1} y_r F_{y_r}$. Then from  (\ref{chap3:eq3.5.65}) we
have  
$$
\gamma^{-1}_1 y G_y \prec \sum\limits^n_{r=2} y_{r-1} F_{y_r} +
\left(1+\sum\limits^n_{r=1} F_{y_r} \right) \Phi.  
$$
Since, by definition, $y_{r-1}/ y_r = \nu_{r-1}/\nu_r$, $y_{r-1}
F_{y_r} = y_r F_{y_r} \nu_{r-1}/ \nu_r$, $r =2, \ldots, n$ we obtain 
$$
\sum\limits^n_{r=2} y_{r-1} F_{y_r} = \sum\limits^n_{r=2}
\frac{\nu_{r-1}}{\nu_r} y_r F_{y_r}.  
$$
Now choose $\nu_1 =1$ and $\nu_{r-1}/ \nu_r$ to be independent of $r$,
say, $\nu_{r-1}/ \nu_r = (2\gamma_1)^{-1}$, $r =2, \ldots, n$. Then 
$$
\sum\limits^n_{r=2} y_{r-1} F_{y_r} \prec (2\gamma_1)^{-1}
\sum\limits^n_{r=1} y_r F_{y_r} = (2\gamma_1)^{-1} y G_y. 
$$
On the other hand, since $\nu_r = 2 \gamma_1 \nu_{r-1} = (2
\gamma_1)^{r-1} \nu_1 \geq 1$, we have $F_{y_r } \prec\nu_r F_{y_r}$,
so that we have  
$$ 
\sum\limits^n_{r=1} F_{y_r} \prec \sum\limits^n_{r=1} \nu_r F_{y_r} =
G_y, 
$$
which implies that $1+ \sum\limits^n_{r=1} F_{y_r} \prec 1 + G_y$. Let
$\gamma_5 = m \gamma_4 (\nu_1+ \ldots + \nu_n)$ and  
$$
\wedge \equiv \sum\limits^\infty_{g=2} \gamma^g_5 (y+G)^g =
\frac{\gamma^2_5 (y+G)^2}{1-\gamma_5 (y+G)}. 
$$
Then it is clear that $\Phi \prec \wedge$. Thus finally we obtain the
majorization 
$$
\gamma^{-1}_1 y G_y \prec (2\gamma_1)^{1} y G_y + (1+ G_y)\wedge, 
$$
or,\pageoriginale
$$
y G_y \prec 2 \gamma_1(1+g_y) \wedge.
$$
This is of the same form as the majorization we had for the
power-series $H$ in the case of simple eigen-values. We proceed
exactly as in that case and prove that $G(y)$ converges for $|y| <
\delta$ for sufficiently small positive $\delta$. Then it follows that
$F^*_k$ and $F_k$ converge for $y_1, \ldots, y_n$ in the complex
region $|y_1| < \nu_1 \delta, \ldots, |y_n| < \nu_n \delta$. We have
thus proved the convergence of $F_k$ in a complex neighbourhood of
$(0, \ldots, 0)$. 

We shall inidicate briefly the construction of $F_k$ and the proof of
its convergence in the case in which for some $k$, $\lambda_k =
\sum\limits^n_{r=1} g_r \lambda_r$. As we have mentioned already, the
$F_k$ in this case are so chosen that $\chi_k (u_1, \ldots, u_n,\break 0,
\ldots,0) = V_k (u_1, \ldots, u_n)$, $k= 1, \ldots, m$, where $V_k$ is
a polynomial with complex coefficients containing only terms of the
form $\alpha_{k,g_1, \ldots, g_n} u^{g_1}_1 \ldots\break u^{g_n}_n$ for
those $g_1, \ldots, g_n$ with $g_1+ \ldots  + g_n \geq 2$  which
satisfy $\lambda_k = \sum\limits^n_{r=1} \lambda_r g_r$. (We recall
that there are only finitely many such $n$-tuples). In $F_k$ we define
the coefficient $c_{k,g_1, \ldots, g_n}$ to be zero whenever
$\lambda_k = \sum\limits^n_{r=1} g_r \lambda_r$. Substituting $u_k =
y_k - F_k (y_1, \ldots, y_n)$ in the polynomials $V_k$, we can express
$V_k$ as a power-series in the complex variables $y_1, \ldots, y_n$
with complex coefficients and starting with quadratic terms. Also,
since $F_k$ start with quadratic terms, we have 
$$ 
u^{g_l}_l = (y_l - F_l (y_1, \ldots, y_n))^{g_1} = y^{g_1}_l + \text{ terms of degree } > g_l,
$$\pageoriginale
and hence
$ 
\alpha_{k, g_1,\ldots, g_n} u^{g_1}_1 \ldots u^{g_n}_n = \alpha_{k,
  g_1, \ldots, g_n} y^{g_1}_1 \ldots y^{g_n}_n + \text{ terms of
  degree } > g_1 + \ldots + g_n$.
Now substituting $u_l = y_l - F_l (y_1, \ldots, y_n)$, $l=1, \ldots,n$
in the condition $\chi_k (u_1, \ldots, u_n, 0, \ldots, 0) =
V_k(u_1,\ldots, u_n)$ and comparing coefficients of $y^{g_1}_1 \ldots
y^{g_n}_n$ on both sides, we find that 
\begin{align*}
& \alpha_{k, g_1, \ldots, g_n} = \left\{ \psi_k - \sum\limits^n_{r=1}
  F_{ky_r} \psi_r\right\}_{g_1, \ldots, g_n} + e_k c_{k-1, g_1,
    \ldots, g_n}\\ 
& -\sum\limits^n_{r=2} e_r(g_r+1) c_{k, g_1,\ldots, g_{r-1} -1, \;
    g_r+1, \ldots, g_n} +  \quad \text{ a polynomial in the } 
\end{align*}
coefficients in $F_1, \ldots, F_n$ and $V_1,\ldots, V_n$ of degrees
$\leq g-1$. We determine from this recurrence formula all the
coefficients $\alpha_{k, g_1, \ldots, g_n}$ in $V_k$ by induction on
the lexicographic ordering of the subscripts $g_1, \ldots, g_n$ and
the natural ordering of $k$; the coefficients $c_{k, h_1, \ldots,
  h_n}$ are determined from (\ref{chap3:eq3.5.61}) in the same way
when $\lambda_k \neq \sum\limits^n_{l=1} g_l \lambda_l$. There remains
the proof of the convergence of $F_k$ and $V_k$ as power-series in
$y_1, \ldots, y_n$ which is trivial for the polynomial $V_k$. Since 
$$
V_k(u_1, \ldots, u_n) = \sum\limits_{g_1, \ldots, g_n, \; \lambda_k =
  \sum g_l \lambda_l} \alpha_{k, g_1, \ldots, g_n} u^{g_1}_1 \ldots
u^{g_n}_n, 
$$
if $\gamma_6 = \max |\alpha_{k,g_1, \ldots, g_n}|$, then we have 
$$
V_k (u_1,\ldots, u_n) \prec \gamma_6 \sum\limits_{h_1 + \ldots + h_n
  \geq 2} u^{h_1}_1 \ldots u^{h_m}_n \prec \gamma_6
\sum\limits^\infty_{h=2} (u_1+ \ldots + u_n)^h.  
$$
Since\pageoriginale $u_l = y_l - F_l(y_1, \ldots, y_n)$ and so $u_l
\prec y_l + F^*_l$, we have  
$$
V_k (u_1, \ldots, u_n) \prec \gamma_6 \sum\limits^\infty_{h=2} (y_1+
\ldots + y_n + F^*_1 + \ldots + F^*_n)^h. 
$$
If we define $F = \sum\limits^n_{k=1} \mu_k F^*_k$, $\mu_k$ positive
constants as before, we have $F^*_k \prec \mu^{-1}_k F$, $k =1,
\ldots, n$, and  
$$
V_k \prec \gamma_6 \sum\limits^\infty_{h=2} \gamma^h_7 (y_1 + \ldots +
y_n + F)^h \prec \frac{\gamma_8(y_1 + \ldots + y_n + F)^2}{1-\gamma_6
  (y_1 + \ldots + y_n + F)}, 
$$
which is of the same form as $\Phi$ and again we get a majorization of
the type 
$$
y  G_y \prec 2 \gamma_1 (1+ G_y) \wedge.
$$
The proof of the convergence of $F_k$ proceeds in the same way. 

We consider next the problem of finding a condition in order that the
solution $x_k = x_k(s)$ of $x' = Ax + \varphi (x)$ be real. When all
the eigen-values were simple we found that $\bar{\lambda}_k =
\lambda_l$ for a uniquely determined $l=l_k$ and in that case it was
enough to prove that 
$$
\bar{F}_k(\bar{y}_1, \ldots , \bar{y}_n) = F_l (y_1, \ldots, y_n), l
=l_k, k =1,\ldots, m_, 
$$
where $\bar{y}_k = u_l$. In the case in which $\lambda_k$ has
multiplicity $>1$, $\bar{\lambda}_k$ also has the (same) multiplicity
$>1$, $A$ being a real matrix. It is clear that $l=l_k$ is not
uniquely determined for those $k$ for which $\lambda_k$ has
multiplicity $>1$. However, we can rearrange the eigen-values
$\lambda_k$ and define $l_k$ in such a way that for each\pageoriginale
$k$, $l_k$ is uniquely determined. We shall illustrate  this in the
case in which $e_k =1$. Then, by (\ref{chap3:eq3.5.56}), $\lambda_k =
\lambda_{k-1}$. We now arrange the $\lambda_k$ in such a way that
$\bar{\lambda}_k = \lambda_l$ and $\bar{\lambda}_{k-1} =
\lambda_{l-1}$; so that $\lambda_l = \lambda_{l-1}$. This means that
we now define $l_k$, $k =1, \ldots, n$, in such a way that if for some
$k$, $\lambda_k = \lambda_{k-1}$, then $l_{k-1} = l_k - 1$. 

Since $x$ is real, $x = Cy = \bar{C} \bar{y}$. If $C_k$ denotes the
columns of the matrix $C$, we can choose again $C_l = \bar{C}_k$, $C_k
= \bar{C}_l$ where $l=l_k$ corresponds to the above ordering of
$\lambda_k$, and hence 
$$
C_k y_k + C_l y_l = C_l \bar{y}_k + C_k \bar{y}_l, \;k , l =1,
\ldots,m, 
$$
so that once again $\bar{y}_k = y_l$, where $l=l_k$ is uniquely
defined as above. Then we apply the method used in the case of simple
eigen-values to prove that 
$$
\bar{F}_k (\bar{y}_1, \ldots, \bar{y}_n) = \bar{F}_k (y_{l_1}, \ldots,
y_{l_n}) = F_l (y_1, \ldots, y_n), l = l_k. 
$$
Formally, if we write $\bar{u}_k = u_l$, $l=l_k$, after the proof of
the convergence of $F_k$ in a complex neighbourhood of $y_1 = 0,
\ldots, y_n = 0$, it would follow that $u_l$ is the complex conjugate
of the complex variable $u_k$ when $l=l_k$ defined uniquely as above. 

Now we proceed to find explicitly the solutions of 
$$
u'_k  = \lambda_k u_k + e_k u_{k-1} + \chi_k (u_1, \ldots, u_m), \; k =1, \ldots, m.
$$
For this we apply a method similar to the one used in the case of
simple eigen-values. Given a sufficiently small $\epsilon > 0$, if we
seek a complex solution $u_k$ such that  $\sum\limits^m_{k=1} |u_k|^2
< \epsilon$, we prove that $u_{n+1} = \ldots = u_m = 0$.\pageoriginale
Let $h_{n+1}, \ldots, h_m$ be $m-n$ positive constants, to be chosen
later. Consider the positive function $v = v(s)$ defined for $s \geq
0$ by 
$$
v = \sum\limits^m_{k=n+1} h_k |u_k|^2 = \sum\limits^m_{k=n+1} h_k u_k
\bar{u}_k. 
$$
Suppose that $0 \leq v(s) < \epsilon$  for all $s \geq 0$. Then 
$$
|u_k| \leq \sqrt{\frac{v(s)}{h_k}} < \sqrt{\frac{\epsilon}{h_k}}, \; k
= n+1, \ldots, m. 
$$
We have $v' = \sum\limits^m_{k=n+1} h_k (u'_k \bar{u}_k+
u_k\bar{u}'_k) $. From the differential equations we have 
$$
u'_k= \lambda_k u_k + e_k u_{k-1} + \chi_k(u), \; \bar{u}'_k =
\bar{\lambda}_k \bar{u}_k + e_k \bar{u}_{k-1} + \bar{\chi}_k
(\bar{u}).  
$$
So
\begin{align*}
v' & = \sum\limits^m_{k=n+1} h_k (\bar{u}_k (\lambda_k u_k + e_k
u_{k-1} + \chi_k(u)) + u_k (\bar{\lambda}_k \bar{u}_k + e_k
\bar{u}_{k-1} + \bar{\chi}_k (\bar{u})))\\ 
& = 2 \sum\limits^m_{k=n+1} h_k \rho_k u_k \bar{u}_k +
\sum\limits^m_{k=n+1} h_k e_k (u_k \bar{u}_{k-1} + \bar{u}_k u_{l-1})\\
&\qquad + \sum\limits^m_{k=n+1} h_k (u_k \bar{\chi}_k (\bar{u}) + \bar{u}_k
\chi_k (u)). 
\end{align*}
On the other hand, 
\begin{align*}
& \sum\limits^m_{k=n+1} h_k \rho_k u_k \bar{u}_k +
  \sum\limits^m_{k=n+1} h_k e_k (u_k\bar{u}_{k-1} + \bar{u}_k
  u_{k-1})\\ 
& \quad  = \sum\limits^m_{k=n+1} h_k \rho_k(u_k + \frac{e_k}{\rho_k}
  u_{k-1}) (\bar{u}_k + \frac{e_k}{\rho_k} \bar{u}_{k-1}) -
  \sum\limits^m_{k=n+1} \frac{h_k}{\rho_k} e^{2}_k u_{k-1}
  \bar{u}_{k-1}. 
\end{align*}
Since $e^2_k = e_k$ and $e_{n+1} = 0$ (because $\lambda_n$ has
negative real part and $\lambda_{n+1}$ positive real part, so that
$\lambda_n \neq \lambda_{n+1}$), we have  
\begin{align*}
\sum\limits^m_{k=n+1} \frac{h_k}{\rho_k} e^2_k u_{k-1} \bar{u}_{k-1} &
= \sum\limits^m_{k=n+1} \frac{h_k}{\rho_k} e_k u_{k-1} \bar{u}_{k-1} =
\sum\limits^{m-1}_{k=n} \frac{h_{k+1}}{\rho_{k+1}} e_{k+1} u_k
\bar{u}_k\\ 
& = \sum\limits^{m-1}_{k=n+1} \frac{h_{k+1}}{\rho_{k+1}} e_{k+1} u_k
\bar{u}_k. 
\end{align*}\pageoriginale
Hence we obtain
\begin{align*}
v' & = \sum\limits^m_{k=n+1} h_k \rho_k u_k \bar{u}_k +
\sum\limits^m_{h=n+1} h_k \rho_k (u_k+ \frac{e_k}{\rho_k}
u_{k-1})(\bar{u}_k + \frac{e_k}{\rho_k}\bar{u}_{k-1}) -\\ 
& - \sum\limits^m_{k=n+1} \frac{h_{k+1}}{\rho_{k+1}} e_{k+1} u_k
\bar{u}_k + \sum\limits^m_{k=n+1} h_k (u_k \bar{\chi}_k (\bar{u}) +
\bar{u}_k \chi_k(u))\\ 
& \geq \sum\limits^{m-1}_{k=n+1} (h_k \rho_k-
\frac{h_{k+1}}{\rho_{k+1}} e_{k+1}) u_k \bar{u}_k + h_m \rho_m u_m
\bar{u}_m\\ 
&\qquad + \sum\limits^m_{k=n+1} h_k (u_k\bar{\chi}_k(\bar{u}) +
\bar{u}_k \chi_k (u)).  
\end{align*}
Now we choose $h_{k+1} = \dfrac{1}{2} h_k \rho_k \rho_{k+1}$, $k=n+1,
\ldots, m-1$, $h_{n+1} =1$, so that $h_k \rho_k -
\dfrac{h_{k+1}}{\rho_{k+1}} e_{k+1} = \dfrac{1}{2} h_k \rho_k$ when
$e_{k+1} = 1$ and $= h_k \rho_k$ when $e_{k+1} = 0$. In any case we
have $h_k \rho_k - \dfrac{h_{k+1}}{\rho_{k+1}} e_{k+1} \geq
\dfrac{1}{2} h_{k} \rho_k $ ans also $h_m \rho_m > \dfrac{1}{2} h_m
\rho_m$ (as $h_m \rho_m$ is positive). Hence 
$$
v' \geq \frac{1}{2} \sum\limits^m_{k=n+1} h_k \rho_k u_k \bar{u}_k +
\sum\limits^m_{k=n+1} h_k  (u_k \bar{\chi}_k (\bar{u}) + \bar{u}_k
\chi_k (u)). 
$$
Let $\beta = \min (\rho_{n+1}, \ldots, \rho_m) \succ 0$. Then we have 
$$
v' \geq \frac{1}{2} \beta v + \sum\limits^m_{k=n+1} h_k (u_k
\bar{\chi}_k (\bar{u}) + \bar{u}_k \chi_k (u)).  
$$
As in our previous discussion, since $\chi_k(u_1, \ldots, u_n, 0,
\ldots, 0) = 0$ implies that each term of $\chi_k (u_1, \ldots, u_m)$
contains at least one of $u_{n+1}, \ldots, u_m$ as a factor and
$\chi_k$ starts with quadratic terms, and since $|u_k| <
\sqrt{\dfrac{\epsilon}{h_1}}$, and the $\chi_k$ are uniformly
convergent, we have, for sufficiently large $s$. 
$$
|\sum\limits^m_{k=n+1} h_k (u_k \bar{\chi}_k (\bar{u}) + \bar{u}_k
\chi_k (u))| \leq \frac{1}{4} \beta v. 
$$\pageoriginale
Thus we obtain the differential inequality 
$$
v' \geq \frac{1}{4} \beta v, \; (ve^{-1/4 \beta s})' \geq 0,
$$
which implies that $ve^{-\dfrac{1}{4} \beta s}$ is non-decreasing, and
since it is non - negative, we necessarily have $v=0$. This means that  
$$
u_{n+1}  = \ldots = u_m=0.
$$

The system of differential equations is therefore reduced to
$$
u'_k = \lambda_k u_k + e_k u_{k-1}, k =1, \ldots, n; u'_k =0,\; k = n+1, \ldots, m,
$$
in case $\lambda_k \neq \sum\limits^n_{r=1} g_r \lambda_r$ and to 
\begin{gather*}
u'_k = \lambda_k u_k + e_k u_{k-1} + V_k (u_1, \ldots, u_{k-1}),\\ 
k =1, \ldots, n; u'_k = 0, \; k=n+1, \ldots, m, 
\end{gather*}
in the contrary case. Consider the first case. Since $e_1 = 0$, we
have $u'_1 = \lambda_1 u_1$, or $(u_1 e^{-\lambda_1s})' = 0$, which on
integration from 0 to $s$ gives 
$$
u_1 = c_1 e^{\lambda_1 s}, \; c_1 = u_1 (0).
$$
We insert this in $u'_2 = \lambda_2 u_2 + e_2 u_1$ where $e_2 = 0$, if
$\lambda_2 \neq \lambda_1$ and $e_2 =0$ or 1, if $\lambda_2 =
\lambda_1$ and integrate from 0 to $s$. We can continue this procedure
and obtain all the $u_k$. In the second case, when\pageoriginale for
some $k$, $\lambda_k =\sum\limits^n_{r=1} g_r \lambda_r$, once again
since $e_1=0$ and $V_1 = 0$, we have $u'_1 = \lambda_1 u_1$ from we
obtain $u_1 = e_1 e^{\lambda_1 s}$. Suppose that we have already
proved that  
$$
u_r = (c_r + \mathscr{P}_r (c_1, \ldots, c_{r-1}, s) e^{\lambda_r s}
\equiv Q_r (c_1, \ldots, c_r, s)e^{\lambda_r s}), 
$$
for $r=1, \ldots, k-1$, where $c_r = u_r (0)$ and $\mathscr{P}_r$ is a
polynomial in $c_1, \ldots, c_{r-1}$ and $s$, which vanishes for
$s=0$. Then we show that  
$$
u_k = (c_k + \mathscr{P}_k (c_1, \ldots, c_{k-1}, s))e^{\lambda_k s}
\equiv Q_k (c_1, \ldots, c_k, s) e^{\lambda_k s}. 
$$
If $e_k = 0$, $u'_k = \lambda_k u_k + V_k (u_1, \ldots, u_{k-1})$ and
inserting $u_1, \ldots, u_{k-1}$ already found, and integrating from 0
to $s$, we get $u_k$ as above, as in the case of simple
eigen-values. If $e_k=1$, then necessarily $\lambda_{k-1} = \lambda_k$
and hence 
\begin{gather*}
u'_k = \lambda_k u_k + e_k u_{k-1} + V_k (u_1, \ldots, u_{k-1})\\
= \lambda_k u_k + Q_{k-1} (c_1, \ldots, c_{k-1}, s) e^{\lambda_{k-1}
  s} + V_k (u_1, \ldots, u_{k-1})\\ 
= \lambda_k u_k + Q_{k-1} (c_1, \ldots, c_{k-1}, s) e^{\lambda_ks} +
V_k (Q_1, \ldots, Q_{k-1})e^{\lambda_k s}, 
\end{gather*}
and we can integrate this to obtain $u_k$. Thus all the $u_k$ are
determined by induction. 

As in the case of simple eigenvalues, it follows that if $c_k =
u_k(0)$, then $\sum\limits^m_{k=1} |c_k|^2 < \epsilon$. However, if
$\sum\limits^m_{k=1} |c_k|^2 < \epsilon$, it may no longer be true
that $\sum\limits^m_{k=1} |u_k (s)|^2 < \epsilon$ for all $s \geq 0$,
because of the presence\pageoriginale of the term $\mathscr{P}_k (c_1,
\ldots, c_{k-1}, s)$ in $u_k$. However, $u_k(s) \to 0$ as $s \to
\infty$. 

Finally, if we arrange the eigen-values ans define $l_k$, $k=1,
\ldots, m$, in such a way that $\lambda_{k-1} =\lambda_k$, $l_{k-1} =
l_k -1$, and so $\bar{\lambda}_{k-1} = \lambda_{l-1}$,
$\bar{\lambda}_k = \lambda_l$, then we can prove as in the case of
simple eigen-values that $\bar{u}_k = u_l$, $l=l_k$, $k=1, \ldots,
m$. Then it follows that $\bar{c}_k = c_l$, $l=l_k$, $k=1, \ldots, n$,
and therefore a general solution such that $\sum\limits^m_{k=1}
|u_{k}|^2 < \epsilon$ contains exactly $n$ real parameters. This
completes the proof of Theorem \ref{chap3:thm3.5.3}. 

\section{Application to the three-body problem}\label{chap3:sec6}
We shall now apply the general theory of stability of solutions of
systems of ordinary differential equations to the special case of the
system of differential equations of the three-body problem near a
general collision. 

We obtained in \S\ \ref{chap3:sec4} the system of differential
equations (\ref{chap3:eq3.5.36}) near the general collision at $t=0$
or, equivalently, at $s=\infty$, where $s = e^{-t}$: 
\begin{equation*}
\delta'_k = \sum\limits^6_{l=1} a_{kl} \delta_l+ \varphi_k (\delta_1,
\ldots, \delta_6), \; k =1, \ldots,
6. \tag{3.6.1}\label{chap3:eq3.6.1} 
\end{equation*}
where $a_{kl}$ are real constants determined uniquely by the masses
and the $\varphi_k$ are power-series in $\delta_1, \ldots, \delta_6$
with real coefficients, also determined uniquely by the masses,
starting with quadratic terms and convergent for small values of
$|\delta_1|, \ldots, |\delta_6|$. We consider the corresponding
normalized system of differential equations in the unknown
functions\pageoriginale $u_k (s)$, $k=1, \ldots, 6$, and shall the
general theory of \S\ \ref{chap3:sec5} of solutions asymptotic to the
equilibrium solution. 

We have to discuss the two cases, the equilateral case and the
colli\-near case, and we had computed, at the end of
\S\ \ref{chap3:sec4}, the eigen-values $\lambda_1, \ldots, \lambda_6$
of the matrix $A = (a_{kl})$ in both the cases. In  the equilateral
case there are three negative eigen-values: 
\begin{gather*}
\lambda_1 = - a_2 = \frac{1}{6} -\frac{1}{6}
\sqrt{13-12\sqrt{1-3a}},\\  
\lambda_2 = - a_1 = \frac{1}{6}-\frac{1}{6} \sqrt{13+12\sqrt{1-3a}},
\; \lambda_3 = -a_o = - \frac{2}{3} 
\end{gather*}
where $a = (m_1 m_2 + m_2 m_3 + m_1 m_3) \; (m_1 + m_2 + m_3)^{-2}$;
$0 < a \leq \dfrac{1}{3}$ and $0 < - \lambda_1 \leq -\lambda_2 < -
\lambda_3$, $\lambda_1 = \lambda_2$ only when $a = \dfrac{1}{3}$, or
$m_1 = m_2 = m_3$.  
In the collinear case, there are two negative eigen-values:
\begin{gather*}
\lambda_1 = - b_o = - \frac{2}{3}, \lambda_2 = - b_1 = \frac{1}{6}
-\frac{1}{6} \sqrt{25+16b},\\ 
\text{where } b = \frac{m_1 (1+(1-\omega)^{-1} + (1-\omega)^{-2}) +
  m_3 (1+\omega^{-1} + \omega^{-2})}{m_1+ m_2(\omega^{-2} +
  (1-\omega)^2) + m_3}\\ 
\qquad > 0 ; 0 < -\lambda, < -\lambda_2.
\end{gather*}

We first consider the case in which $\lambda_k \neq
\sum\limits^p_{l=1} g_l \lambda_l$ for non-negative integers $g_1,
\ldots, g_p$ with $g_1 + \ldots + g_p \geq 2$ ($p=2$ in the collinear
case and $p=3$ in the equilateral case). In the equilateral case, 
\begin{equation*}
u_1 = c_1 e^{-a_2 s}, u_2 =c_2 e^{-a_1 s}, u_3 = c_3 e^{-a_o s}, 
\tag{3.6.2}\label{chap3:eq3.6.2}
\end{equation*}
provided that not all $m_1, m_2, m_3$ are equal, and in the collinear case,
\begin{equation*}
u_1 = c_1 e^{-b_os} , \; u_2 = c_2 e^{-b_1 s}. \tag{3.6.3}\label{chap3:eq3.6.3}
\end{equation*}
From (\ref{chap3:eq3.6.2})\pageoriginale and (\ref{chap3:eq3.6.3}) we
find that the essential difference between the equilateral and
collinear cases consists in the fact that the general solution
asymptotic to the  equilibrium solution as $s \to \infty$ contain 3
real parameters in the former case and only 2 in the latter. We
observe that $a_1$, $a_2$ and $b_1$ are in general irrational, whereas
$a_o = b_o = \dfrac{2}{3}$ is rational. Since $e^{-s} =t$, we have
from (\ref{chap3:eq3.6.2}) and (\ref{chap3:eq3.6.3}), 
\begin{align*}
u_1 & = c_1 t^{a_2}, \; u_2 = c_2 t^{a_1}, \; u_3 = c_3 t^{a_o} \quad
\text{in the equilateral case, }\tag{3.6.4}\label{chap3:eq3.6.4}\\ 
u_1 & = c_1 t^{b_o}, \; u_2 = c_2 t^{b_1} \quad \text{in the collinear
  case. }\tag{3.6.5}\label{chap3:eq3.6.5} 
\end{align*}
 
We have to examine the possibility of multiple eigen-values. Since in
the collinear case, the possible double roots $b_2, b_3$ have positive
real parts and the solution depends only on the eigen-values with
negative real parts, it follows that the only solutions are those
given by (\ref{chap3:eq3.6.5}) whenever $\lambda_k \neq
\sum\limits^p_{r=1} g_r \lambda_r$. In the equilateral case, the only
possible double eigen-value is $a_1 = a_2$. Then $\lambda_1 =
\sum\limits^3_{r=1} g_r \lambda_r$ is satisfied with $g_1 = 0$, $g_2 =
g_3 = 0$. 

We now consider the case in which for some $k$, we have $\lambda_k =
\sum\limits^p_{r=1} g_r \lambda_r$, $g_1 + \ldots + g_p \geq 2$. We
take the equilateral case first. We have to discuss the possibility of
the existence of non-negative integers $g_1, g_2, g_3$ with $g_1 + g_2
+ g_3 \geq 2$ such that $-\lambda_k = g_1 a_2 + g_2 a_1 + g_3 a_o$,
where $-\lambda_1 = a_2$, $-\lambda_2 = a_1$, $-\lambda_3 = a_o(a_o >
a_1 \geq a_2)$. Since the polynomial $V_1 \equiv 0$ always, it is
enough to consider only the two cases $k=2,3$ corresponding to
$-\lambda_2 =a_1$ and $-\lambda_3=a_o$. 

If $k=2$,\pageoriginale then $-\lambda_2 = a_1$ and $g_2 = g_3 =0$, so
that $V_2(u_1) = \alpha u^{g_1}_1$. We have $a_1 = g_1 a_2$, or
$\dfrac{a_1}{a_2} = g_1 \geq 2$. Denoting this integer by $h$, we have  
\begin{equation*}
h = \frac{a_1}{a_2} = \frac{\sqrt{13+w}-1}{\sqrt{13-w}-1} \text{ where
} w = 12 \sqrt{1-3a}. \tag{3.6.6}\label{chap3:eq3.6.6} 
\end{equation*}
For each integer $h=2, 3, \ldots, $ we can determine $w$ and $a$ from
this equation and $V_2 (u_1) = \alpha u^h_1$. 

If $k =3$, then $-\lambda_3 = a_o = g_1 a_2 + g_2 a_1$ with $g_1 + g_2
\geq 2$, since in this case $g_3 = 0$. We observe that $2a_1 >a_o$, so
$g_2 =0$ or 1. Hence, either $g_1 \geq 2 $ (if $g_2 = 0$) or $g_1 \geq
1$ (if $g_2 =1$). We show that $g_2$ cannot be 1, so $g_1 \geq 2$
necessarily. In fact, since we see that $a_2 + a_1 >a_o$, we have $g_2
= 0$ and so $g_1 \geq 2$. Hence $a_o = g_1 a_2$ or $\dfrac{a_o}{a_2} =
g_1 \geq 2$. Denoting this integer by $g$ we have 
\begin{equation*}
g = \frac{a_o}{a_2} = \frac{4}{\sqrt{13-w}-1} \text{ where } w = 12
\sqrt{1-3a}.  
\tag{3.6.7}\label{chap3:eq3.6.7}
\end{equation*}
Again we can determine $w$ and a from (\ref{chap3:eq3.6.7}) for each
$g \geq 2$. In this case $V_3 (u_1, u_2) = \beta u^g_1$. 

We next prove that only one of the possibilitie
$$
V_2 (u_1) = \alpha u^h_1, \; V_3 (u_1, u_2) = 0 ; \; V_2 (u_1) = 0, \;
V_3 (u_1, u_2) = \beta u^g_1,  
$$
can occur. (It cannot happen that $V_2 (u_1) = \alpha^h_1$ and $V_3
(u_1, u_2) = \beta u^g_1$). In order to prove this, we show that
(\ref{chap3:eq3.6.6}) and (\ref{chap3:eq3.6.7}) cannot be satisfied
simultaneously for integers $g, h \geq 2$. We have, from
(\ref{chap3:eq3.6.6}) and (\ref{chap3:eq3.6.7}),  
$$
\sqrt{13-w} =1 + \frac{4}{g} ,\; \sqrt{13+w} = 1 + \frac{4h}{g}.
$$\pageoriginale
Eliminating $w$ by squaring and adding we get
$$
26 g^2 = (g+4)^2 + (g+4h)^2.
$$
So it is enough to prove that this diophantine equation does not have
integer solutions $g, h \geq 2$. We can write this as $(25 g - 4)^2 =
416 + 25 (g+4h)^2$ from which we have  
\begin{gather*}
416 =(25g - 4)^2 -25 (g+4h)^2 = (30g + 20 h -4) \; (20 g - 20 h - 4)\\
\text{or } \qquad (15 g + 10 h -2) \; (5g - 5h -1) = 52.
\end{gather*}
Setting $p=15 g + 10 h - 2$, $q = 5g  - 5h-1$, we have to show that
there are no integers $p,q$ such that  
\begin{equation*}
pq = 52, \; p-3q -1 = 25h,\tag{3.6.8}\label{chap3:eq3.6.8}
\end{equation*}
By definition of $p$, $p>0$ and since $pq = 52$, $q > 0$. The only
integer factorisations $pq$ of 52, with $q<p$, are $p=52$, $q=1$,
$p=26$, $q=2$ and $p=13$, $q=4$. It is easy to check that none of
these factorisations satisfies (\ref{chap3:eq3.6.8}) with integer $h
\geq 1$. This proves that only one of the exceptional cases 
\begin{align*}
V_2 (u_1) & = \alpha u^h_1, \; V_3 (u_1, u_2) = 0;  \tag{3.6.9}\label{chap3:eq3.6.9}\\
V_2 (u_1)& = 0, \; V_3 (u_1, u_2)  = \beta u^g_1, \tag{3.6.10}\label{chap3:eq3.6.10}
\end{align*}
can occur.

Suppose\pageoriginale that the possibility (\ref{chap3:eq3.6.9})
holds. Then $u_1 = c_1 e^{\lambda_1s} = c_1 t^{a_2}$, since $\lambda_1
= - a_2$ and $e^{-s} = t$, $c_1 = u_1(0)$. Inserting this in $u'_2 =
\lambda_2 u_2 + V_2 (u_1)$ we get 
$$
u'_2 = - a_1 u_2 + \alpha u^{g_1}_1 = - a_1 u_2 + \alpha c^{h}_1
e^{\lambda_1 hs} = - a_1 u_2 + \alpha c^{h}_1 e^{-a_1s},  
$$
so $(u_2 e^{a_1 s})' = \alpha c^h_1$, and integrating from 0 to $s$
and putting $u_2(0) = c_2$, 
$$
u_2 e^{a_1 s} = c_2 + \alpha c^h_1 s, \text{ or } u_2 = (c_2 + \alpha
c^h_1 s) e^{-a_1s}. 
$$
Thus, if (\ref{chap3:eq3.6.9}) holds, then
$$
u_1 = c_1 t^{a_2}, u_2 = (c_2 - \alpha c^h_1 \log t) t^{a_1}, u_3 =
c_3 t^{a_o}. (h \text{ integer } \geq 2) 
$$
Similarly, if (\ref{chap3:eq3.6.10}) holds, we have 
$$
u_1 = c_1 t^{a_2}, u_2 = c_2 t^{a_1}, u_3 = (c_3 - \beta c^g_1 \log t)
t^{a_o}. (g \text{ integer } \geq 2) 
$$
In the equilateral case, if we have a double root $a_1 = a_2$ (when
$m_1 = m_2 = m_3$), we have  $h=1$. So $V_2(u_1) = \alpha u$ and hence 
$$
u_1  = c_1 t^{a_2}, u_2 = (c_2 - \alpha c_1 \log t) t^{a_1}, u_3 = c_3 t^{a_o}.
$$
The value of $\alpha$ will be just $e_2 (=0 \text{ or }1)$ since
$\lambda_1 = - a_2 = - a_1 = \lambda_2$. 

Finally we consider the collinear case. In this case there is only
condition $\lambda_k = \sum\limits^2_{r=1} g_r \lambda_r$, i.e. $b_1
=j b_o$, $j$ integer $\geq 2$, because $\lambda_1 = - b_o$,
$\lambda_2=-b_1$ and $0 < -\lambda_1 < -\lambda_2$ this gives $b = j^2
+ \dfrac{j-3}{2}$. Moreover, we have $V_2 (u_1) = \gamma u^j_1$ so
that we have  
$$
u_1 = c_1 2^{\lambda_1s} = c_1 e^{-b_o s} = c_1 t^{b_o}, u_2 = (c_2+
\gamma c^j_1 s) e^{\lambda_2 s} = (c_2 -\gamma c^j_1 \log t) t^{b_1}.  
$$\pageoriginale
We collect together the results in the two cases. 

\medskip
\noindent{\textbf{Equilateral case.}}
\begin{itemize}
\item[{\rm(i)}] If $\dfrac{a_o}{a_2}$ and $\dfrac{a_1}{a_2}$ are not
  integers $u_1 = c_1 t^{a_2}$, $u_2 = c_2 t^{a_1}$, $u_3 = c_3
  t^{a_o}$, $u_k = 0$, $k=4, 5,6$. 

\item[{\rm(ii)}] If $\dfrac{a_o}{a_2} = $ an integer $g \geq 2$,
$$
u_1 = c_1 t^{a_2}, u_2 = c_2 t^{a_1}, u_3 = (c_3 - \alpha c^g_1 \log
t) t^{a_o}, u_k =0, k=4,5,6, 
$$

\item[{\rm(iii)}] $u_1 = c_1 t^{a_2}$, $u_2 = (c_2 - \beta c^h_1 \log
  t) t^{a_1}$, $u_3 = c_3 t^{a_o}$, $u_k = 0 $, $k=4,5,6$, where
  $\dfrac{a_1}{a_2} = $ an integer $h \geq 1$. 
\end{itemize}

\medskip
\noindent{\textbf{Collinear case.}}
\begin{itemize}
\item[{\rm (i)}] If $\dfrac{b_o}{b_1}$ is not integral then
$$
u_1 = c_1 t^{b_o}, \; u_2 = c_2 t^{b_1}, \; u_k = 0, \; k = 3,4,5,6.
$$

\item[{\rm (ii)}] If $\dfrac{b_1}{b_o} = $ an integer $j \geq 2$,
$$
u_1 = c_1 t^{b_o}, u_2 = (c_2 - \gamma c^j_1 \log t)t^{b_1}, u_k =0, k
= 3,4,5,6. 
$$ 
The constants of integration $c_k$ have small absolute values.
\end{itemize}

By taking the inverse of the transformation $u_k = y_k - F_k (y_1,
\ldots, y_n)$, $k=1, \ldots, m$, we have $y_k = u_k + G_k (u_1,
\ldots, u_n)$, $k=1, \ldots,m$, where $G_k$ are power series, with
coefficients complex in general, starting with quadratic terms and
converging for small $|u_k|$. In our case, $m=6$ and $n=3$ or 2
according us we are in the equilateral or in the\pageoriginale
collinear case. So we see after inserting the above solutions $u_k$,
that $y_1, \ldots, y_6$ are power-series, without constant terms, in
$u_1, u_2, u_3$ in the equilateral case, and in  $u_1, u_2$ in the
collinear case. Since $y_k$ are obtained from $\delta_k$ by a linear
transformation $\delta = Cy$, $|C| \neq 0$, it follows that $\delta_1,
\ldots, \delta_6$ are power-series without constant terms, in $u_1,
u_2$, in the collinear case and in $u_1,u_2,u_3$ in the equilateral
case; these power-series converge for small $|u_k|$, and hence for
sufficiently small $t$. Since the power-series do not have constant
terms and $u_k (t) \to 0$ as $t \to 0$, it follows that  
$\delta_k \to 0$ as $t \to 0$.

We recall that $p^*_k = \bar{p}_k + \delta_k$, $q^*_k = \bar{q}_k+
\delta_{k+3}$, $k=1,2,3,$ where $\bar{p}_k, \bar{q}_k$ are uniquely
determined by the masses. We also have $\bar{p}_1 > 0$. Since $q_4 =0$
for a collision orbit, we have $\delta_7 = 0$. We have by definition
$p_4 = p^*_4= \delta_8$ and $\delta_8$ satisfies the differential
equation 
\begin{equation*}
\delta'_8 = \sum\limits^6_{l=1} a_{8l} \delta_l+ \varphi_8 (\delta_1,
\ldots, \delta_6),  
\tag{3.6.11}\label{chap3:eq3.6.11}
\end{equation*}
where $a_{8l}$ are real constants determined uniquely by the masses
and $\varphi_8$ is a power-series with real coefficients, again
uniquely determined by the masses, and starting with quadratic
terms. Hence the right side of (\ref{chap3:eq3.6.11}) is a
power-series $Q= Q(\delta_1, \ldots, \delta_6)$ with real coefficients
and without constant terms. We have determined $\delta_1,
\ldots,\delta_6$ as power-series in $u_k (k=3 \text{ or }2)$ without
constant terms and converging for small $|u_k|$. Hence $Q$ is also a
power-series in $u_k$ without constant term and converging for small
$|u_k|$. So $p_4 = \delta_8$ can be obtained by integrating
$Q$\pageoriginale from 0 to $s$. In order to study the behaviour of
$p_4$ as $t \to 0$ (or, equivalently, as $s \to \infty$), we have to
prove the convergence, as $s \to \infty$, of the integral 
$$
\int\limits^s_o Q (\delta_1, \ldots, \delta_6) ds.
$$
(We do this only in the case in which the relation $\lambda_k \neq
\sum\limits^p_{r=1} g_r \lambda_r$ holds; the discussion in the other
case is similar). A typical term in $Q$ is $\beta_{h_1, \ldots, h_n}
u^{h_1}_1 \ldots u^{h_n}_n$, where $n=3$ or 2 according as we are in
the equilateral or collinear case. In the equilateral case, $u_1, u_2,
u_3$ are given by (\ref{chap3:eq3.6.4}) when $a_o, a_1, a_2$ are
distinct and so a typical term of $Q$ is $\beta_{h_1 h_2 h_3}
c^{h_1}_1 c^{h_2}_2 c^{h_3}_3 e^{-(h_1 a_2 + h_2 a_1 + h_3
  a_o)s}$. This term is integrable in $0 \leq s \leq \infty$ and we
have 
$$
\int\limits^s_0 e^{-(h_1a_2 + h_2 a_1+ h_3 a_o)s} ds = \frac{e^{-(h_1
    a_2 + h_2 a_1 + h_3 a_o)s}-1}{-(h_1 a_2+ h_2 a_1 + h_3 a_o)} 
$$
and this tends to $(h_1 a_2 + h_2 a_1+ h_3 a_o)^{-1}$ as $s \to
\infty$. Since the constants $c_1, c_2, c_3$ have small absolute
values, so have $u_1, u_2, u_3$. Since $Q$ converges uniformly for
small $|u_k|, k =1,2, 3,$ we can integrate the power-series and obtain 
$$
p_4 = \delta_8 = \int\limits^s Q (\delta_1, \ldots, \delta_6) ds + \bar{p}_4
$$
where $\bar{p}_4$ is a constant of integration, and so we have $p_4 =
\bar{p}_4 + $ a unique power-seires in $u_1, u_2, u_3$ withtout
constant term. The power-series\pageoriginale converges for small
$|u_k|$ and so for small $t$. This proves that $p_4$ tends to a finite
limit $\bar{p}_4$ as $t \to 0$. 

Since the differential equations of motion remain invariant under an
orthogonal transformation of coordinates, we can perform a fixed
orthogonal transformation in the plane of motion, so that $\bar{p}_4$
becomes $0$  in the new coordinate system. In other words, we may
assume that $p_4 \to 0$ as $t \to 0$. A similar argument can be
carried out in the case of double eigen-values, and also in the
collinear case, and we see that $p_4$ tends to a finite limit, which
may be assumed to be 0, as $t \to 0$, in all cases. 

We shall now go back to the original coordinate system and find $x_k$,
$y_k$, $k =1,2,3$, the coordinates of $P_1, P_2, P_3$. First we have 
\begin{align*}
p_k & = p^*_k t^{2/3}  = (\bar{p}_k + \delta_k) t^{2/3}, \bar{p}_k > 0, \\
q_k & = q^*_k t^{-1/3} = (\bar{q}_k + \delta_{k+3}) t^{-1/3}, k =1,2,3,\\
p_4 & = p^*_4 = \delta_8, q_4 = q^*_4 = 0. 
\end{align*}
Hence $p^*_k$ are power-series in $u_k$, $k=1,2,3$ or $k=1,2$
according as we are in the equilateral or collinear case, which
converge for small $|u_k|$. And $p_k= p^*_k (u_l) t^{2/3}$, $q_k =
q^*_k (u_l) t^{-1/3}$, $k =1,2,3$. We can also calculate $c = \cos
p_4$ and $s = \sin p_4$ as power-series in $u_l$ which converge for
small $|u_l|$. Since 
\begin{align*}
\xi_1 & = p_1 c, \xi_2 = p_1 s, \xi_3 = p_2 c - p_3 s, \xi_4 = p_2 s +
p_3 c, \\ 
\eta_1 & = q_1 c - q_o s, \eta_2 = q_1 s  + q_o c, \; \eta_3 = q_2 c -
q_3 s , \eta_4 = q_2 s + q_3 c, 
\end{align*}
we see\pageoriginale that the relative coordinates $(\xi_1, \xi_2)$
and $(\xi_3, \xi_4)$ of $P_1$ and $P_2$ with respect to $P_3$ and the
corresponding components of momenta $\eta_1, \ldots, \eta_4$, are also
given by convergent power-series in $u_k$: 
$$
\xi_k = \xi^*_k (u_1, u_2, u_3) t^{2/3}, \; \eta_k = \eta^*_k (u_1,
u_2, u_3) t^{-1/3} 
$$
in the equilateral case, and 
$$
\xi_k = \xi^*_k (u_1, u_2) t^{2/3}, \; \eta_k = \eta^*_k (u_1, u_2) t^{-1/3}
$$ 
in the collinear case. We can now go back to the absolute coordinates 
$x_k, y_k$. If $v$ denotes any of the six coordinates $x_k, y_k, k
=1,2,3$, then we have 
$$
v= t^{2/3} P(u_1, u_2, u_3)\text{ and } v = t^{2/3} P(u_1, u_2)
$$
in the equilateral and collinear cases respectively, where $P$ is a
power-series convergent for small $|u_k|$, and hence for small
$|c_k|$. The components of momenta also have power-series expansions 
$$
w = t^{-1/3} H (u_1, u_2, u_3), w = t^{-1/3} H(u_1, u_2),
$$
$H$ converging for small $|u_k|$ and hence for small $|c_k|$. 

We consider now the manifold of all the collision
orbits. Corresponding to different values of the real parameters $c_1,
c_2, c_3$ in the equilateral case and $c_1, c_2$ in the collinear
case, we obtain different collision orbits in a neighbourhood of $t
=0$. Let us determine the dimensions of this manifold in the
neighbourhood of $t=0$. The coordinates\pageoriginale $x_k, y_k, k
=1,2,3,$ being power-series in $u_k$ ($k=1,2,3$ or $k=1,2$), we have
three real parameters in the equilateral case and two real parameters
in the collinear case. We have proved that the motion takes place in a
fixed plane, chosen as the $(x,y)$-plane in our discussion. Its
position is determined by the three angles of a normal to it: this
involves three extra independent real parameters in both the limiting
cases. We have assumed that the centre of gravity remains fixed at the
origin, and the centre of gravity integrals involve six real
parameters. Finally, the points on a orbit are parametrised by the
real variable $t$. Thus in the equilateral case we have 13 independent
real parameters, and in the collinear case 12. Since the coordinate
functions are regular analytic functions of these parameters, we
conclude that the manifold of all collision orbits is a real analytic
manifold, of dimension 13 in the equilateral case and 12 in the
collinear case, in a neighbourhood of $t =0$. Further, in the
collinear case there are three distinct orderings of the points
$P^*_1, P^*_2, P^*_3$ and hence, corresponding to these, there are
three distinct real 12-dimensional analytic manifolds. (In the case of
a simple collision we have seen that we have power-series expansions
for the coordinates in the variable $t^{1/3}$; the manifold of
collision orbits is there a real analytic manifold of dimension 16 in
the neighbourhood of a simple collision). 

We remark that since our solutions are described only near $t  =0$,
the above description of the collision orbits is purely local. It is
not\pageoriginale possible to describe the manifold of collision
orbits in the large, that is, for all $t$, by our method. 

We consider the nature of the singularity when there is a general
collision at $t=0$. Since the coordinates are power-series in $u_1,
u_2, u_3$ or in $u_1, u_2$, the nature of the singularity depends on
the arithmetical nature of the eigen-values $a_o, a_1, a_2$ in the
equilateral case and $b_o, b_1$ in the collinear case. If $a_2, a_1$
in the equilateral case and $b_1$ in the collinear case are rational,
then we have an algebraic branch point at $t = 0$, so that the
solutions of the three-body problem can be uniformised in a
neighbourhood of $t=0$. If $a_2, a_1$ are irrational and $c_1, c_2
\neq 0$ in the equilateral case, and $b_1$ irrational and $c_2 \neq 0$
in the collinear case, we have an essential singularity at $t = 0$. In
this case it is not possible to continue the solutions analytically
beyond the general collision. 

\end{proof}
