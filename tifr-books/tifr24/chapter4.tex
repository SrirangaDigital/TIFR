\chapter{Multi-dimensional Brownian Motion}%chap 3

We\pageoriginale have already studied one-dimonsional Brownian
motion. We shall now 
define $k$-dimensional Brownian motion, determine its generator and
deduce the main result of Potential Theory using properties of the
$k$-dimensional Brownian motion. 

\section{Definition}\label{chap3-sec1}%Sec 1.

We first define $k$-dimensional Wiener process. Let $x (t,w) = (x_i
(t,w), i= 1,2,\ldots, k)$ be a k-dimensional stochastic process on
a probability space $\Omega (P)$. $x (t,w)$ is called a
$k$-{\em{dimensional Wiener process}} if (1) its components $x_i (t,w)$
are one-dimensional Wiener processes, and (2) $x_i (t,w), 1 \leq i \leq
k$, are stochastically independent processes.  

It is easy to construct a $k$-dimensional Wiener process $x (t,w)$ on
$\Omega (P)$ from a $1$-dimensional Wiener process $\xi (t, \lambda)$
on $\Lambda (Q)$. It is sufficient to take $\Omega = \Lambda^k$  and $P=$ the
product probability $Q^k$, and define for $w = (\lambda_1 ,\ldots,
\lambda_k)$,  
$$
x (t, w) = (\xi (t, \lambda_1) ,\ldots, \xi (t, \lambda_k)).
$$

We now study the $k$-dimensional standard Brownian motions. Let $S =
R^k$, $W=$ the set of all continuous functions into $S$ and define  
$$
P_a (B) = P [a+x (., w) \in B].
$$

Here\pageoriginale $a = (a_1 ,\ldots,a_k)$. It is easily verified that
$\mathbb{M} = 
(S,W,P_a)$ is a Markov process $\mathbb{M}$ is called the \textit
{$k$-dimensional standard Brownian\break motion}. The transition probability
of the process is 
$$
\displaylines{\hfill
P(t, a, E) = \int_E N_k (t, a, b) db,\hfill \cr
  \text{where}\hfill 
  N_k (t,a,b) = N (t,a_1,b_1) \cdots N (t,a_k,b_k)\hfill }
$$

Since, for $f \in C (S)$,
$$
H_t f (a) = \frac{1}{(2 \pi t)^{k/2}} \int e^{-|b|^2/2t} f (a+b)db,
|b|^2 = b^2_1 +\cdots+b^2_k 
$$
is also in $c (S), \mathbb{M}$ is a strong Markov process.

Let $\theta$ denote the group of congruence (distance-preserving)
transformations of $R^k$. If $O\in\theta$, then $O$ indues a
transformation, which again we denote by $O$, of $W \to W$ defined by
$$
(Ow) (t) = O W (t).
$$
$O$ carries measurable subsets of $W$ into measurable subsets. For any
subset $L \subset W$, we define 
$$
O L = (Ow : w \in L).
$$

The following facts are easily verified
\begin{itemize}
\item[(0.1)] $P (t,Oa, OE) = P (t, a, E)$
\item[(0.2)] $P_{O{_a}} (O B) = P_a (B)$.
\end{itemize}

If\pageoriginale $O \in \theta$ is a rotation around $a$, i.e.  if $Oa = a$,
$P_a (OB) = P_a(B)$, so that $O$ is a $P_a$-measure preserving
transformation of $W$ onto $W$. 

\section{Generator of the $k$-dimensional Brownian
  motion}\label{chap3-sec2}%Sec 2 

Let $\mathscr{D} = \mathscr{D} (R^k)$ be the space of all $C^\infty$
function with compact supports. For $\varphi \in \mathscr{D}$,
put 
$$
\theta (t, a) = H_t \varphi (a),
$$
and
$$ 
\psi (a) \equiv \psi (a, \alpha) = G_\alpha \varphi (a) = \int^\infty_0
  e^{-\alpha t} \theta (t, a)dt.
$$

Now
\begin{align*}
\theta(t,a) & =\int\limits_{R^k} \frac{1}{(2 \pi t)^{k/2}} e^{-|b|^2/2t}
  \varphi (a+b)db\\ 
  & = \int_{R^k} \frac{1}{(2 \pi)^{k/2}} e^{-|b|^2 /2} \varphi (a+b \sqrt{t})db, 
\end{align*} 
and a simple calculation gives
$$
\frac{\partial \theta}{\partial t} = \frac{1}{2} \Delta \theta,\quad
\theta (0+a) = \varphi (a). 
$$
 
Taking Laplace transform, the last equation gives
$$
(\alpha - \frac{1}{2} \Delta) \psi = \varphi.
$$
 
In order to show that $\psi$ is the unique solution of this equation,
it is enough to show that if $\psi \in C^2$, $\psi (a) \to 0$ as
$|a| \to \infty$\pageoriginale and $(\alpha - \dfrac{1}{2} \Delta)\psi
= 0$, then $\psi \equiv 0$. To prove this, suppose that $\psi (a) > 0$ for
some $a$. Then since $\psi (a) \to 0$ as $|a| \to \infty$, the maximum
of $\psi(a)$ is attained at a finite point $a_0$ and  
$$
\psi (a_0) = \max \psi(a) > 0.
$$

Therefore 
$$
\Delta \psi (a_0) \leq 0,
$$
and hence 
$$
(\alpha - \dfrac{1}{2}\Delta) \psi (a_0) > 0.
$$

Thus $\psi (a) \leq 0$. Replacing $\psi$ by $-\psi$, we see that $\psi
\equiv 0$. This proves our contention. 

Now, let $f \in \mathscr{B}(R^k)$. Then
$$
u(a) = G_\alpha f (a) = \int G(\alpha, |b-a|)f(b)db,
$$
where 
$$
G(\alpha, |b-a| = \int^\infty_0 \dfrac{e^{- \alpha
    t-|b-a|^2/2t}}{(2 \pi t)^{k/2}} dt.
$$ 

Note that $G(\alpha, |b-a|)$ is continuous in $(a, b)$. It is
immediate that $u \in \mathscr{B}(R^k)$ and we can consider $u$ as a
distribution in the Schwartz sense. Then  by the definition of the
derivative of a distribution, for any $\varphi \in \mathscr{D}$, 
\begin{align*}
\left(\alpha - \frac{1}{2}\Delta\right) u(\varphi) & = \int u(a)
  \left(\alpha -
  \frac{1}{2} \Delta\right)\varphi (a) da \\ 
  & = \iint G(\alpha, |a-b|)f(b) \left(\alpha- \frac{1}{2} \Delta\right) \varphi
  (a) da ~db\\ 
  & =\int f(b) db \int G (\alpha, |b-a|) \left(\alpha- \frac{1}{2} \Delta\right)
  \varphi(a) da \\ 
  & = \int f(b) \psi (b) db ;
\end{align*}\pageoriginale
where
$$
\psi (b) = \int G(\alpha, |a-b|) \left(\alpha -
\frac{1}{2}\Delta\right)\varphi(a) da. 
$$

If $\theta = \left(\alpha - \frac{1}{2} \Delta\right) \varphi$, then $\theta \in
\mathscr{D}$ and   
$$
\psi = G_\alpha \theta,
$$
and from the above we get
$$
\left(\alpha- \frac{1}{2}\Delta\right) \psi = \theta = \left(\alpha -
\frac{1}{2}\Delta \right)\varphi, 
$$
and hence\quad $\psi = \varphi$.

Thus 
$$
\left(\alpha - \frac{1}{2} \Delta\right) u (\varphi) = \int f(b)
\varphi(b) db,  
$$
and this means that the distribution $\left(\alpha - \dfrac{1}{2}
\Delta\right)u$ 
is defined by the function $f$. (Of course, any function equal to $f$
almost every-where defines the same distribution.) 

What we have above also shows that if $u = 0$ then the distribution
$\left(\alpha - \dfrac{1}{2}\Delta\right) u = 0$ so that $f=0$ a.e. Hence 
$$
\mathfrak{N}= \left\{ f : f = 0 \text{ a.e.}\right \}. 
$$
\begin{tabular}{lr@{\;\;}p{8.5cm}}
  Let  & $\mathscr{R}$ & = $\left \{u : u, \Delta u \in \mathscr{B}(R^k),
  \Delta u ~\text{ is the distribution sense } \right \}$\\ 
  & & = $\bigg \{ u : u \in \mathscr{B}(R^k)$  and the distribution
  $\Delta u$ is defined\\ 
  & & \hfill by a function in $\mathscr{B}(R^k)\bigg\}$. 
\end{tabular}

We\pageoriginale see form the above that $\mathscr{R} \subset
\mathscr{R}_+$. Now suppose $u \in \mathscr{R}_+$. Then $u \in
\mathscr{B}(R^k)$ and 
$\Delta u$ is defined by a function in $\mathscr{B}(R^k)$. Let
$(\alpha - \dfrac{1}{2}\Delta)u$ be defined by $f \in
\mathscr{B}(R^k)$. Put $G_\alpha f = v$ and from the above we see that
$(\alpha - \dfrac{1}{2}\Delta)v$ is defined by $f$. Hence $(\alpha u_1
- \dfrac{1}{2} \Delta u_1) = 0$ where $u_1 = u - v$. We prove that
$u_1 = 0$ a.e. Now 
$$
\int \left(\alpha - \dfrac{1}{2}\Delta\right) \varphi(a) u_1 (a) da = 0
$$
for every $\varphi \in \mathscr{D}(R^k)$, so that
$$
\int\left(\alpha - \dfrac{1}{2}\Delta\right) \varphi(a + b) u_1 (a) da = 0
$$
for every $\varphi \in  \mathscr{D}(R^k)$. Also
\begin{align*}
  \iint  G(\alpha, |b| ) | u_1 (a) |&| \left(\alpha - \frac{1}{2}\Delta\right)
  \varphi (a+b) | da~db \\ 
  & = \int G(\alpha, |b|)db \int |u_1(a)||\left(\alpha -
  \frac{1}{2}\Delta\right)  \varphi (a+b) | da \\ 
  & = \int G(\alpha, |b|)db \int \limits_{a \in K}|u_1(a-b)||\left(\alpha -
  \frac{1}{2}\Delta\right) \varphi (a)|da \\ 
  & \leq M \int G(\alpha, |b|)db,
\end{align*}
where $K$ is the compact set outside whish $\varphi$ is zero and 
$$
M = (\diam. K). \Sup |u(a-b)(\alpha-\frac{1}{2}\Delta)\varphi (a)|.
$$

Therefore, 
\begin{align*}
  0 & = \int G(\alpha, |b|) db \int \left(\alpha - \frac{1}{2}\Delta\right)
  \varphi (a+b)u_1 (a) da \\ 
  & = \int u_1(a) da \int G(\alpha, |b|) \left(\alpha-\frac{1}{2} \Delta\right)
  \varphi (a+b)db \\ 
  & = \int u_1(a) da \varphi(a).
\end{align*}\pageoriginale

Hence\quad $u_1 = 0$ ~~ a.e. \quad Thus
$$
\mathscr{R}= \left\{u : u, \Delta u \in \mathscr{B}(R^k) \right \}
$$
and $\mathscr{G} u = \dfrac{1}{2}\Delta u$ in the distribution sense.

\section{Stochastic solution of the Dirichlet
  problem}\label{chap3-sec3}%sec 3 

Let $U$ be a bounded open set and $f$ a function which is bounded and
continuous on the boundary $\partial U$ of $U$. The problem of
finiding a function $h(a : f,U)$, defined and harmonic in $U$ and such
that $h(a: f, U)\to f(\xi)$ as $a \to \xi$ from within $U$, is called
the \textit{Dirichlet problem}. $h(a)$, if it exists, is unique and is
called the \textit{classical solution}. The definition of a solution
can be generalized, in various ways, so as to include cases in which
the classical solution does not exist. The generalized solution will
still be harmonic in $U$, but will tend to the boundary value $f(\xi)$
in a slightly weaker sense. 

The \textit{stochastic solution}. which we shall discuss, gives one
way of defining a generalized solution. Let 
$$
\tau_U = \text{ first leaving time from } U = \inf \left \{ t : x_t
\notin U \right\} 
$$

By definition, $u(a) \equiv u (a : f, U) = E_a (f (x_{\tau_u}))$ is
the stochastic solution of the Dirichlet problem with boundary value
$f$. We shall see that the stochastic solution is identical with the
classical solution,\pageoriginale in case the latter exists. 

We first establish some results on $\tau_U$.

\setcounter{thm}{0}
\begin{thm}\label{chap3-sec3-thm1} %Theorem 1
$P_a (\tau_u < \infty) = 1$ if $U$ is a bounded domain.
\end{thm}

This is a corollary of the following stronger

\begin{thm}\label{chap3-sec3-thm2}%\theorem 2
$$
E_a [\tau_U] <\infty,\text{ \ if } U \text{ is bounded }.
$$
\end{thm}

\begin{proof}
Since $P_a[B+a] = P_0 [B]$, we can assume that $a = 0$. Further,
  since $\tau_u \geq \tau_v$ for $U \supset V$, we can assume that $U$
  is the sphere $\Gamma = \left\{ x : |x| < r \right\}$. Let $u \in
  \mathscr{D}(\mathscr{G})$ be such that $\mathscr{G}u$ has a version
  satisfying $\mathscr{G}u(a) > \in_0$ in $\Gamma$ for some $\in_0 >
  0$.  For example, if $u(a)=-e^{-|a|^\frac{2}/{4r^2}}$, then 
  $$
  \mathscr{G}u(a)= \frac{1}{2}\Delta u(a) =
  \frac{-1}{2}\left[\frac{k}{2^{r^2}}-\frac{|a|^2}{4^{r^4}}\right]
  u(a) > 0, ~\text{ if } |a| \leq r. 
  $$

Let $\tau_n = \tau_U \wedge n$. Then $\tau_n$ is a
Markov time, and $E_0(\tau_n) \leq n < \infty$. Therefore, from
Dynkin's formula, 
$$
E_0\left(\int^{\tau_n}_0 \mathscr{G}u(x_t) dt\right) = E_0 (u(x_{\tau_u})) - u(0).
$$

For $0 \leq t \leq \tau_n, x_t \in \Gamma$ and $\mathscr{G}u(x_t) \geq
\in_0$ and $\in_0 E_0 (\tau_n) \leq 2 || u||$ Therefore 
$$
E_0 (\tau) = \lim E_\circ (\tau_n) \leq  \frac{2 || u
  ||}{\epsilon_\circ} < \infty.
$$
\end{proof}

\begin{thm}\label{chap3-sec3-thm3} % \Theorem 3
  If $U$ is open and bounded, if $f$ is continuous on $\partial U$
  and\pageoriginale 
  if there exists a classical solution $h(a) = h(a : f, U)$, then 
  $$
  u(a : f, U) = h(a : f, U)
  $$
\end{thm}

\begin{proof}
For any open subset $V$ of $U$ such that $\bar{V} \subset U$, let
  $h$ denote a $C^\infty$ function which vanishes outside $U$ and such
  that $h_V = h$ or $\bar{V}$. Such a function can easily be
  constructed. Then $h_V \in \mathscr{D}(\mathscr{G})$. Since $V$ is
  bounded, $E_a (\tau_V) < \infty$ and Dynkin's formula gives 
  $$
  E_a \left(\int^{\tau_v}_0 \mathscr{G}h_V (x_t) dt\right) = E_a
  (h_V(x_{\tau_v})) - h_V(a). 
  $$

For $t < \tau_V, x_t \in V$ and $\mathscr{G}h_V(x_t) =
\dfrac{1}{2}\Delta h_V (x_t) = \dfrac{1}{2} \Delta h(x_t)= 0$. If
$\tau_V < \infty, x _{\tau_v} \in \partial V$ so that $h_V(x \tau_v) =
h(x_{\tau_v})$, and since $V$ is bounded, $P_a(\tau_V < \infty)
=1$. Therefore $E_a (h(x_{\tau_v})) = h_V(a)$. Hence, if $a \in
\bar{V}$, then $E_a (h(x_{\tau_v})) = h(a)$. 

Now let $\left\{V_n \right\}$ be an increasing sequaence of open
subsets of $U$ such that $\bar{V}_n \subset U$ and $V_n \uparrow
U$. Then $\tau_u = \lim\limits_{n- \infty} \tau_{V_n}$. Since $P_a
(\tau_u < \infty)=1$, we have with $P_a$-measure. $1$, 
\begin{align*}
  f(x_{\tau_u} & = \lim_{n \to \infty} h(x_{\tau_{V_n}}) \\
  u(a) = E_a (f(x_{\tau_u})) & = \lim_{n \to \infty} E_a (h(x_{\tau_{v_n}})) = h(a)
\end{align*}
for every $a \in U$. This completes the proof.
\end{proof}

A natural question is ``When does the classical solution exist?'' The
simplest case is that of a ball $\Gamma = \Gamma (a_0 ; r)$. For $a
\in \Gamma$, let $a'$ denote the inverse of a with respect to
$\Gamma$, i.e., 
$$
a' = a_0 +\frac{r^2}{||a-a_0||^2} (a-a_0).
$$\pageoriginale

Let
\begin{align*}
  G(b, a) & = \frac{1}{|b-a|^{k-2}}-\frac{r^{k-2}}{|a-a_0|^{k-2}}
  \frac{1}{|b-a'|^{k-2}},\quad k \geq 3; \\ 
  & = \log \frac{1}{| b-a|}- \log \frac{1}{| b-a'1}- \log
  \frac{r}{|a-a_0|},\quad k = 2, 
\end{align*}
and
\begin{align*}
  \Pi_\Gamma (a, \xi) &= - \frac{1}{k-2} \frac{\partial}{\partial
    \rho_b} G(b, a) \bigg ]_{b=\xi} x r^{k-1} \text{\ for\ } k \geq 3,\\ 
  \Pi_\Gamma (a, \xi) &= - \frac{\partial}{\partial \rho_b}  G(b, a)
  \bigg ]_{b= \xi} r^{k-1} \text{ for } 1 \leq k \leq 2, 
\end{align*}
where $\dfrac{\partial}{\partial \rho_b}$ denotes the derivative in
the radial direction of $\Gamma$. Then, if $f$ is defined and
continuous on the boundary of the ball, and if $\theta (d \xi)$ is the
uniform probability distribution (i.e. the normed rotation invariant
measure on the boundary of $\Gamma$), the classical solution is given
by the Poisson integral 
$$
h(a : f, U) = \int_{\partial \Gamma} \Pi_\Gamma (a, \xi) f (\xi) \theta (d \xi).
$$

The concrete form of $\Pi_\Gamma (a, \xi)$ is not of importance to
us. The only fact we need is  

\begin{thm}\label{chap3-sec3-thm4} % Theorem 4
  If $\Gamma, V$ are two concentric balls, with radii $r, \rho (r >
  \rho)$ ; then 
  $$
   c_1  = \min_{a \in \bar{V}, \xi \in \partial \Gamma} \Pi_\Gamma (a,
    \xi)
$$
and\pageoriginale
$$
c_2   = \max\limits_{a \in \bar{V}, \xi \in
      \partial \Gamma} \Pi_\Gamma (a, \xi)
$$
depend only on $\rho/r$ and $c_1, c_2 \to 1$ as $\rho/r \to 0$.
\end{thm}

The \textit{hitting measure} $\Pi_U (a, E)$ of $E$ is defined as
$$
\Pi_U (a, E) = P_a(x_{\tau_U} \in E), E \in \mathbb{B} (\partial U).
$$

Clearly
$$
u(a : f, U) = \int\limits_{\Gamma}\Pi_U (a, d \xi) f(\xi).
$$

We have the following
\begin{thm}\label{chap3-sec3-thm5}% Theorem 5
  If $\Gamma$ is a ball, for $a \in \Gamma$ we have
  \begin{align*}
    \Pi_\Gamma (a, d \xi) & = \Pi_\Gamma (a, \xi) \theta (d \xi) \\
    & = \text{ the harmonic measure on }\partial \Gamma \text{ with
      respect to } a. 
  \end{align*}
\end{thm}

\begin{proof}
The proof is immediate since, from the above, for every continuous
  function $f$ on $\partial \Gamma$, 
$$
  \int\limits_{\partial \Gamma} (a, d\xi) f(\xi) = \int \limits
  _{\partial \Gamma} \Pi_\Gamma (a, \xi) \theta (d \xi) f (\xi), 
$$
and hence the same equation holds for all bounded Borel functions on
  $\partial \Gamma$. 
\end{proof}

Using the notation of Theorem \ref{chap3-sec3-thm4}, we have
\begin{thm}% theorem 6
$$
c_1 \theta(E) \leq \Pi_\Gamma (a, E) \leq c_2 \theta (E).
$$
\end{thm}

We\pageoriginale now proceed to prove that if the boundary of a
bounded open set $U$ 
is smooth in a certain sense, then the stochastic solution is also the
classical solution.
 
\begin{defi*}
  Let $\xi \in \partial U$, where $U$ is an open set. If there exists
  a cone $C \subset U^c$, with vertex at $\xi$ then $\xi$ is called a
  Poincare point for $U$. 
\end{defi*}

\begin{thm}\label{chap3-sec3-thm7}% theorem 7
If $\xi$ is a Poincare point for a bounded open set $U$, then for
any $\in > 0$ and for any neighbourhood $\Gamma$ of $\xi$, there
exists a smaller neighbourhood $\Gamma'$ of $\xi$ such that 
$$
P_a (x_{\tau_u} \notin \Gamma) < \in
$$
for any $a \in \Gamma' \cap U$.
\end{thm}

\begin{proof}
Let $C \subset U^c$ be a cone with vertex at $\xi$. We can assume
  that $\Gamma$ is a ball of radius $r$ such that $C-\Gamma \neq
  \phi$. Let $\Gamma_n$ be the ball with the same centre as $\Gamma$
  and radius $r_n = \alpha^n r$, where $\alpha < 1$ is to be chosen
  subsequently. Let $\tau_n$ be the first leaving time from
  $\Gamma_n$. If $x_{\tau_U} \notin \Gamma, \tau_\Gamma \leq
  \tau_u$ and since $P_a(\tau_n \leq \tau_\Gamma) = 1$ for any $a \in
  \Gamma$ we have $x_{\tau_n} \in U$. Therefore $x_{\tau_n}\notin
  C$. But $x_{\tau_n} \in \partial \Gamma_n$ so that $x_{\tau_n} \in
  \partial \Gamma_n - c$. Therefore for any $a \in \Gamma_n$, 
  \begin{multline*}
    P_a (x_{\tau_u} \notin \Gamma) \leq P_a(x_{\tau_{n-1}} \in
    \partial \Gamma_{n-1} - C, \ldots, x_{\tau_1} \in \partial
    \Gamma_1 - C) \\ 
     = P_a (x_{\tau_{n-1}} \in \partial \Gamma_{n-1} - C, \ldots,
    x_{\tau_2} \in \partial\\ 
    \Gamma_2-C, x_{\tau_1 (\omega_{\tau_2}+)}
    (\omega_\tau^+) \in \partial \Gamma_1 - C) 
  \end{multline*}
  since\pageoriginale $\tau_1 = \tau_2 + \tau_1 (w^+_{\tau_2})$. Also
  since $\tau_i 
  < \tau_2$, for $i > 2$ we have  
  \begin{align*}
    x_{\tau_i} = x(\tau_i(w), w) & = x (\tau_i (w),w^-_{\tau_2}) \\
    & = x (\tau_1 (w^-_{\tau_2}), w^-_{\tau_2}) \in
    \mathscr{B}_{\tau_2} \subset \mathscr{B}_{\tau_{2+}}. 
  \end{align*}
\end{proof}

Using the strong Markov property we have
\begin{multline*}
  P_a (x_{\tau_u} \notin \Gamma) \leq  E_a (x_{\tau_{n-1}} \in
  \partial \Gamma_{n-1} -C, \ldots,x_{\tau_2} \in \partial\\ 
  \Gamma_2 -C : P_{x_{\tau_2}}(x_{\tau_1} \in \partial \Gamma_1 - C)\\ 
  \leq  c_2 \theta P_a (x_{\tau_{n-1}}\in \partial \Gamma_{n-1} - C,
  \ldots , x_{\tau_2} \in \partial \Gamma_2 - C), 
\end{multline*}
if $a \in \Gamma_n \cap U$. where $\theta = \theta (\partial \Gamma_1
-C)< 1$. Since $\theta$ depends only on the solid angle at the vertex
$\xi, \theta (\partial \Gamma_1 -C)=\theta  (\partial \Gamma_2 -C) =
-- \cdots = \theta(\partial \Gamma_{n-1}-C)$. We have repeating the
argument, 
$$
P_a (x_{\tau_U} \notin \Gamma) \leq (c_2 \theta)^{n-1}
$$

Since $c_2 \to 1 $ as $\alpha \to 0$, we can choose $\alpha$ so small
that $c_2 \theta < 1$. Now choose $n$ large enough so that $(c_2
\theta)^{n-1} < \in $. 

\begin{thm}\label{chap3-sec3-thm8}%theorem 8
  For any open set $U$ and any bounded Borel function $f$ on $\partial U$,
  $$
  u(a) = u(a : f, U)
  $$
  is harmonic in $U$.
\end{thm} 

\begin{proof}
  Let $a \in U $ and $\Gamma$ be a ball with centre at a and contained
  in $U$. Then since $\tau_U = \tau_\Gamma + \tau_U
  (w^+_{\tau_\Gamma)}$, we have 
  \begin{align*}
    u(a : f, U) & = E_a (f(x_{\tau_U}))= E_a
    (f(x_{\tau_{U(w^+_{\tau_\Gamma})}} (w^+_{\tau_\Gamma}))) \\ 
    & = E_a (E_{x_{\tau_\Gamma}}(f (x_{\tau_U})) \\
    & = E_a (u(x_{\tau_\Gamma})) \\
    & = \int P_a (x_{\tau_\Gamma} \in d \xi ) u(\xi)\\
    & = \int \Pi_\Gamma (a, \xi)u(\xi) \theta (d \xi),
  \end{align*}\pageoriginale
  and the last term is harmonic for $a \in \Gamma$. This proves that
  $u$ is harmonic in a neighbourhood of every $a \in U$. Hence $u$ is
  harmonic in $U$. 
\end{proof} 

\begin{thm}\label{chap3-sec3-thm9}% theorem 9
  If $U$ is a bounded open set such that every point of $U$ is a
  Poincare point and if $f$ is continuous on $\partial U$, then the
  stochastic solution $u=u(a:f, U)$ is also the classical solution. 
\end{thm} 

\begin{proof}
  By Theorem \ref{chap3-sec3-thm8}, $u$ is harmonic in $U$. Let $\xi
  \in \partial 
  U$. Since $f$ is continuous, we can choose a ball $\Gamma =
  \Gamma(\xi)$ such that $| f(\eta) - f(\xi)|< \in $ for $\eta \in
  \Gamma$. By Theorem \ref{chap3-sec3-thm7} we can choose $\Gamma'$ so that 
  $$
  P_a (x_{\tau_U} \notin \Gamma) < \in, a \in \Gamma'.
  $$

 
For $a \in \Gamma'$,
\begin{align*}
  |u(a) -f(\xi)| & \leq E_a (|f(x_{\tau_U})-f(\xi)|) \\
  & = E_a(|f(x_{\tau_U})-f(\xi)|:x_{\tau_U} \in \Gamma)\\
  & \hspace{2cm}+E_a
  (|f(x_{\tau_U}) - f(\xi)|:x_{\tau_u} \notin \Gamma)\\ 
  & \leq \in + 2 || f || \in .
\end{align*}
\end{proof}

\begin{remark*}
When\pageoriginale $k=1$, harmonic functions are linear functions. If
$(a_1, a_2)$ is an interval and $f(a_1), f(a_2)$ are given; then  
  \begin{align*}
    h(a:f, (a_1,a_2)) & = \frac{a_2
      -a}{a_2-a_1}f(a_1)+\frac{a-a_1}{a_2-a_1}f(a_2)\\ 
    & = u(a;f,(a_1, a_2))\\
    & = f(a_1)P_a(x_{\tau_{(a_1, a_2)}}= a_1) +f(a_2) P_a(x_{\tau_{(a_1, a_2)}}=a_2)\\
    & = f(a_1) P_a (\sigma_{a_1}< \sigma_{a_2})+f(a_2) P_a
    (\sigma_{a_2} < \sigma_{a_1}), 
  \end{align*}
  where $\sigma_{a_i}, i=1, 2$, is the first passage time for $a_i, i=1,
  2$. Since $f$ is arbitrary, 
  \begin{align*}
    P_a(\sigma_{a_1}< \sigma_{a_2}) & = \frac{a_2 -a}{a_2 - a_1},\\
    P_a(\sigma_{a_2}< \sigma_{a_1}) & = \frac{a -a_1}{a_2 - a_1},
  \end{align*}
\end{remark*}

We have seen that if $U$ is bounded and open and if every point of
$\partial U$ is a Poincare' point, the Dirichlet problem for $U$ has a
solution. We now define a generalized solution. 

Suppose that $U$ is open and bounded and that $f$ is bounded and
continuous on $\partial U$. Let $\left\{U_n \right\} \uparrow U$ be an
increasing sequence of open sets with $\bar{U}_n \subset U_{n+1}$ and
such that every point of $\partial U_n$ is a Poincare point. Let $F$
be a continuous extension of $f$ to $\bar{U}$ and $F_n = F \Gamma
\partial U_n$. Denote the classical solution for $U_n$ with boundary
values\pageoriginale $F_n$ by $h(a; F_n,U_n)$. Then $\lim\limits_{n -
  \infty} h (a; F_n , U_n)$ is, by definition, \textit{the generalized
  solution} (in the Wiener sense) of the Dirichlet problem with
boundary values $f$. We have of course to show that the limit exists
and is independent of the choice of $U_r$ and of $F$. 

\begin{thm}\label{chap3-sec3-thm10}%theorem 10
  For a bounded open set $U, u(a;f,U)$ is the generalized solution.
\end{thm}

\begin{proof}
We have only to show that $h(a:F_n, U_n) \to u(a:f,U)$. In fact
since $\tau_{u_n}\uparrow \tau_u < \infty$ with probability $1$, 
  \begin{align*}
    h(a: F_n, U_n) & = u(a:F_n,U_n) = E_a(F(x_{\tau_{u_n}}))\\
    &\quad \to E_a (F(x_{\tau_u}))\\
    & = E_a (f(x_{\tau_U})) = u(a: f,U).
  \end{align*}
\end{proof}

\begin{remark*}
  $u(a) = u(a:f,U)$ does not always satisfy the boundary condition
  $\lim\limits_{a \in U, a \to \xi} u(a) = f(\xi)$ for $\xi \in
  \partial U$. In $\S 7$ we shall discuss these boundary conditions. 
\end{remark*}

\section{Recurrence}\label{chap3-sec4}%section 4

\begin{defi*}
 A Markov process $\mathbb{M}$ is called {\rm recurrent} if 
  $$
  P_a(x_t \in U \text{ for some }t) \equiv P_a(\sigma_U < \infty)=1
  $$
  for any $a \in S$ and any open $U$; otherwise it is called {\em
    non-recurrent.} 
\end{defi*}

We shall now show that the standard Brownian motion is recurrent for
$k \leq 2 $ and is non-recurrent for $k \geq 3$. 

\setcounter{thm}{0}
\begin{thm}\label{chap3-sec4-thm1}%theorem1 
  Let\pageoriginale $\Gamma_1, \Gamma_2$ be the balls with centres
  $a_0$ and radii 
  $r_1, r_2(r_2 > r_1)$. If $\sigma_1 = \sigma_{\partial \Gamma_1},
  \sigma_2 = \sigma_{\partial \Gamma_2}$ are the first passage times
  for $\partial \Gamma_1$ and $\partial \Gamma_2$, then for $a \in
  \Gamma_2 - \bar{\Gamma_1}$, 
  \begin{equation*}
    P_a (\sigma_1 < \sigma_2 ) =
    \begin{cases}
      \medskip
      \frac{r^{-k+2}-r^{-k+2}_2}{r^{-k+2}_1 - r^{-k+2}_2} , k \geq 3; \\      
      \medskip
      \frac{\log \frac{1}{r} - \log  \frac{1} {r_2}}{\log
        \frac{1}{r_1} - \log  \frac {1} {r_2}}  ~ , ~  k=2; \\       
      \frac{r_2 - r}{r_2 - r_1}, k =1;
    \end{cases}
\end{equation*}
where $r= |a-a_0|$.
\end{thm}

\begin{proof}
In fact, if $U = \Gamma_2 - \bar{\Gamma}_1, \partial U = \partial
  \Gamma_1 \cup \partial \Gamma_2$, and the function $f$ which is $1$
  as $\partial \Gamma_1$ and $0$ as $\partial \Gamma_2$ is continouous
  on $\partial U$. Since every point in $\partial U$ is a Poincar\'e
  point, the classical solution $h(a; f, U) = u(a; f,U)$ exists and 
  $$
  p(a) \equiv P_a(\sigma_1 < \sigma_2) = P_a (x_{\tau_U} \in \partial
  \Gamma_1) = u(a; f,U). 
  $$

The function given in the statement of the theorem is harmonic in $U$
and takes the boundary value $f$. Since such a function is unique, we
get the result. 
\end{proof}

\begin{thm}\label{chap3-sec4-thm2}%theorem 2
Let $\Gamma = \Gamma (a_0, r)$ be a ball with centre $a_0$ and
radius $r$ and let $\sigma_\Gamma$ be the first passage time for
$\Gamma$. For $a \notin \Gamma$ and $\rho = |a-a_0|$,\pageoriginale 
  \begin{equation*}
    P_a (\sigma_\Gamma < \infty)=
    \begin{cases}
      (r / \rho)^{k-2}, \qquad  &  k \geq 3 \\
      ,& k \leq 2
    \end{cases}
  \end{equation*}

  Therefore $k$-dimensional Brownian motion is recurrent or not
  according as $k \leq 2$ or $k \geq 3$. 
\end{thm}

\begin{proof}
  Observe that $\sigma_\Gamma = \sigma_{\partial \Gamma}$ for any path
  whose starting point is not in $\Gamma$. Let $\Gamma' = \Gamma'(a_0,
  r')$ and $\sigma' = \sigma_{\partial \Gamma'}$. If $t < \sigma_
  \infty(w)$, then since $w(t)$ is continous, $F_t = \left\{ x_s : 0
  \leq s \leq t \right \}$ is a compact set and hence we can find $r'$
  such that $\Gamma' \supset F_t$. Then $\sigma (w) \geq t$. It
  follows that 
  $$
  \lim_{r' \to \infty} \sigma'  =\infty.
  $$

  Therefore
  $$
  P_a (\sigma_\Gamma < \infty) = P_a (\sigma_\Gamma < \lim_{r' \to
    \infty} \sigma') = \lim_{r' \to \infty} P'_a (\sigma_\Gamma <
  \sigma'). 
  $$

  Now take $r_2 = r'$ and $\sigma_2 = \sigma'$ in Theorem
  \ref{chap3-sec4-thm1} and we get the result. 
\end{proof}

\begin{thm}\label{chap3-sec4-thm3}%theorem 3
  If $k \geq 3, P_a (| x_t | \to \infty$ as $t \to \infty) = 1$. If k
  $\leq 2, P_a(w : (x_s, s \geq t$, is dense in $R^k$ for all $t$))
  =$1$. 
\end{thm}

\begin{proof}
 {\bf Case {\boldmath$k \geq 3$}}.~ We can, without loss of
 generality, assume   that $a=0$. Let $\Gamma_n = \Gamma^{(0, n)}$ and
 $\sigma_n = \sigma_{\partial \Gamma_n}$. For any path $w, |x_t|\to \infty$ if
  and only if for every given $n$ we can find $s$ such
  that\pageoriginale the image
  of $[0, \infty]$ by $w^+_s$ is contained in $\Gamma^c_n$. Therefore
  $|x_t | \nrightarrow \infty$ if and only if we can find $n$ such
  that for every $s \geq 0$, the image of $[0, \infty]$ by $w^+_s$ has
  a non-empty intersection with $\Gamma_n$ and therefore if $w^+_s (0)
  \notin \Gamma_n$, then $\sigma_n (w^+_s) < \infty$. Therefore 
  \begin{align*}
    P_0   \left[ |x_t| \nrightarrow \infty \right ] & = P_0 [
      \exists ~ n \text{ such that for every } s\\ 
      & \geq 0 \text{ with
      }w^+_s(0) \notin \Gamma_n, \sigma_n (w^+_s) < \infty ] \\ 
    & \leq  \sum_n P_0 \left [\text{for every } s \geq 0 \text{ with
      }w^+_s(0) \notin \Gamma_n, \sigma_n (w^+_s) < \infty \right] \\ 
    & \leq \sum_n P_0 \left[\text{for every } m > n,
      \sigma_n(w^+_{\sigma_m}) < \infty \right]. 
  \end{align*}

  Now
  \begin{align*}
    P_0 (\text{ for every } m, \sigma_n (m^+_{\sigma_m}) < \infty ) &
    \leq P_0 (\sigma_n (w^+_{\sigma_m}) < \infty  \text{ for some }m)
    \\ 
    & = E_0 (P_{x_{\sigma_m}} (\sigma_n < \infty) \\
    & = \left(\frac{n}{m}\right)^{k-2} \to 0, \text{ as } m \to \infty.
  \end{align*}

\medskip
\noindent
\textbf{Case {\boldmath$k \leq 2$}}.~ Let $\Gamma$ be any ball and
$\sigma_\Gamma$ = the first passage time for $\Gamma$. We have 
$$
P_a(\sigma_\Gamma < \infty) =1
$$
for every $a$, so that for any $t$,
$$
P_a (\sigma_\Gamma (w^+_t) < \infty) = E_a (P_{x_t}(\sigma_\Gamma <
\infty)) = 1. 
$$

Now\pageoriginale 
\begin{align*}
  P_a (\text{ for every } t , \sigma_\Gamma (w^+_t) < \infty) & = P_a
  (\text{ for every } n, \sigma_\Gamma (w^+_n) < \infty) \\ 
  & = 1.
\end{align*}

Let $\Gamma_1, \Gamma_2 , \ldots$ be a complete fundamental system of
neighbour\-hoods. Then 
$$
P_a (\text{ for every }n, \text{ for every }t,
  \sigma_{\Gamma_n}(w^+_t) < \infty) = 1,
$$
i.e.,
$$
P_a ((x_s(w) : s \geq t\text{ \ \ is dense in \ \ } R^k )) =
  1. 
$$
\end{proof}

\section{Green function}\label{chap3-sec5}% \section5

\noindent
\textbf{Case {\boldmath$k \geq 3$.}}

\begin{defi*}
  Let $U$ be a bounded open set. Then
  $$
  G_U(a,b) = \frac{1}{| a-b|^{k-2}} - \int_{\partial U} \frac{\Pi_U
    (a, d \xi)}{| \xi - b|^{k-2}} 
  $$
  is called the {\em Green function} for $U$, where $\Pi_U (a, d \xi)$
  is the harmonic measure on $\partial U$ with respect to $a$. This is
  the potential at $b$ due to a unit charge at $a$ and the induced
  charge on $\partial U$. 
\end{defi*}

As the limiting case, when $U \to R^k$, we can define the Green
function (relative to the whole space $R^k$ by 
$$
G(a, b) = \frac{1}{|a-b |^{k-2}}.
$$

\setcounter{thm}{0}
\begin{thm}\label{chap3-sec5-thm1}% theorem 1
If\pageoriginale $f$ is bounded, Borel and has compact support, then $E_a (
  \int^\infty_0 f(x_t)dt) < \infty$ and 
  $$
  E_a (\int^\infty_0 f(x_t) dt) = \frac{2}{K} \int \frac{f(b)
    db}{|b-a|^{k-2}}, \text{ where } K= 4 \Pi^{\frac{k}{2}}/\Gamma
  (\frac{k}{2}-1) 
  $$
\end{thm}

\begin{proof}
  It is enough to prove the theorem for $f \geq 0$. We have 
  \begin{align*}
    E_a \left( \int^\infty_0 f(x_t)dt\right) & = \int^\infty_0 E_a (f(x_t)) dt \\
    & = \int^\infty_0 dt \int_{R^k} \frac{1}{(2 \Pi
      t)^{\frac{k}{2}}}e^{- \frac{|b-a|^2}{2t}} f(b) db \\ 
    & = \int_{R^k} f(b) db \int^\infty_0 \frac{1}{(2 \Pi
      t)^{\frac{k}{2}}} e^{- \frac{|b-a|^2}{2t}} dt \\ 
    & = \int_{R^k}\frac{f(b) db}{|b-a|^{k-2}}\frac{\Gamma(k/2-1)}{2
      \Pi^{\frac{k}{2}}}\\ 
    & = \frac{2}{K} \int_{R^k} \frac{f(b) db}{|b-a|^{k-2}} \\
    & < \infty,
  \end{align*}
  because, if $\Gamma$ is a ball containing the support of $f$,
  $$
  \int\limits_\Gamma \frac{f(b)db}{(b-a)^{k-2}} \leq ||f|| 
\int\limits_\Gamma \frac{db}{(b-a)^{k-2}} <\infty.
  $$
\end{proof}

\begin{thm}\label{chap3-sec5-thm2}%theorem 2
  Let\pageoriginale $v(a) = E_a (\int^\infty_0 f(x_t) dt)$. Then $v(a)
  \in \mathscr{D} (\mathscr{G})$,  
  $$
  \frac{1}{2} \Delta v = -f \text{ \ a.e., and \ } v(a) \to 0 \text{
    \ as  \ }
  | a | \to \infty 
  $$

Therefore, if
\begin{align*}
u(a)&  = \int G(a, b) f(b) db, \\
\Delta ~u & = - k f  \quad \text{a.e. ( Poisson's equation })
  \end{align*}
  and $u(a) \to 0$ as $/a/ \to \infty$.
\end{thm}

\begin{proof}
By Theorem \ref{chap3-sec5-thm1}, $v(a)$ is bounded and Borel. If
  $$
  G_\in f(a) = E_a \left(\int^\infty_0 e^{- \in t} f (x_t) dt\right),
  $$
  we have
  $$
  v(a) = \lim_{\in \to 0} G_\in f(a)
  $$
  and the resolvent equation gives
  $$
  G_\alpha f - G_\in f + (\alpha - \in) G_\alpha G_\in f = 0.
  $$


Letting $\in \to 0$,
$$
G_\alpha f - v+ \alpha G_\alpha v=0,
$$
or
$$ 
v = G_\alpha (f + \alpha v) \in \mathscr{D} (\mathscr{G}).
$$

Also, since $\mathscr{G} v\ = \alpha v - G^{-1}_\alpha v = \alpha v -
f - \alpha v = -f$, a.e., 
$$
\frac{1}{2}\Delta v = - f ~ a. e.
$$
\end{proof}

\begin{defi*}
Let\pageoriginale $A$ be a bounded subset of $R^k$. Then
$$
S(A, w) = \text{ the Lebesgue measure of } \left\{ t : x_t (w) \in A
\right \} 
$$
is called the \textit{sojourn (visiting) time} for the set $A$.
\end{defi*}

From Theorem \ref{chap3-sec5-thm1}, we have
\begin{thm}\label{chap3-sec5-thm3}% theorem 3
$$
\frac{E_a (S(db))}{db} = \frac{2}{K} G(a, b).
$$
\end{thm}

Let now $U$ be a bounded open set and $f \in \mathscr{B}(U)$. Let
$$
v_U(a) = v_U (a; f, U) = E_a \left[\int^{\tau_U}_0 f(x_t) dt\right],
$$
$\tau_U$ being the first leaving time from $U$.

\begin{thm}\label{chap3-sec5-thm4} % theorem 4
  $$
  v_U (a) = \frac{2}{K} \int_U G_U (a, b) f(b) db. 
  $$
\end{thm}

\begin{proof}
Extend $f$ by putting $f=0$ in $U^c$. Then
  $$
  v_0 (a) = E_a \left(\int^\infty_0 f(x_t) dt\right) = \frac{2}{K} \int_U
  \frac{f(b) db}{| b-a|^{k-2}}, 
  $$
  by Theorem \ref{chap3-sec5-thm1}. Also
  \begin{align*}
    v_0 (a) & = E_a \left(\int^{\tau_U}_0 f(x_t) dt\right) + E_a
    \left(\int^\infty_{\tau_U} f(x_t) dt\right) \\ 
    & = v_U (a) + E_a \left(\int^\infty_0 f(x_t(w^+_U)) dt\right) \\
    & = v_U (a) + E_a \left(Ex_{\tau_U} \left(\int^\infty_0 f(x_t)
    dt\right)\right)\\ 
    & = v_U(a) + E_a (v_0(x_{\tau_U})).\\
    & = v_U (a) + \int_{\partial U} \pi_U (a, d \xi ) v_0 (\xi ) \\
    & = v_U (a) + \frac{2}{K} \int f(b)db \int_{\partial U}
    \frac{\pi_U (a,d)}{|b - \xi |^{k-2}} 
  \end{align*}\pageoriginale

This gives the result.
\end{proof}

\begin{thm}\label{chap3-sec5-thm5}%Thm 5
$v_U(a)$ satisfies
$$
\frac{1}{2} \Delta v_U = - f, \text{ \ a.e.,}
$$
and $v_U (a) \rightarrow 0$ as $a \rightarrow \xi, \xi$ being a
regular point of $\partial U$. 
\end{thm}

\begin{proof}
$v_U(a) = v_0 (a) - E_a (v_0( x_{\tau_U}))$. Since $E_a (v_0
  (x_{\tau_U}))$ is harmonic in $U$ and $\dfrac{1}{2} \Delta v_0(a)
  = - f$ a.e., we have 
$$
\dfrac{1}{2} \Delta v_U(a) = - f,\text{ \  a.e.}
$$
\end{proof}

Further if $\xi \in \partial U$ is regular, $E_a (v_0(x_{\tau_U}))
\rightarrow v_0 (\xi)$ as a $\rightarrow \xi$ and since $v_0(a)$ is
continuous by Theorem \ref{chap3-sec5-thm1} $v_0(a) \rightarrow v_0
(\xi)$ as $a\rightarrow \xi$. The result follows.  

\begin{thm}\label{chap3-sec5-thm6}%Thm 6

  Let $S (A/U, w) = $ the Lebesgue measure of $\{t : x_t \in A, t <
  \tau_U \}$. Then 
  $$
  \frac{E_a (S(db/U))}{db} = \frac{2}{K} G_U(a, b).
  $$
\end{thm}

As an example we compute $v_U(a)$ for $U=$ the open cube $(0, 1)^3 ,
k=3$. Since every boundary point of the unit cube is regular (in fact
every point is a Poincar\'e point), $v_U = 0$ as $\partial U$. Therefore
$v = v_U(a)$ is the solution of  
$$
\dfrac{1}{2} \Delta v = - f \text{ \ and \ } v = 0 \text{ \ on \ }
\partial U. 
$$\pageoriginale

Since $v = 0$ as $\partial U$ we can put
$$
v(x, y,z) = \sum_{l + m + n > 0} a_{lmn} \sin l\ \pi x \sin m \pi y \sin n \pi z
$$

Then 
$$
\frac{1}{2} \Delta_{v} = \frac{\pi^2}{2}  \sum_{l + m + n > 0}(1^2 +
m^2 + n^2) a_{lmn} \sin l \pi x \sin m \pi y \sin n \pi z 
$$

If
$$
f(x, y, z) = \sum b_{lmn} \sin l \pi x \sin m \pi y \sin n \pi z,
$$
we have therefore
\begin{align*}
  a_{lmn}  &= \frac{2b_{lmn}}{\pi^2 (l^2 + m^2 + n^2)} \\
  &= \frac{16}{\pi^2 (l^2 + m^2 + n^2)}\\
&\quad  \int^1_0 \int^1_0 \int^1_0 f
  (\xi, \eta, \zeta) \sin l \pi \xi \sin m \pi \uparrow \text{ \ in
    \ } 
n \pi \xi d\xi d\eta d\eta 
\end{align*}


This gives
\begin{multline*}
  v(x, y, z) = \iiint f (\xi , \eta, \zeta) \frac{16}{\pi^2}\\ 
  \sum \frac{\sin
    l \pi \xi \sin l \pi x \sin m \pi \eta \sin m \pi y \sin n \pi \zeta \sin
    n \pi z}{l^2 + m^2 + n^2} d\xi d \eta d \zeta. 
\end{multline*}

Hence 
{\fontsize{10pt}{12pt}\selectfont
$$
G_U (x, y, z; \xi , \eta, \zeta) = \frac{32}{\pi} \sum \frac{\sin l \pi \xi
  \sin l \pi x \sin m \pi \eta \sin m \pi y \sin n \pi \zeta \sin n \pi
  z} {1^2 + m^2 + n^2} 
$$}\relax
in\pageoriginale the distribution sense.


\medskip
\noindent
\textbf{Case {\boldmath$k \le 2$}}.
\smallskip

We cannot apply the preceding method to discuss the Green function for
$k \le 2$ because $E_a (\int^\infty_0 f (x_t) dt)$ may be infinite
even if $f$ has compact support. We therefore follow a different
method. 

Let $\Gamma = \Gamma (o, r)$ be a ball. If $u \in C^{\infty} (R^2)$,
[i.e. compact support and $C^\infty$ ] then $u \in \mathscr{D}
(\mathscr{G})$ and Dynkin's formula gives  
\begin{align*}
E_a & \left(\int\limits^{\tau_\Gamma}_0\right) \frac{1}{2} \Delta u (x_t)dt) = E_a
  (u(x_{\tau_\Gamma}))- u (a)\\ 
  & = \int_{\partial \Gamma} \pi_\Gamma (a, \xi ) u (\xi) \theta (d
  \xi) - u (a) , \pi_{\Gamma}(a, \xi )= \frac{r^2 - a^2}{| a - \xi
    |^2}, \\ 
  & = - \int_{\partial \Gamma} \frac{G (a,b)} {\partial n} \bigg ]_{b
    = \xi \in \partial \Gamma} x r u (\xi) \theta (d \xi) - u (a), G_\Gamma
  (a,b)= \log \frac{|a \bar{b} - r^2|} {|a - b|} \\ 
  & = \int_{\Gamma} \frac{1}{2 \pi} G_{\Gamma} (a, b) \frac{1}{2}
  \Delta u(b) 2 db. 
\end{align*}

If $\varphi \in C^{\infty} (R^2)$ and $v(a) = \frac{1}{\pi}
\int_{\Gamma} G_{\Gamma} (a,b) \varphi (b) db$, then $\frac{1}{2}
\Delta v = \varphi$ Therefore we have for any $\varphi \in
C^{\infty} (R^2)$, 
$$
E_a \left(\int\limits^{\tau_\Gamma}_0 \varphi (x_t) dt\right) = \frac{1}{\pi}
\int_{\Gamma} G_{\Gamma} (a,b) \varphi (b) db. 
$$

It follows that the same equation holds for any $f \in \mathcal{B}
(R^2)$, i.e., 
$$ 
E_a \left(\int\limits^{\tau_\Gamma}_0 f(x_{t})dt\right) = \frac{1}{\pi}
\int\limits_{\Gamma} G (a,b)f (b) db. 
$$

Now let $U$ be a bounded domain, $\bar{U} \subset \Gamma$, a ball. Then
\begin{align*}
  E_a \left(\int\limits^{\tau_\Gamma}_0 f (x_t) dt\right) & = E_a
  \left(\int\limits^{\tau_U}_0 f (x_t) dt\right) + E_a
  \left(\int\limits^{\tau_\Gamma}_{0} (w^+ \tau_U) f(x_0
  (w^+_{\tau_U})) dt\right)\\ 
  & = E_a \left(\int\limits^{\tau_U}_0 f (x_t) dt\right) + E_a \left(E_{x_{\tau U}}
  \left(\int^{\tau_\Gamma}_0 f(x_t) dt\right)\right), 
\end{align*}\pageoriginale
so that
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
E_a  \left(\int\limits^{\tau_U}_0 f (x_t) dt\right)  &= \frac{1}{\pi}
  \int\limits_{\Gamma} G_{\Gamma} (a, b) f (b) db -
  \int\limits_{\partial U} \pi_U (a, d_\xi) E_{\xi}
  \left(\int\limits^{\tau_\Gamma}_{0} f(x_t) dt\right)\\  
  & = \frac{1}{\pi} \int\limits_{\Gamma} G_{\Gamma} (a, b) f (b) db -
  \frac{1}{\pi} \int\limits_{\partial U} \pi_U (a, d) \int\limits_{\Gamma}
  G_{\Gamma} (\xi, b) f(b) db\\ 
  & = \frac{1}{\pi} \int\limits_{\Gamma} G_U (a,b) f (b) db,
\end{align*}}\relax
where
\begin{align*}
  G_U (a,b) &= G_{\Gamma}(a,b) - \int\limits_{\partial U} \pi_{U} (a,
  d_\xi ) G_{\Gamma} (\xi , b)\\ 
  &= \log \frac{1}{|a - b|} - \int\limits_{\partial U} \pi_U (a, d_\xi
  ) \log \frac{1}{|\xi - b|}\\
&\quad - \log \frac{1}{|a \bar{b} - r^2|} + \int\limits_{\partial U} \pi_U (a, d_\xi ) \log \frac{1}{|\xi \bar{b} -r^2|} 
\end{align*}

Since $b \in U \subset \bar{U} \subset \Gamma$, $|a \bar{b}| < r^2$ for
$a \in U$ and $\log | a \bar{b} - r^2 |$ is harmonic for $a \in U$,
with boundary values $\log |\xi \bar{b} - r^2|$. Hence if every point
of $\partial U$ is regular, 
$$
\log  |a \bar{b}- r^2| = \int\limits_{\partial U} \pi_U (a, d) \log
|\xi \bar{b} - r^2|. 
$$

Thus we have
$$
G_U (a, b) = \log \frac{1}{|a - b|} - \int\limits_{\partial U} \pi_U
(a, d) \log \frac{1}{|\xi - b|}. 
$$

\setcounter{thm}{0}
\begin{thm}\label{chap3-sec5-addthm1}%Thm 1
  If $U$ is a bounded open set such that every point of $\partial U$
  is\pageoriginale regular and if $u (a = E_a (\int\limits^{\tau_U}_0
  f (x_t) dt)$, then  
  $$
  \frac{1}{2} \Delta u = f \text{ \ and \ } u (a) \rightarrow 0 \text{
  \  as \ } a \rightarrow \xi \in \partial U.  
  $$
\end{thm}

\begin{proof}
In fact
$$
u (a) = E_a \left(\int^{\tau_U}_0 f (x_t)dt\right) = \frac{1}{\pi} \int_{U} G_U
  (a, b) f (b) db 
$$
and the theorem follows from the definition of $G_U(a, b)$.
\end{proof}

\begin{thm}\label{chap3-sec5-addthm2}%Thm 2
$$
\frac{E_a (S(db/U))}{db} = \frac{1}{\pi} G_U (a, b).
$$
\end{thm}

If $k =1$, we can proceed directly. Suppose that $U = (\alpha , \beta)$.

Then
\begin{align*}
E_a \left(\int\limits^{\tau_{\alpha, \beta}}_0 \frac{1}{2}u'' (x_t)dt\right) & =
  E_a(u(x_{\tau(\alpha , \beta)})) - u(a)\\ 
  & = \frac{\beta - a}{\beta - \alpha} u (\alpha) + \frac{a - \alpha}
  {\beta - \alpha} u (\beta )- u(a)\\ 
  & = \int\limits^\beta_\alpha G_{(\alpha , \beta)} (a, b) \frac{1}{2}
  u'' (b) 2 db, 
\end{align*}
where 
$$
G_{(\alpha , \beta)} (x, y) = G_{(\alpha , \beta)}(y, x) =
\frac{(\beta - y) (x - \alpha)}{\beta - \alpha},\quad \alpha \leq x \leq y
\leq \beta. 
$$

Threfore we have

\begin{thm}\label{chap3-sec5-addthm3} % theo 3
$$
E_a \left(\int\limits^{\tau_{(\alpha , \beta)}}_0 f (x_t) dt\right) = \int^\beta_\alpha
  G_{(\alpha , \beta)} (a, b) f (b) 2db 
  $$
\end{thm}

\begin{thm}\label{chap3-sec5-addthm4} % theo 4
$$
\frac{E_a (S(db / (\alpha, \beta))}{db} = 2 G_{(\alpha, \beta)}(a, b).
$$
\end{thm}

\section{Hitting probability}\label{chap3-sec6} %Sec 6
\pageoriginale

We have already discussed the hitting probability for spheres. Here we
shall discuss it for more general sets, especially compact sets. 

\medskip
\noindent
{\bf Absolute hitting probability {\boldmath$(k \geq 3)$}.}
\smallskip

For simplicity we consider the case $k = 3$.

Let $F$ be a compact set and $\sigma_F = \inf \{ t : t > 0$ and $x_t
\in F \}$. Put $p_F(a) = P_a (\sigma_F < \infty) = P_a (x_t \in F$ for
some $t > 0); p_F(a)$ is called the \textit{absolute hitting
  probability} for a $F$ with respect to $a$. 

\begin{lemma*}
Let $\Gamma = \Gamma (a, r)$ and $\tau_r = \tau_\Gamma =$ the first
  leaving time for $\Gamma$. Then $P_a (\tau_r \rightarrow 0
  \text{ \ as  \ } r \rightarrow 0) = 1$. 
\end{lemma*}

\begin{proof}
Clearly $\tau_r$ decreases as $r$ decreases. If $\tau = \lim\limits_{r \rightarrow 0} \tau_r$, we have only to show that $P_a
  (\tau > 0) = 0$. Now $P_a (\tau > t)  \leq P_a (\tau_r > t) \leq P_a
  (x_t \in \Gamma) = (2 \pi t)^{-3/2} \int\limits_{\Gamma} \exp (-(x-a)^{2}
  \frac{1}{2t}) dx \rightarrow 0$ as $r \rightarrow 0$. 
\end{proof}

\setcounter{thm}{0}
\begin{thm}\label{chap3-sec6-thm1}% Thm 1
$P_F (a)$ is expressible as a potential induced by a bounded measure
  $\mu_F$ i.e. $P_F(a) = \int \dfrac{\mu_F (db)}{|b - a|}$, where
  $\mu_F$ is concentrated  on $\partial F$. Further $\forall_F$ is
  uniquely determined by $F$. 
\end{thm}

\begin{proof}
Firstly we show that $p_F(a)$ is harmonic in $F^c \cap F^0$. Since
  $p_F (a) \equiv 1$ in $F^0$, we have only to show that it is
  harmonic in $F^c$. Let $\Gamma$ be ball such that $\bar{\Gamma}
  \subset F^c$. If $\tau_{\Gamma}$ is the first leaving time for
  $\Gamma$, $\tau_{\Gamma} < \sigma_F$ and $p_F(a) = P_a (\tau_{\Gamma}
  < \sigma_F < \infty) = P_a (\sigma_F (w^+_\tau) < \infty ) = E_a
  (P_{x_{\tau_{\Gamma}}}(\sigma_F < \infty)) = E_a (p_F(x_{\tau_\Gamma}))
  = \int\limits_{\partial \Gamma} \pi_{\Gamma} (a, d_\xi) p_F (\xi) =
  \int_{\partial \Gamma} \pi_{\Gamma} (a, \xi) p_F (\xi) \theta (d
  \xi)$, showing that $p_F(a)$ is harmonic for $a \in \Gamma$. 

Let\pageoriginale $\Gamma$ be a ball and $a \in \Gamma$. Then 
$$
p_F(a) = P_a (\sigma_F < \infty) \geq p_a (\sigma_F (w^+_{\tau_\Gamma}) < \infty) = \int_{\partial \Gamma} \pi_{\Gamma} (a, \xi)
p_F (\xi) \theta (d \xi). 
$$

This show that $p_F(a)$ is super harmonic in the wide sense (i.e. its
value at the centre of a ball in not less than the average value on
the boundary). 

Finally we show that $p_F(a)$ is lower semi-continuous. It is enough
to show this for $a \in \partial F$. Let $a_0 \in \partial F$, and
$\Gamma (a_0 , r) = \Gamma_r$, $\tau_r = \tau_{\Gamma_r}$. Then
$p_F(a_0) = P_{a_0} (x_t \in F$ for some $t > \tau_r) + \eta_r$, $\eta_r
\rightarrow 0$ (from the lemma) $= \int\limits_{\partial \Gamma_r}
p_F (\xi) \theta (d \xi) + \eta_{r}$. On the other hand 
$$
p_F(a) \geq \int\limits_{\partial \Gamma_r} \pi_{\Gamma} (a, \xi) p_F
  (\xi) \theta (d \xi) 
$$
so that
{\fontsize{10pt}{12pt}\selectfont
$$
\lim_{a \rightarrow a_0} p_F (a) \geq \int_{\partial \Gamma_r} \lim_{a
    \rightarrow a_0} \pi_\Gamma (a, \xi) p_F (\xi) \theta (d \xi) =
  \int_{\partial \Gamma_r} p_F (\xi)^{\theta (d \xi)}_\lambda = p_F(a_0)
  - \eta_r.
$$}\relax

Now letting $r \rightarrow 0$, $\lim\limits_{a \rightarrow a_0} p_F(a)
\geq p_F (a_0)$, showing that $p_F(a)$ is lower semi-continuous. Now
if $\Gamma$ is a ball containing $F$, $\sigma_{\Gamma}$, the first
passage time for $\Gamma$, then we have seen that $P_a
(\sigma_{\Gamma} < \infty ) = r \rho^{-1}$, $\rho= |a|$. Therefore $P_a
(\sigma_{\Gamma} < \infty) \rightarrow 0$ as $a \rightarrow \infty$
and since $P_a (\sigma_F < \infty) \leq P_a (\sigma_{\Gamma} <
\infty)$,  $P_a (\sigma_F < \infty) \rightarrow 0$ as $a \rightarrow
\infty$. Since $p_F(a)$ is super harmonic in $R^3$, from the Reisz
representation theorem there exists a unique bounded measure $\mu_F$
with $p_F(a) = \int\dfrac{\mu_{F} (db)}{|b - a|} + H(a)$ where $H(a)$
is harmonic in $R^3$. But $p_F (a) \rightarrow 0 |a| \rightarrow
\infty$\pageoriginale and also $\int \frac{\mu_F (db)}{|b - a|}
\rightarrow 0$ as 
$|a| \rightarrow \infty$ since $\mu_F$ is a bounded measure. It
follows that $H(a) \rightarrow 0$ as $|a| \rightarrow \infty$
i.e. $H(a) \equiv 0$. Therefore $p_F (a) = \int \dfrac{\mu_F (db)}{|b -
  a|}$. Since $\mu_F$ is concentrated in the set where $p_F$ is not
harmonic, $\mu_F$ is concentrated in $\partial F$. This proves  the
theorem completely. 
\end{proof}

\begin{thm}\label{chap3-sec6-thm2} % theo 2
  If $u(a)$ is any potential induced by a measure $\nu$ which is
  concentrated in $F$ and if $u(a) \leq 1$, then 
  $$
  u(a) \leq p_F (a) \text{ and } \nu (F) \le \mu_F (F) (= \mu_F (\partial F)).
  $$
\end{thm}

\begin{proof}
  We have $u(a) = \int_F \dfrac{\nu (db)}{|a -b|}$. Since $F$ is
  compact, for fixed a we can find $n$ such that $|a -b| \leq n$. It
  follows that $\nu (F) < \infty$ and therefore $u(a)$ is harmonic in
  $F^c$. Let $G_n \uparrow F^c$ be a sequence of bounded open sets
  such that $\bar{G}_n \subset G_{n + 1}$. Let $\tau_n = \tau_{G_n} =$
  the first leaving time from $G_n$. If we put $f = u/G_n$ then $u$ is
  the classical solution with boundary values $f$. Therefore for every
  $a \in G_n$ 
  \begin{align*}
&  u(a) = E_a(f(x_{\tau_n}))\\
& = E_a (u(x_{\tau_n})) - E_a (u(x_{\tau_n})
  : \sigma_F = \infty ) + E_a (u(x_{\tau_n}) : \sigma_F < \infty). 
  \end{align*}
\end{proof}

Now $\tau_n \uparrow \sigma_F$. If $\sigma_F = \infty , \tau_n
\uparrow \infty$ and $x_{\tau_n} \uparrow \infty$ with probability
$1$; and by the formula for $u , u (x_{\tau_n}) \rightarrow 0$. Since
$u(a) \leq 1$ we have therefore  
$$
u(a) \leq E_a (\sigma_F < \infty) = p_F(a). 
$$

If\pageoriginale $a \in F^0,  p_F(a) = 1$ and $u(a) \leq 1 = p_F(a)$.

Let now $a \in \partial F$ and $\Gamma = \Gamma (a,r)$ and $\tau_r$
the first leaving time for $\Gamma$. Then 
\begin{align*}
  p_F(a) & \geq P_a (x_t \in F ~\text{for some}~ t \geq \tau_r)\\ 
  & = P_a (x_t(w^+_{\tau_r}) \in F ~\text{for some}~ t \geq 0)\\
  & = E_a [ P_{x_{\tau_r}} (x_t \in F \text { for some } t \geq 0)] \\
  & = E_a  P_{x_{\tau_r}} (x_t \in F \text { for some } t \geq 0) :
  x_{\tau_r} \in F^c\\ 
  & \qquad +  E_a  P_{x_{\tau_r}} (x_t \in F \text { for
    some }t \geq 0) : x_{\tau_r} \in F) \\
  & \geq E_a  [ {x_{\tau_r}} \in F^c  : u(x_{\tau_r}) ] + E_a [x_{\tau_r}
  \in F : 1] \geq E_a [u (x_{\tau_r})] 
\end{align*}
since $P_a (x_t \in F$ for some $t \geq 0) = 1$ for $a \in F$. Letting
$r \rightarrow 0$ we get 
$$
p_F (a) \geq \varliminf_{r \rightarrow 0} E_a (u(x_{\tau_r})) \geq E_a
(\varliminf_{r \rightarrow 0} u(x_{\tau _r})) \geq u (a) 
$$
since $u(a)$ is lower semi-continuous. It remains to prove that $\nu
(F) \leq \mu_F (F)$. 

Let $E$ be a compact set with $E \supset E^0 \supset F$ and consider
$p_E(a)$. Then $p_E(a) = \int \dfrac{\mu_E (db)}{|a - b|}$ and $p_E(a)
= 1$ for $a \in E^0 \supset F$. Since 
$$
\int \frac{\mu_F (db)}{|a - b|} \geq \int \frac{\nu (db)}{|b -
    a|}
$$
we have
$$
\iint \frac{\mu_F (db)}{|a - b|} \mu_E (da) \geq \iint \frac{\nu
    (db)}{|a - b|} \mu_E (da)
$$
i.e.,\pageoriginale
$$
\int_F \mu_F (db) \geq \int_F \nu (db).
$$

An alternative proof of the last fact is the following. Since $p_F(a)
\geq u(a)$ 
$$
\int_F |a| \frac{\mu_F (db)}{|b - a|} \geq \int_F |a| \frac{\nu
  (db)}{|b - a|} 
$$
Letting $a \rightarrow \infty$ we get the result.	

From the above theorem we have
\begin{thm}\label{chap3-sec6-thm3}% theo 3
  $C(F) = \mu_F (\partial F)$ is the maximal total charge for those
  charge distributions which induce potentials $\le 1$. 
\end{thm}

\begin{thm}[Kakutani]\label{chap3-sec6-thm4}% theo 4
  $C (F) > 0$ if and only if $p_F(a) > 0$ i.e.
  $$
  P_a (x_t \in F \text{ for some } t > 0) > 0.
  $$
  $C(F)$ is called the \textit{ capacity } of $F$.
\end{thm}

\begin{thm}\label{chap3-sec6-thm5} %theo 5
  $$
  \frac{C(F)} {\max \limits_{b \in F} |a - b|} \leq p_F (a) \leq
  \frac{C(F)} {\min \limits_{b \in F} |b - a|} \text{ and } C(F) =
  \lim_{|a| \rightarrow \infty} |a| p_F (a). 
  $$
\end{thm}

We shall now prove the subadditivity of $p_F(a)$ and $C(F)$ following
Hunt. This means that $p_F(a)$ and $C(F)$ are both strong capacities
in the sense of Choquet. 

\begin{thm}\label{chap3-sec6-thm6} % theo 6
  $p_F(a)$ and $C(F)$ are subadditive in the following sense.
  
  $\varphi (F_1 \cap \cdots \cap F_n) \leq_i \sum \varphi (F_i) - _i
  \sum_ j \varphi (F_i \cup F_j) + \sum \limits_{i < j < k} \varphi
  (F_i \cup F_j \cup F_k)\cdots$ where $\varphi(F)$ denotes either of
  $p_F(a)$ and $C(F)$. 
\end{thm}

\begin{proof}
  Put\pageoriginale $F^* = \{w : \sigma_F (w) < \infty \}$. Then $(F_1
  \cup \cdots 
  \cup F_n)^* = F^*_1 \cup \cdots \cup F^*_n, (F_1 \cap \cdots \cap
  F_n)^* \subset F^*_1 \cap \cdots \cap F^*_n$ and $p_F (a) = P_a
  (F^*)$. Using the dual inclusion - exclusion formula of Hunt, we
  have 
\begin{align*}
  p_{F_1 \cap \cdots \cap F_n} (a) & = P_a [(F_1 \cap \cdots \cap
    F_n)^*] \leq P_a [(F^*_1 \cap \cdots \cap F^*_n)] \\ 
  & = \sum_i P_a (F^*_i) - \sum_{i < j} P_a (F^*_i \cup F^*_j) + \cdots  \cdots \\
  & =\sum_i P_{F_i} (a) - \sum_{i < j} p_{F_i \cup F_j} (a) + \sum_{i
    < j < k} p_{F_i \cup F_j \cup F_k}(a) \cdots 
\end{align*}
\end{proof}

Multiplying by $|a|$ both sides and letting $|a| \rightarrow \infty$
we get the said inequality for $C(F)$. 

\medskip
\noindent
\textbf{Hitting probability for open sets.}
\smallskip

Let $U$ be a bounded open set and define $\sigma_U$ and $P_U(a)$ as in
the case of compact sets $F$. Then $p_U(a)$ is harmonic outside
$\partial U$ and super harmonic in the whole space. Therefore $p_U(a)
= \int \limits_{\partial U}\dfrac{\mu_U(db)}{|a -b|}$ in $(\partial
U)^c$, and $\mu_U (\partial U) = \lim \limits_{|a| \rightarrow \infty}
|a| p_U(a)$. Let $ F_n \uparrow U$ be compact subsets of $U$. Then
$C(F_n) = \lim \limits_{|a| \rightarrow \infty} |a| p_{F_n} (a)$ and
the convergence is uniform in $n$ since $F_n$ are contained in a
bounded set. Also since 
$$
P_U (a) = P_a (\sigma_U < \infty) = \lim_{n \rightarrow \infty} P_a
  (\sigma_{F_n} < \infty) = \lim p_{F_n}(a)
$$
we have
$$ 	
\mu_U (\partial U) = \lim_{n \rightarrow \infty} C (F_n).
$$

Therefore\pageoriginale $\mu_U \partial U$) is the supremum of
capacities of compact 
sets contained in $U$; it is the \textit{capacity} $C(U)$ of $U$ by
definition. Again $p_U(a) \leq \dfrac{C(U)}{\min \limits_{b \in
    \partial U} |b - a|}$. 

\begin{remark*}
  The capacity of any set is defined as follows. We have already
  defined the notion of capacity for compact sets. The capacity of any
  open set is by definition the supermum of the capacities of compact
  sets contained in it. The \textit{outer capacity} of a set is the
  infimum of the capacities of open sets containing it, while the
  \textit{inner capacity} is the supremum of the capacities of compact
  sets contained in it. If both are equal the set is called
  \textit{capacitable} and the outer (or inner) capacity is called the
  \textit{capacity} of the set. Choquet has proved that every Borel
  (even analytic) set is capacitable. 
\end{remark*}

\medskip
\noindent
\textbf{Relative hitting probability {\boldmath$(k \geq 1)$}.}
\smallskip

Let $F$ be a compact set contained in a bounded open set $U$ and put
$\sigma_{F / U} = \inf \{t : \tau_U > t\to 0 \text{\ and \ } x_t \in F \}$ where
$\tau_U$ is the first leaving time from $U$. Let $p_{F/U}(a) = P_a
(\sigma_{F/U} < \infty) = P_a \{$ for some $t > 0 x_t$ reaches $F$
before it leaves $U$. $p_{F/U}(a)$ is called the \textit{(relative)
  hitting probability} for $F$ with respect to a and relative to
$U$. Using the same idea as before we can prove 	 

\setcounter{dashthm}{0}
\begin{dashthm}\label{chap3-sec6-dashthm1}% theo 1
  $p_{F/U}(a)$ is expressible as a potential induced by a bounded
  measure $\mu_{F/U}$ with the Green function $G_U (a, b)$, i.e. 
  $$
  p_{F/U}(a) = \int G_U (a, b) \mu_{F/U}(db), a \in U,
  $$
  where\pageoriginale $\mu_{F/U}$ is concentrated on $F$. Further
  $\mu_{F/U}$ is uniquely determined by $F$. 
\end{dashthm}

We can define the \textit{relative capacity} $C_U(F)$ of $F$ as
$\mu_{F/U}(\theta F)$ and carry out similar discussions. 

\medskip
\noindent
\textbf{Remark on absolute hitting probability {\boldmath$(k \leq 2)$}.}
\smallskip

In case $k = 1$, $p_F (a) \equiv 0$ or $\equiv 1$ according as $F \neq
\phi$ or $= \phi$. 

In case $k =2$, we contend that $p_F(a) \equiv 1$ or $0$ according as
$C_U (F) > 0$ or $= 0$, where $U$ is a bounded open set containing
$F$. To prove this let $V$ be another bounded open set such that $F
\subset V \subset \bar{V} \subset U$. Let $\sigma_1(w) = \tau_U(w) +
\sigma_V (w^+_U)$, $\sigma_2(w) = \sigma_1 (w) + \sigma_1
(w^+_{\sigma_1})$, $\sigma_3(w) = \sigma_2(w) + \sigma_{1} (w^+_{\sigma_2}),
\ldots , \sigma_n(w) = \sigma_{n-1}(w) + \sigma_1 (w^+_{n-1})$,
etc. Then evidently $x_{\sigma_n} \in \partial V$ and $\sigma_n
\uparrow \infty$; for let, $\sigma'_n(w) = \sigma_{n-1}(w) + \tau_U
(w^+_{\sigma_{n-1}})$. Then $\sigma_{n-1} \leq \sigma'_n \leq
\sigma_n$, and $x_{\sigma_n} \in \partial V$, $x_{\sigma_n'} \in
\partial U$ so that if $\sigma_n \uparrow \sigma$, $x_\sigma \in
\partial V \cap \partial U = \phi$ which is a contradiction. Hence
$\sigma_n \uparrow \infty$ with $P_a$-probability 1. If $C_U(F) =
0$, then $p_{F/U}(x_{\sigma_n}) = 0$. Now 
\begin{multline*}
  P_a (x_t \in F, \sigma _n < t \leq \sigma_{n+1} = P_a (\sigma_F
  (w^+_{\sigma_n}) < \tau_U (w^+_{\sigma_n}))\\ 
  = E_a (P_{x_{\partial_n}}(\sigma_F (w) < \tau_U (w))) = E_a
  (p_{F/U}(x_{\sigma_n})) = 0. 
\end{multline*}

Hence $P_F(a) = P_a (x_t \in F$ for some $t > 0) \leq \sum P_a (x_t
\in F, \sigma_n < t \leq \sigma_{n+1}) = 0$. Now 
\begin{align*}
  1 - p_F (a)& \leq P_a (x_t \notin F, o < t < \sigma_n \\
  &\leq P_a (\sigma _F (w^+_{\sigma_r}) > \tau _U (w^*_{\sigma_r})
  (\leq r \leq n) 
\end{align*}\pageoriginale	

The set $\left(\sigma_F(w^+_{\sigma_r})\right) > \tau_U
(w^+_{\sigma_r}), 1 \leq r \leq 
n-1)$ is $\mathcal{B}_{\sigma_{n^+}}$-meansurable. For	 
$$
(\sigma_F (w^+_{\sigma_r}) < \tau_U (w^+_{\sigma_r})) = ( \sigma_F
[w^-_{\sigma +1})^{+}_{\sigma_r (w^-_{r _1})}] > \tau_U 
[(w^-_{\sigma_{r +1}})^{+}_{\sigma_r (w^-_{r+1})}]) 
$$

Hence $(\sigma_F (w^+_{\sigma_r}) < \tau_U (w^+_{\sigma_r})) \in
\mathbb{B}_{\sigma_{r+1}} \subset \mathbb{B}_{\sigma_n}$, for $r+1= n$.
[Note that if $\sigma_1$, $\sigma_2$ are two Markov times and
  $\sigma_{1}< \sigma_2$ then $\mathbb{B}_{\sigma_1} \subset
  \mathbb{B}_{\sigma_2}$]. Hance by strong Markov property	 
\begin{multline*}
P_a (\sigma_F (w^+_{\sigma_r}) > \tau_{U}(w^+_{\sigma_r}), 1 \leq r
\leq n)\\ 
= E_a [p_{x_{\sigma_n}}(\sigma_F > \tau_F) : \sigma_F
  (w^+_{\sigma_r}) > \tau_U (w^+_{\sigma_r}),\quad 1 \leq r \leq n -1] 
\end{multline*}

If $C_U (F) > 0$, since $p_{F/U}(a)$ is continuous on $\partial V$ and
always $> 0$ it has a minimum $\epsilon> 0$. Then  
\begin{multline*}
P_a (\sigma_F(w^+_{\sigma_r}) > \tau_U(w^+_{\sigma_r}), 1 \leq r
  \leq n)\leq (1 - \epsilon) P_a (\sigma_F (w^+_{\sigma_r})\\ 
  > \tau_U
  (w^+_{\sigma_r}), 1 \leq r \leq n - 1) \leq \ldots \leq (1 - \epsilon)^n
  \rightarrow C. 
\end{multline*}

This proves our contertion.

\section{Regular points ($k \geq 3$)}\label{chap3-sec7}% 7

In order to decide whether the garalied solution (the stochastic
solution) $u (a) = u(a:f,v)$ satisfies the boundary conditions 
$$
\lim_{\substack{a \in U,\\ a \rightarrow \xi}} u(a) = f(\xi),\quad
\xi \in \partial U 
$$\pageoriginale
we introduce the notion of regularity of boundary points.

Let $U$ be an open set and $\xi \in \partial U$. Let 
$$
\tau^*_U = \inf \{ t : t > 0 \text{ and } x_t \notin U  \},
$$
and consider the event $\tau^*_U = 0$. This clearly belongs to
$\mathbb{B}_{o^+}$ and Blumenthal $0-1$ law gives $P_\xi (\tau^*_U =
0) = 1$ or $0$. If it is $1$, $\xi$ is called a \textit{regular point} 
for $U;$ if it is zero it is called \textit{irregular} for
$U$. Regularity is a local property. In fact, if $\xi$ is regular for
$U$, $\xi$ is regular for $\Gamma \cap U$ for any open neighbourhood
$\Gamma$ of $\xi$ and vice versa. We state here two important criteria
for regularity. 

\setcounter{thm}{0}
\begin{thm}\label{chap3-sec7-thm1} % them 1
Let $\xi \in \partial U$.
\begin{enumerate}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item $\xi$ is regular for $U$ if and only if $\lim\limits_{a \in
    U, a \rightarrow \xi} P_a (x(\tau^*_U) \in \partial U \cap \Gamma)
    = 1$. 

\item $\xi$ is irregular for $U$ if and only if $\lim\limits_{\Gamma
    \downarrow \xi} \varliminf\limits_{a \rightarrow \xi} P_a (x(\tau^*_U)
    \in \partial U \cap \Gamma) = 0$. 
\end{enumerate}
\end{thm}

\begin{thm}[Winer's test]\label{chap3-sec7-thm2}% theo 2
If $\xi \in U$ and 
$$
F_n = (b : 2^{-(n+1)(k - 2)} \leq |b-\xi| \leq 2^{-n(k - 2)}, b \in U^c)
$$
is regular or irregular according as $\sum\limits_{n} 2^{-n (k -2)}
  C(F_n) = \infty$ or $< \infty$. 
  
We can prove the above two theorems using the same idea we  used for
  the proof of Poincare's test.  
\end{thm}

The\pageoriginale following theorem, an immediate corollary of Theorem
\ref{chap3-sec7-thm1} gives the boundary values of the stochastic solution. 

\begin{thm}\label{chap3-sec7-thm3} %Thm 3
If $U$ is a bounded open set, if $\xi$ is regular for $U$ and if $f$
  is bounded Borel on $\partial U$ and continuous at $\xi$, then 
$$
\lim\limits_{a \in U,a \to \xi} u(a:f,U)=f(\xi).
$$

On the other hand if $\xi$ is irregular for $U$, then there exists a
continuous funtion $f$ on $\partial U$ such that the above equality
is not true. 
\end{thm}

The following thorem, which we shall state whithout proof, shows that
the set of irregular points is very small compared with the rest. 

\begin{thm}\label{chap3-sec7-thm4}% Thm 4
Let $U$ be a bounded open set. Then the set of irregular points has
  capacity zero. 
\end{thm}

Using Theorem \ref{chap3-sec7-thm3} and \ref{chap3-sec7-thm4} we prove
the following 

\begin{thm}\label{chap3-sec7-thm5}% Thm 5
  If $U$ is a bounded open set and if $f$ is continuous on $\partial
  U$ the stochastic solution $u(a)=u(a:f,U)$ is the unique bounded
  harmonic function defined in $U$ such that 
  $$
  \lim_{a \in U,a \to \xi} u(a)=f(\xi),\xi \in \partial U
  $$
  except for a $\xi$- set of capacity zero.
\end{thm}

\begin{proof}
It folllows at once from Theorem \ref{chap3-sec7-thm3} and 
\ref{chap3-sec7-thm4} that the
stochastic solution\pageoriginale is a bounded harmonic function with
boundary values $f$ at 
  regualar points. Conversely let $v$ be any bounded harmonic funtion
  with the boundary values $f$ upto capacity zero. Let $N$ be the 
  set of all points $\xi$ such that $v(a)\nrightarrow
  f(\xi)$. Then $C(N)=0$ by assumption. Therefore there exists a
  decreasing sequence of open sets $G_m \supset N$ such that
  $\bar{G}_{m+1}\subset G_m$ and $C(G_m) \to 0$. Since $N$ is bounded
  we can assume that $G_m$ are also bounded and since $N \subset
  \partial U$, we can assume that $\bigcap \limits_m G_m \subset
  \partial U$. Let $a \in U$. Then $\rho (a,G_m)= \inf \limits_{b \in
    G_m}\rho (a,b)>$ some positive constant for large $m$. Therefore  
{\fontsize{10pt}{12pt}\selectfont
  $$
  P_a (x_{\tau_U} \in N)\leq P_a \left(x_{\tau_U}\in \bigcap_m G_m\right)\leq P_a
  [\sigma_{G_m}< \infty] \leq \frac{C(G_m)}{(\rho(a,G_m))^{k-2}} \to 0 
  $$}\relax
  so that $P_a (x_{\tau_U} \in N)=1$. Let now $U_n$ be open sets, $U_n
  \uparrow U$ such that $\bar{U}_n \subset U$ and every boundary point
  of $U_n$ is a Poincar\'e point for $U_n$. Then
  $v(a)=E_a(b(x_{\tau_{U_n}}))$, a $\in U_n$ so that 
  \begin{align*}
    v(a)& = \lim_ {n \to \infty} E_a(v(x_{\tau_{U_n}}))=E_a (\lim_ {n \to
      \infty} v(x_{\tau_{U_n}}))\\
    &= E_a (\lim_ {n \to \infty} v(x_{\tau_{U_n}}):\lim_ {n \to
      \infty} x_{\tau_{U_n}}=x_{\tau_{U}} \notin N)\\ 
    &=E_a(f(x_{\tau_{U}}): x_{\tau_{U}} \notin N)=E_a(f(x_{\tau_{U}}))=u(a:f,U).
  \end{align*}
\end{proof}

\section[Plane measure of a two dimensional...]{Plane measure of a two dimensional Brownian\hfil\break motion
  curve}\label{chap3-sec8}% section 8        

We have seen in Theorem \ref{chap3-sec4-thm3} of \S\ \ref{chap3-sec4} that the two-dimensional
Brownian motion is dense in the plane. We now prove the following
interesting theorem due to Paul L\'evy. 
\setcounter{thm}{0}
\begin{thm}[P.~Levy]\label{chap3-sec8-thm1} %Them 1
  The\pageoriginale two dimensional Lebesgue measure of a
  two-dimensional Brownian 
  motion curve is zero with probability $1$ i.e. if $C(w)=\{ x_s:0
  \leq s < \infty \}$, and $|C|=$ the Lebesgue measure of $C(w)$ then
  $P_a(|C|=0)=1$. 
\end{thm}

We first prove the following lemma.
\begin{lemma*}
  Let $S$ be a Hausdorff space with the second countability axiom and
  $W$ a class of continuous functions fo $[0,t]$ into $S$. Let
  $\mathbb{B}$ be the Borel algebra generated by the class of all sets
  of the form $\{ w : w \in W$ and $w(s)\in E \}$ where $0 \leq s \leq
  t$ and $E \in \mathbb{B}(S)$, $\mathbb{B}(S)$ being the class of Borel
  subsets of $S$ (i.e. the Boral algebra generated by open sets of
  $S$). Let $C(w)=\{ w(s): 0 \leq s \leq t \}$. Then the function
  defined by 
  \begin{align*}
    f(a,w) &=1 \text{ if } a \in C(w)\\
    &= 0 \text{ if }a \notin C(w)
  \end{align*}
is $\mathbb{B} (S) \times \mathbb{B}$-measurable in the pair $(a,w)$.
\end{lemma*}

\begin{proof}
It is clearly enough to prove that
  $$
  \{ (a,w):a \notin C(w) \} \in \mathbb{B} (S)\times \mathbb{B}.
  $$

For any open set $U \subset S$ we have
  $$
  (w:C(w)\subset U^c)=-\bigcap _{\substack {r \leq t\\{r, \text{
          rational }}}} \{ w:w(r)\in U^c \} 
  $$
  so that
  $$
  (w: C(w)\subset U^c)\in \mathbb{B}.
  $$

  Let now $U_n$ be a countable base for $S$. Then it is not difficult
  to see that 
  $$
  \{ (a,w):a \notin C(w) \} = \bigcup^\infty_ {n=1} \left[U_n \times \left\{ w:
    C(w)U^c_n \right\} \right] 
  $$
  using\pageoriginale the fact that $C(w)$ being the continuous image
  of $[o,t]$ is 
  closed.\hfill Q.E.D. 
\end{proof}

\begin{pot*}
  To prove the theorem it is enough toi consider two dimensional
  Brownian motion curves starting at zero i.e. a two-dimen\-sional
  Wiener process. Let $x_t(w)$ be a two-dimensional Wiener process on
  $\Omega (\mathbb{B},P)$. It is enough to show that $E(|c_t|)=0$,
  where $c_t= \{ x_s: 0 \leq s \leq t \}$ and $|c_t|=$ the two
  dimensional Lebesgue measure of $c_t$. From the lemma the function
  $\chi (a,c_t)$ defined as 
  \begin{align*}
    \chi (a,c_t) & = 1 \text{ if } a \in c_t\\
    &= 0 \text{ if }a \notin c_t
  \end{align*}
  is measurable in the pair $(a,w)$. Since $|c_t|=\int
  \limits_{R^2}\chi (a,c_t)da, |c_t|$ is measurable in $w$. Consider
  the following four processes: 
  \begin{enumerate}
  \item $x_s (w),0 \leq s \leq t$
  \item $y_s(w)=x_{s+t}(w)-x_t(w),0 \leq s \leq t$
  \item $z_s (w)=x_{t-s}(w)-x_t(w),0 \leq s \leq t$
  \item $u_s(w)=\dfrac{x_{2s}(w)}{\sqrt{2}} \qquad ,0 \leq s \leq t$.
  \end{enumerate}

  All the four processes are continuous processes i.e. processes whose
  sample functions are continuous. Let 
  $$
  c^x_t=\{ x_s : 0 \leq s \leq t \}
  $$
  with similar meanings for $c^u_t,c^y_t$ and $c^z_t$. Now the form of
  the Gaussian distribution shows that all the above four processes
  have the same joint distributions at any finite system of points. It
  follows that\pageoriginale the distributions induced on $[R^2]^{[o,t]}$ by the
  above processes are the same. Also $\chi (a,c^x_t)=f(a,x)$ where $f$
  is the function in the lemma and $x$ denotes the path. Thus we have 
  $$
  E(\chi(a,c^x_t))=E(\chi(a,c^y_t))=E(\chi(a,c^z_t))=E(\chi(a,c^u_t)).
  $$

  Hence
  $$
  E(|c^x_t|)=\int_{R^2}E (\chi(a,c^x_t))da=\int_{R^2}E (\chi(a,
  c^u_t))da=E(|c^u_t|). 
  $$

  We have
  \begin{align*}
    c^x_{2t}&= \{ x_s:0\leq s \leq 2t =c^x_t \cup [c^y_t+x_t]\\
    & \equiv [c^x_t - x_t] \cup c^y_t=c^z_t \cup c^y_t,
  \end{align*}
  where $\equiv$ denotes congruency under translation. Therefore
  $|c^x_{2t}|+|c^y_t \cap c^z_t|=|c^z_t|+|c^y_t|$, and 
  $$
  E(|c^x_{2t}|)+E(|c^y_t \cap c^z_t|)=E(|c^z_t|)+E(|c^y_t|)=2E(|c^x_t|).
  $$

  Also
  $$
  E(|c^x_t|)=E(|\sqrt{2}c^u_t|)=E(2|c^u_t|)=2E(|c^u_t|)=2E(|c^x_t|)
  $$

  Therefore
  $$
  E(|c^y_t \cap 0^z_t|)=0\text{\  i.e.\ } \quad \int \limits_{R^2}E
  (\chi(a,c^x_t)E \chi(a,c^y_t))da=0. 
  $$

  Since the process $y$ and $z$ are easily seen to be independent
  $$
  E (\chi(a,c^x_t)E \chi(a,c^y_t))=E (\chi(a,c^z_t))E
  (\chi(a,c^y_t))=[E (\chi(a,c^x_t))]^2. 
  $$

  Therefore\pageoriginale $\int [E (\chi(a,c^x_t))]^2 da=0$ giving $E 
  (\chi(a,c^x_t))=0$ for almost all $a$. Hence $\int E
  (\chi(a,c^x_t))da=0$ i.e $E(|c^x_t|)=0$. This proves the theorem. 
\end{pot*} 
