\chapter{Stochastic Differential Equations}% chap5.

\section{Introduction}\label{chap5-sec1}\pageoriginale 
% sec 1.

The standard Brownian motion is a one-dimensional diffusion whose
generator is $\dfrac{1}{2} \dfrac{d^2}{da^2}$. We shall here construct
a more general one-dimen\-sional diffusion whose generator $\mathscr{G}$
is the differential operator  
$$
D = \frac{1}{2}p^2 (a) \frac{d^2}{da^2} + r(a) \frac{d}{da};
$$
precisely if $u \in C_2 (R') = \{u: u, u', u''$ continuous and 
bounded$\}$ then $u \in \mathscr{D}(\mathscr{G})$ and $\mathscr{G} u =
Du$. To do this we consider the stochastic differential equation  
$$
dx_t = p(x_t) d \beta_t + r(x_t) dt,
$$
where $\beta_t$ is a Wiener process. The meaning of the above equation is 
$$
x_u - x_t = \int^u_t p(x_s) d \beta_s + \int^u_t r(x_s)ds, 0 \leq t <
u < \infty. 
$$

The meaning of $\int^u_t p(x_s) d \beta_s $ has to be made clear; we
do this in article 3. Note that it cannot be interpreted as a Stieltjs
integral for a fixed path because it can be shown that as a function
of $s, \beta_s$ is not of bounded variation for almost all paths. We
make the following formal considerations postponing the definition of
the integral to \S\ \ref{chap5-sec3}. 

Let\pageoriginale $x_t^{(a)}$ be a solution of the differential
equation with the 
initial condition $x^{(a)}_0 = a$, i.e. let $x_t^{(a)}$ be a solution
of the integral equation 
$$
x_t = a + \int^t_0 p(x_s) d \beta_s + \int^t_0 r(x_s) ds.
$$

Then, under certain regularity conditions on $p$ and $r$, we can
define a strong Markov process $\mathbb{M} = (S, W, P_a)$ with $S =
R', W = W_c (R')$,\break $P_a(B) = P(x^{(a)} \in B)$ and such that  
$$
\mathscr{G} u(a) = \frac{1}{2} P^2 (a) \frac{d^2u}{da^2} + r(a)
\frac{du}{da}, u \in C_2 (R') 
$$
where $\mathscr{G}$ is the generator in the restricted sense. The same
can be done in multi-dimensional case replacing $\beta , p, r$ by a
multi-dimensional Wiener process, a matrix valued function and a
vector valued function respectively. Componentwise we will have  
$$
dx^i_t = \sum_j p^i_j (x_t) d \beta^j_t + r^i (x_t) dt, i = 1, \ldots, n
$$
and the generator will be given by  
$$
\mathscr{G} u(a) = \frac{1}{2} \sum_{i, j} q^{ij}(a) \frac{\partial^2
  u (a)}{\partial a^i \partial a^j} + \sum_i r^i (a) \frac{\partial u
  (a)}{\partial a^i} 
$$ 
where $q^{ij} = \sum\limits_k p^i_k p^j_k$.

Taking local coordinates we can extend the above to the case in which
the state space $S$ is a manifold. 

Coming back to stochastic integrals we prove the following theorem
which show that $\int^u_t f(s, w) d \beta (s, w)$ cannot be
interpreted as a Stieltjes integral. 

\begin{theorem*}
  Let\pageoriginale $\Delta$ be the subdivision $t = s_0 < s_1 <
  \ldots < s_n = u$, 
  and $\delta (\Delta) = \max\limits_i(s_{i+1} - s_i)$. Then  
  \begin{enumerate}
  \item $r_2 (\Delta) = \sum\limits_i (\beta(s_{i+1}) - \beta(s_i))^2
    \to u - t\ L^2$-mean as $\delta (\Delta) -0$ 

  \item $r(\beta, t, u) = \sup\limits_{\Delta} \sum\limits_i| \beta
    (s_{i+1}) - \beta (s_i)| = \infty$ with probability $1$. 
  \end{enumerate}
\end{theorem*} 

\begin{proof}
\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item $E (r_2 (\Delta))  = \sum\limits_i E[\beta(S_{i+1}) - \beta(S_i)]^2 =
      \sum\limits_i(s_{i+1} - s_i) 
      = u - t$ \ and 
\begin{align*}
E(r_2 (\Delta)^2)& = \sum\limits_i E((\beta (s_{i+1)}) -\beta
(s_i))^4)\\
&\quad + 2 \sum\limits_{i < j} E((\beta(s_{i+1}))^2 (\beta(s_{j+1}) -
      \beta(s_j))^2)\\
      & = \sum\limits_i 3(s_{i+1} - s_i)^2 + 2 \sum\limits_{i < j}
      E((\beta (s_{i+1}) -\beta (s_i))^2)\\
&\qquad  E(( \beta (s_{j+1})- \beta
      (s_j))^2)\\ 
      & = \sum\limits_i 3(s_{i+1} - s_i)^2 + 2 \sum\limits_{i < j}
      (s_{i+1} - s_i) (s_{j+1} - s_j)\\ 
      & = 2 \sum_i (s_{i+1} - s_i)^2 +
      \sum_i (s_{i+1}-s_i)^2\\
&\qquad + \sum_{i < j} 2 (s_{i+1} - s_i) (s_{j+1}
      - s_j)\\ 
      &= 2 \sum_i (s_{i+1}-s_i)^2 + \left[\sum
        (s_{i+1}-s_i)\right]^2\\
 & = 2 \sum
      (s_{i+1} - s_i)^2 + (u-t)^2 
    \end{align*}
because $\beta(s_{i+1}) - \beta(s_i)$ and $\beta (s_{j+1}) -
    \beta(s_j)$ are independent for $i \neq j$ and $E (( \beta (t) -
    \beta (s))^4) = 3(t - s)^2$. We thus have  
    \begin{multline*}
      E((r_2 (\Delta ) - (u - t))^2) = E (r_2 (\Delta )^2) - (u -
      t)^2\\ 
      = 2 \sum_i (s_{i+1} - s_i)^2 \leq 2 \delta (\Delta) \sum_i (s_{i+1}
      - s_i) \to 0 \text{ as } \delta (\Delta ) \to 0. 
    \end{multline*}
 
\item From (1) we can find a sequence $\Delta^n = (t = s^{(n)}_0 <
    \ldots < s^{(n)}_{P_{n}} = u)$ such that $r_2 (\Delta^n ) \to u -
    t$ with probability $1$. We have  
    $$
    r(\beta, t, u) \geq \sum | \beta(s^{(n)}_{i+1}) -
    \beta(s^{(n)}_i)| \geq \frac{\sum |\beta(s^{(n)}_{i+1}) -
      \beta(s^{(n)}_i)|^2}{\max_i |\beta (s^{(n)}_{i+1}) - \beta
      (s^{(n)}_i)|} \to \infty 
    $$\pageoriginale
    since $\sum|\beta(s^{(n)}_{i+1}) - \beta(s^{(n)}_i)|^2 \to u-t$ and
    $\max_i\sum|\beta(s^{(n)}_{i+1}) - \beta(s^{(n)}_i)| \to 0$
    because of continuity of path functions. 
\end{enumerate}
\end{proof}

\section{Stochastic integral (1) Function spaces $\mathscr{E}$,
  $\mathscr{L}^2$, $\mathscr{E}_s$}\label{chap5-sec2} 

Let $T$ be a time interval $[u, v), 0 \leq u < v < \infty$ and
  $\beta_t$, $t \in T$ be a Wiener process i.e. (1) the sample
  functions are continous for almost all $w$, (2) $P(\beta_t - \beta_s
  \in E) = \int \limits_E \dfrac{1}{\sqrt{2 \pi (t - s)}} e^{-x^2/2
    (t-s)} dx$ and (3) $\beta_{t_{1}}, \beta_{t_{2}} - \beta_{t_{1}},
  \ldots , \beta_{t_n} - \beta_{t_{n-1}}$ are independent if $t_1<
  \ldots < t_n \in T$. Let $\mathbb{B}^t$, $t \in T$ be a monotone
  increasing system of Borel subalgebras of $\mathbb{B}$ such that
  $\mathbb{B}^t$ includes all null sets for each $t, \beta_t \in
  (\mathbb{B}^t)$ and $\beta_{t+h}- \beta_t$ is independent
  of $\mathbb{B}^t$ for $h>0$. We shall use the notation $f \in
  (\mathbb{B})$ to denote that $f$ is $\mathbb{B}$-measurable. 

Let $\mathscr{L}_s$ be the set of all functions $f$ such that (1) $f$
is measurable in $(t, w)$, (2) $f_t \in (\mathbb{B}^t)$ for almost all $t
\in T$ and (3) $\int_T f^2_t dt < \infty$ for almost all $w \in
\Omega$. Instead of $3) $ we also consider the two stronger conditions  
\begin{itemize}
\item[(3$'$)] $\int_{\Omega} \int_T f(t, w)^2 dt\ dp < \infty$

\item[(3$''$)] there exist a subdivision $u = t_0 < t_1 < \ldots < t_n = v$
and $M < \infty$ such that  
$$
f_t(w) = f_{t_{i}}(w),\quad t_i \leq t <  t_{i+1},\quad 0 \leq i \leq n-1
$$
and\pageoriginale $|f_t(w)| < M$.
\end{itemize}

We define the function spaces $\mathscr{L}^2$ and $\mathscr{E}$ by 
\begin{align*}
  \mathscr{L}^2 & = \{ f : 1), 2) \text{ and } 3') \text{ hold } \}\\
  \mathscr{E} & = \{ f : 1), 2) \text{ and } 3'') \text{ hold } \} .
\end{align*}

Clearly $\mathscr{E} \subset \mathscr{L}^2 \subset
\mathscr{L}_s$. $\mathscr{L}^2$ is a (real) Hilbert space with the
norm $|| f||^2 = \int_{\Omega} \int_{T}|f|^2 dt d \rho$ and
$\mathscr{L}_s$ is a (real) Fr\'echet space with the norm $|| f
||_{\mathscr{L}_s} = \int\limits_{\Omega} \dfrac{1}{1+ \sqrt{\int_T
    |f|^2 dt}}$. $\sqrt{\int_T |f|^2} dt$. ``$|| f|| \mathscr{L}_s \to
0$'' is equivalent to ``$\int_T |f|^2 dt \to 0$ in probability'' and
if $\fint \in \mathscr{L}^2$ then $|| f||_{\mathscr{L}_s} \leq ||
f||$. 

\setcounter{thm}{0}
\begin{thm}\label{chap5-sec2-thm1}%theorem1
\begin{enumerate}[1]
\item $\mathscr{E}$ is dense in $\mathscr{L}^2$ (with the norm $|| ~ ||$)

\item $\mathscr{E}$ is dense in $\mathscr{L}_s$ (with the norm $|| ~
    ||_{\mathscr{L}_s}$).  
\end{enumerate}
\end{thm}

\begin{proof}%pro1.
\begin{enumerate}
\item 
 We shall prove that, given $f \in \mathscr{L}^2$ there exists a
  sequence $f_n \in \mathscr{E}$ such that $|| f_n - f|| \to 0$. We
  can assume that $f$ is bounded. Put $f(t, w) = 0$ for $t \notin
  T$. Then $f$ is defined for all $t$ (this is to avoid changing $T$
  each time) and  
  
  $\int^{\infty}_{- \infty} f^2 dt\ dp < \infty$ so that
  $\int^{\infty}_{- \infty} f^2 dt < \infty$ so almost all $w$. 

Therefore 
$$
\int^{\infty}_{- \infty}|f(t+h) - f(t) |^2 dt \to 0 \text{ as } h \to 0.
$$

Also $\int^{\infty}_{- \infty} |f(t+h) - f(t) |^2 dt \leq 4
\int^{\infty}_{- \infty} f(t)^2 dt \in L' (\Omega)$. We get 
$$
\int \limits_{\Omega} \int^{\infty}_{- \infty} |f(t+h) -f (t) |^2
dt\ dp 
\to 0 \text{ as } h \to 0. 
$$

If\pageoriginale $\varphi_n (t) = \dfrac{[2^n t]}{2^n}, n \geq 1$ then 
$$
\int\limits_{\Omega} \int^{\infty}_{- \infty} | f (s+ \varphi_n (t))
- f(s+t)|^2 ds\ dp \to 0 \text{\ as\ } n \to \infty 
$$

Also 
$$
\int\limits_{\Omega} \int^{\infty}_{- \infty}|f(s + \varphi_n
(t)) - f(s+t)|^2 d\  sd\ P \leq 4 \int\limits_{\Omega}
\int^{\infty}_{- \infty} f(s)^2  ds\  dP.
$$ 

Since $T= [u, v) $ is a finite interval
$$
\int^v_{u-1} \int_{\Omega} \int^{\infty}_{- \infty} | f (s + \varphi_n
  (t)) - f(s+t) |^2\ ds\  dP\  dt \to 0 \text{ as } n \to
  \infty.
$$
i.e.,
$$
\int^{\infty}_{- \infty} ds \int^v_{u-1}
  \int\limits_{\Omega} |f(s+ \varphi_n (t)) - f(s+t)
  |^2\  ds\  dP\  dt \to 0 \text{ as } n \to \infty.
$$ 

Therefore there exists a subsequence $\{n_i \}$ such taht 

$\int^v_{u-1} \int_{\Omega} |f(s+ \varphi_{n_{i}} (t)) - f(s+t) |^2  dP\
 dt \to 0$ for almost all $s$. Choose $s \in [0, 1]$ and fix
it. Then  
$$ 
\int^v_{u-1}  |f(s+ \varphi_{n_{i}} (t)) - f(s+t) |^2  dP ~ dt
\to 0 \text{ as } n_i \to \infty. 
$$

Changing the variable
$$
\int\limits^{v+s}_{u-1+s} \int \limits_{\Omega} |f(s+ \varphi_{n_{i}} (t-s)) - f(t)
|^2  dP\ dt ~  \to ~ 0 \text{ as } n_i \to \infty 
$$
since $0 \leq s \leq 1$
$$
\int\limits^v_{u} \int\limits_{\Omega} |f(s+ \varphi_{n_i} (t-s)) 
- f(t) |^2\ dP\ dt \to 0.  
$$

Let $h_i(t) = f(s+ \varphi_{n_i}(t-s))$. Then $h_i \in \varepsilon$
and $|| h_i -f|| \to 0.$ 

\item Let\pageoriginale $ f \in \mathscr{L}_s$. We prove that there exista
  a sequence $f_n \in \mathscr{E} $ with $|| f_n - f || \mathscr{L}_s
  \to 0$. We can assume that $f$ is bounded so that $ f \in
  \mathscr{L}^2$. We can find $f_n \in \mathscr{E}$ such that $||f_n -
  f || \to 0$. But $|| f_n - f|| \mathscr{L}_s \leq \int_\Omega \sqrt{
    \int _T | f_n - f |^2 } dt ~ d ~ p \leq \sqrt{\int_\Omega
    \int\limits_T | f_n - f|^2 dt d p} = || f_n - f|| \to 0$.  
\end{enumerate}
\end{proof}

\begin{remark*}
 Let $f^M$ be the truncation of $f$ by $M$ i.e. 
  $$
  f^M = (f V - M ) \wedge M 
  $$
  and for a subdivision $\Delta = (u = t_0 < t_1 < \cdots< t_n = v
  )$ let $f_\Delta$ be the function$f_\Delta (t, w) = f (t_i ,
  w), t_i \leq t < t_{ i +1}, 0 \leq i \leq n - 1$. Then the
  approximating functions $f_n$ in the above theorem are of the form
  $f_n = f_{\Delta_n}^{M_n}$ for some $M_n, \Delta _n$.  
\end{remark*}

\section{Stochastic Integral (II) Definitions and
  properties}\label{chap5-sec3} % section 3  

Let $L^2 (\Omega)$ be the real $L^2$-space with the usual $L^2$ -
norm$|| ~~||$ and $S(\Omega)$ be the space of all measurable functions
with the norm $|| f ||_s = \int \dfrac{1}{ 1 + | f (w)|}$ $|f (w)| d P
(w)$. $S(\Omega)$ is a real Fr\'echet space and ``$|| f || _s \to 0$''
is equivalent to ``$f \to 0$ in probobility''. Clearly $L^2 (\Omega)
\subset S (\Omega)$ and if $f \in L^2 (\Omega), || f ||_s \leq || f ||$.   

We first define $I(f) = \int_T f d \beta $ for $f \in \mathscr{E}$,
show that it is continuous in the norms $|| ~ || , || ~ || _s$ and
hence that it is extendable to $\mathscr{L}^2$ and $\mathscr{L}_s$. 

We define for $f \in \mathscr{E}$ 
$$
I(f) = \int_T f_t d \beta_t = \sum^{ n - 1}_{ i = 0} f (t_i)(\beta
(t_{i + 1}) - \beta (t_i)) 
$$
where\pageoriginale $t_0 = u < t_1 < \ldots < t_n = v$ is any
subdivision by which 
$f$ is expressed. This definition is independent of the division
points 	with respcect to which $f$ is expressed and $I(f) \in L^2
(\Omega) \subset S (\Omega)$. That $I$ is linear is easy to see and  
{\fontsize{10pt}{12pt}\selectfont
$$
E(I (f)) = \sum_i E (f (t_i)) (\beta (t_{ i +1})) - \beta ( t_i))) =
\sum_i E(f(t_i)) E ( \beta ( t_{i+1})-\beta(t_{i})) = 0 
$$}\relax
since $f (t_i)$ and $\beta (t_{ i +1}) - \beta (t_i)$ are independent
and $E ( \beta( t )) = 0$.  

Now we prove the following 
\begin{itemize}
\item[(A)] $|| f || = || I (f) ||$ Though we use the same notation, note that 
  $$
  f \in \mathscr{L}^2 , I(f) \in L^2 (\Omega).
  $$

\item[(B)] $||I(f)||_s = 0 (|| f || \mathscr{L}_s ^{ 1/3})$. 
\end{itemize}

\noindent
\textbf{Proof of (A)}. Let $(f, g ) = E(\int\limits_T fg ~ dt)$. It is
enough to show that  
$$
(f, g) = (I(f), I(g)). 
$$

Let $f$, $g$ be expressed by the division points $(t_i)$. Then 
$$
(I (f), I(g)) = (\sum f_i X_i, \sum g_j X_j)
$$
with $f_i = f(t_i)$, $g_j = g(t_j)$, $X_i = \beta (t_{i+1} ) - \beta
(t_i)$. Note that $f_i \in (\mathbb{B}^{t_i})$ and $X_i$ is
independent of $ \mathbb{B}^{t_i}$. We have  
\begin{align*}
 ( I (f), I(g)) & = \sum_i E (f_i g_i ) E (X^2 _i) + \sum_{ i < j} E
  (f_i g_i X_i)E(X_j)\\ 
  & = E\sum_i E(f_i g_i) (t_{ i+1} - t_i) \\ 
  & = \left[ \sum f_i g_i (t_{ i +1} - t_i)\right] = E 
  \left(\int\limits_T fg dt\right) = (f, g). 
\end{align*}

\noindent
\textbf{Proof of (B).} Let\pageoriginale $f$ be expressed by the
division points $(t_i)$ and put $f_i = f(t_i)$, $X_i = \beta (t_{ i
  +1}) - \beta (t_i)$,
$\Delta_i = t_{i+1} - t_i$ and $\delta = || f ||_{\mathscr{L}_s}$. Then 
$$
P \left(\int\limits_T f^2 dt > \epsilon^2 \right) \leq \delta \frac{1 + \in}{\in}.
$$ 

Let 
$$
Y_i =
\begin{cases}
  1 \text{ if } \sum^ i _{ j = 0} f^2_j \Delta _j \leq \epsilon \\
  0 \text{ if } \sum^ i _{ j = 0} f^2_j \Delta _j > \epsilon.
\end{cases}
$$

Then $Y_i \in (\mathbb{B}^{t_i})$ and since $X_i$ is independent of
$\mathbb{B}^{t_i}$  
$$
E \left[ \left( \sum_{ i = 0}^{ n - 1}  Y_i f_i X_i \right)^2\right] = \sum^{ n
  -1}_{ i = 0} E (Y^2 _i f^2 _i ) \Delta _i = E \left[ \sum_{ i =
    0} ^{ n - 1} Y_i f_i^2 \Delta_i\right] 
$$
since from the definition of $Y_i$, $Y^2_i = Y_i$. Again From the
definition of $Y_i, \sum\limits^{ n - 1} _ { i = 0} Y_i f^2_i
\Delta _i \leq \epsilon^2$ so that $E(S^2) \leq \epsilon^2$, where $S =
\sum\limits_{ i = 0}^{ n - 1} Y_i f_i X_i$. Now $P(|S| > \eta ) \leq
\epsilon^2/\eta^2$. If $\int_T f^2 dt \equiv \sum_i
f^2_i \Delta_i \leq \epsilon^2$ then $Y_0 = Y_1 = \ldots = Y_{n-1} =
1$ so that $S = \sum f_i X_i = I(f)$. Therefore  
\begin{gather*}
  P(I(f)\neq S)\leq P \left(\int\limits_T f^2 dt > \epsilon^2\right) \leq \delta
  \frac{1 + \in}{\in}\\ 
  P (|I (f) | > \eta ) \leq \delta \frac{1 + \epsilon}{\epsilon} + \frac{\epsilon^2}{\eta^2}
\end{gather*}
and 
\begin{align*}
  || I (f) ||_s & = \int \frac{1}{1 + | I (f) |} | I (f) d P\\ 
  & = \int\limits_{ |I(f)|\leq \eta} \frac{1}{1 + | I (f) |} |I(f)|d P +
  \int\limits_{|I(f)|> \eta} \frac{1}{1 + | I(f) |} | I (f)| dP\\ 
  &\leq \eta + \delta \frac{1 + \epsilon}{\epsilon} + \frac{\epsilon^2}{\eta^2}. 
\end{align*}\pageoriginale

Putting $\epsilon = \delta ^{ 2 / 3}$, $\eta = \epsilon^{\frac{1}{2}}$, we get
$|| I(f) ||_s \leq 4 \delta ^{ 1 / 3}$.  

Using linearity of $I$ and the fact $|| I(f) || = || f ||$ for $f \in
\mathscr{E}$, we can extend $I$ to $\mathscr{L}^2 (|| ~ ||)$ [since
  $\mathscr{E}$ is dence in $\mathscr{L}^2(|| ~ ||)]$ such that $I$
is lienar. For $f \in \mathscr{L}^2$, $I(f) \in L^2 (\Omega)$ and $||
  f || = || I(f)||$, and $E (I(f)) = 0$.  

The linearity of $I$ and the fact $|| I(f) ||_s \leq 4 || f ||)_{
  \mathscr{L}_s}^{1/3}$ imply that we can extend $I$ to the closure of
$\mathscr{E}$ in $||~||_{\mathscr{L}_{s}}$ i.e.
to $\mathscr{L}_s$. Since for $f \in
\mathscr{L}^2$, $|| f ||_{\mathscr{L}_s} \leq || f||$ we see that this
extension coincides with the above for $f \in \mathscr{L}^2$. Further
for $f \in \mathscr{L}_s$ we have $|| I(f) ||_s \leq 4 || f
||_{\mathscr{L}_{s}^{ 1 /3}}$.   

Using the remark at the end of the previous article we can show that
for $f \in \mathscr{L}^2$  
$$
I(f) = \lim_{n \to \infty} \sum_i f^{M_n} (t^{(n)}_{i})
\left[\beta(t_{i+1}^{(n)}) - \beta (t_i^{(n)})\right] 
$$
for some $\Delta_n = (t^{(n)})_i$ and $M_n$. 

Finally if $f$, $g$, $\epsilon \mathscr{L}_s$ and if $f = g$ on a measurable
set $\Omega_1$ then \break $I(f) = I(g)$ a.e. $\Omega_1$.  

\section{Definition of stochastic integral (III) Continuous
  version}\label{chap5-sec4}% section 4 

Let $\mathbb{B}^t$, $0 \leq t < \infty$ be a monotone increasing
system of Borel subalgebras of $\mathbb{B}$ such that $\mathbb{B}^t$
includes all null sets for each $t$. Let $\beta _t , 0 \leq t <
\infty$ be a Wiener process such that $\beta _t \in (\beta^t )$ and
$\beta _{ t + h} - \beta _t$ is independent of $\mathbb{B}^t$ for $h >
0$.  

Let\pageoriginale $f_t = f_t (w) = f (t, w ), 0 \leq t < \infty$ be
such that  
\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item $f$ is measurable in the pair $(t, w )$. 

\item $f_t \in (\mathbb{B}^t)$ for almost all $t$.

\item $\int^v _u f^2_t ~ dt < \infty$ for almost all $w \in \Omega$
  for any finite interval $[ u, v] \subset [ 0, \infty)$. Consider
    also the following conditions besides $1$ and $2$.  
\begin{itemize}
\item[(3$'$)] $\int_\Omega \int^v _u f^2 _t ~ dt ~ dP < \infty $ for
    any finite interval $[ u, v ] \subset [ 0, \infty)$.  

\item[(3$''$)] There exist point $0 \leq t_{0} < t_1 < t_2 < \ldots \to
  \infty$ and constants $M_i$ independent of $w$ such that  
  $$	
  f_t (w) = f(t_i, w ), | f (t_i) | \leq M_i, t_i \leq t < t_{i +1}, i \geq 0. 
  $$	
\end{itemize}
\end{enumerate}  

In the same way as in $\int 2 $ we introduce theree function
classes $\mathscr{E}$, $\mathscr{L}^2$ and $\mathscr{L}$ as follows  
\begin{align*}
  \mathscr{E} & = \{ f : 1, 2, 3'' \text{ hold }\} \\
  \mathscr{L}^2 & = \{ f : 1, 2, 3 \text{ hold }\}\\
  \mathscr{L} & = \{ f : 1, 2, 3 \text{ hold }\} . 
\end{align*}

From \S\ 3 we can define $I(u, v) = \int_{v}^{u} f (t, w)d \beta (t,
w)$, for $f \in \mathscr{L}$ and for any bounded interval $[ u , v]
\subset [ 0, \infty )$.  

Now we shall show 
\setcounter{thm}{0}
\begin{thm}\label{chap5-sec4-thm1}% theorem 1
  $I (u, v)$ has a continuous version in $[ u, v]$ i.e., there
  exists $I(u, v)$ such that  
  $$
  P[ I(u, v ) = \int^v_u f d ~ \beta ] = 1 \text{ for any pair } (u, v )
  $$
  and $I(u, v)$ is continuous in the pair $(u, v )$ for almost all $w$;
  $I(u, v)$\pageoriginale is uniquely determined in the sense that if
  $I_i (u, v)\, i = 1, 2$ satisfy the above conditions, then  
  $$
  P\left[ I_1 (u, v ) = I_2 (u, v ) ] \text{ for all } u, v \right] = 1. 
  $$
\end{thm}

\begin{proof} % proof
  It is enough to show that $I(t, f) = \int^t_0 f d \beta$ has a
  continuous $(in\ t)$ verson $I^* (t, f)$ in $0 \leq t \leq v$ for
  any given $v > 0$, because $I (u, v) = I (0, v) - I(0, u)$. If $f
  \in \mathscr{E} $ then $I(t)$ itself is such a version and  
  \begin{equation*}
    P[\sup_{ 0 \leq t \leq v} | I (t, f) | > \epsilon ] \leq
    \frac{1}{\epsilon 2}|| f ||^2 \tag{1} 
  \end{equation*}
where $|| f^2 || = \int^v_0 \int_\Omega f^2 dt\  dP$. 
\end{proof}

To prove (1) let the restriction of $f$ to $[ 0, v)$ be expressed by
  the division set $\Delta = (0 = t_0 < t_1 < \ldots < t_n = v)$
  and $s_0$, $s_1,\ldots$ be a dense set in $[0, v)$ such that $t_i =
    s_i ,0\leq i \leq n$. Let now $\tau_{1},\ldots,\tau_{m}(m\geq n)$
    be a rearrangement of $a_{0}$, $s_{1},\ldots,s_{m}$ in order of
    magnitudes. Then  
    $$
    I(\tau_i, f) = \sum_{ j < 1} f (\tau _ j) (\beta (\tau_{ j +1}) -
    \beta (\tau_j)) 
    $$

Using arguments similar to those empolyed in the proof of
Kolomogoroff's inequality we can prove the following  

\begin{lemma*}% Lemma
If $x_1,\ldots,x_n,y_1,\ldots,y_n$ are random variables satisfying 
  \begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item $y_i$ is independent of $(x_1,\ldots, x_i,y_1,\ldots,y_{ i - 1})$ 

\item $E(y_i) = 0$ and $E(x^2_i)$, $E(y^2_i) < \infty$
\end{enumerate}
then 
$$
P \left(\max_{ 1 \leq k \leq n} | \sum^k_{ i = 1} x_i y_i | \geq \epsilon
\right) \leq \frac{1}{\epsilon^2}) \sum E(x^2_i) E(y^2_i).  
$$\pageoriginale
\end{lemma*}

Thus we have 
$$
\displaylines{\hfill 
    P \left(\max_{ 0 \leq i \leq n} | I(\tau_i, f) | > \epsilon \right)\leq
    \frac{1}{\epsilon^2} 
    \int_\Omega \int^v_0 f^2 dt ~ dP  \hfill \cr
    \text{i.e.,}\hfill   
    P\left(\max _{ 0 \leq i \leq n} | I(s_i, f ) | > \epsilon \right) \leq
    \frac{1}{\epsilon^2} || f||^2\hfill }
  $$
  
Letting $n \to \infty $ we have (1). 

Let $C_s$ denote the space of all functions $(h(t, w), 0 \leq t \leq
v, w \in \Omega)$ which are continuous in [$0, v$] and introduce the
norm $|| ~ || c_s$ by  
$$
|| h ||_{ c_s } = E \left(\frac{1}{ 1 + \sup_{ 0 \leq t \leq v} | h (t,
  w)|} \sup_{0 \leq t \leq v} | h (t, w)|\right) 
$$

We shall prove that for $f\in \mathscr{E}$
$$
\displaylines{\hfill 
|| I(f)||_{c_s} = O (|| f || _{\mathscr{L}_s^{ 1/3} }) \hfill(2) \cr
  \text{where}\hfill  
  || f || _{\mathscr{L}_s} = E \left[ \frac{1}{1 + \sqrt{
      \int^v_0 |f|^2 dt}} \sqrt{\int\limits^t_0 f^2
    dt}\right]\hfill } 
$$

Define $Y_i = 1$ if $\sum\limits_{ j = 0}^{ i - 1} f^2_j (t_{ j +1} -
t_j)\leq \epsilon^2 $ and $Y_i = 0$ if $\sum\limits^{ i - 1}_{ j = 0} f^2
_j (t_{ j +1} - t_j) > \epsilon^2$ and let $g(t) = Y_i f (t) = Y_i f(t_i)$
for $t_i \leq t < t_{ i +1}$. Then $g(t) \in \mathbb{B}^t$ and $||
g||^2 = \int\limits_\Omega \int^t_0 g^2 dt ~ dP \leq \epsilon^2$
and\pageoriginale   
$$
P(I (t, f))\neq I(t, g) \text{ for some } t \in [ 0, v) =
  P \left(\int\limits^v _0 f^2 dt > \epsilon^2\right) < \delta \frac{1 + \epsilon}{\epsilon} 
$$
where $\delta = || f ||_{\mathscr{L}s}$. Thus 
$$
P\left(\sup_{0 \leq t \leq v} |I(t,f)| > \eta\right) \leq p \left(\sup_{0 \leq t \leq v}|I
(t, g) | > \eta \right) + \delta \frac{1+\epsilon}{\epsilon} \leq \frac{\epsilon^2}{\eta^2}
+ \delta \frac{ 1 + \epsilon}{\epsilon} 
$$
from (1). Therefore 
$$
|| I(. , f)|| _{c_s}\leq \eta + \delta \frac{1+\epsilon}{\epsilon} +
\frac{\epsilon^2}{\eta^2}.  
$$

Putting $\epsilon = \delta^{2/3}$ and $\eta = \epsilon^{\frac{1}{2}}$ we
get  
$$
|| I(. , f) || _{c_s} \leq \epsilon^{\frac{1}{2} } [2 + \epsilon +
  \epsilon^{\frac{1}{2}} \leq 4 \epsilon^{\frac{1}{2}} = 0
  (\epsilon^{\frac{1}{2}}) = 0 
  (\delta^{1/3}) = 0(||f||^{1/3}_{\mathscr{L}_s}). 
$$

Since $C_s$ is complete in the norm $|| ~ ||_{c_s}$ we can extend the mapping 
$$ 
\mathscr{E} \ni f \to I(. , f) \in C_s
$$ 
to the closure of $\mathscr{E}$ with respect to $|| ~
||_{\mathscr{L}_s}$ i.e. to $\mathcal{L}_s$. This extension gives the
continuous version of $I(t, f) , 0 \leq t \leq v$. Since $(2)$ is also
true for this extension, we have  

\begin{thm}\label{chap5-sec4-thm2}% theorem 2
  If 
  $$
  \int^v _0 | f_n - f|^2 dt \to 0 \text{ in probability }
  $$
  then\ $\sup\limits_{0 \leq t \leq v}| I(t, f_n) - I(t, f) | \to 0$
  in probability.  
\end{thm}

For any Borel set $E \in [ u, v )$ we define 
$$
\int_E f_\theta d \beta _\theta = \int^v_0 f \chi _E d \beta_\theta. 
$$

For\pageoriginale $f \in \mathscr{L}^2$ we have seen that 
$$
|| I(t, f) || = || f ||. 
$$

Let $f \in \mathscr{L}_s$ and consider the truncation $f^M$. Since
$\int^v_u | f^M - f |^2 \chi_E\break ds \to 0$ we see that $\sup\limits_{ u
  \leq t \leq v} | I(t, f \chi_E) - I (t, f^M \chi_E) | \to 0$
in probability.  

Since $\chi_E f^M \in \mathscr{L}^2 $ we have, if $E$ has Lebesgue
measure zero  
$$
|| I (t, f ^M \chi _E) || =  \int^t_0 E(f^M) \chi_E ~ ds = 0. 
$$

Thus $\int\limits_E f_\theta d \beta _\theta = 0 $ if the Lebesgue
measure of $E$ is zero.  

\begin{remark*}% Remark
Henceforth when we speak of the stochastic integral we shall always
understand it to mean the continuous version.  
\end{remark*}

If $f \in \mathscr{L}^2$ we have 

\begin{thm}\label{chap5-sec4-thm3}% theorem 3
  If $f \in \mathscr{L}^2$ and $\epsilon > 0$, 
  $$
  \displaylines{\hfill 
  P[|| | I(t, f) ||| > \epsilon ]\leq \frac{1}{\epsilon^2} || f||^2\hfill \cr
  \text{where}\hfill ||| I(t, f) ||| = \sup\limits_{ 0 \leq t \leq v}
  || I(t, f ||). \hfill }
  $$
\end{thm}

\begin{proof}% proof
Let $f_n \in \mathscr{E}$ be such that $|| f_n - f || \to 0$. Then
  for any $\delta > 0$.  
  \begin{equation*}
    P[ ||| I(t, f_n - f) ||| > \delta ] \to 0 \tag{1}
  \end{equation*}

For $g\in\mathscr{E}$ we have proved that 
\begin{equation*}
 P [ ||| I(t, g) ||| > \epsilon] \leq \frac{1}{\epsilon^2}|| g||^2 \tag{2}
\end{equation*}

Therefore\pageoriginale if $\eta > 0$, 
\begin{multline*}
    P[ ||| I(t, f) ||| > \epsilon + \eta]\leq P [ ||| I (t, f_n - f) ||| +
      ||| I (t, f_n)||| > \epsilon + \eta ] \\ 
    \leq [ ||| I(t, f_n- f)|||] + P[ ||| I (t, f_n) |||>\epsilon]\\ 
      \leq 
     P [||| I (t, f_n - f) ||| > \eta]+ \frac{1}{\epsilon^2}|| f_n ||^2,  
  \end{multline*}
  from (2). From $(1)$ if $n \to \infty , P [ ||| I (t, f) ||| > \epsilon +
    \eta] \leq \dfrac{1}{\epsilon^2} || f ||^2$. Letting $\eta \to \infty$ we
  get the result.  
\end{proof}

\section{Stochstic differentials}\label{chap5-sec5}% section5

Let $\beta^t$, $\beta_t$ be defines as before. If $x_t = x_0 + \int^t
_0 ~ f_s d ~ \beta _s + \int^t_0 ~ g_s ds$, where
$x_0(w)\in\mathbb{B}^0$ and   
\begin{enumerate}
\item $f$, $g$ are measurable in the pair $(t, w)$ 

\item $f_s$, $g_s \in (\mathbb{B}^s)$ for almost all $s$, $0 \leq s <
  \infty$  

\item $\int^t _0 f^2 _s ds < \infty, \int^t_0 | g_s | ds < \infty$ for
  almost all $w$, for any finite $t$ then we write  
  $$
  dx_t = f_t d \beta _t + g_t dt. 
  $$
\end{enumerate}

If $dx_t = f_t d \beta _t + g_t dt, dx^i_t = f^i_t d \beta_t + g_t^i
dt, f_t = \sum_i \varphi ^i_t f^i_t$, $g_t = \sum\limits_i \varphi^i_t
g^i_t + \psi (t)$ then we shall write  
$$
dx_t = \sum_i \varphi^i_t dx^i_t + \psi _t dt. 
$$

\begin{theorem*}
  If $F (\xi^1, \ldots , \xi ^k, t)$ is $C^2$ in $(\xi^1, \ldots , \xi
  ^k, )$ and $C^1$ in $t$, if $dx^i_t = f^i_t d_t + g^i _t dt $ and if
  $y_t = F(x^1_t , \ldots , x^k_t , t)$, then   
  $$
  \displaylines{\hfill 
  dy_{t} = \sum_i F_i dx^i_t + \left[y_{2} \sum^k_{ i, j = 1} F_{ij} f^i_t
    ~  f^j_t + F_{ k +1}\right] dt\hfill \cr  
  \text{where}\hfill F_i = \dfrac{\partial F}{\partial \xi _i} ,
  F_{ij} = \dfrac{\partial ^2 F}{ \partial \xi^1 \partial \xi ^j}, F_{
    k +1} = \dfrac{\partial F}{\partial t}. \hfill }
  $$\pageoriginale
\end{theorem*}

\begin{remark*}
We can get the result formally as follows: 
\begin{enumerate}
\item Expand $dy_t$ i.e. $dy_t = dF (x^1_t , \ldots , x^k_t , t) =
    \sum\limits_i F_i dx^i _t + F_{ k+1} dt + \dfrac{1}{2}
    \sum\limits_{ i, j = 1}^k f _{ij} dx^i_t dx^j_t + \cdots $
 
  \item Put $dx^i _t = f^i _t d \beta_t + g^i_t dt$. 

  \item Use $d \beta _t \simeq \sqrt{ dt}$

  \item Ignore $0 (dt)$. 
  \end{enumerate}
\end{remark*}

\begin{lem}\label{chap5-sec5-lem1}% lemma
  If $f$, $g \in \mathscr{L}_s$ (as defined in \S\ \ref{chap5-sec2}) then 
  $$
  \left[\int\limits^u_t f_s d \beta _s\right] \left[\int\limits^u_t \epsilon_s d
    \beta_s\right] = \int\limits^{u}_{t} f_s G_s d \beta_s +
  \int\limits^u_t g_s F_s d \beta_s + \int\limits^u_t f_s g_s ds,
$$
where
$$
F_s = \int\limits_{t}^{s} f_\theta d \beta_\theta , G_s
  \int\limits_{t}^{s} g_\theta d \beta_\theta.
$$
\end{lem}

\begin{proof}
\setcounter{case}{0}
\begin{case}% case 1
 $f$, $g \in  \mathscr{E}$ (as defined in \S\ \ref{chap5-sec2}). 
\end{case}
  
We can express $f$ and $g$ by the same set of division points
  $\Delta = ( t = t_0^{(n)}<\ldots < t^{(n)}_n = u $). Now let
  $\Delta_n = (t = t_0^{(n)}) < t_1^{(n)} < \cdots < t_n^{(n)} = u
  ) (n \geq m)$ be a sequence of sets of division points containing
  $\Delta$ such that $\delta (\Delta _n) = \max\limits_{ 0 \leq
    i \leq n - 1} |t^{(n)}_{i +1} - t^{(n)}_i |\to 0$. Put $ X_i^{(n)} =
  f (t^{(n)}_i) , Y^{(n)}_i = g(t_i^{(n)}), B_i^{(n)} = \beta
  (t_{i+1}^{(n)}) - \beta(t_i^{n})$. We have   
\begin{multline*}
 \left[ \int \limits^u _t f _s d \beta _s \right] \left[
      \int\limits^u _t g_s d \beta _s \right] = \left[ \sum^{ n - 1}_{
        i =0} X_i ^{(n)} B^{(n)}\right] \left[ \sum^{ n -1}_{ j = 0}
      Y^{(n)}_J B^{(n)}_J\right] \\ 
    = \sum^{ n - 1}_{ i = 1} X_i^{(n)} G (t_i ^{(n)}) B_i ^{(n)} +
    \sum_{ i = 1}^{ n - 1} Y_i^{(n)} F(t_i ^{(n)}) B_i^{(n)}+ \sum^{ n
      - 1}_{ i = 0} X_i ^{(n)} Y_i ^{(n)} (B_i ^{(n)})^{2}.  
\end{multline*}

Put\pageoriginale $\varphi _n (s) = t_i ^{(n)}$ for $f^{(n)}_i \leq s < t_{ i +
    1}^{(n)}$ and let $G_n, F_n$ be defined as  
  $$
  G_n (S, W) = G(\varphi_n (S), w), F_n (s, w) = F(\varphi_n (s), w). 
  $$

  Then $G_n$, $F_n \in \mathscr{E}$ and since the set $\Delta_n$
  contains $\Delta_{m}$, $f G_n$, $g F_n \in \mathscr{E}$. Thus  
  $$
  \int\limits^u_t f_s d \beta _s \int\limits^u_t g_s d \beta _s =
  \int\limits^u_t f (s) G_n (s) d \beta_s + \int\limits^u_t g_s F_n d
  \beta_s + \sum^{ n - 1}_{ i = 0} X_i ^{(n)} Y^{(n)}_i (B^{(n)}_i)^2.  
  $$

  Now $\int\limits^u_t | f (s) G (\varphi_n (s))- f (s) G(s)|^2 ds \leq
  \max\limits_{t \leq s \leq u}| G_n (s) - G(s) | \int\limits^u_t|
  f(s)|^2$ $ds \to 0$ with probabulity $1$ since $G(s)$ is continuous in
  $s$. Similarly  
  
  $\int^u_t | g(s) F_n (s) - g(s) F (s) |^2ds \to 0$ with probability
  $1$. Further  
\begin{align*}
  & E \left[ \left( \sum^{ n - 1}_{ i = 0} X_i^{(n)} Y_i ^{(n)} \left[ (B_i
        ^{(n)})^2 - t^{(n)}_{i +1} - t_i^{(n)}\right] \right)^2 \right] \\ 
    & = \sum_{i = o}^{n - 1} E(( X_i ^{(n)})^2 (Y_i^{(n)})^2 \left[ \left(\left[
        (B_i ^{(n)})^2 - (t^{(n)}_{i +1} - t^{(n)}_i )^2
        \right]\right)^2 \right]\\
    & + 2 \sum_{ i < j} E \left\{ X_i^{ (n)} Y_i^{(n)} X_j^{(n)} Y_j^{(n)}
    \left[ \left(B_i^{(n)}\right) ^2 - \left(t_{ n+1}^{(n)}\right)
      \right] \left[ \left(B_j^{(n)}\right)^2 
      - \left(t^{(n)}_{j+1}-t^{(n)}_{j}\right)\right] \right\} \\ 
    & = \sum^{ n - 1}_{ i = 0}E (( X_i ^{(n)} Y_i ^{(n)} ) ^ 2 E \left[
      \left((B_i ^{(n)})^2 - (t_{i +1} - t_i^{(n)})\right)^2 \right],\\
    & \hspace{4cm}\text{ since } E (( B_j ^{(n)})^2 ) = t_{ j
      +1}^{(n)} - t^{(n)}_{ j} \\ 
    & = 2 \sum^{ n - 1}_{ i = 0} E(( X_i ^{(n)} Y_i ^{(n)}) ( t^{ (n)}_{
      i +1}-t^{(n)}_{i} )^2\leq 2 \delta(A_n) E \left[ \sum^{ n - 1}_{ i = 0} (X_i ^{(n)}
      Y_i ^{(n)}) ^2 (t^{(n)}_{ i +1} - t_i ^{(n)})\right] \\ 
    & = 2\delta (\Delta _n) E \left(\int \limits^{u} _{t} f^2 (s) g^2 (s)
    ds\right), \\
    & \hspace{3cm} \text{ since } \sum^{ n -1 }_{ i =0}
    \left(X_i^{(n)}Y_i^{(n)}\right)^2 
    \left(t_{ i +1}^{(n)} - t_{ i}^{(n)}\right) = \int\limits^u_t f^2 (s) g^2 (s)
    ds.  
  \end{align*}\pageoriginale
  
The lemma for $f$, $g \in \mathscr{E}$, then follows
Theorem \ref{chap5-sec4-thm2} of \S\ \ref{chap5-sec4}.  
  \begin{case}% case 2
    Let $f, g \in \mathscr{L}_s$. There exist sequences $f_n$, $g_n \in
    \mathscr{E}$ such that $\int\limits^{ u}_t |f_n - f|^2 ds$ and
    $\int\limits^u_t | g_n - g|^2 ds \to 0$ in probability.  
  \end{case}
  
  Therefore $\sup\limits_{ t \leq s \leq u_s} | F_n - F|$ and
  $\sup\limits_{ t \leq s \leq u_s} |G_n - G| \to 0$ in probability
  where $F(s) = \int\limits_t f(\theta) d \beta _\theta , G (s) =
  \int\limits^s_{t} g (\theta) d \beta _\theta, F_n (s) =
  \int\limits^{s}_t f_n (\theta) d \beta _\theta , g_n (s) =
  \int_{t}^{s} g_n (\theta) d \beta_\theta$. Choosing a subsequene if
  necessary we can assume that the above limits are true almost every
  where. Then for any $w$  
\begin{multline*}
\int\limits^{u}_t | f_n G_n - fG | ds \leq 2 \int\limits^u_t |
    f_n - f|^2 G^2_n ds + 2 \int\limits^u_t f^2|G_{n}-G|^{2}ds\\ 
    \leq 2 \sup_{ t \leq s \leq u} G^2 _n (s) \int^u_t | f_n - f| ^2 ds
    + 2 \sup_{ t \leq s \leq u} | G_n - G |^2 \int\limits^u_t f^2 ds \to
    0.  
  \end{multline*}
  
The proof of the lemma can be completed easily. 
\end{proof}

Proceeding on the same lines and noting that 
$\sum_i f (t^{(n)}_i ) g(t_i^{(n)}) (\beta(t^{(n)}_{i +1})\break
-\beta(t_i^{(n)})) (t^{(n)}_{i+1}-t_i^{(n)}) \to 0$,  for $f, g \in
\mathscr{E}$, as\pageoriginale $n \to \infty$ we can prove 

\begin{lem}\label{chap5-sec5-lem2}% lemma 2
  $(\int ^u_t f_s d \beta _s) (\int^u_t g_s ds ) = \int^u_t f_s G_s d
  \beta _s + \int ^u_t g_s F_s ds$ where $F_s = \int^s_t f_\theta d
  \beta _\theta , G_s = \int^s_t g_\theta d_\theta$.  
\end{lem}

\begin{pot*}
 Write $F(x^1_t , \ldots , x^k_t , t ) = F(x_t) $. Let $\Delta^n =
  (0 = t_0^{(n)} < t_1^{(n)} < t^{(n)}_{1}<\ldots < t^{(n)}_n = t)$ be a
  sequence of sub divisions such that $\delta (\Delta _n) \to
  0$. Then  
\begin{align*}
    y_t &= y_0 + \sum^{ n - 1}_{ l = 0 } \sum^k_{ i = 1} F_1 \left(
    x(t_l^{(n)})\right) \left(x^i (t_{ l +1}^{(n)} ) - x ^i (t_l^{(n)})\right)\\ 
    & \hspace{2cm} + \sum_{
      l = 0}^{ n - 1} F_{ K +1} \left(x(t_l^{(n)})\right) \left(t^{(n)}_{l + 1} -
    t^{(n)}_l\right) \\ 
    &+ \frac{1}{2} \sum_{ l = 0}^{ n - 1} \sum^{k}_{i, j = 1} F_{ij}
    \left(x(t_l^{(n)})\right) \left(x^i(t_{l+1}^{(n)})  -
    x^i (t_l^{(n)}) \right) \left(x_j (t_{l+1}^{(n)})-x^j
    (t_l^{(n)})\right)\\  
    &+ \frac{1}{2} \sum_{l=0}^{n-1} \sum_{i, j=1}^{k} \epsilon^{(n)} _{
      ijl} \left(x^i (t_{l +1}^{(n)})-x^{i}(t_l^{(n)})\right) \left( x^i
    (t_{l+1}^{(n)}) - x^j (t_{l}^n)\right)\\ 
    & = y_0 + \sum^{k}_{i = 1} I^1_{in} + I^2_{n} + \frac{1}{2}
    \sum_{ i, j = 1}^k I^3_{ijn} + \frac{1}{2} \sum^{k}_{ i, j =1 }
    I^4_{ijn}, \text{\ say}.  
  \end{align*}

From the hypotheses on $F$ and the continuity of $x^j (t)$, 
$$  
\epsilon^{(n)}_{ijl} \to 0\text{\ uniformly in \ } i, j,
l\text{\ as\ } n \to \infty.
$$  
  
Let $\varphi_n(t) = t_l^{(n)}$ for $t^{(n)}_{l} \leq t <
t_{l+1}^{(n)}$. Then we have   
\begin{align*}
I^1_{in} & = \sum^{n-1}_{l=0} F_i (x(t_l^{(n)})) \left[
      \int\limits^{t_{ l +1}}_{t_l^{(n)}} f^i_s d \beta_s +
      \int\limits^{t_{l +1}} _{ t_l^{(n)}}g^{i}_{s}ds\right]\\ 
    & = \sum^{n-1}_{l=0} \left[ \int\limits^{t_{ l +1} }_{t_l^
        {(n)}} F_i (x( \varphi_n (s))) f^i_s d \beta_s +
      \int\limits^{t_{ l +1} }_{t_l^{(n)}} F_i (x( \varphi_n (s)))
      g^i_s ds\right] \\ 
    & = \int\limits^t_0 F_i (x( \varphi _n (s))) f^i _s d \beta _s +
    \int^t_0 F_{i}(x( \varphi_n (s))) g_s^i ds.  
\end{align*}\pageoriginale
  
Also 
\begin{multline*}
    \int^t_0 | F_i ( x ( \varphi _n (s))) - F_i (x(s)) |^2 (f^i_s)^2 ds\\
    \leq \max_{ 0 \leq s \leq t} | F_i ( x ( \varphi_n (s))) - F_i (x(s)
    )|^{2} \times \int^t _0 (f^i _s)^2 ds \to 0 
  \end{multline*}
  for every $w$. Thus 
  $$
  \sum^k_{ i = 1} I^1_{in} \to \sum_{i=1}^k \left[ \int ^t_0 F_i
    (x(s)) f^i _s d \beta _s + \int^t _0 F_i (x(s)) g^i _s ds \right] 
  $$
  in probability. Similarly 
  $$
  I^2_n = \int^t_0 F_{K + 1 }(x(\varphi_n (s))) ds \to \int\limits^t_0
  F_{ k +1} (x(s) )ds.  
  $$

Using Lemma \ref{chap5-sec5-lem1} and \ref{chap5-sec5-lem2} we have 
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
 (x^i (v) - x^i (u))&  ( x^j (v) - x^j (u))\\
    & = \int^v _u \left[f^i_s
      (x^j (s) - x^j(u)) + f^j _s ( x^i (s) - x^i (u) \right] d
    \beta_s\\  
    &\qquad + \int\limits^v _u f^i_s f^j_s ds + \int\limits^v_u g^{i}_{s}
    \left(\int\limits^s_u f^j_\theta d\theta \right)ds\\
&\qquad +
    \int\limits^v_u g^j_s
    \left(\int\limits^{s}_{u}f^{i}_{\theta}d\theta\right)
     +\left(\int\limits^{v}_{u}g^{i}_{s}ds\right)\left(\int\limits^{v}_{u}g^{j}_{s}ds\right)\\ 
    & = \int\limits^v_u \left[f^i_s (x^j (s) - x^j (u)) + f^j_s (x^{i}(s)
      - x^i (u))\right] d \beta_s + \int\limits^v_u f^i_s f^j_s
    ds\\ 
    & + \int\limits^v_u \left[g^i_s (Y^j_s - Y^j_u) + g^j_s
      (Y^i_s - Y^i_u) \right] ds\\ 
     \text{since\ } & \left( \int\limits^v_u g^i_s ds \right) 
\left(\int\limits^{v}_{u}g^{j}_{s}ds\right)
 =   \int\limits^v_u g_s^i \left(\int\limits^s_u g^j_\theta d\theta\right)
      + \int\limits^v_u g_s^j \left(\int\limits^s_u g^i_{\theta}
      d\theta\right) ds \\ 
      \text{where } \qquad & Y^i_s  = \int\limits^s_0 
\left[f^i_\theta + g^i_\theta \right] d\theta, Y^{j}_{s} = \int\limits^s_0 \left[ f^j_\theta + g^j_\theta \right] d \theta.  
  \end{align*}}\relax\pageoriginale

Thus 
{\fontsize{10pt}{12pt}\selectfont
\begin{multline*}
I^3_{ijn} = \int\limits^t_0 F_{ij}(x(\varphi_n (s))) \left[
      f^i_s (x^j (s) - x^j (s))) + f^j_s (x^i(s)-x^{i} (\varphi_n (s)))
      \right]d\beta_{s} \\
    + \int\limits^t_0 F_{ij} (x(\varphi_n(s))) f^i_s
    f^j_s ds + \int\limits^t_0 F_{ij} (x(\varphi_n(s)))\\ 
    \left[ g^i _s (
      Y^j (s) - Y^j ( \varphi _n (s))) + g^j_s (Y^i (s) - Y^i (
      \varphi _n (s)))\right] ds \to \int\limits^t_0 F_{ij} (x(s)) f^i_s
    f^j_s ds
  \end{multline*}}\relax
in probability because othet terms can, without
  difficulty, be shown, to tend to zero in probability. Again  
\begin{multline*}
 | I^4_{ijn}| \leq \max_{0 \leq l\leq n - 1} | \epsilon^{(n)}_{ijl}|
    \sum^{n-1}_{l=0} |x^i(t_{l+1}^{(n)})-x^{i}(t^{(n)}_{l}) || x^j
    (t_{l + 1}^{(n)}) - x^j (t_l^{(n)})|\\ 
    \leq \frac{1}{2} \max_{0 \leq l \leq n - 1} | \epsilon^{(n)}_{ijl}|
    \sum^{n-1}_{l=0} \left[\left(x^i(t_{l+1}^{(n)}) - x^i
      (t^{(n)}_{l})\right)^2 + \left(x^j
      (t^{(n)}_{l+1})-x^{j}(t^{(n)}_{l})\right)^2 \right]  
  \end{multline*}

In the same ways as above we can show that 
$$
\sum^{ n - 1}_{ l = 0} \left(x^i (t^{(n)}_{ l +1}) - x (t_l ^{(n)})\right)^2
\to \int\limits^t_0 f^i_s f^i_s ds.  
$$
  
  Thus $|I^4 _{ ijn}| \to 0 $ in probability. We have proved the theorem 
\end{pot*} 

\section{Stochastic differential equations}\label{chap5-sec6}% section

The notation in this article is as in the previous ones. 
\setcounter{thm}{0}
\begin{thm}\label{chap5-sec6-thm1}% theorem  1
Let $p(\xi)$, $r (\xi)$, $\xi\in R'$ satisfiying Lipschitz condition 
$$
| p (\xi) - p (\eta)| \leq A | \xi - \eta |, |r(\xi) - r (\eta) |
  \leq A | \xi - \eta |.  
$$\pageoriginale
  
Then 
$$
dx_t = p(x_t) d \beta_t + r(x_t) dt, x_0 (w) = \alpha (w) \in \mathbb{B}^0 
$$
has one and olny one solution. 
$$
[ | \alpha (w) | < \infty \text{ for almost all } w ]
$$
\end{thm}

\begin{proof} %proof
\begin{itemize}
\item[(a)] {\em Existence}. We show that 
$$
x_t (w) = \propto (w) + \int\limits^t _0 p (x_s) d \beta _s
+\int\limits^t _0 r (x_s) ds  
$$
has a solution. We use successive approximation to get a
solution. Let $\propto ^M (w)$ be the truncation of $\propto$ at $M$
(i.e., $( \propto V - M ) \wedge M$) and put  
$$
x^0 (t, w ) \equiv \propto ^M (w). 
$$
  
Define by induction on $k$
\begin{align*}  
x^{k +1} (t, w) &= \alpha^M (w) + \int\limits^t_0 p (x^k_s) d
  \beta_s + \int\limits^t_0 r(x^k_s) ds\\
& = \alpha^M + y^k (t) +
  z^k (t), \text{\ say.} 
\end{align*}

Note that if $f \in \mathscr{L}^2$ then $I (t, f ) \in
  \mathscr{L}^2$ and 
$$
E (| I (t, f)|^2) = \int\limits^t_0 E (| f  (s)|^2) ds,
$$ 
where $I (t, f) = \int\limits^t _0 f_s d \beta
  _s$. From the hypotheses on $p$ and $r$,\break $x^k (t, w) \in
  \mathscr{L}^2$ for all $k$. Now  
  \begin{align*}
&    E (| x^{ k +1} (t) - x^k (t))|^2 )\\
& \leq 2 E \left[ | y^k (t) -y^{k
        - 1} (t)|^2 + 2E \left[ |z^k (t)-z^{k-1} (t)|^2\right] \right] \\ 
&    \leq 2 \int_0^t E(|p (x^k (s)) - p(x^{k-1}(s))|^2) ds\\
&\qquad + 2t
    E\left(\int_0^t |r (x^k (s) - r (x^{k-1}(s))|^2 ds )\right)\\ 
&    \left[\ \text{ since } | z^k (t) - z^{ k -1} (t) |^2 \leq t
      \int\limits^t _0 | r (x^k (s) ) -r (x ^{ k -1} (s)) |^2 ds
      \right] \\ 
&    \leq 2A^2 (1 + t)\int\limits^t _0 E (| x^k (s) - x^{ k - 1} (s)
    |^2 ) ds\\
& \leq 2A^2 (1 + v) \int\limits^t _0 E (| x^{k - 1}_s -
    x^{k-1}_x | ^2 ) ds  
  \end{align*}\pageoriginale
where $0 \leq t \leq v < \infty$ and $v$ is fixed for the
present. Therefore $E(| x^{k+1} (t) - x^k (t)|^2 ) \leq [ 2A^2 (1
 + v)]^k \int\limits^t_0 ds_1 \int\limits^{s_{1}}_{ 0} ds_2
  \ldots \int\limits^{s_{ k - 1}}_{ 0}E(|
  x^1 (s_k) - x^0 (x_k) |^2 ) ds_k$ 
  \begin{multline*}
     \leq [ 2 A^2 (1 + v)]^k \int\limits^t_0 ds_1 \ldots
    \int\limits^{s_{k-1}}_0 2 E (p^2 (\alpha^M) s_k + r^2
    (\alpha^M) s^2_k) ds_k\\ 
    = [2A^2 (1 + v)]^k 2 \left[ E ( p^2 (\alpha^M))
      \frac{t^{k+1}}{(k+1)!}+ 2E (r^2 (\alpha^M))\frac{t^{k +2}}{(k +
        2)!}\right] 
  \end{multline*}
  which gives 
  \begin{multline*}
    \int\limits^v_0 E (| x^{k+1} (\theta) - x^k(\theta) |^2 )d \theta \\
    \leq 2
    \left[2A^2 (1 + v)\right]^k E (p^2 (\alpha^M)) \frac{v^{k +2}}{(k + 2)!} +
      2 E (r^2(\alpha^M)) \frac{v^{k+3}}{(k+3)!}] 
  \end{multline*}

Let $||| F (t, w) ||| =\sup\limits_{ 0 \leq t \leq v } | F (t, w)$. Then 
\begin{align*}
P & \left[ ||| x ^{ k + 1} (t) - x^k (t) |||> \in \right]\\ 
  & \leq P \left[
      ||| y^k (t) - y^{ k - 1} (t) ||| > \frac{\in}{2} + P\left[ ||| z^k
        (t) - z ^{ k - 1} (t) ||| > \frac{\in}{2}\right]  \right] \\ 
    & \leq \frac{4}{\epsilon^2} \int\limits^v _0 E \left[ | (p (x^k (s) ) -
      p(x^{k-1}(s))|^2 \right] ds\\ 
    & \hspace{2cm}+ \frac{4}{\epsilon^2} v \int\limits^v_0 E
    \left[ | r (x^k (s)) -r ( x^{k -1}(s) ) ^2 \right]ds. 
\end{align*}
(from Theorem \ref{chap5-sec4-thm3} of \S\ \ref{chap5-sec4})
\begin{align*}
& \leq \frac{4A^2 (1 + v)}{\epsilon^2}2\\ 
& \qquad \left[ 2A^2 (1 + v)\right]^{ k - 1}
    \left[ E (p^2 (\alpha ^M) ) \frac{ v^{k+1}}{(k+1)!} + 2E (r^2
      (\alpha^M )) \frac{v^{k+2}}{(K + 2)!} \right]\\ 
    & < \frac{B}{\epsilon^2} \frac{[2A^2 v (1 + v)]^k}{k!} \text{\ where\ }
    B= 2 \left[ E ( P^2 (\alpha^M ) ) v + 2 v^2 E (r^2 (\alpha^M
      ))\right] . 
\end{align*}
  
Putting\pageoriginale $\epsilon_k = \dfrac{[ 2 A^2 v (1 + v)]^{ k /
    3}}{(k! ) ^{ 1/3}}$ we get  
$$
P \left[||| x^{ k +1} (t) - x^k (t) ||| > \in _k \right]\leq B \epsilon_k . 
$$
\end{itemize}
\end{proof}

Since $\sum \epsilon_k$ is a convergent series Borel-Cantelli lemma
implies that, with probability $1$, $w$ belongs only to a finite
number of sets in the bracket of the last inequality. Therefore  

$P [ ||| x^{ k +1} (t) - x^k (t) ||| < \in _k$ for all $k \geq$ some $l] = 1$. 

Since $\sum \epsilon_k < \infty$, we get 

$||| x^m (t) - x^n (t) ||| \to o$ with probability $1$ as $m, n \to \infty$
i.e., $P [ x^k (t)$ converges uniformly for $0 \leq t \leq v] = 1$. 

Taking $v = 1, 2, 3, \ldots$ we get 

$P[x^k (t)$ converges unifolmly for $0 \leq t \leq n$] for every
$n$] = 1. Let $x^M (t, w )$ be the limit of $x^k (t, w) $. This is
    clearly continuous in $t$ for almost all $w$. Also for any $v <
    \infty$,  
$$
P \left[ ||| x^k (t) - x^M (t) ||| \to 0 \text{ as } k \to \infty \right] = 1. 
$$
so that $\int ^t _0 p (x^k (s)) d \beta _s \to \int^t _0 p (x^M)(s)) d
\beta_s $ in probability. Now we prove without difficulty that  
$$
X^M (t, w) = \alpha ^M (w) + \int^t_0 p (x^M (s, w )) d \beta (s, w )
+ \int\limits^t _0 r (x ^M (s, w ) )ds.  
$$

Let\pageoriginale $\Omega_M = (w : |\alpha (w) | \leq M)$ and define
$x (t, w) = x^M 
(t, w)$ on $\Omega_M$. If $M< M'$ then on $\Omega_M$, $\alpha^M =
\alpha^{M'}$ so that from the construction [and the fact that if $f =g$
  on a measurable set $B$ then $I (t, f) =I (t, g)$ a.e. on $B$] it
follows that $x^M (t, w) = x^{M'} (t, w)$. Also since on $\Omega_M, x
(t, w) = x^M (t, w), x (t, w)$ is a solution.  

\textbf{(b) ~ Uniqueness}. Let
\begin{align*}
  x_t & = a + \int^t_0 p (x_s) d \beta_s + \int^t_0 r (x_s) ds \qquad
  0 \leq t \leq v, a \in (\mathbb{B}^0). \\ 
  y_t & = a + \int^t_0 p (y_s) d \beta_s  + \int^t_0 r (y_s) ds
\end{align*}

\setcounter{case}{0}
\begin{case}%case 1.
 $E (x^2_t)$ and $E(y^2_t)$ are bounded by some $G < \infty$ for $0
  \leq t \leq v$. We have 
  \begin{multline*}
    E((x(t) -y(t))^2) \leq 2E \left(\left[ \int^t_0 (p(x(s)) -p (y(s))) d
      \beta (s)\right]^2 \right)\\ 
    + 2E \left(\left[ \int^t_0 (r(x(s)) -r (y(s)))
      ds\right]^2 \right) \\ 
    \leq 2 \int^t_0 E (\left[(p(x(s)) -p (y(s)))^2\right] ds + 2 t
    \int^t_0 E \left[(r(x(s))-r (y(s)))^2\right]ds  
\end{multline*}
since\quad $\left[\int^t_0 \varphi (s) ds \right]^2 \leq t \int^t_0
  \varphi^2 (s) ds$. Thus 
  \begin{multline*}
    E \left[ (x_t -y_t)^2\right] \leq 2A^2 (1 + t) \int^t_0 E \left[
      (x_s - y_s)^2\right]ds\\ 
    \leq 2A^2 (1+v) \int^t_0 E \left[ (x_s - y_s)^2\right] ds 
\end{multline*}
put \quad $C_t = E ((x_t - y_t)^2) \left[ \leq 4 G^2 \right]$. Then
$$
C_t \leq 2A^2 (1+v) \int^t_o c_s ds \leq \left[ 2A^2 (1+v)\right]^2
\int^t_0 ds \int^s_0 c_\theta d_\theta \leq \cdots 
$$
\end{case}

Therefore
$$
C_t \leq \frac{[2A^2 (1+v)]^n}{n!} t^n 4 G^2 \rightarrow 0\text{\ as\ } n
\rightarrow \infty. 
$$\pageoriginale

\begin{case}%case 2.
Let $x_{tM}= (x_t \Lambda M) \forall (-M), y_{tM} = (y_t \Lambda M)
  \forall (-M) $ and $a_M=(a \forall -M)\Lambda M$. Define $x^0, x^1,
  \ldots , y^0, y^1, \ldots$, inductively as follows  
\begin{align*}
 x^0_t &= x_{tM}, x^{n+1}_t = a_M + \int^t_o p(x^n_s) d \beta_s +
    \int^t_o r (x^n_s)ds\\ 
y^0_t &= y_{tM}, y^{n+1}_t = a_M + \int^t_o p(y^n_s) d \beta_s +
    \int^t_o r(y^n_s) ds. 
  \end{align*} 
\end{case}

Arguments similar to those used in the proof of existence of a
solution prove that 
$$
\tilde{x}_t = \lim\limits_{n-\infty} x^n_t,\quad \tilde{y}_t =
\lim\limits_{n-\infty} y^n_t 
$$
exist and  
\begin{align*}
\tilde{x}_t &= a_M +\int^t_0 p(\tilde{x}_s)d \beta_s + \int ^t_0
  r(\tilde{x}_s) ds\\ 
  \widetilde{y}_t(t) & = a_M + \int^t_0 p (\tilde{y}_s)d 
\beta_s + \int^t_0 r (\tilde{y}_s)ds
\end{align*} 
and 
$$
\sup\limits_{0 \leq t \leq v} E(\tilde{x}^2_t) < \infty,\quad
\sup\limits_{0 \leq t \leq v} E(\tilde{y}^2_t) < \infty. 
$$

Therefore from Case 1, $\tilde{x}_t = \tilde{y}_t$ for $0 \leq t
\leq v$. 

Let $\Omega_M = (w : |a | < M, \sup\limits_{0 \leq t \leq v}
\mid x_t \mid < M, \sup\limits_{0 \leq t \leq v} \mid y_t \mid <
M)$. Then since $x_t$ and $y_t$ have continuous paths $p[U_M \Omega_M]
= 1$. But on $\Omega_M$, 
\begin{equation*}
  \left.
  \begin{aligned}
    y_t = y_{tM} = y^0_t = y^1_t = \cdots \\
    x_t = x_{tM} = x^0_t = x^1_t = \cdots
  \end{aligned}
  \right\}
  \quad 0 \leq t \leq v
\end{equation*}\pageoriginale

Note that if $f, g \in \mathcal{L}_s$ and $f = g$ on a measurable
subset $B$ then  
$I (t, f ) =  I (t, g)$ on $B$ with probability 1.

Thus $x_t = y_t$ on $\Omega_M$. We have proved the theorem.
\begin{coro*}
Let $\alpha (w) \in L^2 (\Omega)$ and $x(t, w)$ satisfy
$$
x(t,w) = \alpha (w) + \int^t_o p (x(s, w)d \beta (s, w) + \int^t_o
r(x(s, w))ds. 
$$
\end{coro*}

Then
$$
E (x^2_t) \leq \beta e^{\mu t}\quad \text{for}\quad 0 \leq t \leq v
$$
where $\beta = 3E(\alpha^2) + 6v p^2 (0) + 6v^2 r^2 (0)$ and $\mu = 6A^2 (1+v)$.

\begin{proof}
From the proof of Theorem \ref{chap5-sec6-thm1} we gather that $x(t, w) \in
\mathcal{L}^2$ (for any $v < \infty)$. If $\mid x(t)||^2 = E (\mid
x(t)|^2)$, 
$$
||x(t) -x (s)||^2  \leq 2 \int^t_S E(p(x(\theta))^2)d\theta + 2 (t-s)
\int^t_s E (r(x(\theta))^2)d\theta 
$$ 
so that $|| x(t)||^2$ is continuous in $t$. Let $l= \sup\limits_{0
\leq t \leq v} || x_t||^2$. 

Now
\begin{align*}
||x_t||^2 &\leq 3E (\alpha^2 ) +3 \int^t_o E (p(x(s))^2) ds + 3t
\int^t_o E (r(x(s))^2)ds\\ 
&\leq 3E (\alpha^2 ) +6 [tp^2 (0) + t^2 r^2(0)] + 6A^2 (1+t)
\int^t_o ||x(s_1)||^2 ds_1.\\ 
&\leq \beta + \mu \int^t_o || x (s_1) ||^2 ds_1 \leq \beta + \mu
\int^t_o ds_1 [\beta + \mu \int^{s_1}_o|| x(s_2)||^2 ds_2] \\
&=\beta (1+ \mu t) + \mu^2 \int^t_o ds_1\int ^{S_1}_o ||x (s_2)||^2
ds_2 \leq \beta (1 + \mu t)\\ 
& \hspace{2cm} + \mu^2 \int^t_o ds_1 \int^{S_1}_o
ds_2 \left[\beta +\mu \int^{S_2}_0 ||x(s_3) ||^2 ds_3\right]\\ 
&=\beta \left[1+ \mu t + \frac{\mu^2 t^2}{2!}\right] + \mu^3 \int^t_0 ds_1
\int^{S_1}_0 ds_2 \int^{S_2}_0 || x(s_3) ||^2 ds_3 
\end{align*}\pageoriginale

Continuting this, for any $n$ we have
\begin{multline*}
  ||x(t)||^2 \leq \beta \left[1+ \mu t + \frac{\mu^2 t^2}{2!} + \cdots +
    \frac{\mu^n t^n}{n!}\right] + \mu^{n+1} \\
  \int^t_0 ds_1 \int^{S_1}_0  ds_2
  \dots \int^{S_n}_0 ||x(s_{n+1})||^2 ds_{n+1}\\ 
  \leq \beta e^{\mu t} + \mu^{n+1} \int^t_o ds_1 \ldots \int^{S_n}_0
  ||x(s_{n+1})||^2 ds_{n+1} \leq \beta e^{\mu t} + \mu^{n+1} l
  \frac{t^{n+1}}{(n+1)!} 
\end{multline*}
Q.E.D.
\end{proof}

\begin{thm}\label{chap5-sec6-thm2}%them 2.
There exists a function $x(t, a, w)$ measurable in the pair $(a, w)$
such that for every fixed $a \in R^1$, 
$$
x(t, a, w) = a + \int^t_0 p(x(s, a, w))d \beta (s, w) + \int^t_0
r(x(s, a, w)) ds 
$$
for every $t $ and for almost all $w$. That is there exists a
version of the solutin of  
$$
dx_t = p (x_t) d \beta_t + r(x_t)dt, x(0) = a
$$
which is measurable in the pair $(a,w)$.
\end{thm}

\begin{proof}
Let $x^0 (t, a, w) \equiv a. x^0 (t,a,w)$ is measurable in the pair $(a, w)$. 
Assume\pageoriginale that $x^1, \ldots , x^k$ have been defined, are
measurable in 
the pair $(a, w)$ and for every fixed a 
\begin{align*}
x^i (t, a, k) &= a + \int^t_0 p(x^{i - 1}(s, a, w))d \beta (s, w)\\
&\qquad +
\int^t_0 r(x^{i-1}(s, a, w))ds, 1 \leq i \leq k. 
\end{align*}
for almost all $w$ and for all $t$. We shall define $x^{k+1}$. Let
$\Delta^n = (0=s^n_o < s^n_1 < \ldots)$ be a sequence of
subdivisions of [$0, \infty$) such that $\delta_n = \sup\limits_i |
  s^n_{i+1} - s^n_i |$ tends to zero. Let $v < \infty$. Then since
  $x^k (s, a, w)$ is continuous in $s$ for almost all $w$, 
$$
\int^v_0 | p(x^k (s, a, w))- p(x^k(\varphi_n (s), a, w))|^2 ds \rightarrow 0
$$
for almost all $w$, where $\varphi_n (t) = s^n_i$ for $s^n_i \leq t <
s^n_{i+1}$. Hence  

$\sup\limits_{0 \leq t \leq v} \big| \int^t_0 [p(x^k(s, a, w)) -
  p(x^k(\varphi_n (s), a, w))]d \beta_s \rightarrow 0]$ in
probability. By the diagonal process we can find a subsequence $n_j$
suct that 
$p[\sup\limits_{0 \leq t \leq v} | \int^t_0 p(x^k(s, a, w))d \beta_s -
  \int^t_0 p(x^k(\varphi_{nj} (s), a, w)) d \beta_s \big| \rightarrow
  0$ for every $v< \infty ] = 1$ 

Since $p(x^k(\varphi_{nj} (s), a, w)) \in \mathscr{E} \int^t_0
p(x^k(\varphi_{nj} (s), a, w)$ is measurable in $(a, w)$. It follows
that $M(t, a, w) = \overline{\lim} \int^t_0 p(x^k(\varphi_{nj}(s), a,
w))d \beta_s$ is measurable in $(a, w)$. Now define 
$$
x^{k+1}(t, a, w ) =  a + M(t, a, w) + \int^t_0 r(x^k(s, a, w))ds.
$$\pageoriginale

Proceeding as in Theorem \ref{chap5-sec6-thm1} we can show that $x^k (t, a, w)$
converges with probability 1. Let now  
$$
x(t, a, w) = \overline{\lim}x^k (t, a, w)
$$
We can show that $x(t, a, w)$ is the required function.
\end{proof}

\begin{remark*}
 We can easily prove that if $a_n \rightarrow a $ then $x(t, a_n, w)
 \rightarrow x(t, a, w)$ in probability. In fact  
\begin{multline*}
E(|x(t, a, w) - x (t, a_n, w) |^2) \leq 3| a-a_n |^2 + 3 \int^t_0
E(|p(x(s, a_n, w)\\ 
- p(x(s, a, w))|^2)ds + 3t \int^t_0 E(|r(x(s, a_n,
w)) - r(x(s, a, w))|^2)ds 
\end{multline*}
so that
\begin{multline*}
\varlimsup_{a_n \rightarrow a} ~ E \left[l x(t, a, w) -x (t,
 a_n, w)|^2\right] \leq 3A^2 (1 + t)\\ 
\varlimsup_{a_n
\rightarrow a} \int^t_0 E (|x (s, a_n, w) - x(s, a, w)|^2)ds 
\end{multline*}
and now using the corollary to Theorem \ref{chap5-sec6-thm1} and Fatou
lemma we get  
\begin{multline*}
\varlimsup_{a_n \rightarrow a} E[l x(t, a, w) - x (t,
 a_n, w)|^2]\\ 
\leq 3A^2 (1+t)  \int^t_o \varlimsup_{a_n
\rightarrow a} E (|n (s, a_n, w) - x(s, a, w)|^2)ds 
\end{multline*}
Q.E.D.
\end{remark*}

\begin{thm}\label{chap5-sec6-thm3}%them 3.
  Let $x(t, a, w)$ be as in Theorem \ref{chap5-sec6-thm2} and $x(t,
  w)$ be the solution of  
  $$
  dx_t = p(x_t) d \beta_t + r(x_t) dt, x(o, w) \equiv \alpha (w),
  \alpha (w) \in (\mathbb{B}^0) 
  $$

Then\pageoriginale
$$
p[x(t, \alpha (w), w) = x(t, w)] = 1
$$
\end{thm}

\begin{proof}
We shall prove that
{\fontsize{10pt}{12pt}\selectfont
$$
x(t, \alpha (w),w) = \alpha (w) + \int^t_0 p(x(s, \alpha (w), w)) d
\beta (s, w) + \int^t_o r(x(s, \alpha (w), w))ds 
$$}\relax
with probability 1; then by uniqueness part of Theorem
\ref{chap5-sec6-thm1} the result will follow. 
\begin{enumerate}
\item Since $x(t, a, w)$ is measurable in $(a, w), x(t, \alpha (w),
  w)$ is measurable in $w$. In fact, $x(t, \alpha (w), w)$ is the
  composite of  
  $$
  w \rightarrow (\alpha (w), w) \text{ and }(a, w) \rightarrow x(t, a, w).
  $$

\item Consider the function-space valued random variable
  $$
  \beta (w) = \beta (. , w) - \beta (o, w).
  $$
\end{enumerate}

This induces a measure on $\mathbb{C}$, the space of all continuous
functions on [$0, \infty$) and with respect to this measure the set of
  coordinate functions is a Wiener process i.e., if for $\tilde{w} \in
  \mathbb{C}$ 
  $$
  \tilde{\beta} (t, \tilde{w}) = \tilde{w}(t)
  $$
  then $\tilde{\beta}(t, w)$ is a Wiener process on $\mathbb{C}$. Let
  $\tilde{\mathbb{B}}^t$ correspond to $\mathbb{B}^t$. There exists a unique
  solution of the equation 
  $$
  \displaylines{\hfill 
    d \tilde{x}_t = p (\tilde{x}_t) d \tilde{\beta}(t) + r
    (\tilde{x}_t) dt, \tilde{x}_o = a. \hfill \cr 
    \text{i.e.,}\hfill  \tilde{x}(t, a, w) = a + \int^t_0 p(\tilde{x}(s, a,
    \tilde{w})) d \tilde{\beta} (s, \tilde{w}) + \int^t_0
    r(\tilde{x}(s, a, \tilde{w}))ds\hfill } 
  $$
  for\pageoriginale almost all $w$. Hence we have by uniqueness
  $$
  x(t, a, w) = \tilde{x}(t, a, \beta (w))\quad\text{a.e.}
  $$

  Let \qquad $L(a, w) = \tilde{x}(t, a, \tilde{w})$ and 
  $$
  R(a, \tilde{w}) = a + \int^t_0 p(\tilde{x}(s, a, \tilde{w})) d
  \tilde{\beta} (s, \tilde{w}) + \int^t_0 r(\tilde{x}(s, a,
  \tilde{w})) ds 
  $$

  If $\alpha (w) \in (\mathbb{B}^0)$ then $\alpha (w)$ and $\beta (w)$
  are independent. Hence the measure induced by $(\alpha, \beta)$ on
  $R^1\times  \mathbb{C}$ is the product $P_\alpha \times P_\beta$
  where $P_\alpha$ is the distribution of $\alpha$ and $P_\beta$ is
  the probability induced on $\mathbb{C}$ by $\beta$. Hence we have 
  \begin{align*}
    P[w : x (t, \alpha (w), w) & = \alpha (w) + \int^t_0 p(x(s, \alpha
      (w), w) d \beta (s, w)\\ 
      & \hspace{3cm} + \int^t_0 r(x(s, \alpha (w), w)ds\\
      &= (P_\alpha \times P_\beta ) [(a, \tilde{w}) : L(a, \tilde{w})
        = R(a, \tilde{w})]\\ 
      &= \int_R P_\beta [(a, \tilde{w}) : L (a, \tilde{w}) = R (a,
        \tilde{w})] P_\alpha (da)\\ 
      & = \int_R 1. P_\alpha (da) = 1. 
  \end{align*}

This proves the theorem.
\end{proof}


\section{Construction of diffusion}\label{chap5-sec7}%sec 7.

In this article we shall answer the question of \S\ \ref{chap5-sec1}
i.e. we shall 
prove that if $\mathfrak{p}$ and $r$ satisfy Lipschitz condition then
there exists a diffusion with state space $R'$ such that if $u$, $u'$,
$u''$ are continuous, $u$ and $\dfrac{1}{2}P^2 u'' + ru'$ are bounded
and $\mathscr{G}$ is the generator in the restricted sense, then 
$$
\mathscr{G} u = \frac{1}{2} P^2 u'' + ru'.
$$

We have proved in \S \ref{chap5-sec6} that
$$
x(t) = a + \int^t_0 p(x(s)) d \beta (s) + \int^t_0 r(x(s)) ds
$$\pageoriginale
has a unique solution $x(t, a, w)$. Let $S = R',  W = W_c (R^1)$ and 
$$
P_a (B) = P(w:x(.,a,w) \in B), B \in \mathbb{B} (W).
$$

Then $\mathbb{M} = (S, W, P_a)$ is a diffusion.

We shall first prove that $\mathbb{M}$ is a Markov process. We verify
the Markov property of $P_a$. 

Let $\beta^-_t (w)$ denote the stopped path at $t$ of $\beta (. , w)$
i.e. $\beta(t \Lambda.,w)$ and let $\beta'(w) = \beta (t + . ,
w) - \beta (t, w)$, $\beta''(s, w) = \beta (t + s, w)$,
${\mathbb{B}''}^{\theta} = \mathbb{B}^{t + \theta}$. 
Then $\beta''(s, w)$ is also
a Wiener process on $\Omega$. Let $x(t, a, w)$, $y(t, a, w)$ denote
solutions with respect to these processes of  
$$
dz_t = p(z_t) d \beta_t + r (z_t)dt
$$
i.e.
\begin{align*}
  x(t, a, w) &= a + \int^t_0 p(x(s, a, w)) d \beta (s, w) + \int^t_0
  r(x(s, a, w)) ds\\ 
  y(t, b, w) &= b + \int^t_0 p(y(s, b, w)) d \beta{''} (s, w) +
  \int^t_0  r (y(s, b, w))ds 
\end{align*}

If $\beta (w) = \beta (. , w) - \beta (0, w)$ then $\beta (w)$ and
$\beta{'}(w)$ induce the same probability on $\mathbb{C}$. Hence (see
the proof of Theorem \ref{chap5-sec6-thm3} of \S\ \ref{chap5-sec6}) 
$$
x(t, a, w) = \tilde{x}(t, a, \beta (w)), y(t, a, w) = \tilde{x} (t, a,
\beta{'} (w)). 
$$

Consider the $\mathbb{C}$-valued random variable $\beta^-_t
(w)$. This induces a probability on $\mathbb{C}$ and with respect to
this the process 
$$
\tilde{\beta}(s,\tilde{w}) =\tilde{w}(s), 0 \leq s \leq t
$$\pageoriginale
is a Wiener process on  $\mathbb{C}$ and there exists a unique solution for
$$
d\tilde{x}_s = p (\tilde{x}_s)d \tilde{\beta}_s + r(\tilde{x}_s)ds,
\tilde{x}_0 = a, 0 \leq s \leq t 
$$
i.e. there exists $f(s, a, \tilde{w})$ such that
$$
f(s, a, \tilde{w}) = a +  \int^s_0 p(f(\theta, a, \tilde{w})) d
\tilde{\beta} (\theta, \tilde{w}) +  \int^s_0 r(f (\theta , a,
\tilde{w}))d \theta, 0 \leq s \leq t. 
$$

Then we have 
\begin{multline*}
f(s, a, \beta^-_t (w)) = a +  \int^s_0 p(f(\; , a, \beta^-_t(w)))d \beta
  (\theta , w)\\ 
  +\int^s_0 r (f(\theta , a, \beta^-_t (w)))d \theta, 0
  \leq s \leq t. 
\end{multline*}

Therefore the stopped path at $t$ of $x( . , a, w)$ is 
\begin{equation*}
  F(s, a, \beta^-_t (w)) = 
  \begin{cases}
    f(s, a, \beta^-_t (w)),\ \ 0 \leq s \leq t\\
    f(t, a, \beta^-_t (w)),\ \  s > t.
  \end{cases}
\end{equation*}

Now
\begin{align*}
  x& (t + s, a, w) \\
  & = x(t, a, w) +  \int^s_0 p (x( + t, a, w)) d \beta
  (\theta + t, w)+  \int^s_o r (x(\theta + t, a, w)) d \theta\\
  & =x(t, a, w) +  \int^s_0 p(x (\theta + t, a, w)) d \beta{''}
  (\theta, w)+  \int^s_0 r(x(\theta + t, a,w)) d \theta. 
\end{align*}

From Theorem \ref{chap5-sec6-thm3} and uniqueness part of Theorem of
\ref{chap5-sec6-thm1} of \S\ \ref{chap5-sec6} we have therefore 
\begin{align*}
  x(t + s, a, w) &= y(s, x (t, a, w), w) = \tilde{x}(s, x(t, a, w),
  \beta{'} (w))\\ 
  &= \tilde{x}(s, E (t, a, \beta^-_t (w)), \beta{'} (w))
\end{align*}

Let\pageoriginale $B_1 \in \mathbb{B}_t (w)$ and $B_2 \in
\mathbb{B}(w)$. Then by definition of $P_a$ 
\begin{multline*}
  P_a [W \in B_1, W^+_t \in B_2]= P[x( . , a, w) \in B_1, x(t + . ,
    a, w) \in B_2] \\ 
  = P[F( . ,a, \beta^-_t (w)) \in B'_1, x( . , F(t, a, \beta^-_t
    (w)), \beta{'}(w)) \in B_2] 
\end{multline*}
where $B_1= (w: w^-_t \in B{'}_1)$. Let $P_{\beta^-_t}$ and
$P_\beta{'}$ be the probabilities induced on $\mathscr{C}$ by
$\beta^-_t$ and $\beta'$; since they are independent they induce the product
probability $P_{\beta_{t}^-} \times P_{\beta{'}}$ on
$\mathscr{C}\times  \mathscr{C}$.  We have therefore, 
\begin{align*}
  P_a & (w \in B_1, w^+_t \in B_2)\\ 
  & = (P_{\beta^-_t} \times P_{\beta
    {'}}) [(\tilde{w}, \tilde{w'}) : F ( . , a, \tilde{w}) \in B'_1,
    \tilde{x}( . , F(t, a, \tilde{w}), \tilde{w}') \in B_2]\\ 
  &= \int P_{\beta^-_t}(d \tilde{w}) P [w' : F ( . , a. \tilde{w}) \in
    B_1, \tilde{x}( . , F(t, a, \tilde{w}), \tilde{w'}) \in B_2]\\ 
  &= \int P_{\beta^-_t}(d w) P [w' : F ( . , a. \tilde{w}) \in B_1,
    \tilde{x}( . ,F(t, a, \tilde{w}), \beta{'} (w')) \in B_2]\\ 
  =& \int P(d w) P [w' : F ( . , a, \beta^-_t (w)) \in B'_1,
    \tilde{x}( . , F(t, a, \beta^-_t (w)),\beta' (w')) \in  B_2]\\ 
  &=\int\limits_{(\omega : F ( . , a, \beta^-_t (w)) \in B'_1)} P[w' :
    \tilde{x} ( . , F(t, a, \beta^-_t (w)), \beta' (w')) \in B_2]
  P(dw)\\ 
  &=\int\limits_{(\omega : F ( . , a, \beta^-_t (w)) \in B'_1)} P[w' :
    \tilde{x} ( . , x(t, a, w), \beta (w')) \in B_2] P(dw) 
\end{align*}
since $\beta$ and $\beta '$ induce the same probability on
$\mathscr{C}$. Thus by definition of $P_b$ we have 
\begin{align*}
  P_a[w : w \in B_1, w^+_t \in B_2] & = \int\limits_{(w : F ( . , a,
    \beta^-_t (w) \in B'_1)} P_{x(t, a, w)}[B_2] P(dw) \\ 
  & = E_a [B_1 : P_{x_t (w)} (B_2)]
\end{align*}

We have derived the Markov property.

From\pageoriginale the remark at the end of Theorem \ref{chap5-sec6-thm2} of
\S\ \ref{chap5-sec6} we see that if 
$a_n \rightarrow a$ there exists a subsequence $a_{nk}$ such that 
$$
x(t, a_{nk}, w) \rightarrow x(t, a, w)\quad\text{a.e.}
$$

Since $H_t f(a) = E_a [f(x_t (w))] = \int\limits_{\Omega} f (x(t, a,
w)) P(dw)$ if $f$ is continuous and $a_n \rightarrow a$ then there
exists a subsequnce $a_{nk}$ such that $H_t f(a_{nk}) \rightarrow H_t
f(a)$. Since this is true of every sequence $a_n \rightarrow a$,we
should have 
$$
\lim\limits_{b \rightarrow a} H_t f(b) = H_t f(a).
$$
$\mathbb{M}$ is therefore a strong Markov process. The definition of
$P_a$ shows that $\mathbb{M}$ is conservative. 

\setcounter{thm}{0}
\begin{thm}\label{chap5-sec7-thm1}%them 1.
If $u$, $u'$, $u''$ are all continuous and if $u$ and $\dfrac{1}{2}p^{2}u'' +
  ru'$ are bounded, then $u \in \mathscr{D} (\mathscr{G})
  (\mathscr{G}$ in the restricted sense) and  
  $$
  \mathscr{G} u = \frac{1}{2} P^2 u'' + ru'.
  $$
\end{thm}

\begin{proof}
  It is enough to prove that $\alpha G_\alpha u - u = G_\alpha
  [\dfrac{1}{2} P^2 u'' + ru']$. From the theorem of
  \S\ \ref{chap5-sec5} we have   
  \begin{multline*}
  u(x(t, a, w)) = u (x(0, a, w)) + \int^t_0 u' (x(s, a, w)) p(x(s, a,
  w))d \beta (s)\\ 
  + \int^t_0 \left[\frac{1}{2} P^2 (x(s, a, w)) u'' (x(s,
    a, w)) + u' (x(s, a, w)) r(x(s, a, w))\right] ds.
  \end{multline*}

Write $F(s, a, w) = \dfrac{1}{2} P^2 (x(s, a, w)) u'' (x(s, a, w)) +
u' (x(s, a, w))$ \break $r(x(s, a, w))$. Then\pageoriginale 
since $x(0, a, w) = a$, 
\begin{equation}
  \int\limits_{\Omega} u(x(t, a, w)) P(dw) = u(a) + \int^t_0 ds
  \int_\Omega F(s, a, w) P(dw) \tag{1}\label{chap5-sec7-eq1} 
\end{equation}
since the expectation of a stochastic integral is zero.

Thus 
\begin{align*}
  \alpha \int^\infty_0&  e^{- \alpha t} dt \int_\Omega u(x(t, a, w)) dP
  (w)\\ 
  & = u (a) + \int^\infty_0 \alpha e^{- \alpha t} dt \int^t_0 ds
  \int_\Omega F(s, a, w) dP (dw) \\
  &= u(a) + \int^\infty_0 ds \int_\Omega F (s, a, w) P (dw)
  \int^\infty_s \alpha e^{- \alpha t}dt\\ 
  &= u (a) + \int^\infty_0 ds \int_\Omega F(s, a, w) P (dw) e^{-\alpha s}
\end{align*}
Q.E.D
\end{proof}

\begin{thm}\label{chap5-sec7-thm2}%them 2
If $u$ satisfies the conditions of Theorem \ref{chap5-sec7-thm1}, then
$$
\lim\limits_{t \rightarrow 0} \frac{H_t u(a) - u (a)}{t} =
\frac{1}{2} P^2 (a) u'' (a) + r(a) u' (a). 
$$
\end{thm}

This is immediate from equation \eqref{chap5-sec7-eq1} above, since all the functions
involved are continuous. 

\begin{thm}\label{chap5-sec7-thm3}%them 3.
  Let $P(t, a, E)$ be the transition probability of the above
  diffusion. Then the following Kolmogoroff conditions are  true.
  \begin{enumerate}[(A)]
  \item $\lim\limits_{t \rightarrow 0} \dfrac{1}{t} P(t, a, U^c_a) = 0$
  \item $\lim\limits_{t \rightarrow 0} \dfrac{1}{t} \int_{U_a} (b-a)
    P(t, a, db) = r(a)$ 
  \item $\lim\limits_{t \rightarrow 0} \dfrac{1}{t} \int_{U_a} (b-a)^2
    P(t, a, db) = P^2(a)$ 
  \end{enumerate}
  where $U_a$ is any bounded open set containing $a$.
\end{thm}

\begin{proof}
We can prove these facts using stochastic differential equations;
but we shall deduce them from Theorem \ref{chap5-sec7-thm2} above. 
\begin{enumerate}[(A)]
\item Let\pageoriginale $V_a$ be any open set containing a with
  $\bar{V}_a \subset 
  U_a$ and let $u$ be a $C_2$ function such that $u = 0$ on $V_a, u
    = 1$ on $U^c_a$ and $0 \leq u \leq 1$ on $U_a - V_a$. Then $u$
    satisfies the conditions of Theorem \ref{chap5-sec7-thm1}. We have 
    $$
    0 \leq \frac{1}{t} P(t, a, U^c_a) \leq \frac{1}{t} [H_t u(a) - u
      (a)] \rightarrow \frac{1}{2} p^2 u'' (a) + ru' (a) = 0. 
    $$

\item Let $V_a \supset \bar{U}_a$ and let $\in$ be a $C_2$ function
    vanishing outside $V_a$, $1$ on $\bar{U}_a$ and $0 \leq \in \leq
    1$. Put $u(b) = (b-a) \in (a)$. Then  
    $$
    \displaylines{\hfill 
    \lim_{t \rightarrow 0} \frac{1}{t} [H_t u(a) - u(a)] = \frac{1}{2}
    p^2 u'' (a)+ ru' (a) = r(a) \hfill \cr
    \text{i.e.,}\hfill  
    \lim_{t \rightarrow 0} \frac{1}{t} \int_{R'} u(b) P(t, a, db) =
    r(a).\hfill }
    $$

    Also 
    \begin{align*}
      \varlimsup_{t \rightarrow 0} | \frac{1}{t} \int_{R'} u(b)
      P(t, a, db)& -  \frac{1}{t} \int_{U_a} (b - a) P(t, a, db) | \\ 
      & \leq \overline{\lim_{t \rightarrow 0}} \frac{1}{t} \int_{V_a -
        U_a} |b-a| | \in (b) -1| P(t, a, db)\\ 
      &\leq \overline{\lim_{t
          \rightarrow 0}} C \frac{1}{t} \int_{U^c_a} P(t, a, db) = 0 
    \end{align*}
    from $(A)$, where $C$ is a bound for $| b-a | | \in (b)-1 |$ on $V_a- U_a$.

\item Take $u(b)= (b-a)^2 \in (b)$ in $(b)$.
  \end{enumerate}
\end{proof}

\begin{remark*}
  Theorem \ref{chap5-sec7-thm3} means (in an intuitive sense)
  \begin{align*}
    P_a (|dx_t | > \in ) &= 0 (dt), E_a (dx_t)  \sim r(a) dt,\\
    V_a(dx_t) &= E_a ((dx_t)^2) \sim p^2 (a) dt.
  \end{align*}
\end{remark*}
