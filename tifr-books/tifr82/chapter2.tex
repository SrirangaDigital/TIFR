\chapter{The Error Terms In Mean Square Formulas}\label{c2}

\section{The Mean Square Formulas}\label{c2:sec2.1}

THE\pageoriginale EVALUATION OF the mean square integral
$\displaystyle{\int^T_0 |\zeta (\sigma + it)|^2 dt}$ is one of the
central problems in zeta-function theory. In view of the functional
equation $\zeta(s)=\chi (s) \zeta(1-s)$ is turns out that the relevant
range for $\sigma$ is the so called ``critical strip'' $1/2 \leq
\sigma \leq 1$. Of particular interest is the case $\sigma = 1/2$
(i.e. the so-called ``critical line''). This problem was considered in
Chapter \ref{c1} (Theorem \ref{c1:eq1.1}). It is possible to push the
analysis much further, and to obtain fairly precise formulas for the
integrals in equation. These in turn yield relevant information about
$|\zeta (\sigma + it)|$ and $|\zeta (1/2 + it)|$. With this in mind we
define
\begin{equation}
  E(T) : = \int\limits_{0}^T \left|\zeta \left(\frac{1}{2} +
  it\right)\right|^2 dt - T \log 
  \left(\frac{T}{2 \pi} \right) - (2 \gamma - 1)T,\label{c2:eq2.1}
\end{equation}
where $\gamma$ is Euler's constant, and for $\frac{1}{2} < \sigma < 1$
fixed
\begin{equation}
  E_\sigma (T) : = \int\limits_0^T |\zeta (\sigma + it)|^2 dt- \zeta
  (2 \sigma) T - \frac{\zeta (2 \sigma -1)\Gamma (2 \sigma-1)}{1-
    \sigma} \sin (\pi \sigma) T^{2- 2 \sigma}.\label{c2:eq2.2} 
\end{equation}

The functions $E(T)$ and $E_\sigma (T)$ are obviously related, and in
fact we have
\begin{equation}
  \lim\limits_{\sigma \to \frac{1}{2} +0} E_\sigma (T) = E(T). \label{c2:eq2.3}
\end{equation}

To show this we use the Laurent expansions
\begin{align*}
\Gamma (s) & = \frac{1}{s} - \gamma + a_1 s + a_2 s^2 + \cdots &
(\text{near}~ s=0),\\
\zeta (s) & = \frac{1}{s-1} + \gamma + \gamma_1 (s-1)+ \cdots &
(\text{near}~ s=1),\\
\zeta(s) & = \zeta(0) + \zeta' (0) s + b_2 s^2 + \cdots\\
& = - \frac{1}{2} - \frac{1}{2} \log (2 \pi) s + b_2 s^2 + \cdots &
(\text{near}~ s=0),\\
\frac{1}{1-s} & = 2- 2(1- 2s) + 2(1- 2s)^2+ \cdots & (\text{near}~ s=
\frac{1}{2}). 
\end{align*}

%till here

Then\pageoriginale for $\sigma \to \frac{1}{2} + 0$ and $T$ fixed we have
\begin{align*}
& \zeta (2 \sigma)  T  + \frac{\zeta(2 \sigma -1) \Gamma (2 \sigma
    -1)}{1- \sigma} \cos (\pi (\sigma - \frac{1}{2}))T^{2- 2 \sigma}= \frac{T}{2 \sigma-1}\\
  &  + \gamma T + O ((2 \sigma -1))
   + \left\{2- 2 (1- 2 \sigma) + O ((2 \sigma -1)^2) \right\}\times\\
  & \left\{- \frac{1}{2} - \frac{1}{2}\log (2 \pi) (2
   \sigma -1)+ O ((2   \sigma-1)^2)  \right\} \times \\
  & \left\{ \frac{1}{2 \sigma-1} - \gamma + a_1 (2
  \sigma -1)+ O ((2 \sigma -1)^2)\right\}\times\\ 
  & \left\{T + (1- 2 \sigma) T \log T + O (( 2
  \sigma -1)^2)\right\}\times\\
  & = T \log T + (2 \gamma -1 - \log (2 \pi)) T + O ((2 \sigma-1)),
\end{align*}
and this proves \eqref{c2:eq2.3}.

An explicit formula for $E(T)$ was discovered in 1949 by
F.V. Atkinson, and its analogue for $E_\sigma (T) (\frac{1}{2} <
\sigma < \frac{3}{4})$ in 1989 by K. Matsumoto. Atkinson's formula
shows certain analogies with the classical Voronoi formula for
\begin{equation}
  \Delta  (x) : = \sideset{}{'}\sum_{n \leq x} d(n) - x(\log x + 2
  \gamma -1)- \frac{1}{4},\label{c2:eq2.4}
\end{equation}
the error term in the Dirichlet divisor problem (see
\eqref{c2:eq2.25}), where $\displaystyle{\sideset{}{'}\sum_{n \leq
    x}}$ means that the last term in the sum is to be halved if $x$ is
an integer. In fact, the analogy between $E(T)$ and $\Delta  (x)$
was one of the primary motivations of Atkinson's work on $E(T)$, and
his formula will be given below as Theorem \ref{c2:eq2.1}, It has
inspired much recent research on the zeta-function. One of its
remarkable aspects is that it provides various results on $E(T)$ and
related topics, some of which will be discussed in Chapter \ref{c3}. Another
important thing is that Atkinso's formula may be generalized in
several ways. Instead of the mean square of $|\zeta (\frac{1}{2} +
it)|$ one may consider the mean square of $|L (\frac{1}{2} + it,
\chi)|$. The possibility of considering the mean square of a Dirichlet
polynomial together with $|\zeta (\frac{1}{2} + it)|$ will be treated
in Section \ref{c2:sec2.8}. The approach to the fourth power moment,
found recently by Y. Motohashi and expounded in Chapter \ref{c5}, is based
on\pageoriginale a generalization of Atkinson's approach. As already
mentioned, it is also possible to obtain the analogue of $E(T)$ for
$E_\sigma (T)$ in the range $\frac{1}{2}< \sigma < \frac{3}{4}$ ($\sigma
= \frac{3}{4}$ appears to be the limit of the present method). The
formula for $E_\sigma (T)$ will be given as Theorem \ref{c2:thm2.2},
and its proof will be given parallel to the proof of Theorem
\ref{c2:thm2.1}. A discussion of the mean square formula for $E(T)$,
which is analogous to the corresponding problem for $\Delta (x)$, is
presented in Section \ref{c2:sec2.6}. Upper bound results on $E(T)$
and $E_\sigma (T)$ are contained in Section \ref{c2:sec2.7}, and some
aspects of $E(T)$ are discussed also in Chapter \ref{c4}, where a general
approach to even moments of $|\zeta (\frac{1}{2} + it)|$ is given.

We present now the formulas for $E(T)$ and $E_\sigma (T)$.

\begin{thm}\label{c2:thm2.1}
  Let $0 < A < A'$ be any two fixed constants such that $AT < N < A'
  T$, $N' =N' (T)= T/(2 \pi) + N/2- (N^2/4 + NT/(2
  \pi))^{\frac{1}{2}}$, and let 
  \begin{gather*}
  f(T, n): = 2 T ar \sinh \sqrt{\frac{\pi n}{2 T}} + (2 \pi n T +
  \pi^2 n^2)^{1/2} - \frac{\pi}{4},\\
  g(T, n) : = T \log \left( \frac{T}{2 \pi n}\right) - T +
  \frac{\pi}{4}, ar \sinh x: = \log (x + \sqrt{x^2+1}).
  \end{gather*}
\end{thm}

Then 
\begin{align}
E(T) &= 2^{- \frac{1}{2}}\sum_{n \leq N} (-1)^n d (n) n^{-\frac{1}{2}}
\left(ar \sinh \sqrt{\frac{\pi n}{2T}}\right)^{-1} \left(\frac{T}{2
  \pi n} + \frac{1}{4} \right)^{\frac{1}{4}}\times \notag\\
& \qquad\qquad \cos (f (T, n)) - 2 \sum_{n \leq N'} d(n) n^{- \frac{1}{2}} \left(\log
\frac{T}{2 \pi n}\right)^{-1}\times\notag\\ 
&\qquad\qquad \cos (g(T, n)) + O (\log^2 T).\label{c2:eq2.5}
\end{align}

\begin{thm}\label{c2:thm2.2}
  Let $0 < A < A'$ be any two fixed constants such that $AT < N <
  A'T$, and let $\sigma$ be a fixed number satisfying $\frac{1}{2} <
  \sigma < \frac{3}{4}$. If $\sigma_a (n) = \sum\limits_{d|n} d^a$, then with
  the notation introduced in Theorem \ref{c2:thm2.1} we have
  \begin{align}
    E_\sigma (T) & = 2^{\sigma-1} \left( \frac{\pi}{T}\right)^{\sigma -
      \frac{1}{2}} \sum_{n \leq N} (-1)^n \sigma_{1-2 \sigma}(n)
    n^{\sigma-1} \left(ar \sinh \sqrt{\frac{\pi n}{2 T}}\right)^{-1}\times\notag\\
     &  \left(\frac{T}{2 \pi n} + \frac{1}{4} \right)^{-\frac{1}{4}}
      \cos (( f (T, n)) - 2 \left(\frac{2 \pi}{T}\right)^{\sigma -
        \frac{1}{2}} \sum_{n \leq N}, \sigma_{1- 2 \sigma} (n)
      n^{\sigma-1}\times\label{c2:eq2.6}\\ 
      & \left( \log \frac{T}{2 \pi n}\right)^{-1} \cos (g
      (T, n))+ O(\log T).\notag
  \end{align}
\end{thm}

Note\pageoriginale that, as $\sigma \to \frac{1}{2} +0$, the sums in
Theorem \ref{c2:thm2.2} become the sums in Theorem \ref{c2:thm2.1}. In
the next sections we shall give a proof of Theorem \ref{c2:thm2.1} and
Theorem \ref{c2:thm2.2}. It will transpire from the proof why the
restriction $\frac{1}{2} < \sigma < \frac{3}{4}$ is a natural one in
Theorem \ref{c2:thm2.2}.

\section{The Beginning of Proof}\label{c2:sec2.2}

We start with the initial stages of proof of Theorem \ref{c2:thm2.1}
and Theorem \ref{c2:thm2.2}. Atkinson's basic idea was to use the
obvious identity, valid for $\re u> 1$ and $\re v > 1$,
\begin{equation}
  \zeta (u) \zeta (v) = \sum^\infty_{m=1} \sum^\infty_{n=1} m^{-u}
  n^{-v}= \zeta (u+v) + f(u, v)+ f(v, u),\label{c2:eq2.7}
\end{equation}
where
\begin{equation}
  f(u, v) : = \sum^\infty_{r=1} \sum^\infty_{s=1}r^{-u} (r+
  s)^{-v}.\label{c2:eq2.8} 
\end{equation}

What is needed is the analytic continuation of $f(u, v)$ to a region
containing the points $u= \frac{1}{2} + it$, $v= \frac{1}{2} -it$, $v=
\frac{1}{2} - it$, so that eventually the integration of
\eqref{c2:eq2.7} will lead to \eqref{c2:eq2.5}. To carry out this
plane we show first that $f(u, v)$ is a meromorphic function of $u$
and $v$ for $\re (u + v)> 0$. Taking $\re v > 1$ and writing 
$$
\psi (x) = x- [x]- \frac{1}{2}, \psi_1 (x) = \int\limits_1^x \psi (y) dy,
$$ 
so that $\psi_1 (x) \ll 1$ uniformly in $x$, it follows on integrating
by parts that
\begin{align*}
  \sum^\infty_{s=1} (r+s)^{-v}& = \int\limits^\infty_{1-0} (r+y)^{-v}
  d[y]= \int\limits_r^\infty ([x] - r)x^{-v-1}dx\\
  & = r^{1-v} (v-1)^{-1} - \frac{1}{2} r^{-v} - v \int\limits_r^\infty
  \psi (x) x^{-v -1} dx\\
  & = r^{1 -v} (v-1)^{-1} - \frac{1}{2} r^{-v} - v(v+1)
  \int\limits^\infty_r \psi_1 (x) x^{-v -2} dx\\
  & = r^{1-v} (v-1)^{-1} - \frac{1}{2} r^{-v} + O (|v|^2 r^{- \re v-1}).
\end{align*}

Hence
$$
f(u, v) = (v-1)^{-1} \sum^\infty_{r=1} r^{1- u -v} - \frac{1}{2}
\sum^\infty_{r=1} r^{- u-v} + O \left(|v|^2\sum^\infty_{r=1} r^{- \re u-
  \re v-1}\right),
$$
and\pageoriginale therefore 
$$
f(u, v)- (v-1)^{-1} \zeta (u+v-1)+ \frac{1}{2} \zeta (u+v)
$$
is regular for $\re (u+v)> 0$. Thus \eqref{c2:eq2.7} holds by analytic
continuation when $u$ and $v$ both lie in the critical strip, apart
from the poles at $v=1$, $u+v=1$ and $u+v=2$.

We consider next the case $-1 < \re u < 0$, $\re (u+v)> 2$. We use the
well-known Poisson summation formula: If $a, b$ are integers such that
$a < b$ and $f(x)$ has bounded first derivative on $[a, b]$, then 
\begin{equation}
  \sideset{}{'}\sum_{a \leq n \leq b} f(n) = \int\limits_a^b f(x) dx +
  2 \sum_{n=1}^\infty \int\limits_a^b f(x) \cos (2 \pi n x)dx,\label{c2:eq2.9}
\end{equation}
where $\sideset{}{'}\sum$ means that the first and the last term in
the sum is to be halved. By using \eqref{c2:eq2.9} with $a= 0$, $b=
\infty$ it follows that 
\begin{align*}
  \sum^\infty_{r=1} r^{-u} (r+s)^{-v} &= \int\limits_0^\infty x^{-u}
  (x+s)^{-v} dx+ 2 \sum^\infty_{m=1} \int\limits_0^\infty x^{-u}
  (x+s)^{-v}\times\\ 
  &\qquad \cos (2 \pi mx)dx = s^{1- u-v}\left( \int\limits^\infty_0 y^{-u} (1+y)^{-v} dy + 2\times\right.\\
 &\qquad\qquad\left.\sum^\infty_{m=1} \int\limits^\infty_{0} y^{-u} (1+y)^{-v} \cos (2
  \pi m ys) dy\right)
\end{align*}
after the change of variable $x=sy$. Recalling the beta-integral
formula
\begin{equation}
  B(a, b) = \int\limits_0^1 x^{a-1} (1- x)^{b-1} dx = \frac{\Gamma (a)
    \Gamma (b)}{\Gamma (a+b)} (\re a > 0, \re b > 0),\label{c2:eq2.10}
\end{equation}
we have with $1+ y = 1/z$
\begin{equation}
  \int\limits_o^\infty y^{-u} (1+y)^{-v} dy = \int\limits_0^1
  (1-z)^{-u} z^{u+v -2} dz = \frac{\Gamma (u+v-1)\Gamma (1-
    u)}{\Gamma (v)}.\label{c2:eq2.11}
\end{equation}

Since $\re (u+v)> 2$, summation over $s$ gives
\begin{align}
  g (u, v): = f(u, v) - \Gamma (u+v-1) \Gamma (1- u)\Gamma^{-1} (v)
  \zeta (u+ v-1)\notag\\
  = 2 \sum^\infty_{s=1}s^{1- u-v} \sum^\infty_{m=1}
  \int\limits^\infty_0 y^{-u} (1+ y)^{-v} \cos (2 \pi m sy) dy.\label{c2:eq2.12}
\end{align}

To\pageoriginale investigate the convergence of the last expression we
note that for $\re u < 1$, $\re (u+v)> 0$, $m \geq 1$,
\begin{align}
   & 2 \int\limits^\infty_0 y^{-u} (1+y)^{-v} \cos (2 \pi n y) dy =
  \int\limits_0^\infty y^{-u} (1+y)^{-v} (e (ny) + e (- ny))dy\label{c2:eq2.13}\\
  & = \int\limits^{i \infty}_0 y^{-u} (1+ y)^{-v} e^{ny}dy +
  \int\limits^{- i \infty}_0 y^{-u} (1+y)^{-v} e (-ny)
  dy \notag\\
  & = n^{u-1} \int\limits^{i \infty}_0 y^{-u} \left(1+ \frac{y}{n} \right)^{-v}
  e(y) dy + n^{u-1} \int\limits^{- i \infty}_0 y^{-u} \left(1+
  \frac{y}{n}\right)^{-v} e(-y) dy\notag\\
  & \ll n^{\re u-1}|u-1|^{-1}\notag
\end{align}
uniformly for bounded $u$ and $v$, which follows after an integration
by parts. Therefore the double series in \eqref{c2:eq2.12} is
absolutely convergent for $\re u < 0$, $\re v > 1$, $\re (u+v)> 0$, by
comparison with 
$$
\sum_{s=1}^\infty |s^{-v}| \sum_{m=1}^\infty |m^{u-1}|.
$$

Hence \eqref{c2:eq2.12} holds throughout this region, and grouping
together the terms with $ms=n$ we have
$$
g(u, v) = 2 \sum_{n=1}^\infty \sigma_{1- u-v} (n) \int\limits^\infty_0
y^{-u} (1+y)^{-v} \cos (2 \pi n y) dy,
$$
where as before $\sigma_a (n) = \sum\limits_{d\mid n} d^a$, so that $\sigma_0
(n)= \sum\limits_{d \mid n} 1= d(n)$ is the number of divisors of $n$.

Therefore if $g(u, v)$ is the analytic continuation of the function
given by \eqref{c2:eq2.12}, then for $0 < \re u < 1$, $0 < \re v < 1$,
$u+v \neq 1$, we have
\begin{align}
  \zeta (u) \zeta (v)&= \zeta (u+v) + \zeta (u+v -1)\Gamma (u + v-1)\notag \\
  &\qquad\left( \frac{\Gamma
    (1-u)}{\Gamma (v)} + \frac{\Gamma (1-v)}{\Gamma
    (u)}\right) + g (u, v) + g(v, u).\label{c2:eq2.14}
\end{align}

So far our discussion was general, but at this point we shall
distinguish\pageoriginale between the cases $\sigma = \frac{1}{2}$ and
$\sigma > \frac{1}{2}$, which eventually lead to the expressions for
$E(T)$ and $E_\sigma(T)$, given by Theorem \ref{c2:thm2.1} and Theorem
\ref{c2:thm2.2}, respectively.

\medskip
\noindent{ \textbf{a) The Case $\sigma = 1/2$.}}

We are interested in \eqref{c2:eq2.14} in the exceptional case
$u+v=1$. We shall use the continuity of $g(u, v)$ and set
$u+v=1+\delta$, $0 < |\delta| < 1/2$, with the aim of letting $\delta
\to 0$. Then the terms on the right-hand side of \eqref{c2:eq2.14} not
containing $g$ become
\begin{align*}
  & \zeta (1+ \delta) + \zeta (\delta) \Gamma (\delta) \left(
  \frac{\Gamma (1-u)}{\Gamma (1- u + \delta)} + \frac{\Gamma (u -
    \delta)}{\Gamma (u)}\right)\\
  & = \zeta (1+ \delta) + \zeta (1- \delta) \frac{(2 \pi)^\delta}{2
    \cos (\frac{1}{2} \pi \delta)} \left( \frac{\Gamma (1- u)}{\Gamma
    (1- u + \delta)} + \frac{\Gamma (u- \delta)}{\Gamma(u)}\right)\\
  & = \delta^{-1} + \gamma + (\gamma - \delta^{-1}) \left(\frac{1}{2}
  + \frac{\delta}{2} \log 2 \pi  \right) \left(1-
  \frac{\Gamma'(1-u)}{\Gamma (1- u)} \delta +1 - \frac{\Gamma'
    (u)}{\Gamma (u)} \delta \right)\\ 
  & \hspace{9.5cm}+ O (|\delta|)\\
  & = \frac{1}{2} \left( \frac{\Gamma' (1-u)}{\Gamma (1- u)}+
  \frac{\Gamma' (u)}{\Gamma (u)}\right) + 2 \gamma - \log (2 \pi) + O
  (|\delta|), 
\end{align*}
where we used the functional equation for $\zeta (s)$. Hence letting
$\delta \to 0$ we have, for $0< \re u < 1$,
\begin{equation}
  \zeta (u) \zeta (1-u) = \frac{1}{2} \left( \frac{\Gamma' (1-
    u)}{\Gamma (1-u)} + \frac{\Gamma' (u)}{\gamma (u)}\right) + 2
  \gamma - \log (2 \pi) + g(u, 1- u)+ g (1- u, u), \label{c2:eq2.15}
\end{equation}
where reasoning as in \eqref{c2:eq2.13} we have, for $\re u < 0$,
\begin{equation}
  g(u, 1- u) = 2 \sum^\infty_{n=1} d(n) \int^\infty_{0} y^{-u} (1+
  y)^{u-1} \cos (2 \pi n y) dy.\label{c2:eq2.16}
\end{equation}

What is needed now is the analytic continuation of $g(u, 1- u)$, valid
for $\re u = 1/2$. This will be obtained in the next section by the
use of the Voronoi formula. Right now, assuming that we have such a
continuation, we may write an expression for $E(T)$ that will be used
in later evaluations. We set $u = \frac{1}{2} + it$ and note that
$\zeta (u) \zeta (1-u)= |\zeta (\frac{1}{2} + it)|^2$. Integration of
\eqref{c2:eq2.15} gives then
\begin{align*}
  & 2 i \int\limits^T_0 |\zeta (\frac{1}{2} + it )|^2 dt =
  \int\limits^{\frac{1}{2} - iT}_{\frac{1}{2} + iT} \zeta (u) \zeta
  (1-u) du\\
  & \frac{1}{2} (- \log \Gamma (1- u)+ \log \Gamma (u))
  \left| \begin{matrix}
    \frac{1}{2} + iT\\[5pt]
    \frac{1}{2} - iT
  \end{matrix}\right. + 2 iT ( 2\gamma - \log 2 \pi)\\
  & \hspace{4cm}+ \int\limits^{\frac{1}{2} + iT}_{\frac{1}{2} - iT} (g (u, 1- u)
  + g(1- u, u)) du\\
  & \log \frac{\Gamma (\frac{1}{2} + i T)}{\Gamma (\frac{1}{2} - iT)}
  + 2 i T (2 \gamma - \log 2 \pi) + 2 \int\limits^{\frac{1}{2} +
    iT}_{\frac{1}{2} - iT} g (u, 1- u) du.
\end{align*}

To\pageoriginale simplify this expression we use Striling's formula in the form
\begin{equation}
  \log \Gamma (s+b) = \left(s+b - \frac{1}{2}\right) \log s -s +
  \frac{1}{2} \log (2 \pi) + O (|s|^{-1}),\label{c2:eq2.17}
\end{equation}
which is valid for $b$ a constant and $|\arg s| \leq \pi - \delta
(\delta > 0)$, if $s=0$ and the neighbourhoods of the poles of $\Gamma
(s+b)$ are excluded. We obtain
\begin{align}
  \int\limits^T_0 |\zeta \left(\frac{1}{2} + iT\right)|^2 dt &= T \log
  \left(\frac{T}{2 \pi} \right) + (2 \gamma -1) T - i\notag\\
  &\qquad\int\limits^{\frac{1}{2} + iT}_{\frac{1}{2} - iT} g (u, 1 -u) du + O
  (1), \label{c2:eq2.18}
\end{align}
or
\begin{equation}
  E(T) = - \int\limits^{\frac{1}{2} + iT}_{\frac{1}{2} - iT} g(u, 1
  -u) du + O(1).\label{c2:eq2.19}
\end{equation}

\medskip
\noindent{
\textbf{b) The Case $1/2 < \sigma < 3/4$.}}

We start again from \eqref{c2:eq2.14}, setting $u= \sigma + it$, $v= 2
\sigma - u = \sigma- it$ and integrating the resulting expression over
$t$. It follows that
\begin{align}
  \int\limits^T_0 |\zeta (\sigma + it)|^2 dt& 
  = \zeta (2 \sigma) T + 2 \zeta (2 \sigma -1) \Gamma (2 \sigma -1)
  \re\times\notag\\ 
   &\quad\left\{ \int\limits^T_0 \frac{\Gamma (1+ \sigma + it)}{\Gamma
    (\sigma + it)} dt \right\}  - i \int\limits_{\sigma - iT}^{\sigma +
  iT} g(u, 2 \sigma -u)du.\label{c2:eq2.20}
\end{align}

To evaluate the first integral on the right-hand side of
\eqref{c2:eq2.20} we use Stirling's formula in the form
\begin{equation}
  \Gamma (s) = \sqrt{2 \pi} t^{\sigma - \frac{1}{2}} \exp \left\{-
  \frac{\pi}{2} t + i \left(t \log t - t + \frac{\pi}{2} \left(\sigma -
  \frac{1}{2}  \right)\right) \right\} \cdot \left(1+ O \left(
  \frac{1}{t}\right) \right) \label{c2:eq2.21}
\end{equation}
for $0 \leq \sigma \leq 1$, $t \geq t_0 > 0$. Then for $t \geq t_0$
\begin{align*}
  \frac{\Gamma (1- \sigma + it)}{\Gamma (\sigma + it)} & = t^{1- 2
    \sigma} \exp \left(\frac{i \pi}{2} (1- 2 \sigma) \right)\cdot
  \left(1+ O \left(\frac{1}{t} \right) \right),\\
  \int\limits_0^T \frac{\Gamma (1- \sigma + it)}{\Gamma (\sigma + it)}
  dt & = \int\limits_{t_0}^T + O (1) = \exp \left(\frac{i \pi}{2}
  \left(1- 2 \sigma \right) \right) \frac{T^{2- 2\sigma}}{2- 2 \sigma}
  + O (1),
\end{align*}
and 
\begin{multline*}
  2 \zeta (2 \sigma -1) \Gamma (2 \delta -1) \re
  \left\{\int\limits^T_0 \frac{\Gamma (1- \sigma + it)}{\Gamma (\sigma
    + it)} dt  \right\}\\
  = \frac{\zeta (2 \sigma -1) \gamma (2 \sigma -1)\cos (\frac{1}{2}
    \pi (1- 2 \sigma))}{1- \sigma} T^{2- 2\sigma}+ O (1).
\end{multline*}

Inserting\pageoriginale the last expression in \eqref{c2:eq2.20} we obtain 
$$
\int\limits_0^T |\zeta (\sigma + it)|^2 dt = \zeta (2\sigma) T +
\frac{\zeta (2 \sigma) \Gamma (2 \sigma-1)\sin (\pi \sigma)}{1-
  \sigma} T^{2- 2 \sigma}+ E_\sigma (T)
$$
with
\begin{equation}
  E_\sigma (T) = -i \int\limits_{\sigma - iT}^{\sigma + iT} g(u, 2
  \sigma -u) du + O(1).\label{c2:eq2.22}
\end{equation}

\section{Transformation of the Expressions for the Error
  Terms}\label{c2:sec2.3} 

We shall transform the integrals in \eqref{c2:eq2.19} and
\eqref{c2:eq2.22}, providing incidentally analytic continuation for
the function $g$ which appears in them. First we shall deal with the
case of $E(T)$ in \eqref{c2:eq2.19}, using the Voronoi formula for
$\Delta  (x)$ (see \eqref{c2:eq2.24}) to transform the integral in
\eqref{c2:eq2.16}. This is 
\begin{equation}
  \Delta  (x) =- \frac{2}{\pi} \sqrt{x} \sum_{n=1}^\infty d(n)
  n^{- \frac{1}{2}} \left(K_1 (4 \pi \sqrt{4x}) + \frac{\pi}{2} Y_1 (4 \pi
  \sqrt{n x})\right), \label{c2:eq2.23}
\end{equation}
where $K_1$, $Y_1$ are standard notation for the Bessel functions. It
is known from Analysis that there exist asymptotic formulas for the
Bessel functions with any desired degree of accuracy. In particular,
one has
\begin{align}
  \Delta  (x) & = \frac{x^{\frac{1}{4}}}{\pi 2^{\frac{1}{2}}}
  \sum^\infty_{n=1} d(n) n^{-\frac{3}{4}} \cos \left(4 \pi \sqrt{n x} -
  \frac{\pi}{4}\right) \label{c2:eq2.24}\\
  & - \frac{x^{-\frac{1}{4}}}{32 \pi 2^{\frac{1}{2}}}
  \sum^\infty_{n=1} d(n) n^{-5/4} \sin \left(4 \pi \sqrt{4 x} -
  \frac{\pi}{4}  \right)\notag\\ 
  & \quad + \frac{15 x^{- \frac{1}{4}}}{2^{11} \pi
    2^{\frac{1}{2}}} \sum^\infty_{n=1} d(n) n^{- 7/4} \cos \left(4x
  \sqrt{nx} - \frac{\pi}{4} \right)+ O(x^{- 5/4}).\notag 
\end{align}

The\pageoriginale series in \eqref{c2:eq2.23} is boundedly convergent when $x$ lies
in any fixed closed subinterval of $(0 , \infty)$, and it is uniformly
convergent when the interval is free of integers. Instead of
\eqref{c2:eq2.23} or \eqref{c2:eq2.24} one often uses a truncated
expression for $\Delta  (x)$, namely
\begin{align}
  \Delta  (x)  & = (\pi \sqrt{2})^{-1} x^{\frac{1}{4}} \sum_{n \leq
    N}d(n) n^{- \frac{1}{4}} \cos \left(4 \sqrt{nx}-
  \frac{\pi}{4}\right)\notag\\
  & \qquad O(x^\epsilon) + O(x^{\frac{1}{2} + \epsilon} N^{-
    \frac{1}{2}})\label{c2:eq2.25} 
\end{align}
when  $1 \ll N \ll x^A$ for any fixed $A > 0$.

Let now $N$ be a large positive integer, $X= N+ 1/2$, and 
\begin{equation}
  h(u, x) : = 2 \int\limits_0^\infty y^{-u} (1 + y)^{u-1} \cos (2 \pi x
  y) dy.\label{c2:eq2.26}
\end{equation}

With $\displaystyle{D(x)= \sideset{}{'}\sum_{n \leq x} d(n)}$ we have
\begin{align*}
  \sum_{n > N} d(n) h (u, n) &= \int\limits_X^\infty h(u, x) dD (x)\\
  &= \int\limits_X^\infty (\log x + 2 \gamma) h(u, x) dx +
  \int\limits_X^\infty h(u, x) d\Delta (x)\\
  &= - \Delta  (X) h (u, x) + \int\limits_X^\infty (\log x + 2
  \gamma) h (u, x) dx\\ 
  &\qquad - \int\limits_X^\infty \Delta  (x)
  \frac{\partial h(u, x)}{\partial x} dx.
\end{align*}

Hence \eqref{c2:eq2.16} becomes 
\begin{align*}
  g(u, 1-u)& =
  \sum_{n \leq N} d(n) h(u, n)- \Delta  (X) h (u, X)\\
  & \qquad +\int\limits_X^\infty (\log x + 2 \gamma)h (u, x) dx -
  \int\limits_X^\infty \Delta  (x) \frac{\partial h (u, x)}{\partial
    x} dx\\
  & = g_1 (u) - g_2 (u) + g_3 (u) - g_4 (u),
\end{align*}
say. Here $g_1 (u)$ and $g_2 (u)$ are analytic functions of $u$ in the
region $\re u < 1$, since the right-hand side of \eqref{c2:eq2.26} is
analytic in this region. Next we have
\begin{align}
  g_3 (u) & = \int\limits_X^\infty (\log x + 2 \gamma)\times\notag\\ 
  & \quad \left\{
  \int\limits_{0}^{i \infty} y^{-u} (1+ y)^{u-1} e(xy) dy +
  \int\limits_0^{- i \infty} y^{-u} (1+y)^{u-1} e(- xy) dy \right\}
  dx.\label{c2:eq2.27} 
\end{align}

The\pageoriginale last integral may be taken over $[0, \infty)$ and
  the variable changed from $y$ to $y/X$. The other two integrals in
  \eqref{c2:eq2.27} are treated similarly, and the results may be
  combined to produce
\begin{align}
  g_3 (u) & = - \pi^{-1} (\log X + 2 \gamma) \int\limits_0^\infty
  y^{-u-1} (1+ y)^{u-1} \sin (2 \pi X y) dy\notag\\
  & \qquad + (\pi u)^{-1} \int\limits_0^\infty y^{-u -1} (1+ y)^u \sin
  (2 \pi Xy) dy.\label{c2:eq2.28}
\end{align}

To treat $g_4 (u)$, write first
$$
h(u, x)= \int\limits_0^{i \infty} y^{-u} (1+ y)^{u-1} e(xy)dy +
\int\limits_0^{- i \infty} y^{-u} (1+ y)^{u-1} e(-xy)dy.
$$

Then 
{\selectfont{\fontsize{10}{8}{
\begin{align*}
  \frac{\partial h (u, x)}{\partial x} &= 2 \pi i \int\limits_0^{i
    \infty} y^{1- u} (1+ y)^{u-1} e(xy)dy - 2 \pi i \int\limits_0^{- i
  \infty} y^{1-u}(1+ y)^{u-1} e (- xy) dy\\
  & = 2 \pi i x^{u-2} \left(\int\limits_0^{i \infty} y^{1- u}
  \left( 1 + \frac{y}{x} \right)^{u-1} e(y) dy - \int\limits_0^{- i \infty}
  y^{1- u} \left(1+ \frac{y}{x} \right)^{u-1} e(- y) dy\right)\\
  & \ll x^{\re u-2}
\end{align*}}}}
for $\re u \leq 1$ and bounded $u$. From \eqref{c2:eq2.25} with $N=
  x^{1/3}$ one has by trivial estimation $\Delta  (x) \ll x^{1/3+
    \epsilon}$, which suffices to show that $g_4 (u)$ is an analytic
  function of $u$ when $\re u < 2/3$. Therefore from \eqref{c2:eq2.19}
  and the expressions for $g_n (u)$ we obtain
\begin{equation}
  E(T)= I_1 - I_2 + I_3 - I_4 + O(1), \label{c2:eq2.29}
\end{equation}
where for $n= 1, 2, 3, 4$
\begin{equation}
I_n = -i \int\limits_{\frac{1}{2} - iT}^{\frac{1}{2} + iT} g_n
(u).\label{c2:eq2.30} 
\end{equation}

Hence
\begin{equation}
  I_1 = 4 \sum_{n \leq N} d(n) \int\limits_0^\infty \frac{\sin (T \log
    (1+ 1/\gamma)) \cos (2 \pi n y)}{y^{\frac{1}{2}} (1+
    y)^{\frac{1}{2}} \log (1+ 1/y)} dy,\label{c2:eq2.31}
\end{equation}\pageoriginale
\begin{equation}
  I_2 = 4 \Delta  (X) \int\limits_0^\infty \frac{\sin (T \log (1+
    1/y))\cos (2 \pi Xy)}{y^{\frac{1}{2}}(1+ y)^{\frac{1}{2}} \log (1+
    1/y)}dy,\label{c2:eq2.32}
\end{equation}
\begin{align}
  I_3 & = - \frac{2}{\pi} (\log X + 2 \gamma)\int\limits_0^\infty
  \frac{\sin (T \log (1+ 
    1/y))\sin (2 \pi Xy)}{y^{\frac{3}{2}}(1+ y)^{\frac{1}{2}} \log (1+
    1/y)}dy,\notag\\
  & \quad + (\pi i)^{-1} \int\limits_0^\infty y^{-1} \sin (2 \pi Xy)
  dy \int\limits_{\frac{1}{2} - iT}^{\frac{1}{2} + iT} (1+ y^{-1})^{u}
  u^{-1} du,  \label{c2:eq2.33}
\end{align}
and lastly
\begin{equation}
  I_4 = -i \int\limits_X^\infty \Delta  (x) dx
  \int\limits_{\frac{1}{2} - iT}^{\frac{1}{2} + iT} \frac{\partial
    h(u, x)}{\partial x} du,\label{c2:eq2.34}
\end{equation}
where $N$ is a positive integer, $X = N + 1/2$, and as in the
formulation of Theorem \ref{c2:thm2.1} we get $AT < N < A'T$. A more
explicit formula for $I_4$ may be derived as follows. Using
\eqref{c2:eq2.26} we have
\begin{align*}
  \int\limits_{\frac{1}{2} - iT}^{\frac{1}{2} + iT} \frac{\partial
    h(u, x)}{\partial x} du &= 4 i \frac{\partial}{\partial x} \left\{
  \int\limits_0^\infty \frac{\sin ( T \log (1+ 1/y)) \cos (2 \pi
    xy)}{y^{\frac{1}{2}}(1+ y)^{\frac{1}{2}} \log (1+ 1/y)}dy
  \right\}\\
  &= 4i \frac{\partial}{\partial x} \left\{\int\limits_0^\infty
  \frac{\sin (T \log (x+y)/y)\cos (2 \pi y)}{y^{\frac{1}{2}}
    (x+y)^{\frac{1}{2}} \log (x+y)/y}dy \right\}\\
  &= 4i \int\limits_0^\infty \frac{\cos (2 \pi
    y)}{y^{\frac{1}{2}}(x+y)^{3/2} \log (x+ y)/y} \\
  &\qquad\left\{ T \cos (T \log (x+y)/y)- \sin (T \log
  (x+y)/y)\left(\frac{1}{2}\right)\right.\\ 
&\hspace{5.3cm}\left. + \log^{-1}\left(\frac{x+y}{y}
  \right)\right\} dy
\end{align*}

Hence replacing $y$ by $xy$ we obtain
\begin{align}
  I_4 & = 4 \int\limits_x^\infty \frac{\Delta (x)}{x} dx
  \int\limits_0^\infty \frac{\cos (2 \pi xy)}{y^{1/2}(1+y)^{3/2} \log
    (1+ 1/y)}\times\notag\\
  &\quad \left\{T \cos \left(T \log \frac{1 + y}{y}\right)- \sin \left( T
  \log \frac{1+y}{y}\right) \left(\frac{1}{2}+ \log^{-1}
  \left(\frac{1+y}{y} \right)  \right)\right\}dy.\label{c2:eq2.35}
\end{align}

We pass now to the discussion of $E_\sigma(T)$, namely the case $1/2<
\sigma < 3/4$. Let
\begin{equation}
  D_{1- 2 \sigma} (x) = \sideset{}{'}\sum_{n\leq x} \sigma_{1- 2
    \sigma} (n),\label{c2:eq2.36}
\end{equation}
where, as in the definition of $\Delta  (x)$, $\sideset{}{'}\sum$
means that the last term in\pageoriginale the sum is to be halved if
$x$ is an integer, and the error term $\Delta _{1- 2 \sigma} (x)$ is
defined by the formula
\begin{align}
  D_{1- 2 \sigma}(x) & = \zeta (2 \sigma) x+ (2- 2 \sigma)^{-1} \zeta
  (2- 2 \sigma)x^{2- 2\sigma}\notag\\
  & \hspace{3cm}- \frac{1}{2} \zeta (2 \sigma -1) +
  \Delta _{1- 2 \sigma}(x).\label{c2:eq2.37}
\end{align}

The analytic continuation of $g(u, 2 \sigma -u)$ in \eqref{c2:eq2.22}
  can be obtained by the use of the analogue of Voronoi's classical
  formula for the function $\Delta _{1- 2 \sigma}(x)$. In the usual
  notation of Bessel functions one has
{\fontsize{10}{12}\selectfont
\begin{align}
  \Delta _{1- 2 \sigma} (X)  &=- x^{1-\sigma} \sum_{n=1}^\infty
  \sigma_{1- 2 \sigma} (n) n^{\sigma-1} \label{c2:eq2.38}\left\{\cos
  (\sigma \pi) J_{2- 2\sigma} (4 \pi \sqrt{n x})\vphantom{\frac{2}{\pi}}\right.\\ 
  &\qquad\left. + \sin
  (\sigma \pi) \left(Y_{2- 2\sigma} (4 \pi \sqrt{nx}) + \frac{2}{\pi} K_{2-
  2\sigma} (4 \pi \sqrt{nx})\right) \right\},\notag
\end{align}}
and it may be noted that the right-hand side of \eqref{c2:eq2.38}
becomes the right-hand side of \eqref{c2:eq2.23} when $\sigma \to
\frac{1}{2} + 0$. Actually we have the generating Dirichlet series
\begin{align*}
  \sum_{n=1}^\infty \sigma_{1- 2 \sigma} (n) n^{-s} & = \zeta (s)
  \zeta (2 \sigma - 1+s) & (\re s > 1),\\
  \sum_{n=1}^\infty d(n) n^{-s} & = \zeta^2 (s) & (\re s > 1),\\
  \intertext{and}
  \lim\limits_{\sigma \to \frac{1}{2} + 0} \sigma_{1- 2 \sigma}(n) & = d(n). 
\end{align*}

The series in \eqref{c2:eq2.38} is boundedly convergent when $x$ lies
in any fixed closed subinterval of $(0, \infty)$, provided that $1/2 <
\sigma < 3/4$, which is a condition whose significance in the analysis
of $E_\sigma (T)$ now becomes apparent. Using the asymptotic formulas
for the Bessel functions we obtain from \eqref{c2:eq2.38}
\begin{align}
\Delta _{1- 2 \sigma} (x) &= O (x^{- \sigma - \frac{1}{4}}+(\sqrt{2
\pi})^{-1} x^{3/4- \sigma} \sum_{n=1}^\infty \sigma_{1- 2 \sigma}
(n) n^{\sigma- 5/4}\left\{\cos \left(4 \pi \sqrt{nx}\right.\right.\notag\\
&\left.\left. - \frac{\pi}{4}\right)- (32 \pi
\sqrt{nx})^{-1} (16(1- \sigma)^2-1) \sin \left(4 \pi \sqrt{nx} -
\frac{\pi}{4}\right)  \right\}.\label{c2:eq2.39}
\end{align}

The analogue of the truncated Voronoi formula \eqref{c2:eq2.25} for
$\Delta _{1- 2 \sigma} (x)$ is, for\pageoriginale $1 \ll N \ll x^A$,
$1/2 < \sigma < 3/4$,
\begin{align}
  \Delta _{1- 2 \sigma} (x) & = (\sqrt{2} \pi)^{-1} x^{3/4- \sigma}
  \sum_{n \leq N} \sigma_{1- 2\sigma}(n) n^{\sigma- 5/4} \cos \left(4 \pi
  \sqrt{nx} - \frac{\pi}{4}\right)\notag\\
  & \qquad + O(x^{1/2- \sigma} N^{\sigma- 1/2 + \epsilon}) + O(x^{\frac{1}{2}+  \epsilon} 
N^{- \frac{1}{2}} ).\label{c2:eq2.40}  
\end{align}

Taking in \eqref{c2:eq2.40} $N= x^{(4 \sigma -1)/(4\sigma+1)}$ we
obtain by trivial estimation 
\begin{equation}
  \Delta _{1- 2 \sigma (x)} \ll x^{1/ (4 \sigma +1)+
    \epsilon}.\label{c2:eq2.41} 
\end{equation}

One can prove \eqref{c2:eq2.40} much in the same way as one proves
\eqref{c2:eq2.35}. We shall now briefly sketch the proof. By the
Perron inversion formula
\begin{equation}
  \sideset{}{'}\sum_{n \leq x} \sigma_{1- 2 \sigma} (n) =\frac{1}{2
    \pi i} \int\limits^{1 + \epsilon + i T}_{1 + \epsilon - i T} L(w)
  \frac{x^w}{w} dw + O (x^\epsilon) + O(x^{1+ \epsilon}
  T^{-1}),\label{c2:eq2.42} 
\end{equation}
where $T$ is a parameter, $w=u + iv(v \geq v_0)$, and 
\begin{align*}
  L(w) & = \zeta (w) \zeta (2 \sigma -1 + w) = \psi (w) L (1- w),\\
  \psi(w) & = \chi (w) \chi (2 \sigma -1 + w)= \left( \frac{v}{2
    \pi}\right)^{2 - 2\sigma - 2u}\\ 
  & \hspace{2cm} \exp \left\{-2 i v \log
  \left(\frac{v}{2 \pi} \right)+ 2 i v + \frac{i \pi}{2}
  \right\}. \left(1+ O \left(\frac{1}{v}\right) \right)
\end{align*}
by \eqref{c1:eq1.8} and \eqref{c1:eq1.9} of Chapter \ref{c1}. The segment of
integration in \eqref{c2:eq2.42} is replaced by the segment $[- \delta
- iT, - \delta + iT]$ ($\delta > 0$ arbitrarily small, but fixed) with
an error which is 
$$
\ll \quad (x T)^\epsilon (x T^{-1} + T^{-1- 2 \delta + 2 \delta}x^{- \delta}).
$$

Let $N$ be an integer and $T^2/(4 \pi^2 x)= N+1/2$. In view of the
poles of the integrand in \eqref{c2:eq2.42} at $w=1$, $w=2-2\delta$
and $w=0$, it follows that
\begin{align*}
  \Delta _{1- 2 \sigma}(x) & = \frac{1}{2 \pi i} \int\limits_{-
    \delta - iT}^{-\delta + iT} \psi (w)L (1-w)\frac{x^w}{w} dw + O\times\\
  & \hspace{3cm}\left\{(x T)^\epsilon (1+ x T^{-1}+ T^{1- 2 \sigma + 2 \delta}
  x^{-\delta}) \right\}\\
  & = \sum_{n=1}^\infty \sigma_{1- 2 \sigma}(n) \left(\frac{1}{2 \pi
    i} \int\limits_{- \delta - i T}^{- \delta + i T} \psi (w)
  n^{w-1} \frac{x^w}{w} dw \right)\\
  & \hspace{2cm} + O \left\{(xT)^\epsilon (1+ x T^{-1} + T^{1- 2 \sigma
    + 2 \delta} x^{-\delta}) \right\}
\end{align*}
because\pageoriginale of the absolute convergence of the above series. Using the
asymptotic formula for $\psi (w)$ it is seen that the terms in the
above series for $n > N$ contribute $\ll N^\epsilon T^{1- 2
  \sigma}$. In the remaining terms we replace the segment $[- \delta -
i T, - \delta + i T]$ by $[- \delta - i \infty, - \delta + i \infty]$
with an admissible error. By using
\begin{multline*}
\psi (w)= 2^{2w + 2 \sigma -1} \pi^{2w + 2 \sigma -3} \sin
\left(\frac{\pi w}{2} \right) \sin \left(\frac{\pi w}{2} + \frac{\pi
  (2 \sigma -1)}{2} \right)\\ 
\Gamma (1-w) \Gamma (2- 2 \sigma -w)
\end{multline*}
and properties of Mellin transforms for the Bessel functions we arrive
at
{\selectfont{\fontsize{10}{8}{
\begin{multline*}
  \Delta _{1- 2 \sigma}(x) = O(x^{1/2 - \sigma}N^{\sigma- 1/2 +
    \epsilon}) + O(x^{1/2 + \epsilon} N^{- \frac{1}{2}} )- x^{1- \sigma}
  \sum_{n \leq N} \sigma_{1- 2 \sigma}(n) n^{\sigma-1}\\
  \left\{\cos (\sigma \pi) J_{2- 2 \sigma} (4 \pi \sqrt{nx}) + \sin
  (\sigma  \pi) \left(Y_{2- 2\sigma} (4 \pi \sqrt{nx})+ \frac{2}{\pi}
  K_{2- 2\sigma} (4 \pi \sqrt{n x})\right) \right\}.
\end{multline*}}}}

Finally with the asymptotic formulas
\begin{align*}
  J_\nu (x) & = \left(\frac{2}{\pi x} \right)^{1/2} \cos \left(x -\left(\nu +
  \frac{1}{2}\right) \frac{\pi}{2}\right) + O_\nu (x^{-3/2}),\\
  Y_\nu (x) & = \left(\frac{2}{\pi x} \right)^{1/2} \sin\left(x-
  \left(\nu + \frac{1}{2} \right) \frac{\pi}{2}\right) + O_\nu (x^{-
    3/2}),\\
  K_\nu (x) & \ll_\nu \quad x^{-1/2} e^{-x},
\end{align*}
the above expression for $\Delta _{1-2 \sigma} (x)$ easily reduces
to \eqref{c2:eq2.40}. The derivation of \eqref{c2:eq2.38} is analogous
to the derivation of the Voronoi formula \eqref{c2:eq2.23} for
$\Delta  (x)$.

Having finished this technical preparation we pass to the evaluation of
the integral in \eqref{c2:eq2.22}. We shall write
\begin{align*}
  g(u, 2 \sigma - u) & = \sum_{n=1}^\infty \sigma_{1- 2 \sigma} (n) h
  (u, n),\\
  h(u, x) & = 2 \int\limits_{0}^\infty y^{-u} (1+ y)^{u- 2 \sigma}
  \cos (2 \pi  xy) dy,
\end{align*}
as\pageoriginale no confusion with \eqref{c2:eq2.26} will arise. As in
the case of $g(u, 1- u)$ we obtain analogously $(X= N + 1/2)$
\begin{align*}
  g(u, 2 \sigma -u) &= \sum_{n \leq N} \sigma_{1- 2 \sigma} (n) h(u, n)
  - \Delta _{1- 2\sigma} (X) h(u, X)\\
 &\quad + \int\limits_X^\infty (\zeta (2 \sigma)+ \zeta (2- 2\sigma) x^{1- 2
  \sigma}) h(u, x) dx\\
&\quad - \int\limits_X^\infty \Delta _{1- 2 \sigma}
  (x) \frac{\partial}{\partial x} h(u, x)dx\\
 & = g_1 (u) - g_2 (u) + g_3(u) - g_4(u),
\end{align*}
say. Here again a slight abuse of notation was made to keep the
analogy with $E(T)$, since $g_n (u) = g_n (\sigma, u)$. We wish to
show that $g(u, 2 \sigma -u)$ can be analytically continued to the
line $\re u = \sigma$, $1/2 < \sigma < 3/4$. Since the integral $h(u,
x)$ is absolutely convergent for $\re u < 1$, the functions $g_1 (u)$
and $g_2 (u)$ clearly possess analytic continuation to the line $\re u
= \sigma$. For $g_3 (u)$ we have, assuming first $\re u < 0$,
\begin{align*}
  g_3 (u) & = \int_X^\infty (\zeta (2 \sigma) + \zeta (2- 2 \sigma)
  x^{1- 2 \sigma}) h (u, x)dx\\
  & = \int\limits_{X}^\infty (\zeta (2 \sigma)+ \zeta (2- 2
  \sigma)x^{1- 2 \sigma}) \int\limits_0^{i \infty} y^{-u} (1+ y)^{u- 2
  \sigma} e (xy) dy dx\\
  & \quad + \int\limits^\infty_{X} (\zeta (2 \sigma) + \zeta (2 - 2 \sigma)
  x^{1 - 2 \sigma})\int\limits^{- i \infty}_0 y^{-u} (1+y)^{u- 2
      \sigma} e(- xy) dy dx\\
    & = I_1 + I_2,
\end{align*}
say. Using Fubini's theorem and integrating by parts we obtain
\begin{align*}
  I_1 & = \int\limits_0^{i \infty} y^{-u} (1+ y)^{u- 2\sigma}
  \left\{\int^\infty_X (\zeta (2 \sigma) + \zeta (2 - 2 \sigma)x^{1-
    2\sigma}) e(xy) dx \right\}dy\\
  & = - \int\limits_0^{i \infty} y^{-u} (1+ y)^{u-2 \sigma}
  \left\{(\zeta(2\sigma) + \zeta (2-2\sigma)X^{1- 2\sigma})
  \frac{e(Xy)}{2 \pi i y} \right\}dy\\
  &\quad - \int\limits_X^{\infty} \zeta (2 - 2 \sigma)(1- 2 \sigma) x^{- 2
    \sigma} \int\limits_0^{i \infty} y^{- u}(1+ y)^{u- 2 \sigma}
  \frac{e(xy)}{2 \pi i y} dy dx\\
  & = -  \frac{1}{2 \pi i} \int\limits_0^{i \infty} \left( \zeta (2 \sigma) +
  \zeta (2- 2 \sigma) X^{1- 2 \sigma}  \right)
  y^{-1 -u}(1+ y)^{u-2\sigma} e(Xy) dy\\
  &\quad - \frac{(1- 2\sigma)\zeta (2- 2 \sigma)}{2 \pi i}
  \int\limits^\infty_X x^{- 2 \sigma} \int\limits_{0}^{i \infty} y^{-1
    -u} (1+ y)^{u- 2\sigma} e(xy)dy\, dx.
\end{align*}

We\pageoriginale denote the last double integral by $J$ and make a change of
variable $xy =w$. Then we obtain
\begin{align*}
  J & = \int\limits_X^\infty x^{- 2 \sigma + 1 + u - u + 2 \sigma-1}
  \int\limits_0^{i \infty} w^{-1 -u} (x+ w)^{u- 2 \sigma} e(w) dw\,
  dx\\
  & = \int\limits_0^{i \infty} w^{-1 -u} e(w) \int_X^\infty (x+ w)^{u
    -2 \sigma} dx\, dw\\
  & = - \int\limits_0^{i \infty} w^{-1 -u} e(w) (X+ w)^{u- 2 \sigma+1}
  (u - 2 \sigma + 1)^{-1} dw.
\end{align*}

Change of variable $y = w/X$ gives now
$$
J= - \frac{X^{1- 2 \sigma}}{u- 2 \sigma +1} \int\limits_0^{i \infty}
y^{-1 -u} (1+ y)^{u- 2 \sigma +1} e(Xy) dy,
$$
and inserting this in the expression for $I_1$ we obtain 
\begin{multline*}
  I_1 =- \frac{1}{2 \pi i} \int\limits_0^{i \infty} (\zeta (2 \sigma)
  + \zeta (2 - 2 \sigma) X^{1- 2 \sigma}) y^{-1 -u} (1+ y)^{u- 2
    \sigma} e(Xy) dy\\
  + \frac{(1- 2 \sigma) \zeta (2- 2 \sigma) X^{1- 2 \sigma}}{2 \pi i
    (u+ 1 - 2 \sigma)} \int\limits_0^{i \infty} y^{-1 -u} (1+
  y)^{u+1-2 \sigma} e(Xy) dy.
\end{multline*}

Similarly\pageoriginale we obtain
\begin{multline*}
  I_2 = \frac{1}{2 \pi i} \int\limits_o^{i \infty} (\zeta (2 \sigma) +
  \zeta (2- 2\sigma)X^{1- 2\sigma}) y^{-1 -u} (1+ y)^{u- 2 \sigma} e(-
  Xy) dy\\
  - \frac{(1- 2\sigma)\zeta (2- 2 \sigma)X^{1- 2 \sigma}}{2 \pi i (u +
    1- 2 \sigma)} \int\limits_0^{- i \infty} y^{-1 -u} (1+ y)^{u+1 - 2
  \sigma} e (- Xy)dy.
\end{multline*}

Since $g_3 (u) = I_1 + I_2$, we finally obtain the desired analytic
continuation of $g_3 (u)$ to the line $\re u= \sigma$ in the form
(after changing the lines of integration to $[0, \infty)$)
\begin{align}
  g_3 (u) & = \frac{1}{\pi} \int\limits_0^\infty (\zeta (2 \delta) +
  \zeta (2 - 2 \sigma) X^{1- 2 \sigma}) y^{-1-u} (1+ y)^{u-2 \sigma}
  \sin (2 \pi Xy) dy\notag\\
  & + \frac{(1- 2 \sigma) \zeta (2- 2\sigma)}{\pi (u + 1- 2 \sigma)}
  X^{1-2 \sigma} \int\limits^\infty_0 y^{-1-u} (1+y)^{u+1 - 2 \sigma}
  \sin (2 \pi Xy)dy.\label{c2:eq2.43}
\end{align}

To show that
$$
g_4 (u) = \int\limits_X^\infty \Delta _{1- 2 \sigma} (x)
\frac{\partial}{\partial x} h(u, x)dx
$$
converges for $\re u \geq \sigma$ we use \eqref{c2:eq2.41} and 
$$
\frac{\partial}{\partial x} h (u, x) \ll x^{\re u-2},
$$
which is obtained as in the corresponding estimate for
\eqref{c2:eq2.26}. Thus the integral for $g_4 (u)$ converges
absolutely for $\re u < 1 - 1/(1+ 4 \sigma)$. But if $\sigma < 3/4$,
then $\sigma < 1 - 1 / (1+ 4 \sigma)$, which means that the integral
representing $g_4 (u)$ converges for $\re u = \sigma$. Now we can
integrate the expression for $g(u , 2 \sigma- u)$ in \eqref{c2:eq2.22}
to obtain 
\begin{equation}
  E_\sigma (T) =- i (G_1 - G_2 + G_3 - G_4) + O(1), G_j =
  \int\limits_{\sigma - iT}^{\sigma + iT} g_j (u) du\label{c2:eq2.44}
\end{equation}
for $1 \leq j \leq 4$ with
\begin{align}
  G_1 & = 4i \sum_{n \leq N} \sigma_{1- 2 \sigma} (n) \int^\infty_0 y^{-
  \sigma} \log^{-1} \left(1 + \frac{1}{y} \right)\times\notag\\ 
  & \hspace{3cm}\cos (2 \pi n y)
  \sin \left(T \log \left(1+ \frac{1}{y}\right)\right) dy,\label{c2:eq2.45}\\
  G_2 & = 4 i \Delta _{1- 2 \sigma}(X) \int_0^\infty y^{- \sigma}
  (1+ y)^{- \sigma} \log^{-1} \left( 1 +
  \frac{1}{y}\right)\times\notag\\ 
  & \hspace{4cm}\cos (2 \pi Xy) \sin \left(T \log \left(1+
  \frac{1}{y} \right)\right) dy,\label{c2:eq2.46}\\
  G_3 & = \frac{- 2 i}{\pi} (\zeta (2 \sigma)+ \zeta (2- 2
  \sigma)X^{1-2 \sigma}) \int\limits_0^\infty y^{- \sigma-1} (1+ y)^{-
  \sigma} \log^{-1} \left(1+ \frac{1}{y} \right)\times\notag\\ 
  & \sin (2 \pi Xy) \sin
  \left(T \log \left(1+ \frac{1}{y}\right)\right)dy
   + \frac{1- 2 \sigma}{\pi} \zeta (2 - 2 \sigma)X^{1-2
     \sigma}\label{c2:eq2.47}\\ 
  & \int\limits_0^\infty y^{-1} (1+y)^{1-2\sigma} \sin (2 \pi Xy)dy
  \int\limits_{\sigma- i T}^{\sigma + iT} (u+1 - 2 \sigma)^{-1}
  \left(1+ \frac{1}{y} \right)^u du.\notag
\end{align}

To\pageoriginale obtain a suitable expression for $G_4$ note that
{\fontsize{10}{12}\selectfont
\begin{align*}
 \int\limits_{\sigma - i T}^{\sigma + iT} \frac{\partial}{\partial x}
  h (u, x) du
  &= 2 \frac{\partial }{\partial x} \left\{ \int\limits_0^\infty
  \int\limits_{\sigma - i T}^{\sigma + i T} y^{- u}(1+ y)^{u- 2\sigma}
  \cos (2 \pi x y)du \, dy \right\}\\
  & = 4 i \frac{\partial }{\partial x} \left\{\int\limits_0^\infty y^{-
    \sigma} (1+ y)^{- \sigma} \log^{-1} \left(1+ \frac{1}{y}
  \right)\times\right.\\
  &\qquad\left.\sin \left(T \log \left( 1 + \frac{1}{y} \right) \right)\cos
  (2 \pi x y) dy \right\}\\
  &= 4 i \frac{\partial}{\partial x} \left\{\int\limits_0^\infty  x^{2
    \sigma -1} y^{- \sigma} (x+ y)^{- \sigma} \log^{-1} \left(
  \frac{x+y}{y}\right)\times\right.\\
  &\qquad\left.\sin \left(T \log \left( \frac{x+y}{y} \right)
  \right) \cos (2 \pi y) dy\right\}, 
\end{align*}}
similarly as in the derivation of \eqref{c2:eq2.35} from
\eqref{c2:eq2.34}. Hence differentiating the last expression under the
integral sign we arrive at 
\begin{align}
G_4&= 4 i \int\limits_X^\infty x^{-1}  \Delta _{1- 2 \sigma} (x) dx
 \int_0^\infty y^{- \sigma} (1+ y)^{- \sigma -1} \log^{-1} \left(1+
\frac{1}{y} \right)\notag\times\\
&\qquad\cos (2 \pi x y)\left( T \cos \left( T \log \left(1+ \frac{1}{y}
\right)\right) + \sin \left(T \log \left(1+ \frac{1}{y} \right)
\right)\right.\times\notag\\ 
& \hspace{1cm}\left.\left\{ (2 \sigma -1) (1+ y) - \sigma - \log^{-1} \left(1+
\frac{1}{1}{y}\right)\right\}\right)dy.\label{c2:eq2.48}
\end{align}

Note that this expression corresponds exactly to \eqref{c2:eq2.35}
when $\sigma \to \frac{1}{2} + 0$.

\section{Evaluation of Some Exponential Integrals}\label{c2:sec2.4}

We\pageoriginale shall first state and prove an elementary result,
often used in the estimation of exponential integrals. This is 

\begin{lemma}\label{c2:lem2.1}
Let $F(x)$ be a real differentiable function such that $F'(x)$ is
monotonic and $F'(X)\geq m > 0$ or $F' (x) \leq - m < 0$ for $a \leq x
\leq b$. Then
\begin{equation}
  \left|\int\limits_a^b e^{iF(x)}dx \right| \leq
  \frac{4}{m}.\label{c2:eq2.49} 
\end{equation}

If in addition $G(x)$ is a positive, monotonic function for $a \leq x
\leq b$ such that $|G(x)| \leq G$, $G(x) \in C^1 [a, b]$, then
\begin{equation}
  \left|\int\limits_a^b G(x) e^{iF(x)}dx \right| \leq
  \frac{4G}{m}.\label{c2:eq2.50}   
\end{equation}
\end{lemma}

\begin{proof}
  The merit of the above estimates is that they do not depend on the
  length of the interval of integration. Recall that by the second
  mean value theorem for integrals
  \begin{equation}
    \int\limits_a^b f(x) g(x) dx= 
    \begin{cases}
      f(b) \int\limits_c^b g(x) dx & \text{if}~ f(x) \geq 0, f' (x)
      \geq 0,\\
      f(a) \int\limits_a^c g(x) dx & \text{if}~ f(x) \geq 0, f' (x)
      \leq 0,
    \end{cases}\label{c2:eq2.51}
  \end{equation}
  where $a < c < b$ and $f' (x)$, $g(x) \in C[a, b]$. We write
\begin{align*}
  e^{iF (x)} & = \cos F(x) + i \sin F(x),\\
  \int\limits_a^b \cos F(x) dx & = \int\limits_a^b (F' (x))^{-1} d
  \sin F(x),
\end{align*}
and use \eqref{c2:eq2.51}, since $(F')^{-1}$ as the reciprocal of $F'$
is monotonic in $[a, b]$. It is seen that
$$
  \left|\int\limits_a^b \cos F(x)dx \right| \leq  \frac{2}{m}.
$$
\end{proof}
and\pageoriginale the same bound holds for the integral with $\sin
F(x)$. Hence \eqref{c2:eq2.49} follows. To obtain \eqref{c2:eq2.50}
write again $e^{iF(x)}= \cos F(x) +i \sin F(x)$, and to each resulting
integral apply \eqref{c2:eq2.51}, since $G(x)$ is monotonic. Using
then \eqref{c2:eq2.49}, \eqref{c2:eq2.50} follows.

From the formulas of Section \ref{c2:eq2.3} it is seen that the proof
of Theorem \ref{c2:thm2.1} and Theorem \ref{c2:thm2.2} is reduced to
the evaluation of certain integrals of the form
$$
I = \int\limits_a^b \varphi (x) e (f(x) + kx) dx,
$$
where $\varphi (x)$, $f(x)$ are continuous, real-valued functions on
$[a, b]$, and $k$ is real. The evaluation of this type of integrals,
which properly represents a theory of its own, is most often carried
out by the so-called ``saddle point'' method, or the method of
``stationary phase''. It consists of considering $I$ as the complex
integral
$$
I = \int\limits_a^b \varphi (z) e(f(z)+ kz)dz,
$$
where naturally one imposes some conditions on $\varphi (z)$ and
$f(z)$ as functions of the complex variable $z$. One then replaces the
segment of integration $[a, b]$ by a suitable contour in the
$z$-plane. If $(f(z)+ kz)'$ has a (unique) zero in $[a, b]$ (``saddle
point''), then in many cases arising in practice the main contribution
to $I$ comes from a neighbourhood of the saddle point $x_0$. There are
several results in the literature which give an evaluation of $I$
under different hypotheses. The one we shall use, due to
F.V. Atkinson, will be stated without proof as

\begin{thm}\label{c2:thm2.3}
  Let $f(z)$, $\varphi(z)$ be two functions of the complex variable
  $z$, and $[a, b]$ a real interval such that:
  \begin{enumerate}
    \item For $a\leq x \leq b$ the function $f(x)$ is real and
      $f''(x)> 0$.
      \item For a certain positive differentiable function $\mu(x)$,
        defined on $a \leq x \leq b$, $f(z)$ and $\varphi(z)$ are
        analytic for $a \leq x \leq b$, $|z- x|\leq \mu (x)$.
        \item There exist positive functions $F(x)$, $\Phi (x)$
          defined on $[a, b]$ such that for $a \leq x \leq b$, $|z-
          x|\leq \mu (x)$ we have
          $$
          \varphi (z) \ll \Phi (x), f' (z) \ll F(x) \mu^{-1} (x), |f''
          (z)|^{-1} \ll \mu^2 (x) F^{-1} (x),
          $$
      and\pageoriginale the $\ll$-constants are absolute.
  \end{enumerate}
  Let $k$ be any real number, and if $f' (x) + k$ has a zero in $[a,
    b]$ denote it by $x_0$. Let the values of $f(x)$, $\varphi(x)$,
  and so on, at $a, x_0$ and $b$ characterised by the suffixes $a, 0$
  and $b$, respectively. Then
{\fontsize{10}{12}\selectfont
\begin{align}
\int\limits_a^b \varphi (x) e (f (x) + kx) dx &= \varphi_0
(f''_0)^{- \frac{1}{2}} e\left(f_0 + kx_0 +
\frac{1}{8}\right)+ O (\Phi_0 \mu_o F_0^{-3/2})\notag\\
&  + O\left( \int\limits_a^b \Phi (x)
\exp \left\{- C |k|\mu (x)- CF (x) \right\} (dx + |d \mu
(x)|)\right)\notag\\ 
&+ O \left(\Phi_a \left(| f'_a + k| + f''_a{}^{\frac{1}{2}}
\right)^{-1}\right)\notag\\
& + O \left(\Phi_b \left(|f'_b + k|+
f''_b{}^{\frac{1}{2}} \right)^{-1} \right).\label{c2:eq2.52}
\end{align}}
  If $f'(x) +k$ has no zeros in $[a, b]$, then the terms involving
  $x_0$ are to be omitted.
\end{thm}

a simplified version of the above result may be obtained if the
conditions 2. and 3. of Theorem \ref{c2:thm2.3} are replaced by 

$2'$. There exists $\mu> 0$ such that $f(z)$ and $\varphi (z)$ are
analytic in the region
$$
D= D(\mu) = \left\{ z : |z- x|< \mu ~\text{for some}~ x \in [a, b]\right\}.
$$

$3'$. There exist $F, \Phi > 0$ such that for $z \in D$
$$
\varphi (z) \ll \Phi, f' (z) \ll F\mu^{-1}, |f'' (z)|^{-1} \ll \mu^2
F^{-1}. 
$$

Then in the notation of Theorem \ref{c2:thm2.3} one has
\begin{align}
  \int\limits_a^b \varphi (x) e (f(x) + kx)dx &= \varphi_0 (f''_0)^{-
    \frac{1}{2}} e\left(f_0 + kx_0 + \frac{1}{8}\right)\notag\\
&\quad + O\left(\frac{\Phi \mu}{F \mu^{-1} \Delta  + F^{\frac{1}{2}}} 
  \right),\label{c2:eq2.53} 
\end{align}
where
$$
\Delta  = \min (|a- x_0|, |b- x_0|).
$$

If $f' (x)+k$ has no zeros in $[a, b]$, then the terms involving $x_0$
are to be omitted.

In Atkinson's Theorem \ref{c2:thm2.3} we would have two additional
error\break terms,\pageoriginale which would be
$$
O (\Phi \mu F^{- 3/2}) + O (\Phi (b-a) \exp (-A (|k|\mu + F))),
$$
where $A > 0$ is an absolute constant. However, these error terms are
negligible if $F \gg 1$, since $b-a \ll \mu$. Indeed,
$$
(b-a) F\mu^{-2} \ll \int\limits_a^b f'' (x) dx = f' (b) - f'(a) \ll
F\mu^{-1}, 
$$
whence $b-a \ll \mu$. If $F \ll 1$, then \eqref{c2:eq2.53} is verified
directly: in any case the left-hand side of \eqref{c2:eq2.53} is
$O(\mu \Phi)$ and the first term on the right-hand side is $O (\mu
\Phi F^{- \frac{1}{2}})$. Both of these are absorbed in the error term
of \eqref{c2:eq2.53}.

Taking $\varphi (x) = x^{-\alpha} (1+ x)^{- \beta} \left(\log \frac{1+x}{x} \right)^{-\gamma}$, $f(x) =
\frac{T}{2\pi} \log \frac{1+x}{x}$, $\phi (x) = x^\alpha (1+x)^{\gamma - \beta}$, $F(x) = T / (1+x)$, $\mu (x) = x/2$,  we obtain after some calculations from
Theorem \ref{c2:thm2.3} the  following

\begin{lemma}\label{c2:lem2.2}
  Let $\alpha, \beta, \gamma, a, b, k, T$ be real numbers such that
  $\alpha, \beta, \gamma$ are positive and bounded, $\alpha \neq 1$,
  $0< a< \frac{1}{2}$, $a < T /(8 \pi k)$, $b \geq  T$, $k \geq 1$ and $T
  \geq 1$. Then
  {\selectfont{\fontsize{10}{8}{
  \begin{align}
&\int\limits_a^b y^{-\alpha} (1+y)^{-\beta} \left(\log
\frac{1+y}{y} \right)^{- \gamma} \exp \left(iT \log \frac{1+y}{y}
+ 2 \pi k i y\right)dy= (2 k \pi^{\frac{1}{2}})^{-1}\times\notag\\
&\quad T^{\frac{1}{2}} V^{-\gamma} U^{-
      \frac{1}{2}} \left(U - \frac{1}{2} \right)^{- \alpha} \left(U + 
    \frac{1}{2} \right)^{-\beta} \exp\left(i T V + 2 \pi i kU - \pi i
    k+ \frac{\pi i}{4} \right)\notag\\
&\quad +O (a^{1- \alpha}T^{-1}) + O (b^{\gamma- \alpha - \beta} k^{-1}) +
    R(T, k)\label{c2:eq2.54}
  \end{align}}}}
uniformly for $|\alpha - 1|> \epsilon$, where
\begin{align*}
  U & = \left(\frac{T}{2 \pi k} + \frac{1}{4} \right)^{\frac{1}{2}}, V
  = 2 ar \sinh \left(\frac{\pi k}{2T} \right)^{\frac{1}{2}},& \\
  R(T, k) & \ll  T^{(\gamma - \alpha - \beta)/2-1/4_k - (\gamma -
    \alpha - \beta)/2- 5/4} & \text{for}~ 1 \leq k \leq T,\\
  T(T, k) & \ll T^{- \frac{1}{2}- \alpha} k^{\alpha-1}& \text{for}~ k
  \geq T.
\end{align*}
\end{lemma}

A similar result holds for the corresponding integral with $-k$ in
place of $k$, except that in that case the main term on the right-hand
side of \eqref{c2:eq2.54} is to be omitted.

For\pageoriginale the next lemma we apply Theorem \ref{c2:thm2.3} with
$a, b$ as the limits of integration, where $b> T$, and
\begin{align*}
  \varphi(x) & = x^{- \alpha} \left(ar \sinh \left(x
  \sqrt{\frac{\pi}{2T}} \right) \right)^{-1} \left( \left( \frac{T}{2
    \pi x^2} + \frac{1}{4} \right)^{\frac{1}{2}} +
  \frac{1}{2}\right)^{-1} \left( \frac{1}{4} + \frac{T}{2 \pi
    x^2}\right)^{- \frac{1}{4}},\\
  f(x) & = \frac{1}{2} x^2 - \left(\frac{Tx^2}{2 \pi} + \frac{x^4}{4}
  \right)^{\frac{1}{2}} - \frac{T}{\pi} ar \sinh
  \left(\sqrt{\frac{\pi}{2 T}} \right).\\
  \mu(x) & = \frac{1}{2} x, \Phi (x) = x^{- \alpha}, F(x) = T.
\end{align*}

Then we obtain
\begin{lemma}\label{c2:lem2.3}
  For $AT^{\frac{1}{2}}< a < A'T^{\frac{1}{2}}$, $0 < A < A'$, $\alpha
  > 0$,
  {\selectfont{\fontsize{9}{7}{
  \begin{align}
&\int\limits_a^b \frac{\exp i\left\{4 \pi x \sqrt{n} - 2T ar \sinh
(x \sqrt{\pi/2T})- (2 \pi x^2 T + \pi^2 x^4)^{\frac{1}{2}}+ x^2
\right\}}{x^\alpha ar \sinh (x \sqrt{\pi 2 T}) 
\left(\left(\frac{1}{2} + \left(\frac{T}{2 \pi x^2} + \frac{1}{4}
\right)^{\frac{1}{2}} \right) \left(\frac{1}{4} + \frac{T}{2 \pi
x^2}\right)^{\frac{1}{4}}\right)}dx=4 \pi T^{-1}\notag\\ 
&\quad n^{\frac{1}{2} (\alpha-1)} \left(\log \frac{T}{2 \pi
n} \right)^{-1} \left(\frac{T}{2 \pi} - n \right)^{3/2-\alpha}
\exp \left\{i \left(T - T \log \left(\frac{T}{2 \pi n} \right)- 2
\pi n+\frac{\pi}{4} \right) \right\}\notag\\
&\quad + O(T^{- \frac{1}{2}\alpha} \min (1, |2 \sqrt{n}+ a - (a^2 +
2T/n)^{\frac{1}{2}}|^{-1})) + O \left(n^{\frac{1}{2}(\alpha-1)}
\left(\frac{T}{2 \pi} -n \right)^{1- \alpha} T^{- 3/2}\right),\label{c2:eq2.55}
\end{align}}}}
  provided that $n \geq 1$, $n < T/(2 \pi)$, $(T/(2 \pi)-n)^2>
  na^2$. If the last two conditions on $n$ are not satisfied, or if
  $\sqrt{n}$ is replaced by $- \sqrt{n}$, then the main term and the
  last error term on the right-hand side of \eqref{c2:eq2.55} are to
  be omitted.
\end{lemma}

\section{Completion of the Proof of the Mean Square
  Formulas}\label{c2:sec2.5}

Having at our disposal Lemmas \ref{c2:lem2.2} and \ref{c2:lem2.3} we
proceed to evaluate $I_n (1 \leq n \leq 4)$, as given by
\eqref{c2:eq2.31}- \eqref{c2:eq2.34}. We consider first $I_1$, taking
in Lemma \ref{c2:lem2.2} $0 < \alpha < 1$, $\alpha + \beta > \gamma$,
so that we may let $a \to 0$, $b \to \infty$. Hence, if
$\frac{1}{2} < \alpha < \frac{3}{4}$, $1 \leq k < AT$, we
obtain 
\begin{align}
&\int\limits_0^\infty \frac{\sin (T \log (1+ 1/y)) \cos (2 \pi
ky)}{y^\alpha (1+ y)^{\frac{1}{2}} \log (1+
1/y)}dy = (4k)^{-1}\times \notag\\
&\quad\left(\frac{T}{\pi} \right)^{\frac{1}{2}} \frac{\sin (TV
+ 2 \pi k U - \pi k+ \pi/4)}{VU^{\frac{1}{2}}\left(U-
\frac{1}{2}\right)^\alpha \left(U+ \frac{1}{2}
\right)^{\frac{1}{2}}} + O(T^{- \alpha/2}k^{(\alpha-3)/2}).\label{c2:eq2.56}
\end{align}

Since\pageoriginale this formula holds uniformly in $\alpha$, we may
put $\alpha= 1/2$. Taking into account that $\sin (x- \pi k)= (-1)^k
\sin x$ we obtain, after substituting \eqref{c2:eq2.56} with $\alpha =
1/2$ into \eqref{c2:eq2.31},
\begin{align}
I_1 &= O (T^{-1/4}) + 2^{- \frac{1}{2}} \sum_{n \leq N} (-1)^n d(n) 
n^{- \frac{1}{2}}\times\notag\\ 
&\quad\left\{\frac{\sin (2 T ar \sinh \sqrt{\pi n/2T} + \sqrt{2 \pi n T+
    \pi^2 n^2} + \pi/4)}{\left(ar \sinh \sqrt{\pi n/2T}) (T/2\pi n)+
  \frac{1}{4}\right)^{\frac{1}{4}}} \right\},\label{c2:eq2.57}
\end{align}
by taking $AT < N < A'T$. Similarly, using $\Delta  (x) \ll x^{1/3+
  \epsilon}$ we obtain from \eqref{c2:eq2.32}
\begin{equation}
  I_2 \ll |\Delta  (X)|X^{-1/2} \ll T^{\epsilon -
    1/6}.\label{c2:eq2.58} 
\end{equation}

To deal with $I_3$ we write \eqref{c2:eq2.33} in the form
\begin{equation}
  I_3 = - \frac{2}{\pi} (\log X +2 \gamma) I_{31}+ (\pi i)^{-1}
  I_{32},\label{c2:eq2.59} 
\end{equation}
and consider first $I_{31}$. We have
$$
\int\limits_0^\infty \frac{\sin (T \log (1+1/y)) \sin (2 \pi
  Xy)}{y^{3/2} (1+y)^{1/2} \log (1+ 1/y)} dy = \int\limits_0^{(2
  X)^{-1}}+ \int\limits^\infty_{(2X)^{-1}} \ll T^{-1/2}.
$$

Here we estimated the integral from 0 to $(2X)^{-1}$ by the second
mean value theorem for integrals (see \eqref{c2:eq2.51}). Namely
setting
$$
f(x) = x^{1/2}(1+x)^{1/2}/\log(1+1/x)
$$
we find that $f'(x)> 0$, hence the integral in question equals
{\fontsize{10}{12}\selectfont
\begin{align*}
 2 \pi X \int\limits_0^\xi \frac{\sin (T \log (1+ 1/y))}{y(1+y)} f(y)
  dy &= 2 \pi X f(\xi)\times\\
&\quad\int\limits_\eta^\xi \frac{\sin (T\log (1+1/y))}{y(1+y)}dy\\ 
&= 2 \pi X \xi^{\frac{1}{2}}(1+ \xi)^{\frac{1}{2}} (\log (1+
  1/\xi))^{-1}\times\\ 
&\quad\left\{T^{-1} \cos (T \log (1+ 1/y))
  \right\}\Bigg|^\xi_\eta \ll T^{- \frac{1}{2}},
\end{align*}}
where\pageoriginale $0 \leq \eta \leq \xi \leq (2 X)^{-1}$. The
remaining integral is estimated by Lemma \ref{c2:lem2.2} by treating
the main terms on the right-hand side of \eqref{c2:eq2.54} as an error
term.

Take next $I_{32}$ and write
\begin{align*}
  I_{32} & = \int\limits_0^\infty y^{-1} \sin (2 \pi X y) dy
  \int\limits_{\frac{1}{2}- iT}^{\frac{1}{2} + iT} \left(\frac{1+y}{y}
  \right)^u \frac{du}{u}\\
  & = \int\limits_0^1 \cdots dy + \int\limits_1^\infty \cdots dy =
  I'_{32} + I''_{32},
\end{align*}
say. Note that
\begin{align}
  \int\limits_0^1 y^{-1} \sin (2 \pi Xy) dy &= \int\limits_0^\infty
  y^{-1} \sin (2 \pi Xy) dy - \int\limits_1^\infty y^{-1} \sin (2 \pi
  X y) dy\notag\\
  & = \int\limits_0^\infty v^{-1} \sin v\, dv + \frac{y^{-1} \cos (2
    \pi Xy)}{2 \pi X}\Bigg|_1^\infty\notag\\
&\quad + \int\limits_1^\infty \frac{\cos (2 \pi Xy)}{2 \pi Xy^2} dy
   = \frac{\pi}{2} + O\left(\frac{1}{X} \right).\label{c2:eq2.60}
\end{align}

In $I'_{32}$ we have $0 < y \leq 1$, hence by the residue theorem
\begin{align*}
  \int\limits_{\frac{1}{2} - iT}^{\frac{1}{2} + iT}
  \left(\frac{1+y}{y}\right)^u \frac{du}{u} & = 2 \pi i - \left[
    \int\limits^{\infty + iT}_{\frac{1}{2} + iT}+ \int\limits_{-
      \infty - i T}^{\frac{1}{2} - i T}\right] \left(\frac{1+y}{y}
  \right)^u \frac{du}{u}\\
  & = 2 \pi i + O(T^{-1}y^{- 1/2}),
\end{align*}
since
$$
\int\limits_{\frac{1}{2} \pm iT}^{- \infty \pm i T}
\left(\frac{1+y}{y}\right)^u \frac{du}{u} \ll T^{-1} \int\limits_{-
  \infty}^{\frac{1}{2}} \left(\frac{1+y}{y}\right)^t dt \ll T^{-1}
y^{- 1/2}. 
$$

Hence 
  {\selectfont{\fontsize{9}{7}{
\begin{align*}
  I'_{32} & = 2 \pi i \int\limits_0^1 y^{-1} \sin (2 \pi Xy) dy + O
  (T^{-1} \int\limits_0^1 |\sin (2 \pi Xy)|y^{-3/2}dy)\\
  & = 2 \pi i \left(\frac{\pi}{2} \right) + O (X^{-1}) + O
  \left(T^{-1} \int\limits_0^{X^{-1}} Xy^{- \frac{1}{2}} dy\right) + O
  \left(T^{-1} \int_{X^{-1}}^\infty y^{- 3/2}dy\right) = \pi^2 i +
  O(T^{- \frac{1}{2}}).
\end{align*}}}}

Next, an integration by parts gives
{\selectfont{\fontsize{9}{7}{
\begin{multline*}
  I''_{32} = \int\limits_1^\infty y^{-1} \sin (2 \pi x y) dy
  \int\limits_{\frac{1}{2}- iT}^{\frac{1}{2} + iT} \left(\frac{1+y}{y}
  \right)^u \frac{du}{u} = \left.\left\{- \frac{\cos (2 \pi Xy)}{2 \pi Xy}
  \int\limits_{\frac{1}{2}- iT}^{\frac{1}{2} + iT} \left(\frac{1+y}{y}
  \right)^u \frac{du}{u} \right\} \right|_1^\infty\\
  - \int\limits_1^\infty \frac{\cos (2 \pi Xy)}{2 \pi X y^2} dy
  \int\limits_{\frac{1}{2}- iT}^{\frac{1}{2} + iT} \left(
  \frac{1+y}{y}\right)^u \frac{du}{u}\\ 
  - \int\limits_1^\infty
  \frac{\cos (2 \pi Xy)}{2 \pi Xy} \int\limits_{\frac{1}{2}-
    iT}^{\frac{1}{2}+ iT} \left(\frac{1+y}{y} \right)^{u-1} y^{-2} du
  \ll T^{-1} \log T,
\end{multline*}}}}
since\pageoriginale for $y\geq 1$
$$
\int\limits_{\frac{1}{2} - iT}^{\frac{1}{2} +iT} \left(\frac{1+y}{y}
\right)^u \frac{du}{u} \ll \int\limits_{\frac{1}{2}- iT}^{\frac{1}{2}+
iT} \left|\frac{du}{u} \right|  \ll \log T,
$$
so that finally
\begin{equation}
  I_3 = \pi + O(T^{- 1/2} \log T). \label{c2:eq2.61}
\end{equation}

It remains yet to evaluate $I_4$, as given by \eqref{c2:eq2.35}, which
will yield the terms $-2 \sum\limits_{n \leq N'} \cdots $ in \eqref{c2:eq2.5}
in the final formula for $E(T)$. We estimate first the inner integrals
in \eqref{c2:eq2.35}, making $a \to 0$, $b \to \infty$ in Lemma
\ref{c2:lem2.2}. We have then in the notation of Lemma
\ref{c2:lem2.2}, for $k= x> AT$,
\begin{multline*}
  \int\limits_0^\infty \frac{\cos (T \log (1+ 1/y)) \cos (2 \pi
    xy)}{y^{1/2} (1+y)^{3/2} \log (1+ 1/y)} dy\\
  = (4x)^{-1} \left(\frac{T}{\pi} \right)^{\frac{1}{2}} \frac{\cos (TV
    + 2 \pi x U - \pi x + \frac{1}{4} \pi)}{VU^{1/2} \left(U-
    \frac{1}{2}\right)^{1/2} \left(U + \frac{1}{2}\right)^{3/2}}+ O
  \left( T x^{-1/2}\right),
\end{multline*}
and similarly for $r= 1, 2$,
\begin{multline*}
  \int\limits_0^\infty \frac{\sin (T \log (1+ 1/y))\cos (2 \pi x
    y)}{y^{1/2} (1+ y)^{3/2} (\log (1+ 1/y))^r} dy\\
  = O \left(T^{1/2} \left(U- \frac{1}{2}\right)^{-1/2}x^{-1}\right) +
  O\left(T^{-1} x^{-1/2} \right)= O(x^{-1/2}).
\end{multline*}

Thus we have
{\fontsize{10}{12}\selectfont
\begin{multline*}
  I_4 = \int\limits_X^\infty \frac{\Delta  (x)}{x}\times\\ 
  \left\{\frac{T
    \cos (2 T ar \sinh \sqrt{\pi x/2T} + (2 \pi xT + \pi^2
    x^2)^{\frac{1}2} - \pi x+ \pi/4}{(\sqrt{2 x} ar \sinh \sqrt{\pi
      x/2 T}) \left( \left( \frac{T}{2 \pi x} +
    \frac{1}{4}\right)^{\frac{1}{2}}+ \frac{1}{2} \right) \left(
    \frac{T}{2 \pi x} + \frac{1}{4}\right)^{1/4}}+ O (x^{-1/2}) \right\}dx.
\end{multline*}}

Now\pageoriginale from \eqref{c2:eq2.25} it follows without difficulty
that $\Delta  (x)$ is $\approx x^{1/4}$ in mean square (see Theorem
\ref{c2:thm2.5} for a sharp result). Hence by the Cauchy-Schwarz
inequality and integration by parts we obtain
\begin{align}
  & \int\limits_X^\infty x^{-1} \Delta  (x) O (x^{-1/2}) dx \ll
  \left(\int_X^\infty x^{- 4/3} dx \right)^{1/2}
  \left(\int\limits_X^\infty x^{-5/3} \Delta ^2 (x) dx
  \right)^{1/2}\label{c2:eq2.62}\\
  & \ll X^{-1/6} \left\{(x^{-5/3} \int\limits_0^x \Delta ^2 (t) dt)
  \Bigg|_X^\infty + \frac{5}{3} \int\limits_X^\infty
  \left(\int\limits_0^x \Delta ^2 (t) dt \right) x^{- 8/3}dx
  \right\}^{1/2}\notag\\
  & \ll X^{-1/6}X^{3/4-5/6}= X^{-1/4}.\notag
\end{align}

Changing the variable $x$ to $x^{1/2}$ in the integral for $I_4$ and
using \eqref{c2:eq2.24} we obtain
\begin{align}
  & I_4 = \frac{T}{\pi} \sum_{n=1}^\infty d(n) n^{-3/4}
  \times \label{c2:eq2.63}\\ 
  &  \int\limits_{X^{1/2}}^\infty \frac{\cos \left\{2T ar \sinh
    (x \sqrt{\pi /2T})+ (2 \pi^2 x^2 T + \pi^2 x^4)^{1/2}- \pi x^2+
    \pi/4 \right\}}{x^{3/2} \left(ar \sinh (x \sqrt{\pi 2/T})
    \left(\frac{T}{2 \pi x^2} + \frac{1}{4} \right)^{1/2} +
    \frac{1}{2}\right) \left(\frac{T}{2 \pi x^2} + \frac{1}{4}
    \right)^{1/4}}\times\notag\\ 
  &  \left\{\cos (4 \pi x\sqrt{n} - \frac{1}{4} \pi) - 3 (32 \pi
  x \sqrt{n})^{-1} \sin (4 \pi x \sqrt{n} - \frac{1}{4}\pi) \right\}dx +
  O(T^{- \frac{1}{4}})\notag\\
  & = \frac{T}{\pi} \sum^\infty_{n=1} d(n) n^{- 3/4}J_n + O(T^{- 1/4}),\notag
\end{align}
say. Strictly speaking, we should write the integral for $I_4$ ad 
$$
\lim\limits_{b \to \infty} \int\limits_{X}^{b^2} x^{-1} \Delta  (x)
(\ldots) dx
$$
and then apply this procedure, since the series for $\Delta  (x)$ is
boundedly convergent in finite intervals only. But it is not difficult
to see that working with $\displaystyle{\lim\limits_{b \to \infty}}$
leads to the same final result.

Now it is clear why we formulated Lemma \ref{c2:lem2.3}. It is needed
to evaluate the integral $J_n$ in \eqref{c2:eq2.63}. Indeed, if
$\left(\frac{T}{2 \pi} - n \right)^2> nX$, $n < T/(2 \pi)$, that is to
say if 
\begin{equation}
  n < \frac{T}{2 \pi} + \frac{X}{2} - \left(\frac{X^2}{4} +
  \frac{XT}{2 \pi} \right)^{1/2}= Z,\label{c2:eq2.64}
\end{equation}
then\pageoriginale an application of Lemma \ref{c2:lem2.3} gives, with
$\alpha= 3/2$, $\alpha = 5/2$,
{\selectfont{\fontsize{10}{8}{
\begin{align*}
  &I_4 = 2 \sum_{n < Z} d(n) n^{- 1/2} \left(\log \frac{T}{2 \pi
    n}\right)^{-1} \cos \left(T \log \left(\frac{T}{2 \pi n} \right)-
  T + \frac{\pi}{4} \right)\\
  &+ O \left( \sum_{n < Z} d(n) n^{- 1/2} (T- 2 \pi n)^{-1}\right) + O
  \left(T^{- 1/2} \sum_{n < Z} d(n) n^{-1/2} \sum_{n < Z} (T- 2 \pi
  n)^{-1/2}\right) \\
  &+ O \left(T^{1/4} \sum_{n=1}^\infty d(n) n^{-3/4} \min (1,|
  2\sqrt{n}+ \sqrt{X} - (X+ T/2\pi))^{1/2} |^{-1}\right) + O
  (T^{-1/4})\\
  & = I_{41} + O(I_{42}) + O(I_{43}) + O(I_{44})+ O(T^{-1/4}),
\end{align*}}}}
say, Here $I_{41}$ contributes the second main term on the right-hand
side of \eqref{c2:eq2.5}, while the contribution of the other error
terms ($I_{42}$comes from applying Lemma \ref{c2:lem2.3} to estimate
the sine terms in \eqref{c2:eq2.63} with $\alpha= 5/2$) is $O(\log^2
T)$. To see this, observe that in view of $AT < X < A'T$ we have $Z\ll
T$, $T/(2 \pi) - Z \gg T$. Hence
\begin{align*}
  I_{42} & \ll T^{-1} \sum_{n \leq Z}d(n) n^{-1/2} \ll T^{-1/2} \log
  T,\\
  I_{43} & \ll T^{- 1/2} T^{-1/2} \sum_{n \leq Z} d(n) n^{-1/2} \ll
  T^{-1/2} \log T,
\end{align*}
and it remains yet to deal with $I_{44}$. Since
$$
\left(\frac{1}{2} \sqrt{X+ \frac{2T}{\pi}} - \frac{1}{2} \sqrt{X}
\right)^2 = \frac{X}{2} + \frac{T}{2 \pi} - \sqrt{\frac{X^2}{4} +
  \frac{XT}{2 \pi}}= Z,
$$
we have
\begin{align*}
   I_{44} & \ll T^{1/4} \sum_{n=1}^\infty d(n) n^{-3/4} \min (1,
  |n^{1/2}- Z^{1/2}|^{-1})\\
  & = T^{1/4} \left( \sum_{n \leq 1/2 Z} + \sum_{\frac{1}{2} Z < n
    \leq  Z- Z^{1/2}} + \sum_{Z- Z^{1/2} < n \leq Z + Z^{1/2}} +
  \sum_{Z+ Z^{1/2} < n \leq 2Z}+ \sum_{n > 2Z}\right)\\
  & = T^{1/4} (S_1 + S_2 + S_3 + S_4 + S_5),
\end{align*}
say. Using partial summation and the crude formula
$\displaystyle{\sum_{n \leq x} d(n) \sim x \log x}$, we obtain
{\selectfont{\fontsize{10}{8}{
\begin{align*}
  S_1 & = \sum_{n \leq \frac{1}{2} Z} d(n) n^{- 3/4} (Z^{1/2} -
  n^{1/2})^{-1} \ll Z^{- \frac{1}{2}} \sum_{n \leq \frac{1}{2}Z} d(n)
  n^{-3/4}  \ll T^{- 1/4} \log T,\\
  S_2 & = \sum_{\frac{1}{2} Z< n \leq Z- Z^{1/2}} d(n) n^{- 3/4}
  (Z^{1/2} - n^{1/2})^{-1} \ll Z^{-1/4} \sum_{\frac{1}{2} Z< n \leq Z-
  Z^{1/2}} d(n) (z- n)^{-1}\\
  & \ll T^{-1/4} \sum_{Z^{1/2} \leq k \leq \frac{1}{2} Z} d([Z]- k)
  k^{-1} \ll T^{-1/4} (Z (\log Z)Z^{-1} + \int\limits_{Z^{1/2}}^{Z} t
  \log t \frac{dt}{t^2})\\
  & \ll T^{-1/4} \log^2 T,\\
  S_3 & = \sum_{Z-Z^{1/2} < n\leq Z+ Z^{1/2}} d(n) n^{- 3/4} \ll
  T^{-1/4 \log T},
\end{align*}}}}
while\pageoriginale
$$
S_4 \ll T^{-1/4} \log^2 T
$$
follows analogously as the estimate for $S_2$. Finally
$$
S_5 \ll \sum_{n > 2Z} d(n) n^{- 3/4} (n^{1/2}- Z^{1/2})^{-1} \ll
\sum_{n > 2Z} d(n) n^{- 5/4} \ll T^{- 1/4} \log T.
$$

Therefore we obtain
\begin{equation}
  I_4 = 2 \sum_{n \leq Z} d(n) n^{-1/2} \left(\log \frac{T}{2 \pi n}
  \right)^{-1} \cos \left(T \log \left(\frac{T}{2 \pi n}\right) - T +
  \frac{\pi}{4} \right)+ O (\log^2 T).\label{c2:eq2.65}
\end{equation}

It remains to note that in \eqref{c2:eq2.61} the limit of summation
$Z$ may be replaced by 
$$
N' = N' (T) = \frac{T}{2 \pi} + \frac{N}{2} - \left(\frac{N^2}{4} +
\frac{NT}{2 \pi} \right)^{1/2},
$$
as in the formulation of Theorem \ref{c2:thm2.1}, with a total error
which is\break $O (\log^2 T)$. Theorem \ref{c2:thm2.1} follows now from
\eqref{c2:eq2.29}, \eqref{c2:eq2.57}, \eqref{c2:eq2.58},
\eqref{c2:eq2.61} and \eqref{c2:eq2.65} if $N$ is an integer. If $N$
is not an integer, then in \eqref{c2:eq2.5} we replace $N$ by $[N]$
again with an error which is $O(\log^2T)$. 

It remains yet to indicate how the expression \eqref{c2:eq2.44} for
$E_\sigma (T)$,\pageoriginale with $G_j (1 \leq j\leq 4)$ given by
\eqref{c2:eq2.45} - \eqref{c2:eq2.48}, is transformed into the one
given by Theorem \ref{c2:thm2.2}. Firstly, applying Lemma
\ref{c2:lem2.2} to \eqref{c2:eq2.45}, we obtain (analogously as for
$I_1$ given by \eqref{c2:eq2.31})
\begin{multline*}
  G_1 = 2^{\sigma-1} i \left(\frac{\pi}{T} \right)^{\sigma- 1/2}
  \sum_{n \leq N} (-1)^n n^{\sigma-1} \sigma_{1- 2 \sigma} (n) \left(ar
  \sinh \sqrt{\frac{\pi n}{2 T}}\right)^{-1}\\ 
  \left(\frac{T}{2 \pi n} +
  \frac{1}{4}\right)^{-1/4} \cos f(T, n) + O(T^{1/4-\sigma}). 
\end{multline*}

Secondly, the estimate
$$
G_2 \ll T^{(1- 4 \sigma)/(2+ 8\sigma)+ \epsilon}
$$
follows from \eqref{c2:eq2.46}, \eqref{c2:eq2.41} and Lemma
\ref{c2:lem2.2}. Thirdly, starting from \eqref{c2:eq2.47} and using a
technique similar to the one used in obtaining \eqref{c2:eq2.61}, we
arrive at 
\begin{equation}
  G_3 = i \pi (1- 2 \sigma) (2 \pi)^{2 \sigma-1} \frac{\zeta (2 - 2
    \sigma)}{\Gamma (2 \sigma) \sin (\sigma \pi)} + O
  (T^{\sigma-1}).\label{c2:eq2.66} 
\end{equation}

Note that in \eqref{c2:eq2.66} the main term tends to $\pi$ as $\sigma
\to \frac{1}{2} +0$ in analogy with \eqref{c2:eq2.61}. We also remark
that in proving \eqref{c2:eq2.66}, instead of \eqref{c2:eq2.60}, we
use the formula
\begin{equation}
\label{c2:eq2.67}
\int\limits_0^\infty w^{- 2 \sigma} \sin w dw = \frac{\pi}{2 \Gamma (2
  \sigma) \sin (\pi \sigma)},
\end{equation}
which follows with $p=1-2 \sigma$ from the well-known formula
\begin{align*}
  \int\limits_0^\infty t^{p-1} \sin t\, dt & = \Gamma (p) \sin
  \left(\frac{p \pi}{2} \right) & (-1 < p < 0)
\end{align*}
and the relation $\Gamma (s) \Gamma (1-s) = \pi /\sin (\pi s)$.

To evaluate $G_4$ we apply Lemma \ref{c2:lem2.2} to the inner integral
in \eqref{c2:eq2.48} to obtain
\begin{align}
   G_4 & = O\left( \int_X^\infty | \Delta _{1- 2\sigma} (x)|
  x^{\sigma-2} T^{1/2 - \sigma} dx\right)\notag\\
 &\quad + 2^{\sigma -1} i \pi^{\sigma- 1/2} \int\limits_X^\infty
   \Delta _{1- 1 \sigma}(x) T^{3/2- \sigma} x^{\sigma -2}\times\notag\\ 
  &\quad\left(ar \sinh
  \sqrt{\frac{\pi x}{2 T}}\right) \left(\frac{T}{2 \pi x} +
  \frac{1}{4} \right)^{-1/4}\times \notag\\
  &\quad\left( \left( \frac{T}{2 \pi x} + \frac{1}{4}\right)^{1/2} +
  \frac{1}{2} \right)^{-1} \cos (f(T, x)) dx,\label{c2:eq2.68}
\end{align}
where\pageoriginale $f(T, x)$ is as in the formulation of Theorem
\ref{c2:thm2.1}. From \eqref{c2:eq2.40} it follows that $\Delta _{1-
  2 \sigma} (x)$ is $\approx x^{3/4 - \sigma}$ in mean square, hence
as in the corresponding estimate in \eqref{c2:eq2.62} for $I_4$ it
follows that the contribution of the error term in \eqref{c2:eq2.68}
is $O (T^{1/4 - \sigma})$.

To evaluate the main term in \eqref{c2:eq2.68} we proceed as in the
evaluation of $I_4$ (see \eqref{c2:eq2.63}), only instead  of
$\Delta  (x)$ we use the formula \eqref{c2:eq2.38} for $\Delta _{1
- 2 \sigma} (x)$. To be rigorous, one should treat $\int_X^\infty$ as
$\displaystyle{\lim\limits_{b \to \infty} \int\limits_X^{b^2}}$, since
$\Delta _{1- 2 \sigma} (x)$ is boundedly convergent only in finite
intervals. However, in both cases the final result will be the same,
namely
{\fontsize{10}{12}\selectfont
$$
G_4 = 2i \left(\frac{2 \pi}{T}\right)^{\sigma- 1/2} \sum_{n \leq N'}
\sigma_{1- 2 \sigma} 
(n) n^{\sigma-1} \left(\log \frac{T}{2 \pi n} \right)^{-1} \cos (g(T, 
n)) + O(\log T).
$$}

The error term $O (\log T)$, which is better than the error term\break
$O(\log^2 T)$ appearing in the expression \eqref{c2:eq2.65} for $I_4$,
comes from the fact that
\begin{align*}
& \sum_{n \leq x} \sigma_{1- 2 \sigma} (n) \sim \zeta (2 \sigma) x &
(\sigma > \frac{1}{2}, x \to \infty),\\
\intertext{whereas}
& \sum_{n \leq x}d(n) \sim x \log x & (x \to \infty),
\end{align*}
and this accounts for the saving of a log-factor. This completes our
discussion of the proof of Theorem \ref{c2:thm2.2}.

\section{The Mean Square Formula For $E(T)$}\label{c2:sec2.6}

From the expressions for $E(T)$ and $E_\sigma (T)$, given by Theorem
\ref{c2:thm2.1} and Theorem \ref{c2:thm2.2}, one can obtain by
squaring and termwise integration the asymptotic formula
\begin{equation}
  \int\limits_2^T E^2 (t) dt = c T^{3/2} + O(T^{5/4} \log^2
  T)\label{c2:eq2.69} 
\end{equation}
with
\begin{equation}
  c= \frac{2}{3} (2 \pi)^{-1/2} \sum_{n=1}^\infty d^2 (n) n^{-3/2} =
  \frac{2}{3} (2 \pi)^{-1/2} \frac{\zeta^4 (3/2)}{\zeta(3)} =
  10.3047\ldots.\label{c2:eq2.70}  
\end{equation}

Similarly\pageoriginale for $\sigma$ fixed satisfying $1/2 < \sigma <
3/4$ we obtain 
\begin{equation}
  \int\limits_2^T E^2 (t) dt = c(\sigma) T^{5/2-2 \sigma} + O
  (T^{7/4-\sigma} \log T)\label{c2:eq2.71}
\end{equation}
with
\begin{equation}
  c(\sigma) = \frac{2}{5- 4\sigma} (2 \pi)^{2 \sigma - 3/2}
  \sum_{n=1}^\infty \sigma^2_{1- 2 \sigma} (n) n^{2 \sigma -
    5/2}\label{c2:eq2.72}  
\end{equation}

From \eqref{c2:eq2.69} and \eqref{c2:eq2.71} we can get weak omega
results for $E(T)$ and $E_\sigma(T)$ (recall that $f(x) = \Omega
(g(x))$ means that $\displaystyle{\lim\limits_{x \to \infty} f(x)
  /g(x)=0}$ does not hold). For example, \eqref{c2:eq2.69} implies
\begin{equation}
  E(T) = \Omega (T^{1/4}),\label{c2:eq2.73}
\end{equation}
and \eqref{c2:eq2.71} gives
\begin{equation}
  E_\sigma (T) = \Omega (T^{3/4-\sigma}).\label{c2:eq2.74}
\end{equation}

There are reasons to believe that \eqref{c2:eq2.73} and
\eqref{c2:eq2.74} are fairly close to the actual order of magnitude of
$E(T)$ and $E_\sigma(T)$ (for $1/2 < \sigma < 3/4$), as one may
conjecture that $E(T) \ll T^{1/4+\epsilon}$ and $E_\sigma (T) \ll
T^{3/4- \sigma + \epsilon}$ (for $1/2 < \sigma < 3/4$). The proofs of
both of these bounds are out of reach at present; the conjectural
bound for $E(T)$ implies by Theorem \ref{c2:thm2.2} that $\zeta (1/2 +
it)\ll t^{1/8+
 \epsilon}$. The main aim of this section is to prove a sharpening of
\eqref{c2:eq2.69}. We remark that by the same method a sharpening of
\eqref{c2:eq2.71} may be also obtained, but we shall treat only the
more interesting case of $E(T)$. In Chapter \ref{c3} we shall deal with the
omega results, and we shall improve \eqref{c2:eq2.74} a little and
\eqref{c2:eq2.73} substantially, obtaining $\Omega_{\pm}-$results for
$E(T)$ which correspond to the best known $\Omega_{\pm}-$results for
$\Delta  (x)$. Our main result is 

\begin{thm}\label{c2:thm2.4}
  With $c$ given by \eqref{c2:eq2.70} we have
  \begin{equation}
    \int\limits_2^T E^2 (t) dt = c T^{3/2} + O(T \log^5
    T).\label{c2:eq2.75}  
  \end{equation}
\end{thm}

This result corresponds to the mean square result for 
$$
\Delta  (x) = \sideset{}{'}\sum_{n \leq x} d(n) - x(\log x + 2
\gamma -1) - \frac{1}{4},
$$
which\pageoriginale also shows the analogy between $E(T)$ and
$\Delta  (x)$. The mean square result for $\Delta  (x)$ is
contained in 

\begin{thm}\label{c2:thm2.5}
  \begin{equation}
    \int\limits_2^X \Delta ^2 (x) dx = \frac{\zeta^4(3/2)}{6 \pi^2
      \zeta(3)} X^{3/2} + O(X \log^5 X).\label{c2:eq2.76}
  \end{equation}
\end{thm}

We note that from \eqref{c2:eq2.75} we can obtain an order estimate
for $E(T)$, which is much better than $E(T) \ll T^{1/2}\log T$, a
result that is a trivial consequence of Atkinson's formula
\eqref{c2:eq2.5}. Namely, from the definition
$$
E(T) = \int\limits_0^T |\zeta (1/2 + it)|^2 dt - T \left(\log
\frac{T}{2 \pi} + 2 \gamma -1 \right)
$$
we find that, for $0\leq x \leq T$,
\begin{equation}
  E(T+x) - E(T) \geq - 2 Cx \log T\label{c2:eq2.77}
\end{equation}
for $T \geq T_0$ and some absolute $C> 0$. Hence
\begin{align*}
  \int\limits_T^{T+x} E(t) dt & = x E (T) + \int_0^x (E(T+ u)- E
  (T))du\\
  & \geq x E(T) - 2 C \log T \int_0^x u \, du = x E(T) - Cx^2 \log T.
\end{align*}

Therefore we obtain
\begin{equation}
  E(T) \leq x^{-1} \int\limits_T^{T+x} E(t) dt + Cx \log T \;  (0 < x \leq
  T, T \geq T_0),\label{c2:eq2.78}
\end{equation}
and analogously
\begin{equation}
  E(T) \geq x^{-1} \int\limits^T_{T-x} E(t) dt - Cx \log T \; (0 < x \leq
  T, T \geq T_0).\label{c2:eq2.79}
\end{equation}

Combining \eqref{c2:eq2.78} and \eqref{c2:eq2.79}, using the
Cauchy-Schwarz inequality and Theorem \ref{c2:thm2.4} we obtain
\begin{align*}
  |E(T)| &\leq x^{-1} \int\limits_{T-x}^{T+x} |E (t) |dt + 2 Cx \log
  T\\
  & \leq x^{-1/2} \left(\int\limits_{T-x}^{T+x} E^2 (t) dt
  \right)^{1/2} + 2 C x \log T\\
  & = x^{-1/2} (c(T + x)^{3/2}- c(T- x)^{3/2} + O(T \log^5 T))^{1/2} +
  2 C x \log T\\
  & \ll T^{1/4} + T^{1/2} x^{- 1/2} \log^{5/2}T + x \log T \ll T^{1/3}
  \log^2 T
\end{align*}
with\pageoriginale the choice $x= T^{1/3} \log T$. Therefore the bound
\begin{equation}
  E(T) \ll T^{1/3} \log^2 T \label{c2:eq2.80}
\end{equation}
is a simple corollary of Theorem \ref{c2:thm2.4}. the foregoing
argument gives in fact a little more. Namely, if we define $F(T)$ by 
\begin{equation}
  \int\limits_2^T E^2 (t) dt = c T^{3/2} + F(T),\label{c2:eq2.81}
\end{equation}
and $\eta$ is the infimum of numbers $d$ such that $F(T) \ll T^d$,
then 
\begin{equation}
  E(T) \ll T^{\eta/3 + \epsilon}. \label{c2:eq2.82}
\end{equation}

Since $E(T) = \Omega (T^{1/4})$ by \eqref{c2:eq2.73}, this means that
we must have $3/4 \leq \eta \leq 1$, the upper bound being true by
Theorem \ref{c2:thm2.4}. It would be interesting to determine $\eta$,
although this problem looks difficult. My (optimistic) conjecture is
that $\eta \geq 3/4$. An $\Omega$-result for $F(T)$, which is sharper
than just $\eta \geq 3/4$, is given by Theorem \ref{c3:thm3.8}. We also
remark here that the bound in \eqref{c2:eq2.80} was obtained without
essentially taking into account the structure of the exponential sums
(with the divisor function $d(n)$) that appear in Atkinson's formula
for $E(T)$. The use of the exponential sum techniques improves
\eqref{c2:eq2.80}. A brief discussion of this topic will be given in
Section \ref{c2:sec2.7}.

Before we pass to the proof of Theorem \ref{c2:thm2.4} and Theorem
\ref{c2:thm2.5}, we remark that Theorem \ref{c2:thm2.4} was recently
proved independently by Y. Motohashi and T. Meurman. We shall only
outline the salient points of Motohashi's proof, as it is based on his
deep work on the approximate functional equation for $\zeta^2 (s)$,
which represents the analogue of the classical Riemann-Siegel formula
for $\zeta(s)$. We state Motohashi's\pageoriginale formula for $E(T)$
as

\begin{thm}\label{c2:thm2.6}
  Let $\delta > 0$ be a small constant and $\delta \leq \alpha < \beta
  \leq 1 - \delta$. Define
  $$
  \lambda (x) =
  \begin{cases}
    1 & \text{if}~ 0 \leq x \leq \alpha,\\
    (\beta -x)/(\beta- \alpha) & \text{if}~ \alpha \leq x \leq
    \beta,\\
    0 & \text{if}~ \beta \leq x \leq 1,
  \end{cases}
  $$
  $\omega (n) = \lambda \left( \frac{2 \pi n}{T}\right), \ob{\omega}
  (n)=1- \lambda \left(\exp \left(-2 ar \sinh \sqrt{\frac{\pi n}{2T}}
  \right)\right)$. Then we have, with an absolute constant $c_0$ and
  $T(\alpha) = \frac{T}{2 \pi \alpha} (1- \alpha)^2$,   
  \begin{align}
     E(T) &= 2^{-1/2} \sum_{n \leq T (\alpha)}  (-1)^{n} \ob{\omega}
    (n) d(n) n^{- 1/2} \left( ar \sinh \sqrt{\frac{\pi
        n}{2}}\right)^{-1} \left(\frac{T}{2 \pi n} + \frac{1}{4}
    \right)^{- 1/4}\notag\\ 
    &\times\cos (f (T, n)) - \sum_{n \leq \beta T/(2 \pi)} \omega (n) d (n) n^{- 1/2}
    \left(\log \frac{T}{2 \pi n} \right)^{-1} \cos (g (T, n))\notag\\ 
    &\qquad + c_0 + O(T^{-1/4}) +  O ( (\beta - \alpha)^{-1} T^{-1/2}\log
    T)\notag\\ 
    &\qquad\qquad + O \left((\beta- \alpha)^{- 1/2} T^{- 1/2}
    \log^{5/2}T\right).\label{c2:eq2.83}
  \end{align}
\end{thm}

In this formula $f(T, n)$ and $g(T, n)$ are as in Theorem
\ref{c2:thm2.1}. If we consider the special case when $\beta = \alpha
+ T^{-1/2}$, replace $\omega$ and $\ob{\omega}$ by 1 with total error
$O (\log T)$, we obtain Atkinson's formula \eqref{c2:eq2.5} with $N=
T(\alpha)$ and $\delta \leq \alpha \leq 1 - \delta$, only with the
error term $O(\log T)$, which is better than $O(\log^2 T)$ in
Atkinson's formula. This saving of a log-factor comes essentially from
the smoothing technique inherent in Motohashi's approach. The
asymptotic formula \eqref{c2:eq2.83} is strong enough to produce, by
standard termwise integration technique, the asymptotic formula
\eqref{c2:eq2.75} with the good error term $O(T \log^5 T)$.

The proof of \eqref{c2:eq2.83} is based on Motohashi's expression for 
\begin{equation}
R_2 (s, x): = \zeta^2 (s) - \sideset{}{'}\sum_{n \geq x} d(n) n^{-s} -
\chi^2 (s) \sideset{}{'}\sum_{n \leq y} d(n) n^{s-1},\label{c2:eq2.84}
\end{equation}
where $xy = (t/(2 \pi))^2$. In the most important case when $x= y=
t/(2 \pi)$, his asymptotic formula reduces to 
\begin{equation}
  \chi (1-s) R_2 \left(s, \frac{t}{2 \pi} \right)= - 2
  \left(\frac{t}{\pi}\right)^{-1/2} \Delta  \left(\frac{t}{2 \pi}
  \right)+ O \left(t^{-1/4} \right),\label{c2:eq2.85}
\end{equation}
which\pageoriginale shows the explicit connection between the divisor
problem and the approximate functional equation for $\zeta^2
(s)$. Using the functional equation $\zeta(s) = \chi (s) \zeta(1-s)$
and \eqref{c2:eq2.84} one can write
\begin{multline*}
|\zeta (1/2 + it)|^2 = 2 \re \left\{\chi (1/2 - it) \sideset{}{'}
\sum_{n \leq t/2\pi} d(n) n^{-1/2- it}\right\} \\
+ \chi (1/2 - it) R_2 \left(1/2 + it, \frac{t}{2 \pi}\right).
\end{multline*}

Integration gives 
$$
\int\limits_0^T |\zeta (1/2 + it)|^2 dt = I_1 (T) + I_2 (T),
$$
say, where
\begin{align*}
  I_1 (T) & = 2 \re \left\{\sum_{n \leq T/2 \pi} d(n) n^{-1/2}
  \int\limits_{2 \pi n}^T \chi (1/2 - it) n^{-it} dt \right\},\\
  I_2 (T) & = \int\limits_0^T \chi (1/2 - it) R_2 \left(1/2 + it,
  \frac{t}{2 \pi}\right) dt.
\end{align*}

Quite a delicate analysis is required to evaluate $I_1(T)$, which
necessitates the use of the saddle-point method to evaluate certain
exponential integrals. The perturbation device
$$
I_1 (T) = \frac{1}{\beta - \alpha} \int_\alpha^\beta I_1 (T) d \xi
$$
induces a lot of cancellations, when the sum over $n$ in the
expression for $I_1(T)$ is split into two parts according to whether
$n \leq \xi T/(2 \pi)$ or $\xi T/(2 \pi) < n \leq T/(2 \pi) \;  (\delta
\leq \xi \leq 1 -\delta)$. The integral $I_2(T)$ is evaluated with the aid
of the $\zeta^2$-analogue of the Riemann-Siegel formula.

In Meurman's proof of Theorem \ref{c2:thm2.4}, which we proceed to
present now, one first derives in fact a special case of Motohashi's
Theorem \ref{c2:thm2.6}, which essentially corresponds to the case
$\beta - \alpha \approx T^{-1/4}$. To obtain this we return to the
proof of Atkinson's formula \eqref{c2:eq2.5}. Analyzing the proof, it
is seen that one gets
\begin{multline*}
  E(T) = \left(\frac{2T}{\pi} \right)^{-\frac{3}{4}} \sum_{n \leq N} (-)^n
  d(n) n^{- 1/4} e (T, n) \cos (f(T, n)) \\
  - \frac{T}{\pi} \sum^\infty_{n=1} d(n) n^{-3/4} J_n (T, \sqrt{N}) +
  \pi+ O (|\Delta  (N)|N^{-1/2})\\ 
   + O \left(\int^\infty_{N} X^{-3/2}
  |\Delta  (x) | dx \right) + O (T^{-1/4}),
\end{multline*}
where\pageoriginale $T \ll N \ll T$, $f(T, n)$ is as in Theorem
\ref{c2:thm2.1}, 
\begin{align*}
  e(T, n) & = \left(1+ \frac{\pi n}{2T}\right)^{-1/2}
  \left(\sqrt{\frac{2T}{\pi n}} ar \sinh \sqrt{\frac{\pi n}{2T}}
  \right)^{-1},\\
  J_n (T, Y) & = \int\limits_Y^\infty g_{3/2} (x) \cos \left(f (T, x^2) - \pi
  x^2 + \frac{\pi}{2} \right)\\
  & \quad \left\{ \cos \left(4 \pi  x \sqrt{n}- \frac{\pi}{4}\right) - Cx^{-1}
  n^{-1/2} \sin \left(4 \pi x \sqrt{n} - \frac{\pi}{4}
  \right)\right\}dx,\\
  g_\alpha (x) & = \left\{x^\alpha ar \sinh \left(x
  \sqrt{\frac{\pi}{2T}}\right) \left( \left( \frac{T}{2 \pi x^2} +
  \frac{1}{4} \right)^{1/2}+ \frac{1}{2} \right) \left(\frac{T}{2 \pi
    x^2} + \frac{1}{4} \right)^{1/4} \right\}^{-1}
\end{align*}
and $C$ is a constant. Now we average the expression for $E(T)$ by
taking $N= (a+ u)^2$, integrating over $0 \leq u \leq U$ and dividing
by $U$. Choosing $T^{1/4} \ll U \ll T^{1/4}$ we have $T^{1/2} \ll a
\ll T^{1/2}$. Since $\Delta  (x)$ is $\approx x^{1/4}$ in mean
square we easily obtain
$$
\int\limits_{N}^\infty x^{-3/2} |\Delta  (x)|dx \ll T^{-1/4}
$$
without the averaging process. But by using Theorem \ref{c2:thm2.5}
(whose proof is independent of Theorem \ref{c2:thm2.4}) we obtain 
{\selectfont{\fontsize{10}{8}{
\begin{multline*}
  U^{-1} \int\limits_0^U |\Delta  (N)| N^{-1/2} du \ll T^{-1/2}
  U^{-1} \int\limits_0^U |\Delta  ((a+u)^2)| du\\
  \ll T^{-1/2}U^{-1/2} \left(\int\limits_{a^2}^{(a+ U)^2} \Delta ^2
  (v) v^{-1/2} dv \right)^{1/2}   \ll T^{- 3/4} U^{-1/2} \left(\int\limits_{a^2}^{a^2 + 4 aU}
  \Delta ^2 (v) dv \right)^{1/2}\\ 
\ll T^{-3/4} U^{-1/2} (Ua^2 + a^2  \log ^5 T)^{1/2} \ll T^{-1/4}.
\end{multline*}}}}

Thus we obtain
\begin{equation}
  E(T) = \sideset{}{^*_1}\sum (T) - \pi^{-1} T \sum^\infty_{n=1} d(n)
  n^{-3/4} K_n + \pi + O (T^{-1/4})\label{c2:eq2.86}
\end{equation}
where
\begin{gather*}
  \sideset{}{_1^*}\sum(T) = \left(\frac{2T}{\pi}\right)^{1/4} \sum_{n \leq
  (a+U)^2} \eta (n) (-1)^n d(n) n^{-3/4} e^{T, n} \cos (f (T, n)),\\
  \eta(n) =1 - \max (0, U^{-1} (n^{1/2}-a)),\\
  K_n = \frac{1}{2} \im (K_{n, 3/2}^+) + O(|K^-_{n, 3/2}|_+ n^{-1/2}|
  K^+_{n, 5/2}|_+ n^{-1/2}|K^-_{n, 5/2}|),\\
  K_{n, \alpha}^{\pm} = U^{-1} \int\limits_0^U
  \int\limits_{a+u}^\infty g_\alpha (x) \exp \left\{i ( \pm 4 \pi x
  \sqrt{n} - f(T, x^2)  + \pi x^2 - \frac{\pi}{4})\right\} dx\, du.
\end{gather*}

The\pageoriginale important feature of \eqref{c2:eq2.86} is that the error term is
$O(T^{-1/4})$. we define now
\begin{align*}
  Z(u) & = Z(T, u) = \frac{T}{2 \pi} + \frac{1}{2} (a+u)^2 -
  \left\{\frac{T}{2 \pi} (a+u)^2 + \frac{1}{4} (a+u)^4
  \right\}^{1/2},\\
  \xi (T, n) & = \max \left\{\min (1, U^{-1} (n^{-1/2}
  \left(\frac{T}{2 \pi}-n\right) -a)), 0\right\},
\end{align*}
and note that $\xi (T, n)=0$ for $n \geq Z(0)$. We can evaluate
$K^{\pm}_{n, \alpha}$ by using the saddle-point method as in Theorem
\ref{c2:thm2.3}  (see also M. Jutila \cite{Jutila9}). One obtains, for $T
\geq 2$, $\alpha > 0$, $T^{1/2} \ll a \ll T^{1/2}$, $1 \leq U \leq a$
and $n \geq 1$
{\fontsize{10}{12}\selectfont
\begin{align}
K^{\pm}_{n, \alpha}  &= \delta^{\pm} \xi (T, n) \frac{4 \pi}{T}
  n^{\frac{1}{2} (\alpha -1)} \left(\log \frac{T}{2 \pi n} \right)^{-1}
  \left(\frac{T}{2 \pi} -n \right)^{\frac{3}{2}-\alpha}\exp  \left\{i
  \left(- g (T, n)+ \frac{\pi}{2}\right) \right\}\notag\\ 
   &\quad  + O \left(U^{-1} T^{-1/2} \sum_{u=0, U} \min \left\{1, (\sqrt{n}-
    \sqrt{Z(u)})^{-2} \right\}\right)
+ O (\delta^{\pm} R(n) T^{- \alpha} n^{\frac{1}{2} (\alpha
  -1)}), \label{c2:eq2.87} 
\end{align}}
where $g(T, n)$ is as in \eqref{c2:eq2.5}, $\delta^+ =1$,
$\delta^-=0$, $R(n) = T^{- 1/2}$ for $n < Z(u)$, $R(n)=1$ for
$z(u)\leq n < Z(0)$, and $R(n)=0$ for $n \geq Z(0)$. Using
\eqref{c2:eq2.87} it follows that 
$$
- \pi^{-1}T \sum^\infty_{n=1} d(n) n^{-3/4} K_n =\sideset{}{_2^*}\sum
(T) + O(R_1) + O(R_2) + O(R_3),
$$
where
\begin{align}
  \sideset{}{_2^*}\sum (T) & = -2 \sum_{n \geq Z(0)} \xi (T, n) d(n)
  n^{-1/2} \left(\log \frac{T}{2 \pi n} \right)^{-1} \cos (g(T,
  n)),\label{c2:eq2.88}\\
  R_1 & = U^{-1} T^{-1/2} \sum_{u=0, U} \sum_{n \geq 1} d(n) n^{-3/4}
  \min (1, (\sqrt{n}- \sqrt{Z(u)})^{-2}),\notag\\
  R_2 & = T^{-1} \sum_{n \leq Z(0)} d(n) n^{-1/2}, R_3= T^{-1/2}
  \sum_{Z(U) \leq n \leq Z(0)} d(n) n^{-1/2}.\notag
\end{align}
Trivially\pageoriginale
$$
R_2 \ll T^{-1/2} \log T,
$$
and since $dZ (u)/du \ll T^{1/2}$ we have
$$
R_3 \ll T^{-1} (Z(0)- Z(U)) \log T \ll U T^{-1/2} \log T \ll T^{-1/4}
\log T.
$$

In $R_1$ the values of $n$ in $(1/2 Z(u), 2Z(u)]$ contribute
\begin{align*}
  & \ll U^{-1} T^{-\frac{1}{2}} \sum_{\frac{1}{2} Z(u) < n \leq 2 Z(u)} d(n) \min (1,
  Z(u) (n-Z(u))^{-2}\\
  & \ll U^{-1} T^{-\frac{1}{2}}\times\\ 
  & \left( \sum_{Z(u) - \sqrt{z(u)}< n < Z (u) +
    \sqrt{z(u)}} d(n) + \sum_{|n- Z(u)| > \sqrt{Z(u)}, 1/2 Z(u) < n
    \leq 2 Z(u)}
    \frac{Z(u) d(n)}{(n - Z(u))^2}\right)\\ 
    & \ll U^{-1} T^{-1/2}
  T^{1/2} \log T \ll T^{-1/4} \log T.
\end{align*}

The remaining $n'$s contribute to $R_1$ only $\ll U^{-1} T^{- 1/2+
  \epsilon} \ll T^{-3/4+ \epsilon}$. Therefore from \eqref{c2:eq2.86},
\eqref{c2:eq2.87} and the above estimates we obtain, for $T\geq 2,
T^{1/4} \ll U \ll T^{1/4}$, $T^{1/2} \ll a \ll T^{1/2}$,
\begin{equation}
  E(T) = \sideset{}{_1^*}\sum (T) + \sideset{}{_2^*}\sum(T) + \quad +
  O(T^{-1/4} \log T).\label{c2:eq2.89}
\end{equation}

We note that by using multiple averaging (i.e. not only over one
variable $u$, but over several) one can remove the log-factor from the
error term in \eqref{c2:eq2.89}, although this is not needed for the
proof of Theorem \ref{c2:thm2.4}. It is the variant of Atkinson's
formula, given by \eqref{c2:eq2.89}, that is suitable for the proof of
Theorem \ref{c2:thm2.4}. We use it with $a= T^{1/2} - U$, $U=T^{1/4}$,
and combine with the Cauchy-Schwarz inequality to obtain
\begin{align}
  \int\limits_T^{2T} E^2 (t) dt &= I_{11} + 2 I_{12} + I_{22} + 2 \pi
  I_1\notag\\
&\quad + O(T^{1/4} I_{11} \log T + T^{1/2} I^{1/2}_{22} +
  T),\label{c2:eq2.90} 
\end{align}
where\pageoriginale
\begin{align*}
  I_{jk} & = \int\limits_T^{2T} \sideset{}{_j^*}\sum (t)
  \sideset{}{_k^*}\sum (t) dt \quad (j, k =1 ~\text{or}~ 2),\\
  I_1 & = \int\limits_T^{2T} \sideset{}{_1^*}\sum (t) dt \ll T^{3/4}
\end{align*}
by Lemma \ref{c2:lem2.1}. From now on the proof is essentially the
same as the one that will be given below by Theorem
\ref{c2:thm2.5}. Termwise integration gives (the factors $\eta(n)$ and
$\xi (t, n)$ will cause no trouble)
\begin{align}
  I_{11} & = c(2 T)^{3/2} - cT^{3/2}+ O(T \log^5 T),\label{c2:eq2.91}\\
  I_{22} & = O(T \log^4 T),\label{c2:eq2.92}
\end{align}
so that the main contribution to \eqref{c2:eq2.75} comes from
\eqref{c2:eq2.91}, which will follow then on replacing $T$ in
\eqref{c2:eq2.90} by $T2^{-j} (j= 1, 2, \ldots)$ and adding all the
resulting expressions. Thus the proof will be finished if the
contribution of $I_{12}$ in \eqref{c2:eq2.90} is shown to be
negligible. To see this let
$$
\sideset{}{_1^*}\sum (t) = \sideset{}{_1'}\sum(t) +
\sideset{}{_2''}\sum (t),
$$
where in $\Sigma'(t)$ we have $n \leq T/A$, and in $\Sigma''(t)$ we
have $T/A < n \leq T$, $A > 0$ being a large constant. Then by
termwise integration we obtain, as in the proof of \eqref{c2:eq2.91},
\begin{align}
  \int\limits_T^{2T} \left(\sideset{}{''}\sum (t)\right)^2 & \ll T^{3/2}
  \sum_{n > T/A} d^2 (n) n^{-\frac{3}{2}} \sum_{n > T/A} d^2 (n) n^{-3/2} + T
  \log^4 T\notag\\ 
  & \ll T \log^4 T.\label{c2:eq2.93}
\end{align}

Set $I_{12} = I'+ I''$, where
$$
I' = \int\limits_2^{2T} \sideset{}{'}\sum (t) \sideset{}{_2^*}\sum (t)
dt, I'' = \int\limits_T^{2T} \sideset{}{''}\sum(t) \sideset{}{_2^*}\sum (t) dt.
$$

Thus by the Cauchy-Schwarz inequality, \eqref{c2:eq2.92} and
\eqref{c2:eq2.93} we infer that $I'' \ll \log ^4 T$. Consider now
$I'$. We have
$$ 
I' \ll \sum_{n \leq T/A} ~\sum_{m \leq Z (2T,  0)} d(n) d (m) n^{-3/4}
m^{-1/2} \left(|J^+_{n, m}| +|J^-_{n, m}| \right).
$$
where\pageoriginale
$$
J^{\pm}_{n, m} = \int\limits_H \xi (t, m) t^{1/4} e (t, n) \left(\log
\frac{t}{2 \pi m} \right)^{-1} \exp \{ i (f (t, n) \pm g(t, m))\} dt
$$
and $H$ is a subinterval of $[T, 2 T]$ such that $m \leq Z(t, 0)$ for
$t \in H$. Now
$$
f' (t, n)= 2 ar \sinh \sqrt{\frac{\pi n}{2t}}, g' (t, m) = \log
\left(\frac{t}{2 \pi m} \right),
$$
and for $n \leq T/A$ we have $|f' (t, n) \pm g' (t, m)|\gg 1$ in $H$,
which was the reason for considering separately $\Sigma'(t)$ and
$\Sigma'' (t)$. Hence by Lemma \ref{c2:lem2.1} we obtain  $J^{\pm}_{n,
m} \ll T^{1/4}$, and consequently $I' \ll T \log^2 T$, $I_{12} \ll T
\log ^4 T$, and Theorem \ref{c2:thm2.4} follows.

It remains to prove Theorem \ref{c2:thm2.5}, whose method of proof
incidentally establishes \eqref{c2:eq2.91}. Suppose $x \geq 1$ is not
an integer and let
$$
\delta_M (x) : = (\pi \sqrt{2})^{-1} x^{1/4} \sum_{n \leq M} d(n)
n^{-3/4} \cos \left(4 \pi \sqrt{nx} - \frac{\pi}{4}\right).
$$

From the Voronoi formula \eqref{c2:eq2.24} one has

\begin{align}
  \Delta  (x) & = \delta_M (x) + O (x^{1/4}(|S_1|+ |S_2|)+
  x^{-1/4}),\label{c2:eq2.94}\\
  S_1 & = \sum_{n > M} (d(n) - \log n - 2 \gamma) n^{-3/4} \exp (4 \pi
  i \sqrt{nx}),\notag\\
  S_2 & = \sum_{n > M} (\log n + 2 \gamma) n^{-3/4} \exp (4 \pi i
  \sqrt{nx}).\notag 
\end{align}

For $\xi > M \geq 2x$ we have
\begin{equation}
  G(\xi): = \sum_{M < n \leq \xi} \exp (4 \pi i \sqrt{nx})= \int\limits_M^\xi
 \exp (4 \pi i \sqrt{tx}) dt + O(1) = O ((\xi/ x)^{1/2}),\label{c2:eq2.95}
\end{equation}
where we used Lemma \ref{c2:lem2.1} and the elementary relation
\begin{equation}
  \sum_{a < n \leq b} e(f(n)) = \int\limits_a^b e(f(x))dx + O_\delta
  (1),\label{c2:eq2.96} 
\end{equation}
provided that $f(x) \in C^2 [a, b]$, $f'(x)$ is monotonic on $[a, b]$
and $|f'(x)|\leq \delta < 1$. Then by partial summation it follows
that
\begin{align*}
  S_2 & = \lim\limits_{A \to \infty} \sum_{M < n \leq A} (\log n + 2
  \gamma)n^{-3/4} \exp (4 \pi i \sqrt{nx})\\
  & = \lim\limits_{A \to \infty} \left\{(\log t + 2 \gamma) t^{-3/4}
  G(t) \Bigg|_M^A - \int\limits_M^\infty ((\log t + 2
  \gamma)t^{-3/4})' G(t) dt \right\}\\
  & \ll x^{-1/2} M^{-1/4} \log M \ll x^{-1/2}.
\end{align*}

For\pageoriginale $S_1$, note that for any $t> 1$
$$
\sum_{n \leq t} (d(n) - \log n - 2 \gamma) = \Delta  (t) + O (t^\epsilon),
$$
since the definition of $\Delta  (t)$ contains
$\displaystyle{\sideset{}{'}\sum_{n \leq t}}$ and $d(n) \ll
n^\epsilon$. Partial summation and the weak estimate $\Delta  (x)
\ll x^{1/3+ \epsilon}$ give
\begin{align*}
  S_1 & = \lim\limits_{A \to \infty} \Bigg\{ (\Delta  (t) +
  O(t^\epsilon)) t^{- 3/4} \exp (4 \pi i \sqrt{tx})\bigg|_M^A \\
  & \quad - \int\limits_M^A (\Delta  (t) + O(t^\epsilon)) (t^{-3/4} \exp (4
  \pi i \sqrt{tx}))' dt\\
  & \ll M^{\epsilon- 5/12} + x^{1/2} \left|\int\limits_M^\infty
  \Delta  (t) t^{5/4} \exp (4 \pi i \sqrt{tx})dt \right| +
  \int\limits_M^\infty t^{\epsilon- 5/4} x^{1/2}dt\\
  & \ll x^{1/2} \left|\int\limits_M^\infty \Delta  (t) t^{- 5/4} \exp
  (4 \pi i \sqrt{tx})dt \right| + x^{1/2} M^{\epsilon - 1/4}\\
  & = x^{1/2} |I| + x^{1/2} M^{\epsilon - 1/4} = x^{1/2}
  \sum_{k=0}^\infty I (2^k M) + x^{1/2} M^{\epsilon - 1/4},
\end{align*}
where we have set
$$
I(Y) : = \int\limits_Y^{2Y} \Delta  (y)y^{-5/4} \exp (4 \pi i
\sqrt{xy})dy. 
$$

To estimate $I(Y)$ we use
{\fontsize{10}{12}\selectfont
$$
\Delta  (y) = (\pi 2^{1/2})^{-1} y^{1/4} \sum_{n \leq Y} d(n)
n^{-3/4} \cos \left(4 \pi \sqrt{ny} - \frac{\pi}{4}\right) +
O (Y^\epsilon)  \; (Y \leq y \leq 2 Y).
$$}

This gives 
$$
I(Y) \ll \sum_{n \leq Y} d(n) n^{-3/4} \left(|I_n^+ (Y)| + |I_n^-
(Y)|\right) + Y^{\epsilon- 1/4},
$$
where\pageoriginale using Lemma \ref{c2:lem2.1} we have
$$
I_n^{\pm} (Y) = \int\limits_Y^{2Y} y^{-1} \exp (4 \pi i (\sqrt{x} +
\sqrt{n}) \sqrt{y}) dy \ll \frac{Y^{-1/2}}{|\sqrt{x} \pm \sqrt{n}|}
$$

The contribution of $I_n^+$ is trivially $O(Y^{-1/2})$, and that of
$I_n^-$ is (since by assumption $x$ is not an integer)
{\selectfont{\fontsize{10}{8}{
\begin{align*}
  & \ll \sum_{n \leq Y} d(n) n^{-3/4} Y^{-1/2} |x^{1/2} -
  n^{1/2}|^{-1}\ll Y^{-1/2} + Y^{-1/2} x^{1/2} \sum_{\frac{1}{2} x< n \leq
    2x} \frac{d(n)n^{-3/4}}{|x-n|}\\
  &\qquad = Y^{-1/2} + 0 (Y^{-1/2} x^{-1/4+ \epsilon} \parallel x
  \parallel^{-1}) + O(Y^{-1/2}) \sum_{|r|\leq x-1, r \neq 0}r^{-1}
  d([x]+ r)x^{-1/4}\\
  & \ll Y^{-1/2} + x^{-1/4 + \epsilon} \parallel x \parallel^{-1} Y^{-1/2},
\end{align*}}}}
where $\parallel x\parallel$ is the distance for $x$ to the nearest
integer, and where we set $n = [x] + r$. Thus
\begin{align*}
  I(Y) & \ll Y^{-1/2} x^{-1/4 + \epsilon} \parallel x \parallel^{-1} +
    Y^{\epsilon - 1/4},\\
    S_1 &  \ll M^{-1/2} x^{1/4+\epsilon} \parallel x\parallel^{-1} +
    x^{1/2} M^{\epsilon- 1/4}.
\end{align*}

But $\epsilon > 0$ sufficiently small
\begin{align*}
& M^{-1/4 + \epsilon} x^{1/2} \leq x^{-1/2}\qquad  \text{for}~ M\geq x^{4
  + 20 \epsilon},\\
& M^{-1/2} x^{1/4+ \epsilon} \parallel x\parallel^{-1}  \leq x^{-1/2}
\qquad\text{for}~ M\leq x^{3/2+ 2 \epsilon} \parallel x \parallel^{-2}.
\end{align*}

The truncated Voronor formula \eqref{c2:eq2.25} gives $\Delta  (x) =
\delta_M (x) + O(x^\epsilon)$ if $x\geq 1$, $M \gg x$, so that we
obtain
{\fontsize{10}{12}\selectfont
$$
\Delta  (x) = \delta_M (x) + R(x) , R(x) \ll 
\begin{cases}
  x^{-1/4} &  \text{if}~ M \geq \max (x^{4 + 20 \epsilon}, x^{3/2 + 2
    \epsilon} \parallel x \parallel^{-2}),\\
  x^\epsilon & \text{otherwise}.
\end{cases}
$$}

If $M \geq x^5 \parallel x \parallel^{-2}$, then $M\geq 4x^5 \geq x^{4
+ 20\epsilon}$, $M \geq x^{3/2+ 2 \epsilon} \parallel x\parallel^{-2}$
for $o < \epsilon < 1/20$, hence $R(x) \ll x^{-1/4}$ for $M \geq x^5
\parallel x \parallel^{-2}$, and $R(x) \ll x^\epsilon$\pageoriginale
otherwise. It is this estimate for $R(x)$, which shows that it is $\ll
x^{-1/4}$ ``most of the time'', that makes the proof of Theorem
\ref{c2:thm2.5} possible. So we take now $M= X^9$, $X \leq x \leq 2X$,
obtaining 
\begin{equation}
  \Delta  (x) = 
  \begin{cases}
    \delta_M (x) + O(X)^{-1/4} & \text{if}~ \parallel x \parallel \gg
    X^{-2},\\
    \delta_M (x) + O(X^\epsilon)& \text{if}~ \parallel x \parallel \ll X^{-2}.
  \end{cases}\label{c2:eq2.97}
\end{equation}

To prove Theorem \ref{c2:thm2.5} it will be sufficient to show
\begin{equation}
  D: = \int\limits_X^{2X} \delta_M^2 (x) dx = \frac{1}{6 \pi^2}
  \left(\sum_{n=1}^\infty d^2 (n) n^{-3/2} \right) ((2X)^{3/2} -
  X^{3/2}) + O (X \log^5 X),\label{c2:eq2.98}
\end{equation}
since using \eqref{c2:eq2.97} we have
$$
\int\limits_{X}^{2X} \Delta ^2 (x) dx = D+ O\left(\int\limits_{X,
  \parallel x\parallel \ll X^{-2}}^{2X} X^{2 \epsilon} dx \right) +
O(X^{1/2} + X^{1/4} D^{1/2}),
$$
by applying the Cauchy-Schwarz inequality. Thus if \eqref{c2:eq2.98}
holds we obtain
$$
\int\limits_X^{2X} \Delta ^2 (x) dx = D + O(X)= d((2X)^{3/2}-
X^{3/2}) + O (X \log^5 X)
$$ 
with
$$
d= \frac{1}{6\pi^2} \left(\sum_{n=1}^\infty  d^2 (n) n^{-3/2}\right)=
\frac{\zeta^4 (3/2)}{6\pi^2 \zeta(3)}.
$$
To prove \eqref{c2:eq2.98} we use $\cos \alpha \cos \beta = \frac{1}{2} \cos
(\alpha + \beta)+ \frac{1}{2} \cos (\alpha- \beta)$. It follows from the
definition of $\delta_M(x)$ that
\begin{align}
  D & = \frac{1}{4 \pi^2} \sum_{m, n \leq M} d(m) d(n) (mn)^{-3/4}
  \int\limits_X^{2X} x^{1/2}\times\notag\\
  &\quad\left\{\cos (4 \pi (m^{1/2}- n^{1/2})x^{1/2}) + \sin (4 \pi
  (m^{1/2}+ n^{1/2})x^{1/2}) \right\}dx\notag\\
  & = \frac{1}{6 \pi^2} \sum_{m \leq M} d^2 (m) m^{-3/2} ((2X)^{3/2}-
  X^{3/2})\notag\\ 
  &\quad + O \left(X \sum_{m, n\leq M, m \neq n}
  \frac{d(m)d(n)}{(mn)^{1/4}|m^{1/2} - n^{1/2}|} \right)\label{c2:eq2.99}
\end{align}
after an integration by parts. Since $M = X^9$, we have
\begin{align*}
  \sum_{m \leq M} d^2 (m) m^{-3/2} & = \sum_{m=1}^\infty d^2 (m) m^{-3/2}
  + O(M^{-1/2} \log^4 M)\\ 
  &= \frac{\zeta^4 (3/2)}{\zeta(3)} + O (X^{-4}). 
\end{align*}

The\pageoriginale double sum in the $O$-term in \eqref{c2:eq2.99} is 
$$
\ll \sum_{n < m \leq M} \frac{d(m) d(n)}{n^{3/4} m^{1/4} (m-n)} \ll
\sum_{r \leq M, n\leq M, n+ r\leq M}\frac{d(n)
  d(n+r)}{n^{3/4}r(n+r)^{1/4}} 
$$
if we write $m=n+r$. In the portion of the last sum with $n \leq M$,
$\frac{1}{2} n< r \leq M$ we have $n+r \geq \frac{3}{2} n$ and $\frac{1}{r}
\leq \frac{3}{n+r}$. Hence
\begin{multline*}
  \sum_{\frac{1}{2} n < r \leq M, n+ r \leq M} \frac{d(n) d(n+r)}{n^{3/4}
    r(n+r)^{1/4}} \leq 3 \sum_{n \leq M} \frac{d(n)}{n^{3/4}} \sum_{n
    +r \leq M, r> 1/2 n} \frac{d (n+r)}{(n+r)^{5/4}}\\
  \ll \sum_{n \leq M} d(n) n^{- 3/4} \sum_{m \geq 3n/2} d(m) m^{-5/4}
  \ll \sum_{n \leq M} d(n) n^{-1} \log n \ll \log^2 M.
\end{multline*}

Also
\begin{align*}
  \sum_{r \leq \frac{1}{2} n, n< M, n+ r\leq M} &\frac{d(n) d(n+r)}{n^{3/4} r
    (n+r)^{1/4}}\\
  &\leq \sum_{r \leq \frac{1}{2} M} r^{-1} \sum_{2r \leq n \leq
    M} \frac{d(n) d (n+r)}{n^{3/4} (n+r)^{1/4}}\\
  &\leq 2 \sum_{r \leq \frac{1}{2} M} r^{-1} \left( \sum_{2r \leq n \leq M}
  \frac{d(n)}{n^{1/2}}\cdot \frac{d(n+r)}{(n+r)^{1/2}} \right)\\
  &\leq 2 \sum_{r \leq \frac{1}{2} M} r^{-1} \left(\sum_{n \leq M} d^2 (n)
  n^{-1}\right)^{1/2} \left(\sum_{m \leq 2M} d^2 (m) m^{-1}
  \right)^{1/2}\\
  & \ll \log^5 M.
\end{align*}

In view of $M= X^9$ this implies
$$
\sum_{r \leq M, n \leq M, n+ r\leq M} \frac{d(n) d(n+r)}{n^{3/4}
  r(n+r)^{1/4}} \ll \log^5 X
$$
and  therefore establishes \eqref{c2:eq2.98}.

Finally we discuss how to improve slightly on the error terms in
Theorem \ref{c2:thm2.4} and Theorem \ref{c2:thm2.5} and obtain
\begin{equation}
  \int\limits_2^T E^2 (t) dt = cT^{3/2} + O(T \log ^4 T),\label{c2:eq2.100}
\end{equation}
and analogously the error term in \eqref{c2:eq2.76} may be replaced by
$O (X \log ^4 X)$.\pageoriginale Instead of the fairly straightforward
estimation of the $O$-term in \eqref{c2:eq2.99} one has to use the inequality
\begin{equation}
  \sum_{r, s\leq R; r \neq s} \frac{a_r \ob{a}_s \exp (ib_r -
    ib_s)}{(rs)^{1/4}(r^{1/2}- s^{1/2})}\ll \sum_{r \leq R}
  |a_r|^2, \label{c2:eq2.101} 
\end{equation}
where the $a_r'$s are arbitrary complex numbers, and the $b_r'$s are
arbitrary real numbers. This follows from a variant of the so-called
Hilbert's inequality: Suppose that $\lambda_1, \lambda_2, \ldots
\lambda_R$ are distinct reals and $\delta_r = \underset{s}{\min}_+
|\lambda_r - \lambda_s|$, where $\min_+ f$ denotes the least positive
value of $f$. Then
\begin{equation}
  \left|\sum_{r, s\leq R; r \neq s} \frac{u_r\ob{u}_s}{\lambda_r -
    \lambda_s}\right| \leq \frac{3 \pi}{2} \sum_{r \leq R}
  |u_r|^{2}\delta_r^{-1}. \label{c2:eq2.102}
\end{equation}

By taking in \eqref{c2:eq2.102} $\lambda_r = r^{1/2}$, $u_r = a_r
r^{-1/4} \exp (ib_r)$ and noting that in this case
$$ 
\delta_r = \underset{s}{\min}_+ |r^{1/2} - s^{1/2}| = \underset{s}{\min}_+
\left|\frac{r-s}{r^{1/2}+ s^{1/2}} \right| \ll r^{-1/2},
 $$

One obtains \eqref{c2:eq2.101}. This possibility was noted recently by
{\small E. Preissmann} \cite{Preissmann2}, who considered the mean square of $\Delta 
(x)$, using a classical differencing technique of E. Landau based on
the Voronoi formula for the integral of $\Delta  (x)$. This seems
more appropriate in this context than the use of the method of proof
of Theorem \ref{c2:thm2.5}, where the choice $M=X^9$ is large for the
application of \eqref{c2:eq2.101}. But for \eqref{c2:eq2.100} this can
be done directly, and it will be sufficient that the non-diagonal
terms in the mean square for $\Sigma_1^* (t)$ are $\ll T \log ^4
T$. The non-trivial contribution to the sum in question comes from
{\fontsize{10}{12}\selectfont
\begin{align*}
  & \sum_{m \neq n \leq T} d(m) d(n) \eta (m) \eta (n) (mn)^{-3/4}
  \int\limits_T^{2T} e(t, m) e (t, n) \cos (f(t, m)- f(t, n))dt\\
  & = \sum_{m \neq n \leq T} d(m) d(n) \eta (m) \eta (n) (mn)^{-3/4}\times\\
  & \qquad \int\limits_T^{2T} \frac{e(t, m) e(t, n)}{2 \left(ar \sinh
    \left(\frac{\pi n}{2 T}\right)^{1/2} - ar \sinh \left(\frac{\pi
      m}{2T}\right)^{1/2}\right)} \frac{d}{dt} \left\{ \sin (f(t, n)- f(t,
  m))\right\} dt\\
  & = \frac{1}{2} \sum_{m \neq n \leq T} d(m) d (n) \eta (m) \eta (n)
  (mn)^{-1/4}\times\\ 
  & \qquad \left\{\frac{e (t, m) e (t, n)}{ar \sinh \left(\frac{\pi
      n}{2 T}\right)^{1/2} - ar \sinh \left(\frac{\pi m}{2 T}
    \right)^{1/2}}  (\sin (f(t, n)- f(t, m)))\right\} \Bigg|_T^{2T}\\
   &  - \frac{1}{2} \sum_{m \neq n \leq T} d(m) d (n) \eta (m) \eta (n)
    (mn)^{- 3/4} \int\limits_T^{2T} \sin (f(t, n)- f(t, m))
    \frac{d}{dt}\times\\ 
    & \hspace{3cm} \left\{\frac{e(t, m)e (t, n)}{ar \sinh
      \left(\frac{\pi n}{2T} \right)^{1/2} - ar \sinh \left(\frac{\pi
        m}{2T}\right)^{1/2}} \right\}dt.
\end{align*}}

The\pageoriginale integrated terms are first simplified by Taylor's
formula and then estimated by \eqref{c2:eq2.101}, giving sums like
$$
T \sum_{m\neq n \leq T} \frac{d(m) d(n) \exp (if\, (T, n) -
if~ (t, m))}{(m^{1/2} - n^{1/2})(mn)^{1/4}}\ll T \log^4 T.
$$

The integral $\int\limits_T^{2T}$ in the above sum is estimated by
Lemma \ref{c2:lem2.1} and the ensuing sums are treated analogously as
in the proof of Theorem \ref{c2:thm2.5}. Their total contribution will
be again $\ll \log^4 T$, and \eqref{c2:eq2.100} follows.

\section{The Upper Bounds for the Error Terms}\label{c2:sec2.7}

We have seen in Section \ref{c2:sec2.6} how by an averaging technique
Theorem \ref{c2:thm2.4} can be used to yield the upper bound $E(T) \ll
T^{1/3} \log ^2T$. Now we shall use another, similar averaging
technique to obtain an upper bound for $E(T)$ which is in many ways
analogous to the truncated formula \eqref{c2:eq2.25} for $\Delta 
(x)$. This bound will have the advantage that exponential sum
techniques can be applied to it, thereby providing the possibility to
improve on $E(T)\ll T^{1/3} \log^2 T$. We shall also give upper bounds
for $E_\sigma(T)$, which will be valid in the whole range $1/2 <
\sigma < 1$, and not just for $1/2 < \sigma < 3/4$, which is the range
for which Theorem 2.2 holds.

We shall consider first $E(T)$. In case $E(T)> 0$ we start from the
inequality 
\begin{equation}
  E(T) \leq H^{-N} \int\limits_0^H \cdots \int\limits_0^H E(T+ u_1 +
  \cdots + u_N) du_1 \cdots du_N + CH \log T,\label{c2:eq2.103}
\end{equation}
whose\pageoriginale proof is analogous to the proof of
\eqref{c2:eq2.78}. A similar lower bound inequality, analogous to
\eqref{c2:eq2.79}, also holds and may be used to estimate $E(T)$ when
$E(T)< 0$. We suppose $T\geq T_0$, $T^\epsilon \ll H \ll T^{1/2}$, and
take $N$ to be a large, but fixed integer. In Atkinson's formula
\eqref{c2:eq2.5}, which we write as
$$
E(T) = \sideset{}{_1}\sum (T) + \sideset{}{_2}\sum (T) + O(\log^2 T),
$$  
take $N= T$, $N' = \alpha T$, $\alpha = \frac{1}{2 \pi}+ \frac{1}{2} -
\left(\frac{1}{4} + \frac{1}{2 \pi} \right)^{1/2}$. First we shall
show that the contribution of 
$$
\sideset{}{_2}\sum (T) = -2 \sum_{n \leq N'} d(n) n^{-1/2} \left(\log
\frac{T}{2 \pi n} \right)^{-1} \cos (g(T, n))
$$ 

is negligible. We have
$$
g(T, n) = T \log \left(\frac{T}{2 \pi n} \right)- T + \frac{\pi}{4},
\frac{\partial g (T, n)}{\partial T}= \log \left(\frac{T}{2 \pi n}
\right) \gg 1 
$$
for $n \leq N$. Hence
\begin{align*}
  & H^{-N}\int\limits_0^H \cdots \int\limits_0^H \sideset{}{_2}\sum (T +
  u_1 + \cdots + u_N) du_1 \cdots du_N \\
  & =- 2H^{-n} \sum_{n \leq N'} d(n) n^{-1/2} \int\limits_0^H\cdots
  \int\limits_0^H \log^{-2} \left(\frac{T+u_1 + \cdots + u_n}{2 \pi n}
  \right) \\
& \hspace{2cm}  \frac{\partial \sin g (T+u_1 + \cdots + u_{N'}n)}{\partial
    u_1} du_1 \cdots du_N\\
  & = - 2H^{-N} \sum_{n \leq N'} d(n) n^{-1/2} \left\{\int\limits_0^H
  \cdots \int\limits_0^H \left(\frac{\sin g (T + H + u_2 + \cdots u_n,
    n)}{\log^2 \left(\frac{T+ H+ u_2 + \cdots + u_N}{2 \pi n}
    \right)} \right. \right. \\
& \hspace{2cm} \left. - \frac{\sin g (T + u_2 + \cdots + u_n, n)}{\log^2
    \left(\frac{T+ u_2 + \cdots + u_N}{2 \pi n} \right)} \right) du_2
  \cdots du_N\\
  & \hspace{2cm} + 2 \int\limits_0^H \cdots \int\limits_0^H \log^{-3}
  \left(\frac{T+ u_1 + \cdots + u_N}{2 \pi n} \right) \\
& \hspace{2cm} \left.  \frac{\sin g (T
    + u_1 + \cdots + u_N, n)}{T+ u_1 + \cdots + u_N} du_1 \cdots
  du_N\right\}\\
  & = - 2 H^{-N} \sum_{n \leq N'} d(n) n^{-1/2} \int\limits_0^H \cdots
  \int\limits_0^H  \left(\frac{\sin g (T + H+ u_2 + \cdots + u_N,
    n)}{\log^2 \left(\frac{T+H+ u_2 + \cdots + u_N}{2 \pi n} \right)} \right.\\
& \hspace{2cm} \left.  - \frac{\sin g (T + u_2 + \cdots + u_N, n)}{\log^2 \left(\frac{T+u_2
      + \cdots + u_N}{2 \pi n}\right)}\right) du_2 \cdots du_N\\
  & \hspace{2cm}+ O(T^{-1/2}\log T)= \ldots =  O(H(^{-N}T^{1/2} \log
  T) \\
& \hspace{4cm} + O(T^{-1/2} \log T)
\end{align*}
after\pageoriginale $N$ integrations by parts. Thus for $N$
sufficiently large the contribution of $\Sigma_2$ is negligible,
namely it is asorbed in the term $CH \log T$ in \eqref{c2:eq2.103}.

Further we have
{\selectfont{\fontsize{9}{7}{
$$
\sideset{}{_1}\sum (T) = 2^{-\frac{1}{2}} \sum_{n \leq T} (-1)^n d(n) n^{-1/2}
\left(ar \sinh \sqrt{\frac{\pi n}{2 T}}\right)^{-1} \left(\frac{T}{2
  \pi n} + \frac{1}{4} \right)^{-1/4} \cos (f(T, n)),
$$ }}}
where
$$
\frac{\partial f(T, n)}{\partial T}= 2 ar \sinh \sqrt{\frac{\pi n}{2T}}.
$$

It will be shown now that the contribution of $n$ for which $T^{1+
  \epsilon} H^{-2} < n\leq T$ ($\epsilon > 0$ arbitrarily small and
fixed) in the integral of $\Sigma_1(T+ u_1 + \cdots + u_N)$ in
\eqref{c2:eq2.103} is negligible. To see this set
$$
F_1 (T, n): = 2^{-3/2} \left( ar \sinh \sqrt{\frac{\pi n}{2T}}
  \right)^{-2} \left(\frac{T}{2\pi n} + \frac{1}{4}\right)^{-1/4}.
$$

On integrating by parts it is seen that the integral in question is
equal to 
{\fontsize{10}{12}\selectfont
\begin{align*}
&   H^{-N} \sum_{T^{1+ \epsilon}H^{-2} < n \leq T} (-1)^n d(n) n^{-1/2}
  \int\limits_0^H \cdots \int\limits_0^H F_1 (T 
  + u_1 + \cdots + u_N , n)\times\\
& \hspace{5cm} \frac{\partial \sin f(T + u_1 + \cdots +
    u_N, n)}{\partial u_1} du_1 \cdots du_N\\
 &\quad  = H^{-N} \sum_{T^{1+\epsilon} H^{-2} < n \leq T} (-1)^n d(n)
  n^{-1/2} \left\{ \int\limits_0^H \cdots \int\limits_0^H (F_1 (T + H +
  u_2 + \cdots + u_N, n) \right.\\
& \qquad   \sin f(T + H + u_2 + \cdots + u_N, n) 
   - F_1 (T+ u_2 + \cdots + u_N, n) \\
& \qquad \sin f (T + u_2 + \cdots + u_N ,  n)) du_2 \cdots du_N
  \int\limits_0^H \cdots \int_0^H \\
& \qquad \left. \frac{\partial F_1 (T+ u_1 +
    \cdots + u_N , n)}{\partial u_1} \sin f(T+ u_1 + \cdots + u_N, n)
  du_1 \cdots du_n \right\}.
\end{align*}}

But\pageoriginale we have 
\begin{multline*}
  \frac{\partial F_1 (T, n)}{\partial T} =- \frac{1}{8\sqrt{8} \pi n}
  \left( \frac{T}{2 \pi n} + \frac{1}{4}\right)^{-5/4} \left(ar\sinh
  \sqrt{\frac{\pi n}{2 T}}\right)^{-2} + 2^{-3/2}\times\\ 
  \left(\frac{T}{2 \pi n} + \frac{1}{4} \right)^{-1/4} (ar \; \sinh)
  \left( \sqrt{\frac{\pi n}{2T}}\right)^{-3} \left(1+ \frac{\pi n}{2T}
  \right)^{-1/2} \left(\frac{\pi n}{2} \right)^{1/2} T^{-3/2}\\ 
\qquad\ll T^{-1/4} n^{-3/4}, 
\end{multline*}
and since $\displaystyle{\sum_{n \geq 1} d(n) n^{5\-5/4}}$ converges,
trivial estimation shows that the contribution of the $N$-fold
integral is negligible. Now set
\begin{align*} 
F_2 (T, n) &:= 2^{-1} \left(ar \sinh \sqrt{\frac{\pi n}{2T}}
\right)^{-1} F_1 (T, n)\\ 
&= 2^{-5/2} \left(ar \sinh \sqrt{\frac{\pi n}{2T}} \right)^{-3}
\left(\frac{T}{2 \pi n} + \frac{1}{4} \right)^{-1/4}.
\end{align*}

Then we have (the other $(N-1)$-fold integral is treated analogously:)
{\fontsize{10}{12}\selectfont
\begin{align*}
&   H^{-N} \sum_{T^{1 + \epsilon} H^{-2} < n \leq T} (-1)^n d(n) n^{-1/2}
  \int_0^H \cdots \int_0^H (-F_1 (T+u_2 + \cdots + u_N, n)\times \\
& \hspace{5cm} \sin f(T+ u_2   + \cdots + u_N, n)) du_2 \cdots du_N\\
&   = H^{-N} \sum_{T^{1+\epsilon} H^{-2} < n \leq T} (-1)^n d(n)
  n^{-1/2} \int\limits_0^H \cdots \int\limits_0^H F_2 (T + u_2 +
  \cdots + u_N, n)\times\\
& \hspace{5cm}  \frac{\partial \cos f (T + u2 + \cdots + u_N,
    n)}{\partial u_2} du_2 \cdots du_N\\
&   = H^{-N} \sum_{T^{1 + \epsilon} H^{-2} < n \leq T} (-1)^n d(n)
  n^{-1/2} \left\{ \int\limits_0^H \cdots \int\limits_0^H (F_2 (T + H+
  u_3 + \cdots + u_N, n)\times\right. \\
& \qquad \cos f(T+ H + u_3 + \cdots + u_N, n)
  - F_2 (T+ u_3 + \cdots + u_N. n)\times \\
& \qquad  \cos f(T+ u_3 + \cdots + u_N, n))  du_3 \cdots du_N
- \int\limits_0^H \cdots \int\limits_0^H\times \\
& \qquad \left.\frac{\partial F_2 (T+ u_2
    + \cdots + u_N , n)}{\partial u_2} \cos f(T + u_2 + \cdots + u_N,
  n) du_2 \cdots du_N\right\}
\end{align*}}

But
$$
\frac{\partial F_2 (T, n)}{\partial T} \ll T^{1/4} n^{-5/4}.
$$

Trivial\pageoriginale estimation shows that the total contribution of
the $(N-1)$-fold integral is 
\begin{multline*}
  \ll H^{N-1} H^{-N} \sum_{n> T^{1+ \epsilon}H^{-2}} d(n) n^{-1/2}
  T^{1/4} n^{- 5/4}\\ 
  \ll H^{-1} T^{1/4} (T^{1+ \epsilon} H^{-2})^{-3/4}
  \log T \ll 1.
\end{multline*}

Thus defining
$$ 
F_k (T, n) : = \left(2^{-1} ar \sinh \sqrt{\frac{\pi  n}{2T}}\right)^{1-k} F_1 (T, n)
$$
we continue integrating, until all $N$ integrations are completed. We
estimate the resulting expression trivially, obtaining a contribution
of all integrated terms as
\begin{align*}
  & H^{-N} \sum_{n > T^{1+ \epsilon} H^{-2}} d(n)n^{-1/2}
  \left(\frac{n}{T} \right)^{\frac{1}{2} (1-N)} F_1 (T, n) \ll H^{-N}T^{\frac{1}{2} N
    + \frac{1}{4}}\times\\ 
  & \hspace{2cm}\sum_{n > T^{1+ \epsilon} H^{-2}} d(n) n^{-\frac{3}{4} - \frac{1}{2} N}\times\\
  & H^{-N} T^{\frac{1}{2} N+ \frac{1}{4}}(T^{1+ \epsilon}H^{-2})^{\frac{1}{4} - \frac{1}{2} N} \log T
  = T^{\frac{1}{2} + \frac{1}{4} \epsilon - \frac{1}{2} \epsilon N} H^{1/2} \log T \ll 1 
\end{align*}
if $N= N(\epsilon)$ is sufficiently large. The contribution of the
remaining $n'$s in $\Sigma_1$ for which $n \leq T^{1+ \epsilon}
H^{-2}$ is estimated trivially, by taking the supremum of the
expression in question. Therefore we obtain

\begin{thm}\label{c2:thm2.7}
  For $T^\epsilon \ll H \ll T^{1/2}$, $0 < \epsilon < 1/2$ fixed, we
  have
\begin{align}
E(T)&\ll H \log T+  \sup\limits_{\frac{1}{2} T < \tau \leq 2T} 
\Bigg| \sum_{n \leq T^{1+\epsilon}H^{-2}} (-1)^n d(n) n^{n^{-\frac{1}{2}}}\notag\\
&\quad\left(ar \sinh\sqrt{\frac{\pi n}{2 \tau}} \right)^{-1}
\left(\frac{\tau}{2 \pi n} +  
\frac{1}{4}\right)^{-1/4} \cos (f(\tau, n)) \Bigg|.\label{c2:eq2.104}
\end{align}
\end{thm}

Estimating the sum over $n$ in \eqref{c2:eq2.104} trivially we obtain 
$$
E(T) \ll H \log T + T^{1/4} \sum_{n \leq T^{1+ \epsilon}H^{-2}} d(n)
n^{-1/4} \ll H \log T + T^{1/2 + \epsilon} H^{-1/2}.
$$

Choosing\pageoriginale $H= T^{1/3}$ we obtain
$$
E(T) = O(T^{1/3+ \epsilon}),
$$
which is essentially \eqref{c2:eq2.80}. A similar analysis may be made
for $E_\sigma (T)$, if one uses \eqref{c2:eq2.6} and follows the
foregoing argument. In this way one obtains

\begin{thm}\label{c2:thm2.8}
  For $T^\epsilon \ll H \ll T^{1/2}$, $0 < \epsilon < 1/2$ and $1/2 <
  \sigma < 3/4$ fixed, we have
{\fontsize{10}{12}\selectfont
  \begin{align}
E_\sigma (T)&\ll H + T^{1/2-\sigma} \sup\limits_{\frac{1}{2} T \leq \tau
      \leq 2 T}\Bigg|\sum_{n \leq T^{1+ \epsilon} H^{-2}} (-1)^n
    \sigma_{1-2\sigma} (n)  n^{\sigma-1}\times\notag\\ 
 &\quad\left(ar \sinh \sqrt{\frac{\pi
      n}{2 \tau}} \right)^{-1} \left(\frac{\tau}{2 \pi n}+ \frac{1}{4}
    \right)^{-1/4} \cos f(\tau, n) \Bigg|.\label{c2:eq2.105}
  \end{align}}
\end{thm}

Choosing $H= T^{1/(1+ 4\sigma)}$ and estimating trivially the sum in
\eqref{c2:eq2.105} one obtains  
\begin{equation}
  E_\sigma (T) \ll T^{1/(1/4\sigma)+\epsilon} \quad (1/2 < \sigma <
  3/4),\label{c2:eq2.106} 
\end{equation}
and using the technique similar to the one employed in deriving
\eqref{c2:eq2.80}, it can be seen that ``$\epsilon$'' in
\eqref{c2:eq2.106} can be replaced by a suitable log-power. The point of
having upper bound such as \eqref{c2:eq2.104} and \eqref{c2:eq2.105}
is that both \eqref{c2:eq2.80} and \eqref{c2:eq2.106} can be improved
by the use of exponential sum techniques on writing $d(n) =
\sum\limits_{kl=n} 1$ and $\sigma_{1- 2 \sigma} (n)=
\displaystyle{\sum_{kl=n} k^{1-2 \sigma}}$, respectively. In this way
one obtains a double sum which is suitable for further transformations
and estimations. We shall consider first the estimation of $E(T)$ in
more detail and treat $E_\sigma(T)$ later on. However, right now we
remark again that \eqref{c2:eq2.104} corresponds to the truncated
formula \eqref{c2:eq2.25} for $\Delta  (x)$, namely 
\begin{equation}
  \Delta (x) = \frac{x^{1/4}}{\pi \sqrt{2}} \sum_{n \leq N}d(n)
  n^{-3/4} \cos \left( 4 \pi \sqrt{\pi x} - \frac{\pi}{4}\right) +
  O\left(x^{1/2 + \epsilon}N^{-1/2} \right),\label{c2:eq2.107}
\end{equation}
where $1 \ll N \ll x$. In this analogy $N$ corresponds to $T^{1 +
  \epsilon} H^{-2}$, $x$\pageoriginale to $T$, $4 \pi \sqrt{nx}-
\frac{\pi}{4}$ to $f(t, n)$, since
$$ 
f(t, n) =- \frac{\pi}{4} + 4 \pi \sqrt{\frac{n t}{2 \pi}} +
O(n^{3/2}t^{-1/2}) \;  (n=o(t)).
$$

Hence we may consider for simplicity the estimation of $\Delta 
(x)$, since the estimation of $E(T)$ via \eqref{c2:eq2.106} will be
completely analogous. Removing by partial summation $n^{-3/4}$ from
the sum in \eqref{c2:eq2.107}, we have by the hyperbola method
$$
\sum_{n \leq N} d(n) e(2 \sqrt{nx})=2 \sum_{m \leq \sqrt{N}} \sum_{n
  \leq N/m} e (2 \sqrt{mnx})- \sum_{m \leq \sqrt{N}} \sum_{n \leq
  \sqrt{N}} e(2 \sqrt{mnx}).
$$

If $(x. \lambda)$ is a one-dimensional exponent pair, then for $1 \ll
M \ll x$
$$ 
\sum_{M < n\leq M' \leq 2M} e(2 \sqrt{mnx}) \ll (mx M^{-1})^{\frac{1}{2} x} M^\lambda = (mx)^{\frac{1}{2}x}M^{\lambda - \frac{1}{2} x}. 
$$

Hence 
\begin{multline*}
  \sum_{n \leq N} d(n) e^{(2 \sqrt{nx})} \ll \sum_{m \leq \sqrt{N}}
  \left\{(mx)^{\frac{1}{2}x} (N/m)^{\lambda - \frac{1}{2} x} + (mx)^{\frac{1}{2} x}N^{\frac{1}{2}
    \lambda - \frac{1}{4} x}  \right\}\\
  \ll x^{\frac{1}{2} x} (N^{\lambda - \frac{1}{2} x} N^{\frac{1}{2} x- \frac{1}{2} \lambda + \frac{1}{2}} +
  N^{\frac{1}{2} \lambda - \frac{1}{2} x} N^{\frac{1}{4} x + \frac{1}{2}})\ll x^{\frac{1}{2} x}N^{\frac{1}{2}(1+ \lambda)}.
\end{multline*}

Partial summation gives
$$
\sum_{n \leq N} d(n) n^{-3/4} \cos \left(4 \pi \sqrt{nx} -
\frac{\pi}{4} \right) \ll x^{1/2 x} N^{1/2 \lambda - 1/4 x}, 
$$
and from \eqref{c2:eq2.107} it follows that
\begin{equation}
  \Delta  (x) \ll x^{1/4 + 1/2 x}N^{1/2 \lambda - 1/4} + x^{1/2 +
    \epsilon}N^{-1/2} \ll x^{(x+ \lambda)/(2 \lambda
    +1)+\epsilon}\label{c2:eq2.108}
\end{equation}
with the choice
$$
N= x^{(1- 2x)/(2 \lambda +1)}.
$$

The same approach can be used to estimate $E(T)$, by considering
separately even and odd $n$ to get rid of the factor $(-1)^n$ in
\eqref{c2:eq2.104}. We therefore obtain the bound
\begin{equation}
  E(T) \ll T^{(x+ \lambda)/(2\lambda +1)+\epsilon}.\label{c2:eq2.109}
\end{equation}

But\pageoriginale if $(x_0, \lambda_0)$ is exponent pair, then so is
also
$$
(x, \lambda) = B(x_0, \lambda_0)= \left(\lambda_0 - \frac{1}{2}, x_0 +
\frac{1}{2}\right). 
$$

This is the so-called $B$-process (Poisson step) in the theory of
exponent pairs. Then
$$
\frac{x+ \lambda}{2 \lambda +1} = \frac{x_0 + \lambda_0}{2(x_0 + 1)},
$$
hence we also have
\begin{equation}
  E(T) \ll T^{(x+ \lambda)/(2x+2)+\epsilon},\label{c2:eq2.110}
\end{equation}
where $(x, \lambda)$ is any exponent pair obtained after applying the
$B$-process. In \eqref{c2:eq2.109} the exponent of $T$ is less than
$1/3$ if $3 x + \lambda< 1$, i.e. in this case \eqref{c2:eq2.109}
improves \eqref{c2:eq2.80}. We remark that we shall also consider the
estimation of $E(T)$ in Section \ref{c4:sec4.5}. There we shall
obtain \eqref{c2:eq2.110} in another way, first from a general
approach to even moments, and then from an explicit formula of
$R$. Balasubramanian on $E(T)$. The latter gives an estimate which is
slightly sharper than \eqref{c2:eq2.110}, namely
$$
E(T) \ll T^{(x + \lambda)/(2x+2)}(\log T)^2.
$$

Nevertheless, it is worth noting that upper bounds for $E(T)$ such as
\eqref{c2:eq2.109} or \eqref{c2:eq2.110}, which involve the explicit
use of one-dimensional exponent pairs, are not likely to produce the
sharpest known upper bound for $E(T)$, which we state without proof as

\begin{thm}\label{c2:thm2.9}
  \begin{equation}
    E(T) \ll T^{7/22+\epsilon}.\label{c2:eq2.111}
  \end{equation}
\end{thm}

This is a recent result of D.R. Heath-Brown ad M.N. Huxley \cite{Huxley1},
which is analogous to the well-known bound $\Delta  (x) \ll x^{7/22+
\epsilon}$ of H. Iwaniec and C.J. Mozzochi \cite{Iwaniec1} (in both cases 
``$\epsilon$'' can be replaced by a suitable log-power). Thus the
analogy between $E(T)$ and $\Delta  (x)$ holds in this case in the
sense that at best known exponents in both problems are $7/22=
0. 3181818 \cdots$, and as we shall see in Chapter \ref{c3}, analogous
$\Omega_{\pm}$-results also hold in both problems.

The\pageoriginale proof of Theorem \ref{c2:thm2.9} is quite involved,
and its presentation would lead us too much astray. It rests on the
application of the {\small Bombieri-Iwaniec} method for the estimation of
exponential sums (see Notes for Chapter \ref{c1}), applied to the sum
$$
S= \sum_{h=H}^{2H-1} \sum_{m=M+ 2H-1}^{2M-2H} e \left(TF
\left(\frac{m+h}{M}\right) - TF \left(\frac{m-h}{M} \right)\right).
$$

As is common in such problems, $F(x)$ is a real-valued function with
sufficiently many derivatives which satisfy certain non-vanishing
conditions for $1 \leq x \leq 2$. Heath-Brown and Huxley succeeded in
treating $S$ by the ideas of Iwaniec-Mozzochi. They obtained a general
mean value bound which gives
$$
\int\limits_T^{T+ \Delta } |\zeta (\frac{1}{2} + it)|^2 dt \ll \Delta 
\log T \quad (T^{7/22} \log ^{45/22} T\ll \Delta  \leq T).
$$ 
 
This bound implies
\begin{equation}
  E(T + \Delta ) - E(T) \ll \Delta  \log T+ T^{7/22}
  \log^{67/22}T,\label{c2:eq2.112}   
\end{equation}
and they obtain \eqref{c2:eq2.111} in an even slightly sharper form,
namely
$$
E(T) \ll T^{7/22} \log^{111/22}T,
$$
by the use of an averaging technique, similar to the one used in
Section \ref{c5:sec5.6} for the fourth moment of $\zeta (\frac{1}{2} + it)$.

For the rest of this section we shall suppose that $1/2 < \sigma < 1$
is fixed, and we shall consider the estimation of 
$$ 
E_\sigma (T) = \int\limits_0^T |\zeta (\sigma + it)|^2 dt - \zeta (2
\sigma)T - \frac{\zeta(2 \sigma -1)\Gamma (2 \sigma -1)}{1-\sigma}
\sin (\pi \sigma) T^{2-2\sigma}. 
$$

To this end we shall introduce also the function
\begin{equation}
  I (T, \sigma; \Delta ): = (\Delta  \sqrt{\pi})^{-1}
  \int\limits_{-\infty}^\infty | \zeta (\sigma + iT + it)|^2
  e^{-t^2/\Delta ^2}dt \label{c2:eq2.113}
 \end{equation}
and 
\begin{equation}
  E(T, \sigma ; \Delta ) : = I (T, \sigma; \Delta ) - \zeta (2
  \sigma) - 2 \zeta (2 \sigma-1) \Gamma (2 \sigma -1) \sin (\pi
  \sigma)T^{1-2 \sigma},\label{c2:eq2.114}
\end{equation}
where $0< \Delta  \leq T/\log T$. Since
$$
\lim\limits_{\Delta  \to o +} I (T, \sigma ; \Delta ) = |\zeta
(\sigma + iT)|^2,
$$
it\pageoriginale follows that $E(T, \sigma ; \Delta )$ may be thought of as a sort
of a derivative of $E_\sigma(T)$. The connection between $E(T, \sigma;
\Delta )$ and $E_\sigma(T)$ will be made in the following lemmas.

\begin{lemma}\label{c2:lem2.4}
  For $1/2 < \sigma < 1$ fixed, $T^\epsilon \leq \Delta  \leq T^{1-
    \epsilon}$ and $L= 100 \sqrt{\log T}$ we have uniformly
  \begin{align*}
  \int\limits_T^{2T} |\zeta (\sigma + it)|^2 dt & \leq \int\limits_{T-
  L \Delta }^{2T + L \Delta } I (t, \sigma; \Delta ) dt + O(1)\\
  \intertext{and}
  \int\limits_T^{2T} |\zeta (\sigma + it)|^2 dt & \geq \int\limits_{T+
  L\Delta }^{2T- L\Delta } I (t, \sigma; \Delta ) dt + O(1).
  \end{align*}
\end{lemma}

\medskip
\noindent{
\textbf{Proof of Lemma 2.4.}} This lemma is similar to Lemma
\ref{c5:lem5.1}, and the proof is similar. We have
\begin{align*}
  \int_{T - L\Delta }^{2T+ L \Delta } I (t, \sigma; \Delta ) dt&
  = \int\limits_{- \infty}^\infty |\zeta(\sigma + iu)|^2 \left((\Delta 
  \sqrt{\pi})^{-1} \int\limits_{T- L \Delta }^{2T + L \Delta }
  e^{- (t-u)^2/\Delta ^2} dt\right) du\\
  & \geq \int\limits_T^{2T} |\zeta (\sigma + iu)|^2 \left((\Delta 
  \sqrt{\pi})^{-1} \int\limits_{T - L \Delta }^{2 T+ L \Delta }
  e^{- (t-u)^2.\Delta ^2} dt \right)du.
\end{align*}

But for $T \leq u \leq 2T$ we have, on setting $t- u = \Delta  v$,
\begin{align*}
  & (\Delta \sqrt{\pi})^{-1} \int\limits_{T- L\Delta }^{2T+ L
    \Delta } e^{- (t-u)^2/\Delta ^2} dt= \pi^{-1/2}
  \int\limits_{(T-u)/\Delta -L}^{(2 T-u)/\Delta  + L} e^{-v^2}
  dv\\
  & = \pi^{-1/2} \int\limits_{- \infty}^\infty e^{-v^2} dv + O
  \left(\int\limits_{100 \sqrt{\log T}}^\infty e^{- v^2} dv +
  \int\limits_{- \infty}^{-100 \sqrt{\log T}} e^{- v^2} dv  \right)\\
  & = 1+ O(T^{-10}).
\end{align*}

Therefore
$$
(1 + O (T^{-10})) \int\limits_T^{2T} |\zeta (\sigma + it)|^2 dt \leq
\int\limits_{T- L \Delta }^{2 T + L \Delta } I (t, \sigma;
\Delta ) dt,
$$
which\pageoriginale proves the upper bound inequality in the
lemma. The lower bound is proved analogously.

\begin{lemma}\label{c2:lem2.5}
  For $1/2 < \sigma < 1$ fixed, $T^\epsilon \leq \Delta  \leq
  T^{1-\epsilon}$ and $L = 100 \sqrt{\log T}$ we have uniformly
  $$
  |E_\sigma (2T)- E_\sigma (T)| \leq \left|\int\limits_{T- L
    \Delta }^{2T + L \Delta } E(t, \sigma; \Delta )dt \right| +
  \left| \int\limits_{T+ L\Delta }^{2T - L \Delta } E(t, \sigma ;
  \Delta ) dt \right| + O (L \Delta ).
  $$
\end{lemma}

\medskip
\noindent{
\textbf{Proof of Lemma 2.5.}} This follows from Lemma \ref{c2:lem2.4}
when we note that the integrals appearing in Lemma \ref{c2:lem2.4} on
the left-hand sides are equal to 
$$
E_\sigma (2T) - E_\sigma (T) + \left\{ \zeta (2 \sigma) + \frac{\zeta
  (2 \sigma -1) \Gamma (2 \sigma -1) \sin (\pi \sigma)t^{2 - 2
    \sigma}}{1- \sigma}\right\} \Bigg|_T^{2T},
$$
and then use \eqref{c2:eq2.114} and simplify.

\begin{lemma}\label{c2:lem2.6}
  For $1/2 < \sigma < 1$ fixed, $T^\epsilon \leq \Delta  \leq
  T^{1-\epsilon}$ we have uniformly
  \begin{multline*}
    \int\limits_0^T E(t, \sigma; \Delta ) dt = O(\log T)+
    2^{\sigma-1} \left(\frac{T}{\pi} \right)^{1/2 - \sigma}
    \sum^\infty_{n=1} (-1)^n \sigma_{1- 2 \sigma} (n) n^{\sigma-1}\times\\
    \left( ar \sinh \sqrt{\frac{\pi n}{2T}}\right)^{-1} \left(
      \frac{T}{2 \pi n} + \frac{1}{4}\right)^{- 1/4}\times\\ 
      \exp \left(-\left(\Delta  ar \sinh \sqrt{\frac{\pi
          n}{2T}}\right)^2\right) \cos (f (T, n)),
  \end{multline*}
where $f (T, n)$ is in Theorem \ref{c2:thm2.1}.
\end{lemma}

\medskip
\noindent{
\textbf{Proof of Lemma 2.6.}}  Follows from the method of Y. Motohashi
\cite{Motohashi8}, which is analogous to the method that he used in proving
Theorem \ref{c5:thm5.1}, the fundamental result on the fourth
moment. However, the case of the fourth moment is much more difficult
than the case of the mean square, hence only the salient points of the
proof of lemma \ref{c2:lem2.6} will be given.

Consider, for $\re u$, $\re v > 1$, $0< \Delta  \leq T/\log T$,
$$
I_o (u, v; \Delta ): = (\Delta  \sqrt{\pi})^{-1}
\int\limits_{-\infty}^\infty \zeta (u+ it) \zeta (v- it)
e^{-t^2/\Delta ^2} dt.
$$ 

Then\pageoriginale $I (u, v; \Delta )$ can be continued
meromorphically to $\mathbb{C}^2$, and by shifting appropriately the
line of integration one has, for $\re u$, $\re v < 1$,
\begin{multline*}
  I_0 (u, v; \Delta ) = (\Delta  \sqrt{\pi})^{-1} \int\limits_{-
    \infty}^\infty \zeta (u+ it) \zeta (v- it) e^{-t^2 / \Delta ^2}
  dt + \frac{2\sqrt{\pi}}{\Delta } \zeta (u+ v-1)\\
   \left\{ \exp \left(
  \left(\frac{u-1}{\Delta } \right)^2\right) + \exp \left(
  \left(\frac{v-1}{\Delta }\right)^2 \right) \right\}. 
\end{multline*}

On the other hand, in the region of absolute convergence we have
$$
I_0(u, v; \Delta ) = \zeta (u+ v) + I^{(1)} (u, v; \Delta ) +
I^{(1)} (v, u; \Delta ),
$$
where
$$
I^{(1)} (u, v; \Delta ): = \sum_{m, n=1}^\infty
 m^{-u} (m+n)^{-v} \exp \left(- \frac{\Delta ^2}{4} \log^2 \left( 1+
 \frac{n}{m}\right)\right).
$$

For $\re v > \re s > 0$ we define
$$
M(s, v; \Delta ): = (\Delta \sqrt{\pi})^{-1} \Gamma (s)
\int\limits_{- \infty}^\infty \frac{\Gamma (v + it -s)}{\Gamma (v +
  it)} e^{-t^2/\Delta ^2} dt,
$$
so that for $x > 0$, $a> 0$ we have
$$
(1+x)^{-v} \exp \left(- \frac{\Delta ^2}{4} \log^2 (1+x) \right) =
\frac{1}{2 \pi i} \int\limits_{a- i \infty}^{a+ i \infty} M(s, v;
\Delta ) x^{-s}ds. 
$$

This gives, for $\re (u +v) > a+1 > 2$,
$$
I^{(1)} (u, v; \Delta ) = \frac{1}{2 \pi i} \int\limits_{a- i
  \infty}^{a + i \infty} \zeta (s) \zeta (u+ v -s) M(s, v; \Delta ) ds.
$$


If $(u, v)$ lies in the region $b > \re (u+v) > a + 1 > 2$, then we
move the line of integration to $\re s=b$. In this way we obtain
\begin{align}
I^{(1)} (u, v; \Delta )&=
  \zeta (u+ v-1) M(u+ v-1, v; \Delta)  + \frac{1}{2 \pi i}\label{c2:eq2.115}\\
  &\qquad\int\limits_{b- i \infty}^{b+ i \infty} \zeta (s) \zeta(u+v-s) M(s,
  v; \Delta ) ds.\notag
\end{align}

From \eqref{c2:eq2.115} we have a meromorphic continuation of
$I^{(1)}$ to the region $b > \re (u+v)$. Then we use the functional
equation for $\zeta (s)$ and obtain from \eqref{c2:eq2.115}, for any
$\sigma<1$  and $b > 2$,
\begin{align}
& (\Delta  \sqrt{\pi})^{-1} \int\limits_{- \infty}^{\infty} |\zeta
  (\sigma + iT + it)|^2 e^{- t^2/\Delta ^2}dt= \zeta (2 \sigma) +
  \zeta(2 \sigma-1)\times\notag\\ 
&\quad \left\{ M(2 \sigma - 1,
  \sigma + iT ; \Delta ) + M(2 \sigma - 1, \sigma- iT; \Delta )
  \right\}\notag\\
&\quad - 2i (2 \pi)^{2 \sigma -2} \sum^\infty_{n=1}  \sigma_{2 \sigma -1}
  (n) \int\limits_{b- i \infty}^{b + i \infty} (2 \pi n)^{-s} \sin
  \left(\frac{2 \sigma \pi - \pi s}{2} \right)\times\notag\\
&\quad\Gamma (s+1 - 2
  \sigma)\left\{M (s, \sigma + i T \Delta ) + M (s \sigma - iT ;
  \Delta ) \right\}ds\notag\\
&\quad - 4 \pi (\Delta  \sqrt{\pi})^{-1} \zeta (2 \sigma-1) \re
    \left\{\exp \left(\frac{\sigma -1+ i T}{\Delta } \right)^2
    \right\}.\label{c2:eq2.116}
\end{align}

In\pageoriginale \eqref{c2:eq2.116} the sine is written by means of the exponential
function, and the resulting integrals are simplified by changing the
order of integration. In this way we obtain from \eqref{c2:eq2.116},
for $T^\epsilon \leq \Delta  \leq T^{1- \epsilon}$, $T\leq t \leq
2T$, and $I$ defined by \eqref{c2:eq2.113},
\begin{align}
I (t, \sigma; \Delta ) & = \zeta (2 \sigma)+ 2 \zeta (2 \sigma -1) \Gamma (2 \sigma -1)
(\Delta \sqrt{\pi})^{-1} \notag\\
&\re\left\{ \int\limits_{-
\infty}^\infty \frac{\Gamma (1- \sigma + it + iu)}{\Gamma (\sigma
+ it + iu)} e^{- u^2/\Delta ^2}du \right\}\notag\\
&+ 4 \sum_{n=1}^{\infty} \sigma_{1- 2 \sigma}(n) \int\limits_0^\infty
y^{- \sigma} (1+ y)^{-\sigma} \cos (2 \pi ny)\times\notag\\
&\quad\cos \left(t \log \left( 1+ \frac{1}{y}\right)\right)\exp \left(- \frac{\Delta ^2}{4}
\log^2 \left(1+ \frac{1}{y} \right)\right)dy\notag\\
& + O \left(\exp \left(-
\frac{1}{2} \log ^2 T \right)\right)\label{c2:eq2.117}
\end{align}

By Stirling's formula we find that, for $T \leq t \leq 2T$,
\begin{align}
  (\Delta  \sqrt{\pi})^{-1} \re & \left\{ \int\limits_{-
    \infty}^\infty \frac{\Gamma (1- \sigma + it + iu)}{\Gamma (\sigma
    + it + iu)} e^{-u^2/\Delta ^2}du\right\}
   t^{1-2\sigma} \sin(\pi \sigma) + O\left(\frac{1}{T}
   \right).\label{c2:eq2.118} 
\end{align}

On inserting \eqref{c2:eq2.118} in \eqref{c2:eq2.117} and integrating
it follows that 
\begin{align}
&\int\limits_T^{2T} E(t, \sigma ; \Delta ) dt=
O(1)+ 4 \sum_{n=1}^\infty \sigma_{1- 2 \sigma}
(\sigma) \label{c2:eq2.119}\\  
&\quad\left\{\int\limits_{0}^\infty \frac{\cos (2 \pi n y) \sin (t \log (1+
1/y))}{y^\sigma (1+y)^\sigma \log (1+ 1/y)} \exp \left( -
\frac{\Delta ^2}{4} \log^2 (1+ 1 /y)\right) dy \right\}
\Bigg|_T^{2T}.\notag 
\end{align}

It\pageoriginale remains to evaluate the exponential integrals in
\eqref{c2:eq2.119}. This can be done directly by the saddle-point, as
was done by Y. Motohashi \cite{Motohashi8}, or one can use Atkinson's Theorem
\ref{c2:thm2.3}. In fact, save for the factor $\exp \left( -
\frac{\Delta ^2}{4} \cdots\right)$, the integrals in question are of
the same type as those considered by Lemma \ref{c2:lem2.2}. By either
method one obtains Lemma\ref{c2:lem2.6}, replacing $T$ by $T2^{-j} (j=
1, 2, \ldots)$ and adding all the results.

As we have
$$
ar \sinh x = x - \frac{x^3}{6} + O(|x|^5) \quad (|x|\leq 1),
$$ 
it follows that we may truncate the series in Lemma \ref{c2:lem2.6} at
$n= 100 T \Delta ^{-2}$ $\log T$ with an error which is $O(1)$,
provided that $\Delta  \ll T^{1/2}$. Hence combining Lemma
\ref{c2:lem2.5} and Lemma \ref{c2:lem2.6} we obtain

\begin{thm}\label{c2:thm2.10}
  For $1/2 < \sigma < 1$ fixed, $T^\epsilon \leq \Delta  \leq
  T^{1/2}$ and $f(T, n)$ as in Theorem \ref{c2:thm2.1}, we have
  uniformly
\begin{align}
&E_\sigma (2T)-E_\sigma(T) \ll \Delta  \sqrt{\log
    T}+\sup\limits_{\frac{2T}{3} \leq \tau \leq 3 T} \tau^{1/2 -
    \sigma}\times\notag \\ 
&\qquad\left|\sum_{n \leq 100 T \Delta ^{-2} \log T} (-1)^n \sigma_{1-
      2 \sigma} (n) n^{\sigma-1} \left(ar \sinh \sqrt{\frac{\pi n}{2
        \tau}}\right)^{-1}\times\right.\notag\\ 
&\qquad\left.\left(\frac{\tau}{2 \pi n}+ \frac{1}{4}
    \right)^{-1/4} \exp \left(- \left(\Delta  ar \sinh
    \sqrt{\frac{\pi n}{2 \tau}} \right)^{2} \right) \cos (f(\tau,
    n))\right|.\label{c2:eq2.120}
  \end{align}
\end{thm}

This result may be compared to Theorem \ref{c2:thm2.9}. The parameter
$H$ in \eqref{c2:eq2.105} corresponds to $\Delta $ in
\eqref{c2:eq2.120}. The latter has two distinct advantages: the range
for $\sigma$ is the whole interval $1/2 < \sigma < 1$, and in the
range for $n$ there is no $T^\epsilon$, but only $\log T$ present.

To obtain bounds for $E_\sigma (T)$ from Theorem \ref{c2:thm2.10}, we
use first the fact that, for $\sigma > 1/2$ and $x \to \infty$,
$$
\sum_{n \leq x} \sigma_{1- 2\sigma} (n) \sim \zeta (2 \sigma) x.
$$

Hence by partial summation and trivial estimation one obtains from
Theorem \ref{c2:thm2.10} 
\begin{multline*}
E_\sigma (2T) - E_\sigma (T) \ll T^{3/4 -\sigma} \sum_{n
  \leq 100 T \Delta ^{-2} \log T} \sigma _{1- 2\sigma} (n) n^{\sigma-5/4}\\
+ \Delta  (\log T)^{1/2} \ll T^{1/2}\Delta ^{- 2 \sigma+ 1/2}
(\log T) ^{\sigma - 1/2}+ \Delta  (\log T)^{1/2}.
\end{multline*}

Choosing\pageoriginale
$$
\Delta   = T^{1/(1+ 4\sigma)} (\log T)^{(4 \sigma-3)/(8\sigma +2)}
$$
we obtain 
\begin{equation}
  E_\sigma (T) \ll T^{1/(1+ 4 \sigma)} (\log T)^{(4 \sigma-1)/ (4
  \sigma+1)} \quad (1/2 < \sigma < 1).\label{c2:eq2.121}
\end{equation}

This bound obviously sharpens and extends \eqref{c2:eq2.106}. Note
that, for 
\begin{equation}
  \sigma > \frac{3+ \sqrt{17}}{8} = 0.890388\ldots,\label{c2:eq2.122}
\end{equation}
we have
$$
\frac{1}{1+ 4\sigma} > 2 - 2 \sigma,
$$
so that in this range the bound \eqref{c2:eq2.121} for $E_\sigma (T)$
becomes larger than the second main term (see \eqref{c2:eq2.2}) in the
asymptotic formula for 
$${\int\limits_0^T |\zeta (\sigma +
  it)|^2}dt.$$ 
Now we shall use the theory of exponent pairs to
estimate $E_\sigma(T)$, which will yield non-trivial bounds in the
whole range $1/2 < \sigma < 1$, and in particular when $\sigma$
satisfies \eqref{c2:eq2.122}. To this end first note that the factor
$(-1)^n$ appearing in the sum in \eqref{c2:eq2.120} is harmless, since
one may consider separately subsums over even and odd integers. Thus,
with a slight abuse of notation, this factor may be discarded. The
functions $ar \sinh (\pi n/2T)^{1/2}$ and $\left(\frac{T}{2 \pi n} +
\frac{1}{4}\right)^{-1/4}$ may be approximated by the first few terms
coming from the Taylor expansion. The factor $\exp (-(\Delta 
\ldots)^2)$ lies between 0 and 1, and since it is monotonically
decreasing as a function of $n$, it may be removed from the sum by
partial summation. Thus the problem is reduced to the estimation
$$ 
S(T, N) : = T^{3/4-\sigma}\left| \sum_{n \leq N} \sigma_{1-2 \sigma}
(n) n^{\sigma -5/4} \exp (if~ (T, n))\right|,
$$
where $N \ll T \Delta ^{-2} \log T$. We estimate first
$$
  \sum (T, N): = \sum_{n \leq N} \sigma_{1- 2 \sigma} (n) \exp
  (if~ (T, n)).
$$

Write\pageoriginale
{\fontsize{10}{12}\selectfont
\begin{align*}
\sum (T, N) &= \sum_{km \leq N} k^{1- 2 \sigma} \exp (\mbox{ if } (T,
  km))\\
&= 2 \sum_{k \leq N^{1/2}} k^{1- 2 \sigma} \sum_{m \leq N/k}
  \exp(if~ (T, km))\\
&\qquad - \sum_{k \leq N^{1/2}} k^{1-2 \sigma}
  \sum_{m \leq N^{1/2}} \exp (if~ (T, km)).
\end{align*}}

The sums over $m$ are split into subsums of the form
$$
S_M : = \sum_{M< m \leq M' \leq 2M} \exp (if~ (T, km)) \qquad
(1 \leq M \leq N).
$$

Note that
$$
\frac{\partial f(T, km)}{\partial m} \sim \left(\frac{2 \pi kT}{m}
\right)^{\frac{1}{2}}  > 1,
$$
and that higher derivatives with respect to $m$ may be also easily
evaluated. Their form is such that $S_M$ may be estimated by the
theory of exponent pairs as
$$
S_M \ll (Tk)^{\frac{1}{2}x}M^{\lambda - \frac{1}{2} x},
$$
and consequently
{\selectfont{\fontsize{10}{8}{
\begin{align*}
  \sum (T, N) & \ll \sum_{k \leq N^{\frac{1}{2}}} k^{1- 2 \sigma} (Tk)^{\frac{1}{2}} 
  (N/k)^{\lambda- \frac{1}{2} x} + \sum_{k \leq N^{\frac{1}{2}}} k^{1-2\sigma}
  (Tk)^{\frac{1}{2}x}N^{\frac{1}{2} \lambda - \frac{1}{4} x}\\
  & \ll T^{\frac{1}{2} x}N^{\lambda - \frac{1}{2} x} \sum_{k \leq N^{\frac{1}{2}}} k^{1-2
    \sigma + x- \lambda} + T^{\frac{1}{2} x} N^{\frac{1}{2} \lambda - \frac{1}{4} x} \sum_{k
    \leq N^{\frac{1}{2}}} k^{1- 2 \sigma + \frac{1}{2} x}\\
  & \ll T^{\frac{1}{2} x}N^{1- \sigma + \frac{1}{2} \lambda}
\end{align*}}}}
if
\begin{equation}
  \lambda - x < 2 - 2 \sigma,\label{c2:eq2.123}
\end{equation}
and if equality holds in \eqref{c2:eq2.123} we get an extra
log-factor. Partial summation gives then
$$ 
S(T, N) \ll T^{\frac{3}{4} - \sigma + \frac{1}{2} x} N^{\frac{1}{2} \lambda - \frac{1}{4}} \ll T^{\frac{3}{4}-
\sigma + \frac{1}{2} x} (T \Delta ^{-2} \log T)^{\frac{1}{2} \lambda - \frac{1}{4}},
$$ 
and consequently we obtain
\begin{equation}
  E_\sigma (2T) - E_\sigma (T) \ll T^{1/2 (x + \lambda+1)- \sigma}
  \Delta ^{1/2 - \lambda} (\log T)^{1/2 \lambda - 1/4} +
  \Delta (\log T)^{1/2}. \label{c2:eq2.124}
\end{equation}

If\pageoriginale we choose
$$
\Delta   =T^{\frac{(1- 2 \sigma) + \chi + \lambda}{2 \lambda +1}} (\log
T)^{\frac{2 \lambda -3}{4 \lambda + 2}}
$$
to equalize the terms on the right-hand side of \eqref{c2:eq2.124}, we
obtain the following

\begin{thm}\label{c2:thm2.11}
  If $(\chi, \lambda)$ is an exponent pair and $1/2 < \sigma < 1$ is a
  fixed number such that $\lambda - x < 2 - 2 \sigma$, then 
  \begin{equation}
    E_\sigma (T) \ll T^{(1- 2 \sigma + \chi + \lambda)/(2 \lambda +1)}
    (\log T)^{(2 \lambda -1)/(2 \lambda +1)}.\label{c2:eq2.125}
  \end{equation}
\end{thm}

By specialising $(\chi, \lambda)$ one can get several interesting
estimates from \eqref{c2:eq2.125}. For example, by taking $(\chi,
\lambda)= (1/2, 1/2)$ it is seen that \eqref{c2:eq2.125} holds for
$1/2 < \sigma < 1$, hence we have

\begin{coro}\label{c2:coro1}
  For $1/2 < \sigma < 1$ fixed we have
  \begin{equation}
    E_\sigma(T) \ll T^{1- \sigma}.\label{c2:eq2.126}
  \end{equation}
\end{coro}

Note that \eqref{c2:eq2.126} provides a bound which is always of a
lower order of magnitude than the main terms, and it improves
\eqref{c2:eq2.121} for $3/4 < \sigma < 1$. 

Another possibility is to choose $(x, \lambda)$ such that $\lambda
= x + \frac{1}{2}$. Then \eqref{c2:eq2.123} reduces to $\sigma <
3/4$, and the estimate given by \eqref{c2:eq2.125} is optimal for
$\chi$ minimal. Taking $(\chi, \lambda)= \left(\frac{9}{56} + \epsilon, 
\frac{37}{56}+ \epsilon \right)$, which is a recent exponent pair of
Hauley-Watt, we see that even the case $\sigma =\frac{3}{4}$ may be
treated, and we obtain

\begin{coro}\label{c2:coro2}
  For $1/2 < \sigma \leq 3/4$ fixed we have
  \begin{equation}
    E_\sigma (T) \ll T^{(51-56 \sigma)/ 65 +
      \epsilon}, \label{c2:eq2.127} 
  \end{equation}
  and in particular
  \begin{equation}
    E_{3/4}(T) \ll T^{9/65+ \epsilon}.\label{c2:eq2.128}
  \end{equation}
\end{coro}

Note that \eqref{c2:eq2.121} gives only $E_{3/4}(T) \ll T^{1/4}(\log
T)^{1/2}$, which is much\pageoriginale weaker than
\eqref{c2:eq2.128}. There are possibilities to choose various other
exponent pairs which, depending on the condition \eqref{c2:eq2.123},
will provide various estimates for $E_\sigma(T)$. We list here only
one more specific example. Namely, with $(\chi, \lambda) =
\left(\frac{11}{30}, \frac{16}{30} \right)$ we see that
\eqref{c2:eq2.123} holds for $\sigma \leq 11/12$, and
\eqref{c2:eq2.125} yields

\begin{coro}\label{c2:coro3}
  For $\frac{1}{2} < \sigma < \frac{11}{30}$ fixed we have
  $$
  E_\sigma (T) \ll T^{(57 -60\sigma)/62} (\log T)^{1/2}
  $$
  and we also have
  $$
  E_{11/12} (T) \ll T^{1/31} (\log T)^{3/2}.
  $$
\end{coro}

Of course, there are possibilities to use the techniques of
two-dimen\-sional exponential sums to estimate $\Sigma (T, N)$. These
techniques may, at least in some ranges of $\sigma$, lead to further
small improvements of our results. Also it may be remarked that
\eqref{c2:eq2.125} in the limiting case when $\sigma \to 1/2 + 0$
reduces to the bound \eqref{c2:eq2.110} for $E(T)$.

\section[Asymptotic Mean Square of the Product....]{Asymptotic Mean
  Square of the Product of the Zeta Function 
  and a Dirichlet Polynomial}\label{c2:sec2.8}

The title of this section refers to the asymptotic formula for the
integral
$$
 I (T, A) : = \int\limits_0^T \left|\zeta \left(\frac{1}{2} + it\right)\right|^2 \left|A \left(\frac{1}{2} + it\right)\right|^2 dt,
$$
where
$$
A(s) = \sum_{m \leq M} a (m) m^{-s}
$$
is a Dirichlet polynomial. Problems involving the application of $I(T,
A)$ with various (complex) coefficients $a(m)$ frequently occur in
analytic number theory. Thus it is desirable to have an asymptotic
formula for $I(T, A)$, or at least a good upper bound. It is natural
to expect that for relatively small $M$ an asymptotic formula for
$I(T, A)$ may be derived, and we know that in the trivial case $M=1$
such a formula\pageoriginale exists. R. Balasubramanian et
al. \cite{Balasubramanian1} established an asymptotic formula in the case when
$a(m)\ll_\epsilon m^\epsilon$ and $\log M \ll \log T$. They proved
that 
\begin{align}
  I (T, A) & = T \sum_{h, k \leq M} \frac{a(h)\ob{a(k)}}{hk} (h, k) \left(\log
  \frac{T(h, k)^2}{2 \pi hk}+ 2 \gamma -1\right)\notag\\
  & \qquad + O_\epsilon (T^\epsilon M^2) + O_B (T
  \log^{-B}T)\label{c2:eq2.129} 
\end{align}
for any $B> 0$, so that one gets a true asymptotic formula from
\eqref{c2:eq2.129} in the range $1 \leq M \ll T^{1/2 - \epsilon}$. In
some special cases, or in the case when some additional conjectures
are assumed (e.g. like Hooley's Conjecture $R^*$ for Kloosterman
sums), the error term in \eqref{c2:eq2.129} can be improved. The
asymptotic formula for $I (T, A)$ can be recovered (by an argument
similar to the one used in Sections \ref{c5:sec5.6} and
\ref{c5:sec5.7}) from the asymptotic formula for
$$
g(u) : = \left(\Delta  \sqrt{\pi}\right)^{-1} \int\limits_{- \infty}^\infty
|\zeta (1/2 + it)|^2 |A(1/2 + it)|^2 e^{(- t-u)^2/\Delta ^2}dt,
$$
where $T \leq u \leq 2T$, $\exp (5 \sqrt{\log T}) \leq \Delta  \leq
T/\log T$. The evaluation of $g(u)$ is rather delicate. Putting $s_0 =
1/2 + iu$ one can write
$$ 
g(u) = \left(i \Delta  \sqrt{\pi}\right)^{-1} \int\limits_{\frac{1}{2} - i \infty}^{\frac{1}{2}
+ i \infty} e^{(s-s_0)^2/\Delta ^2} \zeta^2 (s) \chi (1-s)
A(s)\ob{A(1-s)} ds
$$
and move the line of integration to $\re s=1+ \eta (\eta>0)$. This
procedure is difficult, as it involves the evaluation of certain
complicated integrals containing the exponential function.

Another approach to the evaluation of $I (T, A)$ is given by
Y. Motohashi [3, Part V]. He observed that the argument of Atkinson
which leads to the proof of Theorem \ref{c2:thm2.1} may be generalized
to produce the asymptotic formula for $I (T, A)$. His result is 

\begin{thm}\label{c2:thm2.12}
  If $A(S) = \displaystyle{\sum_{m \leq M}} a (m) m^{-s}$ with $a(m)
  \ll_\epsilon m^\epsilon$ and $\log M \ll log T$, then 
  {\selectfont{\fontsize{10}{8}{
  \begin{equation}
    I (T, A)= T \sum_{h, k \leq M} \frac{a(h) \ob{a (k)}}{hk} (h, k)
    \left(\log \frac{T(h, k)^2}{2 \pi hk} + 2\gamma -1\right) +
    O_\epsilon \left(T^{\frac{1}{3}+ \epsilon} M^{4/3}\right).\label{c2:eq2.130}
  \end{equation}}}}
\end{thm}

\begin{proof}
  Only\pageoriginale an outline of the proof will be given, since full
  details require considerable technicalities. Unlike
  \eqref{c2:eq2.129}, from \eqref{c2:eq2.130} one can obtain the
  estimate $E(T)\ll_\epsilon T^{1/3+ \epsilon}$ simply by taking
  $m=1$. We have, for $\re u >1$, $\re v > 1$,
  \begin{align*}
    \zeta (u) \zeta (v) A (u) \ob{A (\ob{v})} & = \zeta (u+v) \sum_{k,
    l \leq M} a (k) \ob{a(l)} [k, l]^{-u-v}\\
    & \qquad M (u, v)+ \ob{M(\ob{v}, \ob{u})},\\
    \intertext{where}
    M (u, v) : &= \sum^\infty_{m=1} \sum^\infty_{n=1} \left(\sum_{k
      \mid m} a(k) \right)~ \left( \sum_{l \mid (m+n)} \ob{a(l)}
    m^{-u} (m+n)\right)^{-v}.
  \end{align*}
\end{proof}

This relation is an easy generalization of \eqref{c2:eq2.7} and
\eqref{c2:eq2.8}  in the initial stage of the proof of Atkinson's
formula. To obtain analytic continuation of $M(u, v)$ to the region
$\re u< 1$ we use the formula
$$
r^{-s} = \frac{1}{\Gamma(s)} \int\limits_0^\infty y^{s-1} e^{-ry} dy
\qquad (r> 0, \re s > 0)
$$
to replace summation over $m$ and $n$ by a double integral over $(0 ,
\infty) \times (0, \infty)$. Then we replace this integral by
integrals over the contour $\mathscr{C}$. This is the loop contour
which starts at infinity, proceeds along the positive real axis to
$\delta(0 < \delta < 1/2)$, describes a circle of radius $\delta$
counterclockwise around the origin and returns to infinity along the
positive axis. This procedure leads to 
{\fontsize{10}{12}\selectfont
$$
M(u, v) = \Gamma (u+ v+1) \frac{\Gamma (1-u)}{\Gamma (v)} \zeta (u+
v-1) \sum_{k, l} \frac{(k, l)^{1- u-v}}{[k, l]} a(k) \ob{a(l)}+ g(u,
v; A),
$$}
where
\begin{multline*}
   g(u, v; A): = \frac{1}{\Gamma (u) \Gamma(v) (e^{2 \pi i u}-1) (e^{2
      \pi i v}-1)} \sum_{k, l} \frac{a(k) \ob{a(l)}}{l} \sum_{f=1}^l\\
  \int\limits_{\mathscr{C}} \frac{y^{v-1}}{e^{y- 2 \pi i f/l}-1}
  \int\limits_{\mathscr{C}} x^{u-1} \left(\frac{1}{e^{kx+ky-2\pi i
      fk/l}-1}- \frac{\delta (f)}{kx+ ky}\right) dx\, dy. 
\end{multline*}

Here\pageoriginale $\delta(f) =1$ if $l\mid kf$ and zero otherwise,
and the double integral is absolutely convergent for $\re u <
1$. Collecting the above estimates it follows that, for $0 < \re u <
1$ and $u+ v\to 1$, we have 
\begin{multline*}
  \zeta (u) \zeta (1-u) A(u) \ob{A (1- \ob{u})}= \sum_{k, l} \frac{a
    (k \ob{a(l)})}{[k, l]} \\
  \left\{\frac{1}{2} \left( \frac{\Gamma' (u)}{\Gamma (u)} +
  \frac{\Gamma' (1-u)}{\Gamma (1-u)}\right)  + \log \frac{(k,
    l)^2}{kl} + 2 \gamma - 2 \log (2 \pi)\right\}\\
  + g(u, l - u; A) + g(1- u, u; \ob{A}).
\end{multline*}

Setting $k^* = k/(k, l)$, $l^* = 1/(k, l)$, $\ob{k^* k}\equiv 1 \mod
l^*$, one transforms $g$ into
\begin{align}
  & g(u, l - u; A)= \sum_{k, l} \frac{a(k) \ob{a(l)}}{[k,
      l]}\label{c2:eq2.131}\\ 
  & \sum_{n \neq 0} d(|n|) \exp (2 \pi i \ob{k^*}/l^*)
  \int\limits_0^\infty \exp((2 \pi i ny)/(k^* l^*)) y^{-u} (1+
  y)^{u-1} dy.\notag
\end{align}

Further transformations of \eqref{c2:eq2.130} may be carried out by
the analogue of the Voronoi formula for the function
\begin{align}
   \Delta  (x, \ob{k^*}/l^*): &= \sum_{n \leq x} d(n) \exp \left(2 \pi
  i \frac{\ob{k^*}}{l^*}n \right)\notag\\ 
  &\quad - \frac{x}{l^*} (\log x + 2 \gamma
  -1 - 2 \log l^*) - D\left(0, \frac{\ob{k^*}}{l^*}
  \right),\label{c2:eq2.132} 
\end{align}
where
$$
D\left(s, \frac{\ob{k^*}}{l^*} \right): = \sum_{n=1}^\infty d(n) \exp
\left(2 \pi i \frac{\ob{k^*}}{l^*} n \right)n^{-s} \qquad (\re s> 1).
$$

Formulas for \eqref{c2:eq2.132} are given by M. Jutila \cite{Jutila9}, and
their use makes it possible to estimate the error term in
\eqref{c2:eq2.130}. If the function in question is denoted by $E(T,
A)$, then the result of Theorem \ref{c2:thm2.12} follows by using an
averaging technique, either the analogue of \eqref{c2:eq2.13}, or by
considering 
$$
\int\limits_{- \infty}^\infty E(T) + t, A) e^{- (t/G)^2}dt,
$$
the\pageoriginale suitable choice for $G$ being $G=
T^{1/3+\epsilon}M^{4/3}$. Note that the error term in
\eqref{c2:eq2.130} does not exceed the error term in
\eqref{c2:eq2.129}. 

\newpage

\begin{center}
  \textbf{\LARGE Notes For Chapter 2}
\end{center}
\bigskip

The definition of $E_\sigma (T)$ in \eqref{c2:eq2.2} differs from
K. Matsumoto's definition in \cite{Matsumoto1} by a factor of 2. This is
because Matsumoto considers $\displaystyle{\int\limits_{-T}^T |\zeta
  (\sigma + it)|^2} dt$, but I thought it more appropriate to define
$E_\sigma (T)$ in such a way that the limiting formula
\eqref{c2:eq2.3} holds. I have followed Matsumoto in the use of the
function $\sigma_{1- 2 \sigma}(n)$ in \eqref{c2:eq2.6} and in the
sequel. This notation is perhaps a little awkward because of the
double appearance of ``sigma'', but I hope this will cause no
confusion. 

In view of \eqref{c2:eq2.4} and the fact that $G(T)=O (T^{3/4})$ (see
\eqref{c3:eq3.1} and Lemma \ref{c3:lem3.2}) perhaps it would be more
consistent to define
$$ 
E(T) = \int\limits_{0}^T |\zeta (1/2 + it)|^2 dt - T \left(\log
\frac{T}{2 \pi} + 2 \gamma -1 \right)- \pi,
$$
which would be stressing the analogy between $E(T)$ and $\Delta 
(x)$. However, I prefer to use the standard notation introduced in the
fundamental paper of F.V. Atkinson \cite{Atkinson2} (see also his work
\cite{Atkinson3} which contains the ideas used in \cite{Atkinson2}). Moreover, one
defines often in the literature $\Delta  (x)$ not by
\eqref{c2:eq2.4} but as
$$
\Delta  (x) = \sum_{n \leq x} d(n) \; - \;  x(\log x + 2 \gamma -1),
$$
which is then more in true with the definition of $E(T)$ is extensively
studied by M. Jutila \cite{Jutila1}, \cite{Jutila4} and \cite{Jutila5}.

Generalizations of Atkinson's method to $L$-functions were made
independently by Y. Motohashi in the series of papers \cite{Motohashi3}, and by
T. Meurman \cite{Meurman2}, \cite{Meurman5}. For example the latter notes that, for
$\re u > 1$\pageoriginale and $\re v > 1$ one has
$$
\sum_{\chi \pmod{q}} L(u, \chi) L(v, \ob{\chi}) = \varphi (q) (L (u+v,
\chi_0) + f_q (u, v)+ f_q (v, u)),
$$
where $\chi_0$ is the principal character modulo $q$ and 
$$
f_q (u, v)= \sum_{r=1, (r, 1)=1}^\infty \sum_{s=1}^\infty r^{-u} (r+ qs)^{-v},
$$
the double series being absolutely convergent for $\re (u+v)> 2$, $\re
v > 1$. Writing
$$
f_q (u, v)= \sum_{k \mid q} \mu(k) \sum_{s=1}^\infty \sum_{r=1}^\infty
(kr + qs)^{-v},
$$
one can apply Poisson's summation formula to the sum over $r$ if $\re
u < -1$, $\re (u+v)>2$, and then carry on the analysis in the manner
of Atkinson. In this fashion Meurman eventually obtains an asymptotic
formula for 
\begin{align*}
  E(q. T): &= \sum_{\chi \pmod{q}} \int\limits_0^T |L (1/2 + it,
  \chi)|^2 dt\\ 
  &\quad - \frac{\varphi^2 (q)}{q} T \left(\log \frac{qT}{2 \pi}
  + \sum_{p \mid q} \frac{\log p}{p-1} + 2 \gamma -1 \right),
\end{align*}
which generalizes Atkinson's formula for $E(T) \equiv E(1,
T)$. Y. Motohashi also obtains several interesting results concerning
applications of Atkinson's method to $L$-functions. Thus in Part I of
\cite{Motohashi3} he proves that, if $t$ is a fixed real and $q$ is a prime,
then
\begin{multline*}
  (q-1)^{-1} \sum_{\chi \pmod{q}} |L (1/2 + it, \chi)|^2 = \log
  \frac{q}{2 \pi} + 2 \gamma + \re \frac{\Gamma'}{\Gamma} (1/2 + it)\\
  + 2 q^{-1/2} |\zeta (1/2 + it)|^2 \cos (t \log q)- q^{-1} |\zeta
  (1/2 + it)|^2 + O (q^{-3/2}),
\end{multline*}
a result that suggests some peculiar relation between the zeros of
$\zeta (s)$ and the values of $L$-functions. Part V of Motohashi
\cite{Motohashi3} is discussed in Section \ref{c2:sec2.8}.

Theorem \ref{c2:thm2.1} is due to F. V. Atkinson \cite{Atkinson2}, and Theorem
\ref{c2:thm2.2}  was proved by K. Matsumoto \cite{Matsumoto1}.

In deriving \eqref{c2:eq2.28} we started from $\re u < 0$, but
\eqref{c2:eq2.28} in fact can be seen to hold for $\re u<1$.

The\pageoriginale classical formula of G.F. Voronoi \cite{Voronoi1}, \cite{Voronoi2}
for $\Delta  (x)$ is discussed also in Chapter \ref{c3} of Ivi\'c \cite{Ivic1}
and by M. Jutila \cite{Jutila9}. The asymptotic formula for
\eqref{c2:eq2.38} for $\Delta _{1- 2 \sigma} (x)$ is due to
A. Oppenheim \cite{Oppenheim1}. The proof of the truncated formula
\eqref{c2:eq2.40} for $\Delta _{1- 2 \sigma}(x)$ is analogous to the
proof of the formula for $\Delta (x)$ given in Chapter 12 of
E.C. Titchmarsh \cite{Titchmarsh1}. for properties of Bessel functions the
reader is referred to the monograph of G.N. Watson\cite{Watson1}.

A more general version of the second mean value theorem for integrals
than \eqref{c2:eq2.51} is as follows. Suppose $f(x)$ is monotonic and
$g(x)$ integrable on $[a, b]$, $a < b$. Then there exists $a \leq \xi
\leq b$ such that
$$ 
\int\limits_a^b f(x) g(x) dx= f(a) \int\limits_a^\xi g(x) dx + f(b)
\int\limits_\xi^b  g(x) dx.
$$

Namely, let $G(x)= \displaystyle{\int\limits_a^x} g(t)
dt$. Integration by parts gives
$$
\int\limits_a^b f(x) g(x) dx = \int\limits_a^b f(x) dG(x) = G(b) f(b)
- \int\limits_a^b G(x) df(x).
$$

Suppose now $f(x)$ is increasing. Then $df(x)$ is a positive Stieltjes
measure, and in view of continuity of $G(x)$ the last integral above
equals 
$$
G(\xi) (f(b)- f(a))\qquad (a \leq \xi \leq b),
$$
so that after rearrangement we obtain the result.

Theorem \ref{c2:thm2.3}, Lemmas \ref{c2:lem2.2} and \ref{c2:lem2.3}
are all from F.V. Atkinson \cite{Atkinson2}. Proofs of these results may be
also found in Chapter \ref{c2} of Ivi\'c \cite{Ivic1}, and results on exponential
integrals in the monographs of M. Jutila \cite{Jutila9}  and E. Kr\"atzel
\cite{Kratzel1}. For this reason and because exponential integrals are not
the main topic of this text, I have not given the proofs of these
results.

The discussion concerning the simplified version of Atkinson's Theorem
\ref{c2:thm2.3} (with the conditions 2.' and 3.') is due to T. Meurman
\cite{Meurman1}. 

The mean square formula \eqref{c2:eq2.69} is due to D.R. Heath-Brown
\cite{Heath-Brown2}.\pageoriginale Theorem \ref{c2:thm2.4} was obtained
independently by T. Meurman \cite{Meurman3} and Y. Motohashi \cite{Motohashi4}, and
the latter work contains a comprehensive account on his important work
on the analogue of the Riemann-Siegel formula for $\zeta^2
(s)$. Theorem \ref{c2:thm2.5} is due to K.-C. Tong \cite{Tong1}. The
asymptotic formula \eqref{c2:eq2.71}, in the range $1/2 < \sigma <
3/4$, is due to K. Matsumoto \cite{Matsumoto1}. He kindly informed me that,
jointly with T. Meurman, he succeeded in improving the error term in
\eqref{c2:eq2.71} to $O(T \log^4 T)$.

Concerning the use of \eqref{c2:eq2.101} for the proof of
\eqref{c2:eq2.100} (and the sharpening of \eqref{c2:eq2.76} by a
log-factor), it may be remarked that E. Preissmann in correspondence
informed me that he also obtained a proof of \eqref{c2:eq2.100}. In
\cite{Preissmann2} he actually treats in detail the circle problem (i.e., the
function $P(x)= \displaystyle{\sum_{n \leq x} r(n)}- \pi x$, where
$r(n)$ is the number of representation of $n$ as a sum of two integer
squares) by the classical method of E. Landau \cite{Landau1} and
\eqref{c2:eq2.101}, getting
$$
\int\limits_1^X P^2 (x) dx = CX^{3/2} + O (X \log^2 X), C =
\frac{1}{2\pi^2} \sum_{n=1}^\infty r^2 (n) n^{- \frac{3}{2}}.
$$  

The divisor problem is closely related to the circle problem (see
Chapter 13 of Ivi\'c \cite{Ivic1}), and similar methods may be applied to
both. However, the above result for $P(x)$ is not new, since it was
proved long ago by I. K\'atai \cite{Katai1}. K\'atai used the estimate
$$
\sum_{n \leq x} r(n) r(n+ k) \ll x \sum_{d \mid k} \frac{1}{d}
\quad (\text{uniformly for}~1 \leq k \leq x^{1/3}),
$$
which he proved by ingenious elementary arguments.

Y. Motohashi remarked that alternatively one can prove 
$$ 
\sum_{r \leq M, n\leq M, n+ r\leq M} \frac{d(n) d (n+r)}{r n^{3/4}
  (n+r)^{1/4}} \ll \log^4 x 
$$
by noting that, for $r \ll x$,
$$
\sum_{n \leq x} d(n) d(n+r) \ll \sigma_{-1} (r) x \log^2 x,
$$
which follows e.g. from a theorem of P. Shiu \cite{Shiu1} on
multiplicative functions.\pageoriginale

Theorem \ref{c2:thm2.7} corresponds to Theorem 15.5 of A. Ivi\'c
\cite{Ivic1}, which is a result of M. Jutila \cite{Jutila1}. Theorem
\ref{c2:thm2.8} is given by K. Matsumoto \cite{Matsumoto1}.

R. Balasubramanian's formula \cite{Balasubramanian1} for $E(T)$ will be discussed in
Chapter \ref{c4}. There it will be shown how a smoothing technique, combined
with the method that gives \eqref{c2:eq2.110}, leads to the estimate
$$ 
E(T) \ll T^{(x + \lambda)/(2 x + 2)} \log^2 T.
$$

This result is superseded by a bound of D.R. Heath-Brown and
M.N. Huxley \cite{Heath-Brown and Huxley1}, contained in Theorem \ref{c2:thm2.9}.

In \cite{Motohashi8} Y. Motohashi uses the argument briefly described in the
proof of Lemma \ref{c2:lem2.6} to prove, for $1/2 < \sigma < 1$ fixed
and $T^\epsilon \leq \Delta  \leq T^{1- \epsilon}$,
\begin{multline*}
  E(T, \sigma; \Delta ) = O\left(\Delta ^2 T^{-1- 2 \sigma}\right) +
  O\left(\Delta ^{1/2  - 2 \sigma} T^{-1/2 \log^5 T}\right)+\\
  + 2^\sigma \left(\frac{T}{\pi}\right)^{1/2 - \sigma}
  \sum_{n=1}^\infty (-1)^{n-1} \sigma_{1- 2 \sigma} (n) n^{\sigma-1}
  \left(\frac{T}{2 \pi n} + \frac{1}{4}\right)^{- 1/4}\\ 
  \sin f(T, n)
  \exp \left(-\left(\Delta\,  ar \sinh \sqrt{\frac{\pi n}{2T}}\right)^2\right) 
\end{multline*}
uniformly in $\Delta$. Motohashi's idea to work with the smoothed
integral \eqref{c2:eq2.113} is used in his fundamental works [3, Part
  VI], \cite{Motohashi6} on the fourth power moment, which is discussed in
Chapter \ref{c5}. There is the reader will find more on the properties of the
function $M(s, v; \Delta )$, which is crucial in establishing
\eqref{c2:eq2.116}. For details on \eqref{c2:eq2.118}, see
\eqref{c3:eq3.19}, where a similar expression is also evaluated by
Stirling's formula.

Proofs of Theorem \ref{c2:thm2.10} and Theorem \ref{c2:thm2.11} have
not been published before.

As was already remarked in Notes for Chapter \ref{c1}, M.N. Huxley and
N. Watt \cite{Huxley and Watt1} discovered new exponent pairs. This means that these
exponent pairs, one of which is the pair $\left( \frac{9}{56} + \epsilon,
\frac{37}{56}+ \epsilon \right)$ used in the text, cannot be obtained
by a finite number of applications of the so-called $A-$,
$B-$processes and convexity to the trivial exponent pair $(x,
\lambda)= (0, 1)$.

For\pageoriginale C. Hooley's Conjecture $R^*$, which is important for
the asymptotic formula \eqref{c2:eq2.129} of R. Balasubramanian
et. al. \cite{Balasubramanian1}, see C. Hooley \cite{Hooley1}.

The Dirichlet series
$$
D\left(s, \frac{k}{\ell}\right) = \sum_{n=1}^\infty d(n) \exp \left(2
\pi i \frac{k}{\ell} n \right) n^{-s} \quad (\re s > 1),
$$
which appears in \eqref{c2:eq2.132} is sometimes called the Estermann
zeta - function. This is in honour of T. Estermann, who in \cite{Estermann1}
studied analytic properties of this function. It will appear again in
Chapter \ref{c5} in connection with the fourth power moment. For its
properties one can also see M. Jutila \cite{Jutila9}.

