

\chapter[The Trajectories of Solutions to Stochastic....]{The Trajectories of Solutions to Stochastic Differential
  Equations}%%%% 2

\section{The Martingale Problem}\pageoriginale%%%% 1
 
In Chapter \ref{chap1} we have discussed diffusion's from $It \hat{o}'s$ point
of view, that is as solutions to stochastic differential
equations. For the purpose of certain applications it is useful to
adopt a slightly different point of view. 

Let $\Omega =C([ 0, \infty), R^n]$ and think of $\Omega$ as a Polish
space with the metric of uniform convergence on compact sets. Denote
by $M$ the Borel field over $\Omega$. Given $t > o$ and $\omega \in
\Omega$, let $x (t,\omega)$ be the position in $R^n$ of $\omega$ at
time $t$ and set $M_t = \sigma (x(s): 0 \leq s \leq t)$.  If $L$ is a
differential operator of the from 
\begin{equation*}
L = \frac{1}{2}\sum^{n}_{i, j=1}a^{ij}(x)\frac{\partial^2}{\partial
  x_i \partial x_j}+ \sum^{n}_{i=1}b^i (x)\frac{\partial} {\partial
  x_i}, \tag{1.1}\label{chap2:eq1.1} 
\end{equation*}
where $a(.)$ and $b(.)$ are continuous functions with values in
$S_n$ (the set of symmetric, non-negative definite real $n \times n$ matrices)
and $R^n$ respectively and if $x \in R^n$, we say that $P$
\textit{solves the martingale} problem for $L$ starting from $x$
(Notation $P \sim L$ at $x$) if $P$ is a probability measure on
$(\Omega, M)$, $P(x(o)=x)=1$ and for all $f \in C^\infty_0(R^n):
(X_f(t),M_t,P)$ is a martingale, where 
$$
X_f(t)\equiv f(x(t)) - \int \limits^{t}_{0}Lf(x(s))ds. 
$$

If for each $x \in R^n$ there is precisely one such $P$, we say that
the \textit{martingale problem for $L$ is well-posed.} 


\setcounter{theorem}{1}
\begin{theorem}\label{chap2:thm1.2} % them 1.2
Let\pageoriginale $\sigma(.)$ and $b(.)$ be as in (I,
\ref{chap1:thm3.3}) and let $L$ 
be gives as 
(\ref{chap2:eq1.1}) with $a(.)=\sigma \sigma^{\ast} (.)$. Then the martingale
problem for $L$ is well-posed. In fact, for each $x \in R^n$ , the
unique $P_x \sim L$ at $x$ coincides with the distribution of $\xi
(.,x)$ is the solution to (I, \ref{chap1:eq2.3}).  
\end{theorem}

\begin{proof}
The first step is the simple remark that, by It$\hat{o}'s$ formula,
if $\xi (.,x)$ is given by (I, \ref{chap1:eq2.3}), then $P \sim L$ at $x$, where
$P$ denotes the distribution of $\xi (.,x)$. Thus for each $x \in R^n$
the martingale problem for $L$ admits a solution. 
\end{proof}

The next step is to show that if $P \sim L $ at $x$ and if $f \in
C^\infty ([o,T) \times R^n)$ for some $T > o$, then 
$$
(f (t \Lambda T, x(t)) - \int \limits^{t \Lambda T}_{o} (\partial/
  \partial s + L)f(s,x(s))ds, \mu_t,P) 
$$
is a martingale. The proof of this fact runs as follows: 
Given $o \leq s < t \leq T$ and $N \geq 1$, let $u_k =
s+k/N(t-s)$. Then for $A \in \mu_s$, 
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
 E^p[f(t, & x(t))- f(s,x(s)), A]\\
 & = \sum^{n-1}_{k=0}E^P[f(u_{k+1}, x(u_{k+1})) -f(u_k, x(u_k)),A]\\ 
 & = E^p [1/n \sum^{n-1}_{k=0} \frac{\partial f}{ \partial u}(u_k, x
   (u_{k+1}))+\sum^{n-1}_{k=0} \int \limits^{u_{k+1}}_{u_k} Lf
   (u_k,x(v))dv,A ] + 0\left(\frac{1}{N} \right)\\ 
 & \to E^p [ \int \limits^t_s (\partial / \partial u + L) f(u,x(u))
   du, A]\text{ as } n \to \infty. 
\end{align*}}\relax

Having\pageoriginale proved the preceding, the identification of $P
\sim L$ at $x$ 
is quite easy. Namely, let $f \in C^\infty_0(R^n)$ and define 
$u_f (t,x)= E[f(\xi (t,x))]$ as in (I, \ref{chap1:eq4.1}). Then
$u_f \in C_b([0, \infty) \times R^n) \cap C^\infty ([0, \infty) \times
    R^n)$ and $u_f$ satisfies (I, \ref{chap1:eq4.6}). 
Given $R > o$, choose $\eta _R \in C^\infty_0 (B(0,2R))$ so that $0
\leq \eta_R \leq 1$ and $\eta_R \equiv 1$ on $\overline{B(o,R)}$. Then
the function 
$$
F_R(t,y)= \eta_R(y) u_f (T-t,y) \in C^\infty _0 ([0,T]\times R^n)
$$
for any $T > 0$. Hence
$$
(F_R(t \Lambda T, x(t \Lambda T)) - \int \limits^t_o (\partial /
\partial s + L) F_R(s,x(s))ds, \mu_t,P) 
$$
is a martingale. Thus by Doob's stopping time theorem,
$$
(u_f(T-t \Lambda \tau_R \Lambda T, x(t \Lambda \tau_R \Lambda T)), \mu_t
, P) 
$$
is a martingale, where 
$$
\tau^R = \inf \{ t \geq o : | x(t) - x(o)| \geq R \}. 
$$

Since $\tau_R \uparrow \infty$ as $R \uparrow \infty$ and $u_f$ is
bounded, it follows that  
$$
(u_f(T-t \Lambda T, x(t \Lambda  T)), \mu_t , P)
$$
is a martingale. In particular, if $0 \leq s \leq T$ then 
\begin{align*}
E^p[f (x(T)) | \mu_s] & = E^p[u_f (o, x(T))| \mu_s]\\
& = u_f (T -s, x(s)) \qquad (a.s., P).
\end{align*}
working\pageoriginale by induction on $N$, it follows easily that if 
$0 \leq t_1 < \cdots t_N$ and $f_1, \ldots ,f_N \in C(R^n)$, then
$$
E^p[ f_1 (x(t_1)) \ldots f_N (x(t_N))]= E[ f_1 (\xi(t_1,x)) \ldots f_N
  (\xi(t_N,x))] 
$$ 

Clearly this identifies $P$ as the distribution of $\xi (.,x)$. 

\setcounter{exercise}{2}
\begin{exercise} % exercise 1.3
Using exercise (I, \ref{chap1:exer4.9}), show that theorem
(\ref{chap2:thm1.2}) 
continues to hold 
if $\sigma (.)$,  $b(.)\in C^3 (R^n)$ and $(\partial^{|\alpha|}
\sigma/\partial x^\alpha) (.)$ and $(\partial^{|\alpha|} b/\partial
x^\alpha) (.)$ are bounded for $1 \leq |\alpha | \leq 3$. 
The point is that under these conditions, one can show that the $u_f$
defined by (I, \ref{chap1:eq4.1}) is in $C^{1,2}([0, \infty] \times R^n)$ and
still satisfies (I, \ref{chap1:eq4.5}). Actually, theorem
(\ref{chap2:thm1.2}) is true even if $\sigma (.)$ and $b(.)$ just
satisfy (I, \ref{chap1:eq2.1}); however, the proof in  
this case is quite different (cf. chapter 5 and 6 of [$S$ \& $V$]). 
\end{exercise}

\begin{exercise} % exercise 1.4
Let $\{ \sigma_m (.)\}^\infty_{m=1}$ and $\{ b_m(.)\}^\infty_{m=1}$ be
sequences of co-efficients satisfying the hypotheses of theorem (I,
\ref{chap1:thm3.3}).  Assume that $\sigma_m(.)\to \sigma(.)$ and that
$b_m(.) \to 
b(.)$ uniformly on compact sets where $\sigma(.)$ and $b(.)$ again
satisfy the hypotheses of (I, \ref{chap1:thm3.3}). For $m \geq 1$ and
$x \in R^n$  
let $P^m_x \sim L_m$ at $x$, where $L_m$ is defined for $\sigma_m(.)$ and
$b_m(.)$; and for $x\in R^n$ let $P_x \sim L$ at $x$ where $L$ is
defined for $\sigma(.)$ and $b(.)$. Show that if $x_m \to x$, then
$P^m_{x_m}\Rightarrow p_x$ on $\Omega$, where ``$\Rightarrow$'' means
convergence in the sense of weak convergence of measures. The idea is
the following.  

In\pageoriginale the first place one can easily check that 
$$
\Sup_{m \geq 1} E^{P^m_{x_m}}[ | x(t) -x(s)|^4]\leq C(T)|t-s|^2, \;\; 0
\leq s \leq t \leq T 
$$
for each $T > o$.

Combining this with the fact that $\{x_m\}^\infty_1$ is relatively
compact, one can use (I, \ref{chap1:eq2.7}) to see that for each
$\varepsilon > 0$ 
there is a compact $K_\varepsilon \subseteq \Omega$ such that
$P^m_{x-m}(K) \geq 1-\varepsilon $ for all $m \geq 1$. By Prohorov's
theorem (cf. Chapter \ref{chap1} of [$S$ \& $V$]) this means that $\{
P^m_{x_m}\}^\infty_{m=1}$ is relatively compact in the weak
topology. Finally, one can easily check that if $\{ P^{m'}_{x_m'}\}$ is
any weakly convergent subsequence of $\{ P^m_{x_m}\}$ and
$P^{m'}_{x_m'}\Rightarrow P$, then $P \sim L$ at $x$; and so
$P^m_{x_m}\Rightarrow P_x$. 
\end{exercise}

\section{Approximating Diffusions by Random Evolutions}

We start with an example:


Let $\tau_1, \tau_2 , \ldots , \tau_n \ldots$ be independent unit
exponential random variables on some probability space (i.e. $P(\tau_1
> s_1, \ldots , \tau_n > s_n) = \exp\break (- \sum \limits^{n}_{j=1}s_j)$ for
all $(s_1, \ldots ,s_n) \quad [0, \infty)^n$). Define $T_o \equiv o$,
  and $T_n = \sum \limits^{n}_{j=1}\tau_j$, $n \geq 1$; and consider
  $N(t)= \max \{n \geq 0 : T_n \leq t\}$. Since $P((\exists n > 1):
  \tau_n = o)=o$, it is clear that a.s. the path $t \to N(t)$ is
  right continuous, non-decreasing, piecewise constant and jumps by
  1 when it jumps. 

\setcounter{lemma}{0}
\begin{lemma}\label{chap2:lem2.1}% lemma 2.1
For $0 \leq s < t $ and $m \geq 0$:
\begin{equation*}
P(N(t)-N(s)= m | \sigma (N(u),u \leq s ))= \frac{(t-s)^m}{m
  !}e^{-(t-s)} a.s.\tag{2.2}\label{chap2:eq2.2}  
\end{equation*}

 That is, $N(t)- N(s)$ is independent of $\sigma(N(u): 0 \leq u \leq
 s)$ and is a Poisson random variable with intensity $(t-s)$.  
\end{lemma}

\begin{proof}
Note\pageoriginale that if $t \geq 0 $ and $h > 0$,
\begin{align*}
P(N(t+h) & - N(t) \geq 1/ \sigma (N(u) : u \leq t))\\
& = P(N(t+h)-N(t) \geq 1 / \sigma (T_1,\ldots ,T_{N(t)}, N(t))) \\
& = P(\tau _{N(t)+1} \leq t+h-T_{N(t)}/ \sigma(T_1,\ldots, T_{N(t)}, N(t)))\\
& = P(\tau _{N(t)+1} \leq t+h-T_{N(t)}/ \tau_{N(t)+1}> t - T_{N(t)})\\
& = 1-P(\tau_{N(t)+1} > t+h -T_{N(t)} /\tau_{N(t)+1}>t -T_{N(t)})\\
& = 1-e^{-h}\\
& = h+ \phi(h)
\end{align*}
Similarly
\begin{align*}
P(N(t+h) & - N(t) \ge 2/ \sigma (N(u) : u \le t))\\ 
& = P(\tau_{N(t)+1} + \tau_{N(t)+2} \le t+h - T_{N(t)}/ \tau_{N+1} >
t-T_{N(t)})\\ 
& = \int \limits^h_0 e^{-u}(1-e^{-(h-u)})du = \Psi (h). 
\end{align*}

Because of the structure of the paths $N(.)$, if $o \leq s < t$ and
$u_{n,k}\equiv s+ \dfrac{k}{n}(t-s)(n \geq 1 $ and $0 \leq k \leq n)$,
then we now see that $P(N(t)-N(s) = m| \sigma(N(u):u \leq s))$ 
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
& =\lim_{n \to \infty} \sum _{\substack { A \subset \{1, \ldots ,n\}
      \\ { |A| =m}}}P(N(u_{n,k})-N(u_{n,k-1}) =X_A(k), 1 \leq k \leq n
  | \sigma (N(u):u \leq s))\\ 
& =\lim_{n \to \infty}(^n_m)(h_n + \phi (h_n) - \Psi
  (h_n))^m(1-h_n-\phi (h_n))^{n-m}\\ 
& = \frac{(t-s)^m}{m!}e^{-(t-s)}
\end{align*}}\relax
where we have used $h_n \equiv \dfrac{t-s}{n}$

Next\pageoriginale define $\theta (t) = (-1)^{N(t)}, t \ge 0$. Clearly
$\theta(.)$ 
is right continuous, piece -wise constant and $\theta(t) \neq \theta
(t -)$ implies $\theta(t) = - \theta (t -)$. 
\end{proof}

\setcounter{lemma}{2}
\begin{lemma}\label{chap2:lem2.3}% lemma 2.3
If $f: R^1 \times \{ -1,  1 \} \rightarrow R^1$ is a function such
that $f(., \theta) \in C^1_b(R'), \theta \{ -1,1 \}$, then 
\begin{multline*}
(f(\int \limits^t_0 \theta(u)du, \theta(t)) - \int \limits^t_0 [\theta
  (s) f' (\int \limits^t_0 \theta (u) du, \theta(s))\\
  + Kf(\int
  \limits^s_0 \theta(u)du, \theta(s))]ds, F_t , p) 
\end{multline*}
is a martingale, where $F_t = \sigma(\theta(u): 0 \le u \le t), f'(x,
\theta) = \dfrac{d}{dx}f(x,\theta)$ and $Kf(x, \theta) = f(x, -\theta)
- f(x, \theta)$. 
\end{lemma}

\begin{proof}
If we can prove the result when $f$ does not depend on $x$, then the
general case follows immediately by the argument given in the second
step of the proof of theorem (II, 4.2). But from (\ref{chap2:eq2.2})
it is easy to see that  
$$
\frac{d}{dt} E[f(\theta(t)) |F_s = E[Kf (\theta(t)) |F_s , 0 \le s \le
    t, 
$$
and so the result holds for $f$ not depending on $x$.  

Consider the process
\begin{equation*}
x_\varepsilon (t) = \varepsilon\int\limits^t_0 \theta(s)ds,
\tag{2.4}\label{chap2:eq2.4}   
\end{equation*}
where $\varepsilon > 0$, and define 
\begin{equation*}
X_\varepsilon (t) = x_\varepsilon(t) + \frac{\varepsilon}{2} \theta
(t). \tag{2.5}\label{chap2:eq2.5}  
\end{equation*}
\end{proof}

\setcounter{lemma}{5}
\begin{lemma} % lemma 2.6
If $\phi \in C^\infty_0 (R^1)$, then there is an $F.-$progressively
measurable process $L_{\phi, \varepsilon}(.)$ such that $|L_{\phi,
  \varepsilon}(.)| < C || \phi ||_{C^3_b}(R^1)$  and 
$$
(\phi (x_\in (t)) - \dfrac{\in ^2}{2}
\int \limits^t_0 \phi''(x_\in(s)) ds - \in^3 \int \limits^t_0 L_{\phi ,
  \in}(s)ds, F_t, P
$$ 
is\pageoriginale a martingale. 
\end{lemma}

\begin{proof}
By (\ref{chap2:lem2.3}) with $f(x, \theta) = \phi (\varepsilon x +
\dfrac{\varepsilon }{2} \theta)$: 
\begin{multline*}
(\phi (X_\varepsilon (t)) - \varepsilon \int\limits^t_0 \theta(s)\phi'
(X_\varepsilon (s))ds \\
- \int\limits^t_0 (\phi '
(X_\varepsilon (s)) - \varepsilon\theta(s))(X_\varepsilon(s)))ds, F_t , P) 
\end{multline*}
is a martingale. By Taylor's theorem
\begin{align*}
\phi(X_\varepsilon(s) & - \varepsilon \theta(s)) - \phi(X_\varepsilon (s)) \\ 
& = \varepsilon \theta(s) \phi' (X_\varepsilon (s)) +
\frac{\varepsilon^2}{2} \phi''(X_\varepsilon(s)) -
\frac{\varepsilon^2}{6} \phi''(X_\varepsilon (s)- \delta(s)\theta(s) 
\end{align*} 
where $0 < \delta(s) < \varepsilon$. Thus we can take
$$
L_{\phi , \varepsilon} = - \frac{1}{6}'' (X_\varepsilon(s) - \delta(s)
\theta(s)). 
$$

We now see that for $\phi \in C_0 (R^1)$:
$$ 
(\phi(X_\varepsilon (\frac{t}{\varepsilon^2})) - \frac{1}{2}
\int\limits^t_0 \phi'' (X_\varepsilon (\frac{2}{\varepsilon^2}))ds -
\varepsilon \int\limits^t_0 L_{\phi , \varepsilon}
(\frac{s}{\varepsilon^2})ds, F_{t/  \varepsilon^2}, P) 
$$
is a martingale. Thus if we let $p^\varepsilon$ be the distribution on
$\Omega(n = 1)$ of $X_\varepsilon (./ \varepsilon^2)$, then, since $|
X_\varepsilon (./ \varepsilon^2)-X_{\varepsilon} (./\varepsilon^2) |
\leq \varepsilon /2$, it is
reasonable to suppose that $P^\varepsilon \Rightarrow \omega$ as
$\varepsilon \downarrow 0$, where $\omega \sim \dfrac{1}{2}
\dfrac{d^2}{dx^2}$ at 0 (i.e. $\omega$ is 1-dimensional Wiener
measure), in the weak topology. If we know that $\{ P^\varepsilon :
\varepsilon > 0\}$ were compact, then this convergence would follow
immediately from the observation that if $\varepsilon _n \downarrow 0$
and $P^{\varepsilon_n} \Rightarrow P$, then  
$$
(\phi(x(t)) - \frac{1}{2} \int\limits^t_0 \phi''(x(s)) ds, \mu_t , P) 
$$
is a martingale for all $\phi \in C^\infty_0(R^1)$. 

In\pageoriginale order to handle the compactness question, we state
the following theorem.  
\end{proof}

\setcounter{theorem}{6}
\begin{theorem}\label{chap2:thm2.7}%%% 2.7
Let $\{ P^\varepsilon : \varepsilon > 0 \}$ be a family of probability
measures on $(\Omega, \mu )$ and let $A(\phi)$, $\phi \varepsilon C^\infty_0
(R^n)$, be a non-negative number such that $A(\phi) < C || \phi
||_{C_k(R^n)}$ for some $C < \infty$ and $k \ge 0$. Assume that for
each $\varepsilon > 0$ there is a $M$.-progressively measurable
function $X_\varepsilon : [0, \infty ) \times \Omega \rightarrow R^n$
  such that  
\begin{enumerate}[(i)]
\item $\lim\limits_{R \uparrow \infty} \Sup\limits_{\varepsilon > 0}
  P^\varepsilon(| X_\varepsilon (0)| \ge R) = 0$ 

\item $X_\varepsilon (.)$ is right continuous and has left limits
  ($P^\varepsilon  -$ a.s.),

\item $\overline{\lim\limits_{\varepsilon}} P^{\varepsilon}
  (\Sup\limits_{0\leq t \leq T} |x(t) - X_{\varepsilon (t)}| \geq
  \delta ) =0$ for each $T >0$ and $\delta >0$,

\item $(\phi(X_\varepsilon(t)) + A(\phi)t, \mu_t , P^\varepsilon )$ is a
  submartingale for each $\varepsilon > 0$.
\end{enumerate}

If $\{\varepsilon_n \}^\infty_1$ is any sequence of positive numbers
tending to zero, then $\{P^{\varepsilon_n} : n \ge 1 \}$ has a weakly
convergent sub-sequence. 
\end{theorem}

\medskip
\noindent\textbf{Comment on the proof:}
We will not give the proof here because it is somewhat
involved. However, in the case when $X_\varepsilon(.) = x(.)$ for all
$\varepsilon > 0$, the proof is given in Chapter \ref{chap1} of [$S \& V$]
(cf. Theorem 1.4.6) and the proof of the general case can be easily
accomplished by modifying the lemma 1.4.1 given there. The
modification is the following. 

\begin{lemma*}
Let $t_1$ and $t_2$ be any pair of points in [0, $T$]. such that $|t_2
- t_1| < \delta_\omega(\rho)$. Then 
$$
|X(t_2 , \omega) - X (t_1 ,\omega) | \le \rho + \Sup_{0 \le t \le T} |  
X_\varepsilon (t,\omega) - x_\varepsilon (t, \omega) | 
$$\pageoriginale
and so 
\begin{gather*}
\Sup  \{ |X(t_2, \omega) - X(t_1 ,\omega) | : 0 \le t_1 \le t_2
\le T  \; \text{ and } \; |t_2 - t_1| < \delta (\rho) \} \\
< \rho + 2 \Sup_{0 \le t \le T} |X_\varepsilon (t, \omega) -
x_\varepsilon (t, \omega)| 
\end{gather*}
\end{lemma*}

The notation is the same as that in lemma 1.1.4.1 of [$S \& V$]. We
now see that the measures $P^\varepsilon$ discussed in the paragraph
preceding (\ref{chap2:thm2.7}) converge weakly, as $\varepsilon
\downarrow 0$, to 
$\omega$. Notice that this convergence result provides some insight
into the structure of Brownian paths. Indeed, the paths of
$x_\varepsilon (./\varepsilon^2)$ are rather simple; they all have
speed  $1/\varepsilon$ and the times at which they change directions
are distributed like sums of independent exponential random variables
having mean $\varepsilon^2$. In the limit, this constant speed
property is reflected by the a.s. constancy of the square variation of
Brownian paths over a given time interval. 

We now want to generalize the preceding in order to get analogous
approximations of more general diffusions. To this end, let $G =
SO(d)$ and let $\lambda$ denote normalized Haar measure on $G$. By
expanding the original probability space on which the $\tau_j$'s were
defined, we may assume that there exist $G-$valued random variables
$g_1 , g_2,\ldots , g_n \ldots$ which are independent of $\tau_j$'s
and independent of one another, and each have distribution
$\lambda$. Let $\theta_0 \in S^{d-1}$ be fixed and define
$\theta(t) = g_{N(t)} g_{N(t)-1} \cdots g_1 \theta_0\break ( \equiv
\theta_0$ if $N(t) = 0)$. Set $F_t = \sigma(\theta(u) : 0 \le u \le
t)$. By the same argument as that used to prove (\ref{chap2:lem2.3}),
one can prove that if  
$$
A(t) = \int\limits^t_0 \gamma (s)\, ds  
$$\pageoriginale
where $\gamma(.)$ is a bounded, right continuous , $R^N-$ valued, $F-$
progressively measurable function and if $f \in C^{1,0}_b (R^N
\times S^{d-1})$, then  
\begin{align*}
(f(A(t), \theta (t)) & - \int\limits^t_0 [< \gamma (s), \text{ grad}_x
    f(\Lambda(s), \theta(s)) > \\
& \qquad + Kf (A(s), \theta(s))]ds, F_t P)  \tag{2.8}\label{chap2:eq2.8}  
\end{align*}
is a martingale; where
$$
Kf(x,\theta) = \int\limits_{S^{d-1}} (f(x,\eta) -f (x, \theta))d \eta
$$
and $d \eta$ denotes normalized uniform measure on $S^{d-1}$

\setcounter{lemma}{8}
\begin{lemma}% lemma (2.9)
For each $\varepsilon > 0$, let $\gamma_\varepsilon(.)$ be a right
continuous $R^N$-valued, $F$.-progressively measurable function such
that 
$$
|\gamma (.)| \le \rho (\varepsilon) 
$$
where $\rho(\varepsilon) \downarrow 0$ as $\varepsilon \downarrow
0$. Set 
$$
A_\varepsilon (t) = \int \limits^t_0 \gamma_\varepsilon (s)ds.
$$
Then for any $h \in C^{1,0}(R^N \times S^{d-1}), T > 0$ and
$\delta > 0$: 
$$
\lim \limits_{\varepsilon \downarrow 0} P(\int \Sup\limits_{0 \le t
  \le T} | \varepsilon \int\limits^{t/\varepsilon}_0 (h(A_\varepsilon(s),
\theta(s)) -\bar{h}(A_\varepsilon (s))) ds \ge \delta | ) = 0 
$$
where $\bar{h}(x) \equiv  \int \limits_{S^{d-1}} h(x, \eta)d\eta$. 
\end{lemma}

\begin{proof}
Clearly we may assume that $h(0 , \theta_0) = 0$. Set
\begin{align*}
& \Delta_\varepsilon (t) = \varepsilon \int\limits^t_0 (h(A_\varepsilon
(s), \theta(s)) - \bar{h}(A_\varepsilon(s)))ds \\
\text{and } \qquad & \tilde{\Delta_\varepsilon}(t) = \Delta_\varepsilon(t) +
\varepsilon h(A_\varepsilon(t), \theta(t)).  \hspace{3cm}
\end{align*}
Clearly\pageoriginale it suffices to prove that
$$
\Sup_{0 \le t \le T} |\tilde{\Delta_\varepsilon} (t / \varepsilon)|
\rightarrow 0 \text { in probability, as } \varepsilon \downarrow 0. 
$$

Thus it is enough to check that if $\phi \in C^2_b (R^1)$ and
$\phi(0) = 0$, then  
$$
E[ \Sup_{0 \le t \le T} \phi (\tilde{\Delta_\varepsilon}(t/
  \varepsilon) ] \rightarrow 0 \text{ as } \varepsilon \downarrow 0. 
$$

To this end, note that by (\ref{chap2:eq2.8})
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
M_{\varepsilon,\phi} (t) & \equiv \phi (\tilde{\Delta}_\varepsilon(t)) -
\varepsilon \int \limits ^t_0 (h - \bar{h}) (A_\varepsilon(s),
\theta(s)) \phi ' (\tilde{\Delta}_\varepsilon(s)) ds \\ 
& - \varepsilon \int \limits ^t_0 < \gamma_\varepsilon (s), grad_x
h(A_\varepsilon(s), \theta(s)) > \phi'(\tilde{\Delta}_\varepsilon(s)) ds
\\ 
& - \int\limits^t_0 (\int\limits_{S^{d-1}}[\phi (\Delta_\varepsilon(s) + h
  (A_\varepsilon (s), \eta)) - \phi (\Delta_\varepsilon (s) +
  \varepsilon h (A_\varepsilon (s), \theta(s))] d\eta) ds 
\end{align*}}\relax
is an $F$.- martingale with respect to $P$.

By Taylor's theorem
$$
\int \limits _{S^{d-1}} [ \phi (x + \varepsilon h (A_\varepsilon(s),
  \eta)) -  \phi (x + \varepsilon h (A_\varepsilon(s), \theta(s))) ] d
\eta 
$$
\begin{align*}
& = \varepsilon \phi '(x) \int \limits _{S^{d-1}} [h
    (A_\varepsilon(s), \eta)-h(A_{\varepsilon} (s), \theta (s))] d \eta \\ 
& + \varepsilon^2 /2 \int \limits _{S^{d-1}} R_\varepsilon (x,  h
  (A_\varepsilon(s), \eta),  h (A_\varepsilon(s) , \theta(s)) d\eta 
\end{align*}
where $|R_\varepsilon (.)| < C || \phi || _{C^2_b (R^1)}$. Thus 
$$
\Sup_{0 \le t \le T} | \phi (\tilde{\Delta}_\varepsilon
(t/\varepsilon)) - M_{\varepsilon , \phi}(t/\varepsilon) | \le
C(\phi)(\varepsilon + \delta(\varepsilon))T 
$$
and so we need only to show that $E[ \Sup\limits_{0 \le t \le T}
  M^2_{\varepsilon , \phi}(t / \varepsilon)] \rightarrow 0$ as
$\varepsilon \downarrow 0$. But $E[ \Sup\limits_{0 \le t \le T}
  M^2_{\varepsilon , \phi}(t / \varepsilon)] \le 4 E[M^2_{\varepsilon
    , \phi}(T/\varepsilon)]$, and
$$
M^2_{\varepsilon ,  \phi}(T/\varepsilon) \leq 2 \phi^2
(\tilde{\Delta}_\varepsilon (T/\varepsilon)) + 2(C(\phi))^2
(\varepsilon + \delta(\varepsilon))^2 T^2.
$$\pageoriginale 

Thus it suffices to show that  $E[\phi^2 (\tilde{\Delta}_\varepsilon(T/
  \varepsilon))] \rightarrow 0$ as $\varepsilon \downarrow 0$. Finally,  
$$
|\phi^2 (\tilde{\Delta}_\varepsilon (T/ \varepsilon)) -
M^2_{\varepsilon , \phi^2}(T/ \varepsilon) | \le C(\phi^2)
(\varepsilon + \delta(\varepsilon ))T, 
$$
and so  
$$
E[\phi^2 (\tilde{\Delta}_\varepsilon (T/ \varepsilon)) \le C(\phi^2)
  (\varepsilon +  \delta(\varepsilon)) T, 
$$
since  
$$
E[M_{\varepsilon , \phi^2}(T/ \varepsilon)] = E[M_{\varepsilon ,
    \phi^2}(0)] = 0. 
$$
\end{proof}

\setcounter{theorem}{9}
\begin{theorem}\label{chap2:thm2.10}% them 2.10
Let $F : R^n \times S^{d-1} \rightarrow R^n$ be a smooth bounded
function having bounded derivatives and satisfying  
$$ 
\int \limits_{S^{d-1}} F(x, \eta)d \eta = 0  \text { for  all } x
\in R^n. 
$$

Let $b : R^n \rightarrow R^n$ be a smooth bounded function having
bounded derivatives. Define 
$$
a^{ij}(x) = \int \limits_{S^{d-1}}\int \limits_{S^{d-1}} (F^i (x,
\eta) - F^i (x, \xi)) (F^j (x, \eta) - F^j(x, \xi)) d\xi d \eta 
$$
and 
$$
c^i(x) = \int \limits_{S^{d-1}} \sum^n_{j=1} F^j(x, \eta)
\frac{\partial F^i}{\partial x_j} (x, \eta)d \eta; 
$$
and define $x_\varepsilon (.,x)$ by 
$$
x_\varepsilon (t,x) = x + \varepsilon \int \limits^t_0 F
(x_\varepsilon (s,x), \theta(s))ds + \varepsilon^2 \int \limits^t_0 b
(x_\varepsilon (s,x), \theta(s))ds, t \ge 0, 
$$
for $\varepsilon > 0$ and $x\in R^n$. 

Let\pageoriginale $P^{(\varepsilon)}_x$ on $(\Omega, \mu)$ be the
distribution of $x_\varepsilon (./ \varepsilon^2)$ and set   
$$
L = \frac{1}{2} \sum^n_{i,j=1} a^{ij}(x) \frac{\partial^2}{\partial
  x_i \partial x_j} + \sum^n_{i =1} (b^i + c^i)(x)
\frac{\partial}{\partial x_i} 
$$

Then $\{ P^{(\varepsilon)}_{x} : 0 < \varepsilon \le 1 \}$ is relatively
compact in the weak topology and if $P^{(\varepsilon_n)}_x \Rightarrow P$ where
$\varepsilon_n \downarrow 0$ then $P \sim L $ at $x$. 
\end{theorem}

\begin{proof}
Without loss of generality, we will assume that $x = 0$ and for
convenience we will use $x_\varepsilon (.)$ in place of $x_\varepsilon
(.,0)$. 

Define
$$
X_\varepsilon (t) = x_\varepsilon (t) + \varepsilon
F(x_\varepsilon(t), \theta(t)). 
$$
Given $\phi \in C^\infty_0 (R^n)$, set
$$
\tilde{\phi}(x, \theta) = \phi(x + \varepsilon F(x, \theta)).
$$
Then by (\ref{chap2:eq2.8}):
\begin{align*}
\phi(x_\varepsilon (t)) & - \int \limits^t_0 < \varepsilon F
(x_\varepsilon (s), \theta(s)) + \varepsilon^2 b(x_\varepsilon (s)) ,
grad_x \tilde{\phi}(x_\varepsilon (s), \theta(s)) > ds \\ 
& -\int\limits^t_0 ( \int \limits_{S_{d-1}} [
  \tilde{\phi}(x_\varepsilon (s), \eta) - \tilde{\phi}_{\varepsilon}(x_\varepsilon
  (s), \theta(s)) ] d \eta ) ds 
\end{align*}
is an $F$ martingale with respect to $P$. Note that
$$
\frac{\partial \tilde{\phi}}{\partial x_i} (x, \theta) =
\frac{\partial \phi}{\partial x_i} (x + \varepsilon F (x, \theta)) +
\varepsilon \sum^n_{j=1} \frac{\partial \phi}{\partial x_i} (x +
\varepsilon F (x, \theta)) \frac{\partial F^j}{\partial x_i} (x,
\theta). 
$$
Next, observe that by Taylor's theorem
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
& \int \limits_{S^{d-1}} [\tilde{\phi}_{\varepsilon}(x, \eta) -
    \tilde{\phi}_{\varepsilon}(x, \theta)]d \eta\\ 
& = \varepsilon \int\limits_{S^{d-1}} < F(x, \eta)- F(x, \theta),
  \text{ grad}_x \phi(x +
  \varepsilon F(x, \theta)) > d \eta \\ 
& + \frac{\varepsilon^2}{2} \sum^n_{i, j = 1} \int\limits_{S^{d-1}}
  (F^i (x, \eta) - F^i (x,  \theta)) (F^j(x, \eta) - F^j(x, \theta)) d
  \eta \frac{\partial^2  \phi}{\partial x_i \partial x_j} (x +
  \varepsilon F (x, \theta)) 
  \\ 
& + \varepsilon^3 R_{\varepsilon , \phi}(x, \theta)  
\end{align*}}\relax
where\pageoriginale
$$
|R_{\varepsilon , \phi}(x, \theta)| \le C|| \phi ||_{C^3_b(R^n)} 
$$
Thus if 
$$
a^{ij}(x, \theta) = \int\limits_{S^{d-1}} (F^i(x, \eta) F^i(x, 
\theta)) (F^j(x, \eta) - F^j (x, \theta)) d \eta  
$$
and 
$$
\beta^i (x, \theta) = b^i(x) + \sum^n_{j=1} (F^j \frac{\partial 
  F^i}{\partial x_j}) (x, \theta), 
$$
then, since  
$$
\int F(x, \eta) d \eta = 0,  
$$
\begin{align*}
\phi(X_\varepsilon(t)) &- \varepsilon^2 \int \limits^t_0 <
\beta(x_\varepsilon(s), \theta(s)), grad_x \phi(X_\varepsilon(s))  >
ds \\
& - \frac{\varepsilon^2}{2} \int \limits^t_0 \sum^n_{i, j = 1}
a^{ij}(x_\varepsilon(s), \theta(s)) \frac{\partial^2 \phi}{\partial
  x_i \partial x_j} (X_\varepsilon(s)) ds \\ 
& -\varepsilon^3 \int \limits^t_0 L_{\varepsilon ,
  \phi}(x_\varepsilon(s), \theta(s))ds 
\end{align*}\pageoriginale
is an $F.$- martingale with respect to $P$, where
$$
|L_{\varepsilon , \phi}(x, \theta)| \le C' || \phi || _{C^3_b(R^n)} 
$$

By (\ref{chap2:thm2.7}) this proves that $\{P^\varepsilon: \varepsilon > 0 \}$ is
relatively compact. Also, it shows that there is an
$\tilde{L}_{\varepsilon, \phi}(x, \theta)$ satisfying	 
$$
|\tilde{L}_{\varepsilon, \phi}(x, \theta)| \le C'' || \phi ||_{C^3_b(R^n)} 
$$
such that 
$$
\phi(x_\varepsilon(t))-\varepsilon^2 \int \limits^t_0 <
\beta(x_\varepsilon(s), \theta(s)), \text{ grad}_x \phi(X_\varepsilon(s))> ds 
$$
\begin{align*}
& - \frac{\varepsilon^2}{2} \int\limits^t_0
  \sum^n_{i,j=1}a^{ij}(x_\varepsilon(s), 
  \theta(s)) \frac{\partial^2 \phi}{\partial x_i \partial x_j}
  (x_\varepsilon(s))ds \\ 
& - \varepsilon^3 \int\limits^t_0 \tilde{L}_{\varepsilon ,
    \phi}(x_\varepsilon(s), \theta(s))ds + (\phi(X_\varepsilon(t))
  -\phi(x_\varepsilon(t)))  
\end{align*}
is an $F$-martingale with respect to $P$. Since
$$
| \phi(X_\varepsilon(t)) - \phi(x_\varepsilon(t)) | \le C'' \varepsilon 
$$
and, by (\ref{chap2:eq2.8}), we know that
\begin{align*}
\Sup_{0 \le t \le T} | \varepsilon^2 & \int \limits^{t/ \varepsilon^2}_0
    [L \phi(x_\varepsilon(s)) - < \beta (x_\varepsilon(s)),
      \theta(s)), {\rm grad}_x \phi(x_\varepsilon(s))  > \\
& - \frac{1}{2}
      \sum^n_{i,j=1} a^{ij}(x_\varepsilon(s), \phi(s))
      \frac{\partial^2 \phi}{\partial x_i \partial x_j}
      (x_\varepsilon(s)) ] ds| 
\end{align*}
$\rightarrow 0$ in probability, it is now clear that $\varepsilon_n 
\downarrow 0$ and $P^{\varepsilon_n} \Rightarrow P$, then $P \sim L$ at
0.
\end{proof}

\setcounter{coro}{10}
\begin{coro}\label{chap2:coro2.11} % cor 2.11
Let $\sigma : R^n \rightarrow R^n \otimes R^d$ and $b : R^N
\rightarrow R^n$ be bounded smooth functions having bounded
derivatives of all orders and define 
\begin{equation*}
L= \frac{1}{2} \sum^d_{k=1} ( \sum^n_{i=1} \sigma^i_k (x)
\frac{\partial}{\partial x_i})^2 + \sum^n_{i=1} b^i (x)
\frac{\partial}{\partial x_i}. \tag{2.12}\label{chap2:eq2.12}  
\end{equation*}

For $\varepsilon > 0$ and $x \in R^n$, let
$x_\varepsilon(.,x)$ be the process determined by 
\begin{equation*}
x_\varepsilon (t,x) = x + \left(\frac{d}{2}\right)^{1/2}\varepsilon
\int\limits^t_0 
\sigma(x_\varepsilon(s,x)) \theta (s) ds+ \varepsilon^2 \int
\limits^t_0 b(x_\varepsilon(s,x)) ds, t \ge 0, \tag{2.13}\label{chap2:eq2.13}  
\end{equation*}
and denote by $P^\varepsilon_x$ the distribution on ($\Omega,$) of
$x_\varepsilon (./ \varepsilon^2 ,x)$. Then as $\varepsilon \downarrow
0, P^\varepsilon_x$ converges weakly to the unique $P_x \sim L$ at
$x$. In particular, if $\mathscr{S}(x; \sigma , b)$ denotes the set of paths
$\phi_\psi(.)$ of the form 
\begin{equation*}
\phi_\psi(t) = x + \int \limits^t_0 (\phi_\psi(s))\psi(s) ds + \int
\limits^t_0 b(\phi_\psi(s)) ds, \tag{2.14}\label{chap2:eq2.14}  
\end{equation*}\pageoriginale
where $\psi : [0, \infty] \rightarrow R^d$ is a locally bounded, right
continuous function possessing left limits, then  
$$
\text{Supp}(P_x) \subseteq \overline{\mathscr{S} (x; \sigma , b)} 
$$
\end{coro}

\begin{proof}
The second part follows immediately from the first, since (\ref{chap2:eq2.13})
shows explicitly that 
$$
\text{Supp}(P^\varepsilon_x) \subseteq \overline{\mathscr{S} (x; \sigma , b)}  
$$
for each $\varepsilon > 0$.

To prove the convergence result, first observe that, by
(\ref{chap2:thm1.2}), the 
martingale problem for $L$ well-posed. Next, define 
$$
F^i(x, \theta) = (d/2)^{1/2} \sum^d_{k=1} \sigma^i_k (x)\theta_k, 1
\le i \le n. 
$$

Then $F(x, \theta)$ satisfies the hypotheses of theorem
(\ref{chap2:thm2.10}). Moreover,  
\begin{align*}
\int\limits_{S^{d-1}} & (F^i(x, \theta) - F^i(x, \eta)) (F^j(x, \theta) -
F^j(x, \eta)) d\eta \\
& = \frac{d}{2} \sum_{k, \ell} \sigma^i_k (x)
\sigma^j_{\ell}(x)\int\limits_{S^{d-1}} (\theta_k   - \eta_k)
(\theta_\ell - \eta_\ell) d\eta \\  
& = \frac{d}{2} \sum_{k, \ell} \sigma^i_k (x)
\sigma^j_{\ell}(x)(\theta_k \theta_\ell +   \frac{1}{d}
\delta_{k,\ell}).  
\end{align*}
Thus in the notation of (\ref{chap2:thm2.10}):
$$
a^{ij}(x) = \sum_{\ell} \sigma^i_\ell (x)\sigma^j_\ell (x).
$$
Next:\pageoriginale
\begin{align*}
\int\limits_{S^{d-1}} & \sum^n_{j=1} F^i(x, \eta) \frac{\partial
  F^i}{\partial x_j} (x, \eta) d\eta \\
& = \frac{d}{2} \sum^n_{j=1} \sum^d_{k, \ell=1} \sigma^j_k (x)
  \frac{\partial \sigma^i_\ell}{\partial x_j} (x) \int\limits_{S^{d-1}} \eta_k
  \eta_\ell d\eta \\ 
& = \frac{1}{2} \sum^n_{j=1} \sum^d_{k=1} \sigma^j_k (x)
  \frac{\partial \sigma^i_k}{\partial x_j} (x) 
\end{align*}
Thus in the notation of (\ref{chap2:thm2.10})
$$
c^i(x) = \frac{1}{2} \sum^d_{k=1} \sum^n_{j=1} \sigma^j_k (x)
\frac{\partial \sigma^i_k}{\partial x_j} (x) 
$$
But this means that 
$$
\frac{1}{2} \sum^n_{i,j=1} a^{ij}(x)\frac{\partial^2}{\partial x_i
  \partial x_j} + \sum^n_{i=1} c^i(x) \frac{\partial}{\partial x_i} =
\frac{1}{2} \sum^d_{k=1} (\sum^n_{i=1} \sigma^j_k (x)
\frac{\partial}{\partial x_j})^2, 
$$
and so the $L$ is given in (\ref{chap2:eq2.12}) is the one associated with $F(x,
\theta)$ and $b(x, \theta)$ via the prescription given in
(\ref{chap2:thm2.10}).  
\end{proof}

\setcounter{exercise}{14}
\begin{exercise}%%% 2.15
 Let $\sigma (.)$ and $b(.)$ be as in the preceding and suppose that
 $\bar{\sigma}(.)$ and $\bar{b}(.)$ are a second pair of such
 functions. Assume that Range $(\bar{\sigma}(x)) \subseteq$ Range
 $(\sigma(x))$ for all $x \in R^n$ and that $\bar{b}(x) - b(x)
 \in$ Range $(\sigma(x))$ for all $x \in R^n$. Show
 that 
$$
\mathscr{S} \overline{(x ; \bar{\sigma}, \bar{b})} \subseteq
\mathscr{S} \overline {(x; \sigma, b)} 
$$
for all $x \in R^n$
 \end{exercise}

 \setcounter{remark}{15}
\begin{remark}%%% 2.16
 An alternative description of $\mathscr{S} \overline {(x; \sigma,
   b)}$ can be obtained as follows: 

Let\pageoriginale
$$
X_k = \sum^n_{i=1} \sigma^i_k (x) \frac{\partial}{\partial x_i}, 1 < k
< d, 
$$
and denote by Lie $(X_1, \ldots , X_d)$ the Lie algebra of vector
fields generated by $\{ X_1, \ldots , X_d \}$. Then $\mathscr{S}
\overline {(x; \sigma, b)}$ is the closure of the integral curves,
starting at $x$, 
of vector fields $Z + Y$, where $Z \in \text{ Lie } \{ X_1, \ldots ,
X_d \}$ and $Y = \sum\limits^n_{i=1} b^i (x) \dfrac{\partial}{\partial
  x_i}$. The derivation of this identification rests on the elementary
facts about how integral curves change under the Lie bracket operation
and linear combinations. 
 \end{remark}

\section{Characterization of Supp$(p_x)$, the non-degenerate case}

In Corollary (\ref{chap2:coro2.11}), we showed that if $L$ is given by
(\ref{chap2:eq2.12}) and 
$P_x \sim L$ at $x$, then Supp$(P_x) \subseteq \overline{\mathscr{S}(x; \sigma
  , b)}$. In the case when $\sigma \sigma^*(.) > 0$, this inclusion
does not provide any information. What we will show in the present
section is that, in fact, if $\sigma \sigma^*(.) > 0$, then Supp
$(P_x) = \Omega _x \equiv \{ w \in \Omega : x(0 ,w) = 0
\}$. Actually, we are going to derive a somewhat more general result. 

\setcounter{lemma}{0}
\begin{lemma}% lem 3.1
Given $\varepsilon > 0$, define 
\begin{equation*}
u_\varepsilon(t,x) = \sum^\infty_{n = - \infty} \int^\varepsilon_{-
  \varepsilon} [\gamma _t (x - y - 4n \varepsilon) - \gamma_t (x -y-(4
  n + 2) \varepsilon ) ]dy \tag{3.2}\label{chap2:eq3.2}  
\end{equation*}
for $t > 0$ and $x \in (- \varepsilon, \varepsilon)$, where  
$$
\gamma_t(z) = \frac{1}{(2 \pi t)^{1/2}} e^{- z^2/2t}.  
$$
Then
$$
u_\varepsilon \in  C^\infty ((0, \infty)\times R^1), \frac{\sigma
  u_\varepsilon}{\sigma t} = \frac{1}{2} \frac{\sigma^2
  u_\varepsilon}{\partial x^2} in (0, \infty) \times R^1, 
$$
 $\lim\limits_{t \downarrow 0} u_\varepsilon (t,x)=1$\pageoriginale
for $x \in (- 
\varepsilon, \varepsilon), u_\varepsilon(t,x) >0$ for $(t,x) \in
(o,\infty) \times (-\varepsilon, \varepsilon)$, $u(t, \pm
\varepsilon)=0$ for $t > 0$ and
$\dfrac{{\partial^2u}_\varepsilon}{dx^2} (t,x) \le 0$ for $(t,x) \le
(0, \infty) \times (-\varepsilon, \varepsilon)$.
Finally, if $(\beta(t), F_t, P)$ is a one dimensional Brownian motion,
$T > 0$ and  $ x\in  (-\varepsilon, \varepsilon)$, then  
\begin{equation*}
u_\varepsilon(T,x)= P(x+ \beta(t)) \in (-\varepsilon, \varepsilon)
\text{ for } t \in [0,T]).\tag{3.3}\label{chap2:eq3.3}  
 \end{equation*} 
  \end{lemma}

 \begin{proof}
Let
$$
S=U((4n-1) \varepsilon, (4n+1)\varepsilon)
$$
and 
$$
S'= 2 \varepsilon+S= \{2 \varepsilon +z: z \in S\}.
$$
Then 
$$
u_\varepsilon(t,x) = \int\limits_{S} \gamma_t(x-y)dy- \int\limits_{S'}
\gamma_t (x-y) dy. 
$$
This proves that 
$$
u_\varepsilon \in C^\infty((0, \infty)\times R^1)
$$
and that 
$$
\frac{\partial u_\varepsilon}{\partial t}= \frac{1}{2}
\frac{{\partial^2u}_\varepsilon}{\partial x^2} 
$$
for $t > 0$. Also, if $x \in (-\varepsilon, \varepsilon)$, then $x \in
S$ and $x \notin \bar{S'}$. 

\noindent
 Hence 
 $$ 
 \lim_{t \downarrow 0} \int\limits_{S} \gamma_t (x-y)dy=1 \text{ and }
 \lim_{t \downarrow 0} \int\limits_{S'} \gamma_t (x-y) dy =0. 
 $$
  Thus
 $$
 \lim_{\varepsilon \downarrow 0} \int\limits_{S} u_\varepsilon(t,x) =1
 \text{ for } x \in (-\varepsilon, \varepsilon). 
 $$
  Next\pageoriginale observe that  
 $$
 u_\varepsilon(t,\varepsilon)=  \int\limits_{S+\varepsilon}\gamma_t(y)
 dy- \int\limits_{S+\varepsilon} \gamma_t (y)  dy=0. 
 $$
  Since $u_\varepsilon(t,.)$ is clearly even, this shown that
  $u_\varepsilon (t,\pm \varepsilon) =  0 $ for $t > 0$.  
 
 We next prove (\ref{chap2:eq3.3}). To this end, let
 $$
 \tau_x= \inf \{ t \ge 0 : x+ \beta(t) \not\in 
 (-\varepsilon, \varepsilon)\}. 
 $$
 
 Then, by It$\hat{o'}$s formula applied to $u_\varepsilon(T-t,x)$: 
 \begin{align*}
u_\varepsilon(T,x) &= E[u_\varepsilon(T- \tau_x \Lambda T, x+
  \beta(\tau_x \Lambda T)\\ 
&=P(\tau_x >T) =P(x+\beta(t) \in (-\varepsilon, \varepsilon)
\text{ for } t \in [0,T]). 
 \end{align*} 
 
 From (\ref{chap2:eq3.3}) it is clear that 
 $$
 \frac{\partial u_\varepsilon}{\partial t} (t,x) \le 0, 
 $$
 and so 
 $$
 \frac{{\partial^2u}_\varepsilon}{\partial x^2} = 2 \frac{\partial
   u_\varepsilon}{\partial t} \le 0. 
$$

Finally, we must show that $u_\varepsilon(t,x)>0$ for all $t > 0$ and
$x \in (-\varepsilon, \varepsilon)$. To this end, Suppose that $x^o \in
(-\varepsilon, \varepsilon)$ and that $u_\varepsilon(T,x^o)=0$ for some
$T >0$. Then we would have that  
$$
E[e^{z \tau_{x^o}}]  
$$
is an entire function of $z \in \mathbb{C}$. On the other hand a given
$0 < \lambda < \pi /2^{3/2} \varepsilon$, consider the function  
$$
\phi_\lambda(x)= \frac{\cos(2^{1/2}\lambda x)}{\cos (2^{1/2}\lambda
  \varepsilon)}. 
$$

Applying\pageoriginale  It$\hat{o'}$s formula to $e^{\lambda^{2_t}}
\phi_\lambda(x)$, one sees that  
$$
\phi_\lambda(x) = E[e^{\lambda^{2_{\tau_{x^o}}}}]
$$
and so
$$
\lim_{\lambda \uparrow
  \pi/2^{{3/2}_\varepsilon}}E[e^{\lambda^{2_{\tau_{x^o}}}}]= \infty, 
$$ 
which clearly is a contradiction. 
 \end{proof}

\setcounter{lemma}{3}
\begin{lemma}\label{chap2:lem3.4} % lem 3.4
Let $(E,F,P)$ be a probability space, $\{F_t: t \ge 0\}$ a
non-decreasing family of sub $\sigma-$algebras of $F$ and $\eta:[0,
  \infty) \times E \to R$ a $P-$a.s. continuous $F$.-progressively
  measurable function. Assume that there exists bounded
  $F.$-progressively measurable functions $a: [0, \infty) \times E\to
    [0,\infty)$ and $b: [0, \infty) \times E \to R$ such that   
$$
(f(t, \eta(t)) - \int\limits_{0}^t (\frac{\partial f}{\partial s}+
        \frac{1}{2} a(s) \frac{\partial^2 f}{\partial x^2}+ b(s)
        \frac{\partial f}{\partial x}) (s, \eta(s)) ds, F_t, P) 
$$
is a martingale for all $f \in C^{1,2}_{b}([0, \infty) \times
  R^1)$. Let $\sigma$ be an $F.$-stopping time and given $\varepsilon >
  0$ define 
$$
\tau_\varepsilon=\inf \{t \ge \sigma :|\eta(t) - \eta(\sigma) |\ge
\varepsilon\}. 
$$

If $b(s) \equiv 0$ for $\sigma \Lambda T \le s \le T \Lambda
\tau_\varepsilon$ and $a(s) \le A$ for $\sigma \Lambda T \le s \le T
\Lambda \tau_\varepsilon$ then  
$$
P(\tau_{\varepsilon} \leq T) \leq (1-u_{\varepsilon}(AT, 0))P(\sigma \leq T).
$$
\end{lemma}

\begin{proof}
Let $F:[0, \infty) \times R^1 \times E \to R^1$ have the properties
  that for each $(t, x)\in [0, \infty) \times R^1 F(t, x)$ is
    $F_t-$measurable and for each $q \in E$, $F(t, x,
    q)\in C_b^{1,2}([0, \infty)\times R^1$. Then by Doob's
      stopping time theorem\pageoriginale plus elementary properties
      of conditional expectations (cf 1.5.7 in [$S \& V $]),  
\begin{align*}
E[F & (T \Lambda \tau_{\varepsilon}, \eta(T \Lambda
  \tau_{\varepsilon}))-F(\sigma \Lambda T, \eta (\sigma \Lambda
  T))|F_{\sigma}] \\
& =E[\int\limits_{\sigma \Lambda T}^{\tau_{\varepsilon}~\Lambda
    ~T}(\frac{\partial F}{\partial s}+\frac{1}{2}a(s) \frac{\partial^2
    F}{\partial x^2})(s, \eta(s)ds|F_{\sigma}] 
\end{align*}
(a.s., $P$) on $\{\sigma < \infty\}$

In particular, with $F(t, x)=u_{\varepsilon}(A(T-T \Lambda t), x-
\eta(\sigma))(\equiv 0$ if $\sigma=\infty)$, we have 
\begin{align*}
P(\tau_{\varepsilon}> T, \sigma \leq T) &=E[u_{\varepsilon}(A(T-T
\Lambda \tau_{\varepsilon}), \eta(\tau_{\varepsilon} \Lambda
  T)-\eta(\sigma)), \sigma \leq T]\\ 
&=E[F(T \Lambda \tau_{\varepsilon}, \eta(T \Lambda
  \tau_{\varepsilon})), \sigma \leq T]\\ 
&=E[F(\sigma \Lambda T, \eta(\sigma \Lambda T)), \sigma \leq T]\\ 
&+ E[\int\limits_{\sigma \Lambda T}^{T \Lambda \tau_{\varepsilon}}
  ( \frac{\partial F}{\partial s}+ \frac{1}{2}a(s)\frac{\partial ^2
    F}{\partial x}(s, \eta(s))ds, \sigma \leq T]. 
\end{align*}
since $F(\sigma \Lambda T, \eta(\sigma \Lambda T))=
u_{\varepsilon}(A(T-\sigma),0) \geq u_{\varepsilon}(AT, 0)$ on
$\{\sigma \leq T\}$ and 
\begin{align*}
(\dfrac{\partial F}{\partial s} & + \dfrac{a(s)}{2}~\dfrac{\partial^2
  F}{\partial x^2})(s, \eta(s))\\
&=(-A\frac{\partial u_{\varepsilon}}{\partial s}+\frac{a(s)}{2}
  \frac{\partial u_{\varepsilon}}{\partial x^2}) (A(T-s),
  \eta(s)-\eta(\sigma))\\ 
&=-\frac{1}{2}(A-a(s))\frac{\partial^2 u_{\varepsilon}}{\partial
    x^2}(A(T-s), \eta(s)-\eta(\sigma))\geq 0 
\end{align*}
for $\sigma \Lambda T \leq s \leq T \Lambda \tau_{\varepsilon}$, we see
that 
$$
P(\tau_{\varepsilon}> T, \sigma \leq T)\geq u_{\varepsilon}(AT, 0)P
(\sigma \leq T). 
$$
Hence
\begin{align*}
P(\tau_{\varepsilon} \leq T)& = P(\sigma \leq T, \tau_{\varepsilon}
\leq T)\\ 
&=P(\sigma \leq T)- P(\sigma \leq T,\tau_{\varepsilon}> T)\\
&<(1-u_{\varepsilon}(AT, 0))P(\sigma \leq T).
\end{align*}\pageoriginale
\end{proof}

\setcounter{lemma}{4}
\begin{lemma}\label{chap2:lem3.5} % lemma 3.5
Let $(\beta(t), F_t, P)$ be a $d$-dimensional brownian motion,
$\alpha(.)$ a bound $F$.-progressively measurable $R^n \otimes R^d-$
valued function, $b(.)$ a bounded $F$.-progressively measurable $R^n$-
valued function and $c(.)$ a bound $F$.-progressively measurable
$R^d$-valued function such that $c(t)\equiv 0$ for $t \geq T$. Define 
\begin{equation*}
R=\exp[\int\limits_0^{\infty}< c(s), d \beta(s)>
  -\frac{1}{2}\int\limits_0^{\infty}|c(s)|^2 ds].\tag{3.6}\label{chap2:eq3.6}  
\end{equation*}

Then $R>0$ (a.s., $P$) $E^p[R]=1$ and if $dQ=RdP$, then for every $f
\in C_b^{1,2}([0, \infty), R^n)$ 
$$
(f(t, \xi(t)))-\int\limits_0^t (\frac{\partial f}{\partial s}+ L_sf+<
  \alpha (s)c(s), grad~f>)(s, \xi(s))ds, F_t, Q) 
$$ 
is a martingale, where
$$ 
L_s \equiv \frac{1}{2} \sum_{i, j=1}^{n}(\alpha \alpha^*
(s))^{i~j}\frac{\partial^2}{\partial x_i \partial x_j}+ \sum^n_{i=1}
b^i(s)\frac{\partial}{\partial x_i}. 
$$
\end{lemma}

\begin{proof}
It is clear that $R>0$ (a.s., $P$). Furthermore, by It$\hat{o}$'s
formula, if 
$$
R(t)= \exp [\int\limits_0^t < c(s), d
  \beta(s)>-\frac{1}{2}\int\limits_0^t | c(s)|^2 ds]. 
$$
then
\begin{equation*}
R(t)=1+\int\limits_0^t R(s)<c(s), d\beta(s)>, t \geq
0. \tag{3.7}\label{chap2:eq3.7}   
\end{equation*}

Thus\pageoriginale $E^P[R]=E^P[R(T)]=1$. Finally, another application of
It$\hat{o}$'s formula shows that 
{\fontsize{10pt}{12pt}\selectfont
$$
(R(t)f(t, \xi(t))-\int\limits_0^t(R(s)(\frac{\partial f}{\partial
  s}+L_sf+<\alpha(s)c(s), \text{ grad } f>)(s, \xi(s))ds, F_t, P) 
$$}\relax
is a martingale. Since, from (\ref{chap2:eq3.7}) 
$$
E^P[R| F_t]=R(t) \quad (a,s., P) 
$$
for all $t \geq 0$, it follows immediately that 
$$
(f(t,\xi(t))-\int\limits_0^t(\frac{\partial f}{\partial
  s}+L_sf+<\alpha(s)c(s), grad~f>)(s, \xi(s))ds, F_t, Q) 
$$
is a martingale.
\end{proof}

\setcounter{theorem}{7}
\begin{theorem}\label{chap2:thm3.8}%Thm 3.8
Let $(\beta(t),F_t,P)$ be a $d$-dimensional Brownian motion and let
$\alpha(.)$ be a bounded $F$.-progressively measurable $R^n \otimes
R^d$-valued function and $\gamma(.)$ a bounded $F$.-progressively
measurable $R^d$-valued function. Set 
$$
\xi(t)=\int\limits_0^t \alpha(s) d\beta(s)+\int\limits_0^t
\alpha(s)\gamma (s)ds 
$$
and assume that
$$
\frac{\text{ Trace }\alpha \alpha^*(s)}{|\alpha
  (s) \; (s)|}\chi_{[\varepsilon, \infty)}(|\xi (s)|)(\equiv 0 \text{  if
  } \alpha(s)=0) 
$$
is uniformly bounded for each $\varepsilon > 0$. Then for each $T > 0$
and $\varepsilon > 0$, 
$$
P(\Sup_{0 \leq t \leq T}|\xi(t)| < \varepsilon)> 0. 
$$
\end{theorem}

\begin{proof}
Set $\eta(t)=|\xi(t)|^2$ and define 
$$
\sigma=\inf \{t \geq 0: |\eta(t)|\geq 2 \varepsilon \}
$$\pageoriginale
and
$$
\tau= \inf \{t \geq 0: |\eta(t)-\eta(\sigma)| \geq \varepsilon \}.
$$
Then
$$
P(\Sup_{0 \leq t \leq T}|\xi (t)|\geq (3 \varepsilon)^{1/2})=P(\Sup_{0
  \leq t \leq T}|\eta(t)| \geq 3 \varepsilon)\leq
P(\tau_{\varepsilon}\leq T). 
$$

Thus we must show that $P(\tau_{\varepsilon} \leq T)< 1$. Without loss
of generality, we will assume that $\alpha(S)\equiv 0$ if 
$$
|\xi(s)|\geq (3 \varepsilon)^{1/2}.
$$

To prove that $P(\tau_{\varepsilon} \leq T)< 1$, first note that
\begin{align*}
\eta(t) & =2\int\limits_0^t < \alpha^*(s)\xi(s), d \beta(s)>+
\int\limits_0^t \text{ Trace } \alpha\alpha^*(s) ds\\
& \qquad +2\int\limits_0^t < \alpha^*(s) \xi(s), \gamma(s)>ds 
\end{align*}
Now define
$$
c(s)=-\gamma(s)-\frac{1}{2}(\frac{\text{ Trace }\alpha \alpha^*
  (s)}{|\alpha^*(s)\xi(s)|^2}\alpha^*(s) \xi (s))\chi_{[0,
    T]}(s)\chi_{[-\varepsilon/2, \infty]}(\eta(s)). 
$$

Then $c(.)$ is uniformly bounded and $c(s)\equiv 0$ for $s \geq
T$. Set 
$$
R=\exp [\int\limits_0^\infty <c(s), d\beta(s)> -\frac{1}{2}
  \int\limits_0^\infty |c(s)|^2 ds] 
$$
and $dQ= RdP$. Since $Q$ and $P$ are mutually absolutely continuous 
$$
Q(\tau_{\varepsilon}\leq T)< 1 \text{ iff } P(\tau_{\varepsilon} \leq
  T) < 1. 
$$
But, by (\ref{chap2:lem3.5}),
$$
(f(t, \eta (t))- \int\limits_0^t (\frac{\partial f}{\partial
  s}+\frac{1}{2}a(s) \frac{\partial^2 f}{\partial
  x^2}+b(s)\frac{\partial f}{\partial x}) \; (s, \eta(s)ds, F_t, Q) 
$$\pageoriginale
is a martingale for all $f \in C_b^{1, 2}([0, \infty) \times
  R^1)$, where 
$$
a(s)=4|\alpha^*(s)\xi(s)|^2 
$$
and
$b(s)=$Trace $\alpha \alpha^*(s)+2 < \alpha^* \xi(s), c(s) > +
<\alpha^*(s)\xi (s), \gamma(s)>$. In particular, $a(.)$ and $b(.)$ are
bounded and $b(s)=0$ for $\sigma \Lambda T \leq s \leq
\tau_{\varepsilon} \Lambda T$. Thus, by (\ref{chap2:lem3.4}), $Q(\tau_{\varepsilon}
\leq T)< 1$.  
\end{proof}


\setcounter{coro}{8}
\begin{coro}\label{chap2:coro3.9}% cor 3.9
Let $(\beta(t), F_t,P)$ be a $d$-dimensional Brownian motion,
$\alpha(.)$ a bounded $F.-$progressively measurable $R^n\otimes
R^d$-valued function and $v(.)$ a bounded $F.-$progressively
measurable $R^n$-valued function. Assume that $\alpha \alpha^*(.)\geq
\lambda I$ for some $\lambda >0$. set 
$$
\xi(t)=x+\int\limits_0^t \alpha(s)\,d \beta(s)+\int\limits_0^t v(s)ds, 
t \geq 0. 
$$

Then for every $\phi \in C_b^1 [0, \infty), R^n)$ satisfying
  $\phi(0)=x$, every $\varepsilon > 0$ and every $T > 0$: 
$$
P(\Sup_{0 \leq t \leq T}|\xi(t)-\phi(t)|< \varepsilon)>0. 
$$
\end{coro}

\begin{proof}
Given $\phi(.)$, set
$$
\xi_{\phi}(t)=\xi(t)-\phi(t)
$$

Then
$$
\xi_{\phi}(t)= \int\limits_0^t \alpha(s)d~(s)+ \int\limits_0^t \alpha
(s)\gamma(s)ds 
$$
where
$$
\gamma(s)=\alpha^*(s)~(\alpha\alpha^*(s))^{-1}(v(s)-\phi'(s)). 
$$

Note\pageoriginale that
$$
|\alpha^*(s)\xi(s)|^2=<\xi(s), \alpha \alpha^*(s)\xi(s)>\geq \lambda
|\xi(s)|^2, 
$$
and so
$$
\frac{\text{ Trace }\alpha \alpha^*(s)}{|\alpha^*(s)~\xi (s)|}
\chi_{[\varepsilon, \infty]}(|\xi(s)|) 
$$
is uniformly bounded for each $\varepsilon > 0$. 

Thus, by theorem (\ref{chap2:thm3.8}),
$$
P(\Sup_{0 \leq t \leq T}|\xi_{\phi}(t)|< \varepsilon) > 0 
$$
for each $\varepsilon > 0$ and $T > 0$.
\end{proof}

\begin{coro}\label{chap2:coro3.10} % 3.10
Let $\sigma : R^n \to R^n \otimes R^d$ and $b:R^n \to R^n$ be
$C_b^{\infty}$-functions and set 
$$
L=\frac{1}{2}\sum_{i,j=1}^n (\sigma 
\sigma^*)^{i,j}(x)\frac{\partial^2}{\partial x_i \partial
  x_j}+\sum_{i=1}^n b^i(x) \frac{\partial}{\partial x_i}. 
$$

Assume that $\sigma \sigma^*(x)>0$ for each $x \in R^n$, let $P_x \sim
L$ at $x$. Then 
$$
\Sup p (P_X)=\{\phi \in C([0, \infty), R^n) : \phi(0)=X \}
$$
\end{coro}

\begin{proof}
As we already know, $P_x$ is the distribution of the process
$\xi(.,x)$ given by 
$$
\xi(t, x)=x+\int\limits_0^t \sigma(\xi (s,
x))s\beta(s)+\int\limits_0^t b(\xi(s, x))ds 
$$
where $(\beta(t), F_t, P)$ is any $d$-dimensional Brownian motion. In
particular, we may assume that 
$$
\beta(,)=(\bar{\beta_1}(.),\ldots,\bar{\beta_d}(.))
$$\pageoriginale
where $(\bar{\beta}(t), F_t, P)$ is a $(d+n)-$dimensional Brownian
motion. Now let $\phi \in C_b^1([0, \infty), R^n)$ with $\phi(0)=x$ be
  given. Set 
$$
K=\Sup_{0 \leq t \leq T}|\phi (t)|+2 \varepsilon 
$$
and define
$$
\bar{\xi}(t)=x+\int\limits_0^t \bar{\sigma}(\xi(s,
x))d\bar{\beta}(s)+\int\limits_0^t b(\xi(s, x))ds, \; t \geq 0.
$$
where $\bar{\sigma}: R^n \to R^n \otimes R^{d+n}$ is given by 
$$
\bar{\sigma}(y)=(\sigma(y),\chi_{[K, \infty)}(y)I). 
$$

Then $\bar{\xi}(t)=\xi(t, x)$ for $0 \leq t \leq \tau_K \equiv \inf
\{s \geq 0:|\xi(s,x)|\leq K \}$. In particular
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
P(\Sup_{0 < t < T}| \xi(t, x)-\phi(t)|<\varepsilon) &=P(\Sup_{0 \leq t
  \leq T}|\xi (t, x)-\phi (t)|<\varepsilon, \tau_K > T)\\ 
&=P(\Sup_{0 \leq t \leq T}|\bar{\xi}(t)-\phi(t)|<\varepsilon, \tau_K > T)\\
&=P(\Sup_{0 \leq t \leq T}|\bar{\xi}(t)-\phi(t)|<\varepsilon).
\end{align*}}\relax

But $\bar{\sigma} \bar{\sigma}^* (y)=\sigma\sigma^*(y)+\chi_{(K,
    \infty)}(Y)I \geq \lambda I$ where 
$$
\lambda= \inf \{< \theta,\sigma\sigma^*(Y)\theta > /
|\theta|^2:|Y|\leq K \text{ and } \theta \in R^n \backslash \{0\}\} > 0\} 
$$
Thus, by corollary (\ref{chap2:coro3.9}), 
$$
P(\Sup_{0 \leq t \leq T}|\bar{\xi}(t)-\phi(t)| < \varepsilon)>0.
$$
We have therefore proved that 
$$
\{\phi \in C_b^1([0, \infty):R^n); \phi(0)=X \} \subseteq \Sup (P_x). 
$$\pageoriginale

Since $\Sup(P_X)$ is closed in $\Omega$, this completes the proof. 
\end{proof}

\begin{coro}[The Strong Maximum
    Principle]\label{chap2:coro3.11}%%% 3.11 
 Let $L$ be as in (\ref{chap2:coro3.10}) and let $G$ 
  be an open subset of $R^1 \times R^n$. Suppose that $u \in
  C^{1,2}(G)$ satisfies 
$$
\frac{\partial u}{\partial t}+L u \geq 0
$$
in $G$ and that $(t_0, x)\in G$ has the property that $u(t_0, x)\geq
u(t, x)$ for all $(t, x)\in G$. Then $u(t_1, x^1)=u(t_0, x)$ for all
$(t_1, x^1)\in G(t_0, x^0)$. where $G(t_0, x^0)$ is the closure of the
points $(t_1, \phi(t_1-t_0))$ such that $t_1 \geq t_0$, $\phi \in C([0,
  \infty): R^n)$, $\theta(0)=x^0$ and $(t, \phi(t-t_0)\in G$ for $t_0
  \leq t \leq t_1$. In particular, if $\mathcal{U}$ is a connected open set in
  $R^n$ and $u \in C^2(\mathcal{U})$ satisfies $Lu \geq 0$ in
  $\mathcal{U}$, then $u$ is   constant in $\mathcal{U}$ if $u$ attains its
  maximum in $\mathcal{U}$.  
\end{coro}

\begin{proof}
By replacing $G$ by $\{(t-t_0, x):(t, x)\in G\}$ and $u(t, x)$ by
$u(t-t_0, x)$. We will assume that $t_0=0$. Furthermore, by
approximating $G$ from inside with relatively compact open regions,
we will assume that $u \in C_b^{1,2}(G)$. 

Note that by It$\bar{o}$'s formula and Doob's stopping time theorem\break
$E^{P_x^{0}}[u(t \Lambda \tau, \times(t \Lambda \tau))]-u(0, x^0)$ 
$$ 
=E^{P_x^0}[\int\limits_0^{t \Lambda \tau}(\frac{\partial u}{\partial 
    s}+Lu)(s,x(s))ds] \geq 0, 
$$
where $P_{x^0} \sim L$ at $x^0$ and $\tau=\inf \{t \geq 0: (t,
x(t)\notin G)\}$ 

Thus\pageoriginale
$$
E^{P_{x^0}}[u(t \Lambda \tau, X (t \Lambda \tau))-u(0, x^0)] \geq 0,t \geq 0.
$$

Now suppose that $\phi \in C([0, \infty): R^n, \phi(0)=x$ and $(t,
  \phi(t)) \in G$ for $0 \leq t \leq t_1$. If $u(t_1, \phi(t_1)) <
  u(0, x^0)$. Then we could find an $\varepsilon >0$ such that
  dist. $((s, \phi(s)), G^C)> \varepsilon $ for all $0 \leq s \leq t $
  and $u(t_1, x) \leq u(0, x)-\varepsilon $ for $|x-\phi(t_1)|<
  \varepsilon $. Thus we would have 
\begin{align*}
E^{P_x^0} & [u(t_1 \Lambda \tau, x (t_1 \Lambda \tau))-u(0, x^0)]\\
& \leq -\varepsilon P_{x^0}(\Sup_{0 \leq t \leq
  t_1}|x(t)-\phi(t)|<\varepsilon )<0, 
\end{align*}
which is a contradiction
\end{proof}

\setcounter{remark}{11}
\begin{remark}%Remk 3.12
The preceding result can be extended in the following way:

Let $a=R^1 \times R^n \to R^n \otimes R^n$ and $b:R^1 \times R^n \to
R^n$ be measurable functions such that $a$ is symmetric and uniformly
positive definite on compact sets and $a$ and $b$ are bounded on
compact sets. Set 
$$
L_t= \frac{1}{2}\sum_{i, j=1}^n a^{ij}(t,x)\frac{\partial^2}{\partial
  x_i \partial x_j}+\sum_{i=1}^n b^i (t, x) \frac{\partial}{\partial
  x_i}. 
$$

If $u \in C^{1, 2}(G)$ satisfies
$$
\frac{\partial u}{\partial t}+ Lu \geq 0
$$
in $G$ and $u$ attains its maximum at $(t_0, x^0)\in G$,
then\pageoriginale $ u (t_1,x^1 )  = u ( t_0,x^0 ) $  for all $(t_1,x^1)
\in G ( t_0,x )$. The  proof can be constructed along the
same lines  as we have just used, the only missing  ingredient is a
more  sophisticated treatment of the  existence theory for solutions
to the  martingale problem  (cf [Berk. Symp.] for the details). 
\end{remark}

\section{The Support of $P_x \sim L$, the Degenerate case}
We are going to show  that if 
\begin{equation*}
L = \frac{1}{2} \sum^{d}_{\ell = 1}  ( \sum \sigma^i_\ell (x)
\frac{\partial}{\partial x_i} )^2 + \sum^{n}_{i = 1} b^i  (x)
\frac{\partial}{\partial x_i}, \tag{4.1}\label{chap2:eq4.1}  
\end{equation*}
and if  $ P_x \sim L $ at $x$, then
\begin{equation*}
\text{supp} ( P_x ) = \mathscr{S} \overline{ ( x;
  \sigma,b)}. \tag{4.2}\label{chap2:eq4.2} 
\end{equation*}

Since we already know that  supp $ ( P_x )\ge \mathscr{S} \overline{(
  x; \sigma, b)} $, it suffices to show that if 
\begin{equation*}
 \phi (t)  = x + \int \limits^{t}_{0} \sigma ( \phi (s)) \dot{\psi}
 (s) ds + \int \limits^{t}_{0}  b ( \phi (s)) ds, t \ge 0,
 \tag{4.3}\label{chap2:eq4.3}  
\end{equation*}
where $ \psi \in  C^2_0 ((0,\infty); R^d )$, then 
\begin{equation*}
P_x( \Sup_{o \leq t \leq T}   \mid x (t) - \phi (t)  \mid  \leq
\varepsilon ) > 0 \tag{4.4}\label{chap2:eq4.4}  
\end{equation*}
for all $ \varepsilon > 0 $ and  $ T > 0 $. Actually, what we are
going to prove  is slightly more refined result than
(\ref{chap2:eq4.4}). Namely, 
suppose that  $ ( \beta ( t), F_t, P ) $ is a 
$d-$dimensional Brownian motion and  that   
\begin{equation*}
\xi (t,x) = x + \int \limits^{t}_{0}  \sigma ( \xi (s,x )) d \beta
(s)  + \int \limits^{t}_{0} \tilde{b} ( \xi (s,x )) ds, t \ge  0,
\tag{4.5}\label{chap2:eq4.5} 
\end{equation*}
where 
\begin{equation*}
\tilde{b} (x)  =  b (x) + \frac{1}{2} \sum^{n}_{j =1} 
\sum^{d}_{\ell=1} \sigma^j_\ell (x)  \frac{\partial
  \sigma_\ell}{\partial x_j} (x) . \tag{4.6}\label{chap2:eq4.6} 
\end{equation*}\pageoriginale

Then $ P_x $  is the distribution of $ \xi (., x ) $. We will show
that  
\begin{equation*}
\lim_{\delta \downarrow 0} P ( \Sup_{ 0 \leq t \leq T}  \mid  \ge
\varepsilon \mid  \Sup_{ 0 \leq t \leq T} \mid \beta (t) - \psi (t)
\mid  \leq \delta ) = 1 \tag{4.7}\label{chap2:eq4.7}  
\end{equation*}
for all $ \varepsilon > 0 $ and  $ T > 0 $. Since 
$$ 
P( \Sup_{ 0 \leq t \leq T}  \mid \beta (t) - \psi (t) \mid  \leq
\delta ) >  0 
$$
for  $ \delta > 0 $  and $ T> 0 $, this will certainly prove
(\ref{chap2:eq4.4}). The  proof of (\ref{chap2:eq4.7}) relies on a few
facts about Brownian  motion.  

\setcounter{lemma}{7}
\begin{lemma}\label{chap2:lem4.8}% lem 4.8
Let $(\beta(t), F_t, P )$  be a $d$-dimensional Brownian
motion.\break Then there exist $A > 0$ and $B > 0$, depending on $d$,
such that  
\begin{equation*}
P ( \Sup_{ 0 \leq t \leq T} ~ \mid \beta (t)  \mid  < \delta ) \ge A
\exp \left( -\frac{BT}{\delta^2} \right), T > 0 \text{ and }  \delta >
0. \tag{4.9}\label{chap2:eq4.9}  
\end{equation*}
\end{lemma}

\begin{proof}
 First note that if 
 $$
 \Phi ( T, \delta )  =  P ( \Sup{ 0 \leq t \leq T} \mid  \beta (t) 
 \mid  < \delta, 
 $$
 then 
 $$
\Phi ( T, \delta )  =   \Phi ( T / \delta^2, 1 ).  
 $$

The reason for this  is that for any  $\lambda > 0, \lambda \beta (.)$
has the same distribution as  $ \beta ( \lambda^2 ,.)$ (cf. exercise 
(\ref{chap2:exer4.11}) below). Thus we need only check that  
$$
 \Phi (t,1 ) \ge  A e^{-Bt}, t > 0.
$$
Next observe that 
\begin{gather*}
P ( \Sup_{ 0 \leq t \leq T}  \mid \beta (t)  \mid  < \delta ) \ge P
( \Sup_{ 0 \leq t \leq T} \max{ 1 \leq \ell \leq d}  \mid
\beta^\ell (t)  \mid  < \delta / d^{1/2} ) \\ 
=  P ( \Sup_{ 0 \leq t \leq T}  \mid \beta^1  (t)  \mid  < \delta /
d^{1/2} )^d 
\end{gather*}\pageoriginale
since the  $ \beta^\ell (.) 's $ are  mutually independent and  each 
has the same distribution. Thus we will restrict our attention to the
case when  $d = 1$. 

To prove that  $ \Phi (T,1) \ge A e^{-Bt} $ when  $ d = 1 $, we will
show that if $f \in C ([-1, \; 1])$  
\begin{align*}
E [ f ( x & +  \beta (T)), \Sup_{ 0 \leq t \leq T} ~  \mid x + \beta (t)
  \mid  < 1 ] \\ 
& = \sum^{\infty}_{m=0} a_m (f) e^{(-m^2 \pi^2 /8 ) T} ~ \sin  (m
\pi /2)   (x +1 )), \tag{4.10}\label{chap2:eq4.10} \\
& \qquad T > 0  \text{ and  } x \in ( -1,1 ),
\end{align*}
where
$$ 
a_m (f) = \int\limits^{1}_{0}  f (y) \sin ( m \pi /2 ) ( y +1 ))  
dy/ \int\limits^{1}_{0} \sin^2 (( m \pi /2 ) (y+1)) dy.
$$
Given (\ref{chap2:eq4.10}), we will have 
{\fontsize{10pt}{12pt}\selectfont
$$
P (\Sup_{0 \leq t \leq T } \mid \beta (t) \mid < 1) = a_1 (1) e^{(-
  \pi^2 /8)T } + \sum^{\infty}_{m=1} a_{2m +1} (1) (-1)^{ m+1}e^{(-(
  2m +1)^2 \pi^2/8) T} 
$$}\relax

Since $ a_1 (1) > 0 $ and  $ \mid a_m (1)  \mid  \leq 1 $ for all
$m$, it is clear form this that   
$$
\lim_{T \uparrow \infty} e^{( \pi^2 /8) T} P(\Sup_{ 0 \leq t \leq
  T} \mid \beta (t) \mid < 1) = a_1 (1) > 0,  
$$
and so estimate will be established. To prove (\ref{chap2:eq4.10}),
first note that     
$$
\left\{ \sin \frac{m \pi}{2} (x+1): m \ge 1 \right\} 
$$
is an  orthonormal basis in $ L^2(( -1, 1 ))$ (cf. exercise
(\ref{chap2:exer4.11}) below).  

Next\pageoriginale observe that for, $ f\in C^\infty_0  ( (-1, 1 ))$,
$$
\sum^{\infty}_{m =0} a_m (f) \sin ( m \pi/2 ( x+1) )
$$
 is uniformly and  absolutely convergent to  $f$  and that if $ u
 (T,x) $ is given by the right hand side of (\ref{chap2:eq4.10}) then  
 \begin{gather*}
\frac{ \partial u}{ \partial T} = \frac{1}{2} \frac{ \partial^2 u}{
  \partial x^2} , T > 0 \text{ and } x \in  R^1 \\ 
\lim_{ T \downarrow 0}  u ( T,x ) = f, -1 < x < 1 \\
u ( T, \pm 1 ) = 0. \\ 
 \end{gather*} 
 
 Hence, $( u ( T -t  \Lambda T, x + \beta ( t )) $,   $ F_t, P ) $
 is a martingale and so  
 $$
 u (T,x ) = E [ f (x)  +  \beta ( T )), \tau_x > T ]  
 $$
 where  $ \tau_x = \inf  \{ t \ge 0 :   \mid x  +  \beta ( t ) \mid
 \ge 1 \} $. Thus (\ref{chap2:eq4.10}) holds for  $ f \in C^\infty_0 (( -1, 1
 )) $. Using  obvious limit procedure, it is  now easy ot  see that 
 (\ref{chap2:eq4.10}) continues  to hold for all  $ f \in ([ -1, 1])$ so long
 as  $ T > 0 $.  
\end{proof}

\setcounter{exercise}{10}
\begin{exercise}\label{chap2:exer4.11}%4.11
 Fill the missing details in the preceding  proof. In particular, use
 the martingale problem characterization of Brownian motion to check
 that $ \lambda \beta ( ./ \lambda^2 ) $ has the same distribution as
 $ \beta (.) $ for any $ \lambda \in R / \{ 0 \} $. Second,
 show that  
 $$
 \{ \sin ( \frac{m \pi}{2} ~ ( x+1 ) : ,m \ge  1 \} 
 $$
 is an  orthogonal basis in  $ L^2 (( -1,1 )) $, that 
 $$
\Sup_{m}  \mid a_m (f) \mid  \ge c \mid\mid f \mid\mid_{ L^2 (( -1,1
  ))}, 
 $$\pageoriginale
 and that 
 $$
 \sum^{\infty}_{m=1} a_m (f) \sin ( \frac{m \pi}{2} ( x + 1 ))
 $$
 is absolutely and uniformly convergent to $ f $ if  $ f \in
 C^\infty_0 ((-1,1 )) $. 
 All these facts are easy consequences of the  elementary theory of
 Fourier  series. 
 \end{exercise}
 
 \setcounter{lemma}{11}
 \begin{lemma}\label{chap2:lem4.12} % lem 4.12
 Let  $ ( B (t), F_t,P ) $  be a  1-dimensional Brownian
 motion. Then  $P-$a.s., there is precisely one solution to the
 equation 
 $$
 \xi (t) = 2 \int \limits^{t}_{0} \mid \xi (s) \mid^{1/2} dB (s) + 2t,
 t \ge 0. 
 $$
 
 Moreover, the unique solution  $ \xi (.) $  is non-negative and $ B
 (.)$-measu\-rable. 
 \end{lemma}  

 
 \begin{proof}
For  $ 0 <  \varepsilon \leq 1 $ and $ n \ge 1 $, consider the
equation  
$$
\xi_n (t, \varepsilon )= \varepsilon  + 2 \int \limits^{t}_{0}
\sigma_n ( \xi_n (s, \varepsilon)) dB (s)  + 2t, 
$$
where $ \{ \sigma_n \}^\infty_1 \subseteq  c^\infty ( R^1 ) $, $ \Sup_{n \ge
  1} $ $ \Sup_{\mid x \mid \leq 1}$ $ \mid \sigma_n (x) \mid \leq 1 $
and  $ \sigma_n (x) = \mid x \mid^{1/2} $ for $ \mid x \mid \ge
\varepsilon / n $. Since $\sigma_n (.) $  is uniformly Lipschitz
continuous, $ \xi_n (., \varepsilon ) $ is  uniformly determined and $
B (.) $-measurable. Moreover, if  
$$
\tau_n = \inf \{ t \ge 0 : \xi_n (t, \varepsilon ) \ge  ~ \varepsilon  
/ n \}, 
$$
then $ \xi_{ n+1 } (t, \varepsilon ) = \xi_b (t, \varepsilon )$, $ 0
\leq t \leq \tau_n $ (a.s., $P$). Next note that $ \tau_n \uparrow
\infty$ (a.s., $P$) as $ n \rightarrow \infty $. To see this, define   
$$
\zeta_{n,R} = \inf~ \{ t \ge 0 : \mid \xi_n ( t, \varepsilon ) \mid
\ge  R \} 
$$
for  $ R > \varepsilon $. Then
$$
 \varlimsup_{n \rightarrow \infty} P ( \tau_n \leq \zeta_{n,R} ) = 0 ~
 \text{ for all }  R > 0 
$$\pageoriginale
 
 Indeed, choose  $ u \in C^2_b ( R^1 ) $ so  that
 $$
 u (x) = \log ( nx/ \varepsilon )^2 / \log  ( nR / \varepsilon )^2 
 $$
for $ \varepsilon / n \leq x \leq R $. Then, by  It'$\hat{o}$s formula 
\begin{align*}
P ( \tau_n > \zeta_{n,R}  )  &=  E [ u (\xi_n ( \tau_n \Lambda
  \zeta_{n,R} )) ] =  u(\varepsilon) \\ 
&= \log (n^2) / \log  (nR / \varepsilon )^2 \rightarrow 1 
\end{align*}
as $ n \rightarrow \infty $. Next note that by Doob's inequality, for
any $ T > 0 $  
\begin{align*}
P &\Sup_{0 \leq t \leq T} \mid  \xi_n (t,\varepsilon ) - \varepsilon-
2t \mid \ge  R ) \\ 
&\leq \frac{1}{2} E [ \mid \xi_n ( T,\varepsilon ) - \varepsilon - 2T
  \mid ] \\ 
&\leq \frac{c (T)}{R} \\ 
\end{align*}
where $ c (T) $ is independent of $n$. Hence  
$$
\Sup_{n \ge 1}  P ( \zeta_{n,R} \leq T ) \rightarrow 0  
$$
as $ R \rightarrow \infty $, and so we conclude that  
$$
\varlimsup P ( \tau_n \leq T ) = 0  
$$
for all $ T > 0 $.

We have now proved that there is for each $ \varepsilon > 0$, $P$-a.s.
a unique continuous $ \xi (., \varepsilon ) $  such that $ \xi (t,
\varepsilon ) = \xi_n ( t, \varepsilon ) $, $ 0 \leq t \leq \tau_n $,
and that $ \tau_n \uparrow \infty$ (a.s., $P$) as  $ n \rightarrow
\infty$. Clearly $ \xi (., \varepsilon ) $  is  $B (.)$-measurable
and non-negative (a.s., $P$). Also  
$$
\xi (t, \varepsilon ) = \varepsilon + 2 \int  \mid (\xi (s, \varepsilon
)) \mid^{1/2} dB (s) + 2t, \;\; t \ge 0.  
$$\pageoriginale

We next show that 
$$
P ( \Sup_{ 0 \leq t \leq T} \mid \xi ( t, \varepsilon)- \xi
(t-\varepsilon') \mid \ge \lambda ) \rightarrow 0  
$$   
as $ \varepsilon, \varepsilon'  \rightarrow 0 $ for each $T > 0 $ and
$ \lambda > 0 $. To this end, note that  
$$
0 < \rho ( \delta ) = \Sup_{\substack{ x,y \leq 1\\ \mid x-y \mid \leq 
    \delta \\}} ( \mid x \mid^{1/2} - |y|^{1/2}) \leq C (\delta^{1/2}
\Lambda \delta), \;\; \delta >0.
$$

Thus we can find  $ \{ \alpha_k \} \subseteq$ $(0, 1)$ so that   
$$
\int \limits^{\alpha_{k-1}}_{\alpha_k} \frac{1}{\rho^2 (\lambda )} d
\lambda = k . 
$$

Choose  $ \{ \phi''_k \} \subseteq C^\infty_0 (( \alpha_{k},
\alpha_{k-1} )) $ so that  $ 0 \leq \phi''_k (.) \leq
\dfrac{2}{k\rho^2 (.)} $  and  
$$
\int \limits^{\infty}_{0} \phi''_k  (\lambda) d \lambda = 1.
$$
Set 
$$
\phi_k (\lambda)= \int\limits^{\mid \lambda \mid}_{0} dt
\int\limits^{t}_{0} \phi''_{k} (s) ds.  
$$
Then, by  It\^o's formula:
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
E [ \phi_k  ( \xi & ( T, \varepsilon ) - \xi ( T, \varepsilon' )) ] \\
&\leq \phi_k ( \varepsilon - \varepsilon' ) + \frac{1}{2} E [ \int
  \limits^{T}_{0} \rho^2 ( \mid \xi (t,\varepsilon)
  -\xi(t,\varepsilon') \mid ) \phi''_k ( \xi (t,\varepsilon ) -\xi ( t,
  \varepsilon' )) dt ]\\ 
&\leq \mid \varepsilon - \varepsilon' \mid  + T/k.\\ 
\end{align*}}\relax

Since  $ \phi_k (x) \uparrow \mid x \mid $ as  $ k \uparrow \infty $, we
now see that  
$$
E [ \mid \xi ( T, \varepsilon ) - \xi ( T, \varepsilon' ) \mid  ] \leq
\mid \varepsilon - \varepsilon' \mid. 
$$

But\pageoriginale $ ( \xi (t, \varepsilon ) -\xi (t,\varepsilon'), ~
F_t, P)$ is a martingale; and so, by Doob's inequality  
$$
P ( \Sup_{0 \leq t \leq T } \mid \xi (t, \varepsilon ) - \xi ( t, 
\varepsilon' ) \mid  \ge \lambda ) \leq  \frac{\mid
  \varepsilon-\varepsilon' \mid}{\lambda} \rightarrow 0.  
$$

We now see that there exist $ \varepsilon_n \downarrow 0 $ such that $
\xi (., \varepsilon_n ) \rightarrow \xi (.)$ (a.s., $P$) uniformly
on finite intervals. Clearly $ \xi (.) $ is  $ B (.) $-measurable, $
P$ - a.s. non-negative and satisfies 
$$
\xi (t) = 2 \int \limits^{t}_{0} \mid \xi (s) \mid^{1/2} dB (s)  +
2t,  \;\; t \ge 0  
$$
(cf. Exercise (\ref{chap2:exer4.13}) below). Finally, if $\eta (t) $
were a second process satisfying the  same equation, then, using the
functions $\phi_k$  constructed above, we would find that  
$$
\eta ( T ) =  \xi ( T )  \;\; ({\rm a.s., } P)
$$
for all  $T > 0 $.
 \end{proof}

\setcounter{exercise}{12}
\begin{exercise}\label{chap2:exer4.13}%4.13
 Show that in fact $ \xi ( ., \varepsilon) \leq  \xi (.,
 \varepsilon')$ (a.s., $P$) if $ 0 \leq 
 \varepsilon < \varepsilon' $. The idea is to  employ a variation on
 the ideas  used prove  
 $$
 P \left( \Sup_{0 \leq t \leq T}  \mid \xi (t,\varepsilon) - \xi
 (t,\varepsilon') \mid \ge \lambda \right) \rightarrow 0 \text{ as }
 \varepsilon,\varepsilon' \rightarrow 0. 
 $$
\end{exercise}

\setcounter{lemma}{13}
\begin{lemma}\label{chap2:lem4.14}%4.14
Let $ ( \beta (t), F_t, P ) $  be a $d-$ dimensional Brownian
motion where $ d \ge 2 $. Given $ 1 \leq i \neq   j \leq d $, define 
\begin{gather*}
L_{ij} (t)  = \int \limits^{t}_{0}  \frac{ \beta_i}{ ( \beta^2_i +
  \beta^2_j )^{1/2}} d \beta_j - \int \limits^{t}_{0}  \frac{
  \beta_i}{ ( \beta^2_i + \beta^2_j )^{1/2}} d \beta_i \\
\left(\text{here we take} \quad  \dfrac{ \beta_i}{ ( \beta^2_i +
    \beta^2_j )^{1/2}}  = 
\dfrac{ \beta_j}{ ( \beta^2_i + \beta^2_j )^{1/2}} = 0 \text{ if }
\beta^2_i + \beta^2_j = 0 \right)
\end{gather*}\pageoriginale

Then $ L_{i j} (t), F_t, P $ is a 1-dimensional Brownian motion and 
the  process $ L_{i j} ( . )$ is independent of the process $
\beta^2_i (.)  + \beta^2_j (.) $.  

In particular, if $ \theta (.) $ is a bounded  $F.$-progressively
measurable\break $R$-valued process, then for $ M \ge 1$ and  $ \delta > 0$:  
\begin{gather*}
P ( \Sup_{0 \leq t \leq T} \mid  \int \limits^{t}_{0} \theta (u) \mid
\beta (u) \mid d L_{ij} (u) \mid > M \delta \mid \Sup_{0 \leq t \leq
  T}  \mid  \beta (t) \mid < \delta ) \\
 \leq 2 \exp  \left( - \frac{M^2}{2 T
  \mid\mid \theta \mid\mid^2_u }\right) \tag{4.15}\label{chap2:eq4.15} 
\end{gather*}
\end{lemma}

\begin{proof}
Without  loss of generality, we will assume that $ i = 1 $ and  $ j =
2 $. Let $ L (t) = L_{1 2} (t) $. To see that  $ ( L (t), F_t, P ) $ is
a 1-dimensional Brownian motion, note that by It\^o's formula  
$$
( f ( L (t)) - \int \limits^{t}_{0} \frac{1}{2} f'' ( L (s)) ds,
F_{t}, P ) 
$$
is a martingale for all $f \in C^2_b(R^1)$.

\noindent
(Remember that $P( \int\limits^{\infty}_{0} \chi_{\{ 0\}} (
\beta^2_1 (s) + \beta^2_2 (s)) ds = 0 ) = 0$ 
since 
$$
E \left[\int\limits^T_{\varepsilon} \chi_{\{0\}} (\beta^2_1(s) + \beta^2_2 
  (s)) ds\right] = \int\limits^T_{\varepsilon} dt \int\limits_{\{0\}}
\frac{1}{2\pi t} e^{-y^2/2t} dy = 0
$$
for all $ 0 < \varepsilon < T < \infty$.) 

We next show that $ L (.) $ is independent of $ \beta^2_1 (.) $ $ +
\beta^2_2 (.) $. Define
$$
B (t) = \int\limits^{t}_{0}  \frac{\beta_1}{\beta^2_1 + \beta^2_2}
d \beta_1 + \int\limits^{t}_{0}  \frac{\beta_2}{\beta^2_1 +
  \beta^2_2} d \beta_2   
$$
(with the same convention when $\beta^2_1  + \beta^2_2 = 0$).

Again\pageoriginale use It\^o's formula to show that 
$$
( f ( L (t), B (t )) - \frac{1}{2} \int \limits^{t}_{0} \Delta f ( L
(s), B (s) ds, F_t, P ) 
$$
 is a martingale for all $f \in  C^2_b ( R^2 ) $. Thus $(( L (t), B
 (t)), F_t, P ) $ is a 2-dimensional Brownian motion, and so $ L
 (.) $ is independent of $ B (.) $. 
 
 Finally, by $It \hat{o}'s$ formula, if $ \xi (t) = $ $ \beta^2_1 (t)
 $ $ + \beta^2_2 (t) $ then  
 \begin{align*}
 \xi (t) &=  2 \int \limits^{t}_{0} ( \beta_1 (s) d \beta_1 (s)  +
 \beta_2 (s) d \beta_2 (s)) + 2t \\ 
 &= 2 \int \limits^{t}_{0}  \mid \xi (s) \mid^{1/2} dB (s) + 2t 
 \end{align*}
 and so, by (\ref{chap2:lem4.12}), $\xi (.)$ is $ B (.)$-measurable. Hence $ L
 (.) $ is independent  of  $ \beta^2_1 (.) + \beta^2_2 (.)$. 
 
 Finally, since $L (.) $ is independent of $ \xi (.) $ and they both
 are independent of  $ \sum\limits^{d}_{3} \beta^2_j (.) $, we can argue as
 in (I. \ref{chap1:lem4.3}) to prove that  
 \begin{gather*}
 P \left( \Sup_{0 \leq t \leq T} \mid \int \limits^{t}_{0} \theta (s) \mid
 \beta (s) \mid  dL (s) \mid \ge \lambda \mid \beta (.) \mid\right ) \\ 
  \leq  2 \exp  \left( - \frac{\lambda^2}{2 \mid\mid \theta \mid\mid
    ^2_u \int \limits^{T}_{0} \mid \beta (s) \mid^2} \right) \\ 
 \end{gather*}
  where $\mid\mid \quad  \mid\mid_u $ denotes the uniform norm. 

 In\pageoriginale particular
 \begin{align*}
P \left( \Sup_{0 \leq t \leq T} \mid \int \limits^{t}_{0} \theta (s) \mid
\beta (s) \mid  dL (s) \mid  \right. & \left.\ge M \delta  \Sup_{0 \leq t \leq
  T}  \mid \beta (t) \mid \leq \delta \right) \\
&  \leq 2 \exp  \left( - M^2 /2 T \mid\mid \theta \mid\mid ^2_u \right)  
 \end{align*}
\end{proof}

 \setcounter{exercise}{15}
 \begin{exercise} % exercise 4.16 
Let $ ( \beta (t), F_t, P ) $ be a 2-dimensional Brownian
motion. Prove that for each $ \varepsilon > 0$, $P ( \mid \beta ( t V
\tau_\varepsilon ) \mid > 0$, $t \ge 0 ) = 1 $, where $
\tau_\varepsilon = \inf \{ t \ge 0 : \beta (t) \mid  \ge \varepsilon
\} $. Next show that $ P (\tau_\varepsilon \downarrow 0 $ as  $
\varepsilon \downarrow 0 ) = 1 $. Conclude that  $ P ( \mid \beta (t)
\mid > 0 $  for all $ t > 0 ) = 1 $. Thus $ P$-a.s., $ \theta (t) =
\arg \beta (t) $ is well defined for all $ t > 0$). In order to
study $ \theta (t)$,  $t > 0 $, write $ z (t) = \beta_1 (t) +
i \beta_2 (t)$. Given an analytic function $f$ on $ \mathbb{C}$ show
that $ df ( z (t)) =f' (z (t)) dz (t) $.  

In particular, show that for fixed $ t_0 > 0 $
$$
\log z (t)  - \log z (t_0) = \int \limits^{t}_{t_0} \frac{1}{\mid z
  (s) \mid } d B (s) + \int \limits^{t}_{t_0}  \frac{dL (s)}{\mid z
  (s) \mid }, \;\; t \ge t_0,   
$$
where  $B (.) $ and $L (.) $ are defined as  in (\ref{chap2:lem4.14}). Hence 
$$
\theta (t)  - \theta (t_0)  = \int \limits^{t}_{t_0}  \frac{dL
  (s)}{\mid z (s) \mid}, \;\; t \ge 0 . 
$$

The conditional distribution of 
$ \theta  (.V t_0) - \theta (t_0 ) $   given  $ \mid z (.) \mid $ is 
the same as the  distribution of  $ \tilde{B} ( \int\limits^{. V
  t_0}_{t_{0}} \mid z (s) \mid^{-2} ds $, where  $ \tilde{B} (.) $ is
a 1-dimensional Brownian motion. 
 \end{exercise}

\setcounter{notation}{16}
\begin{notation} % not 4.17
 If\pageoriginale  $ \theta  : [ 0, \infty )   \times R^N $ and  $ T >
   0 $, we will use  $ \mid\mid \theta (.) \mid\mid^0_T $ to denote $
   \Sup\limits_{0 \leq t \leq T} \mid \theta (t) \mid $.  
\end{notation}

\setcounter{lemma}{17}
\begin{lemma}\label{chap2:lem4.18} % lem 4.18
Let $ ( \beta (t), F_t, P ) $ be a $d-$dimensional Brownian motion and
suppose that $ \theta (.) $  is an  $P$ -a.s. continuous
$F.$-progressively measurable $R^d$ - valued function with the property 
that for some $ \alpha > 1 $ 
$$
\lim_{M \rightarrow \infty} \Sup_{ 0 < \delta < 1}  P ( \mid\mid 
\theta (.) \mid\mid^0_T \ge  M \delta^\alpha|  \mid\mid \beta
(.) \mid\mid^0_T \leq \delta ) = 0. 
$$

Then 
\begin{align*}
\lim_{ \delta \downarrow 0}  P ( \mid\mid \int\limits^{\cdot}_{0} <
\theta (u),  d \beta (u) & > \mid\mid^0_T > \varepsilon \mid  \mid\mid
\beta (.) \mid\mid^0_T \leq \delta )\\
&  \text{is 0 for all } \varepsilon > 0 . 
\end{align*}
\end{lemma}

\begin{proof}
Let $ 0 < \delta < 1 $ and $ M \ge 1 $ be given and define 
$$
\zeta = \inf \{ t \ge 0 : ~ \mid \theta (t) \mid \ge M \delta^\alpha
\}. 
$$

Then
\begin{align*}
& P ( \mid\mid \int \limits^{.}_{0} < \theta (u),  d \beta (u)   > 
\mid\mid^0_T,  \ge \varepsilon,  \mid\mid \beta ( . ) \mid\mid^0_T 
\leq \delta ) \\ 
& \quad \leq P ( \mid\mid \int \limits^{.}_{0} < \theta (u),  d \beta (u)   >
 \mid\mid^0_T  \ge \varepsilon,  \mid\mid \theta ( . )
\mid\mid^0_T  \leq  M \delta^\alpha ) \\ 
 & \qquad \quad +  P ( \mid\mid \theta ( . )  \mid\mid^0_T \ge M
\delta^\alpha,  
 \mid\mid \beta ( . ) \mid\mid^0_T \leq \delta ) \\ 
 & \quad \leq P  ( \mid\mid \int \limits^{.}_{0} < \theta ( u  \Lambda 
 \zeta ),  d \beta (u)   >  \mid\mid^0_T,  \ge \varepsilon) \\ 
& \qquad  \quad +  P ( \mid\mid \theta ( . )  \mid\mid^0_T \ge M
 \delta^\alpha,  
 \mid\mid \beta ( . ) \mid\mid^0_T \leq \delta ). 
\end{align*}

By the argument used to prove (I. \ref{chap1:lem4.3}):
$$
P ( \mid\mid \int \limits^{.}_{0} < \theta ( u ~ \Lambda ~ \zeta ),  ~
d\beta (u)  > \mid\mid^0_T \ge \varepsilon )  \leq 2 \exp ( - \frac{
  \varepsilon^2}{M^2 \delta^{2  \alpha_T}}). 
$$
Thus,\pageoriginale by (\ref{chap2:eq4.9}):
\begin{align*} 
P(|| \int\limits_o^. & < \theta(u), d\beta(u) >  ||^0_T \ge
\varepsilon || \beta(.) ||^0_T \le \delta )\\ 
& \le  \frac{2}{A} \exp ( - \frac{\varepsilon^2}{M^2\delta^{2\alpha_T}}
+ \frac{BT}{\delta^2}) +  P( ||\theta (.) ||^0_T \ge
M\delta^\alpha  ||  \beta(.)  ||^0_T \le \delta) 
\end{align*}

By assumption, given $\lambda > 0$ we can choose $M_\lambda < \infty $
so that the  second term is less than $\lambda/2$ for $0 < \delta <
1$. We can then choose $0 < \delta_\lambda <  1$ so that the first
term is less than $\lambda/2$ for this choice of $M_\lambda$. 
\end{proof}

\setcounter{lemma}{18}
\begin{lemma}\label{chap2:lem4.19}% 4.19
Let $(\beta(t),F_t,P)$ be a $d-$dimensional Brownian motion and let
$\xi(.,x)$ be given by (\ref{chap2:eq4.5}). Then for any $\varepsilon
> 0$, $T > 0$, 
$f\in C^\infty_b(R^n)$ and $1 \le k \le d$ or $1 \le k \neq \ell \le d$: 
$$ 
\lim_{\delta \downarrow 0} P( | \int\limits^._0 f(\xi(u,x))
d(\beta^k(u))^2  ||^0_T \ge \varepsilon ||\beta(.) ||^0_T < \delta) 
= 0 
$$
and   
$$ 
\lim_{\delta \downarrow 0)}  P(|| \int f(\xi(u,x))
\beta^k(u)d\beta^\ell(u) ||^0_T \ge \varepsilon  || \beta(.) ||^0_T
< \delta) = 0. 
$$
\end{lemma}

\begin{proof} 
 Set $\xi(.) = \xi(.,x)$. Then for $1 \le k, \ell \le d$, we see, by 
 It$\hat{o}'$s formula, that 
  \begin{align*}
\int\limits_0^t f(\xi(u)) d (\beta^k\beta^\ell(u)) & =
\beta^k\beta^\ell(t)f(\xi)(t))\\ 
& - \int\limits^t_0 \beta^{k} \beta^{\ell}(u) < \sigma^{*} (\xi (u))
\text{ grad } f(\xi (u)),d\beta (u) >\\ 
& - \int\limits^t_0\beta^k\beta^\ell (u)Lf(\xi (u)) du 
 \end{align*} 
 $$
 - \int\limits_0^t [\beta^k(u)(\sigma^*(\xi(u)) \text{ grad } f(\xi
   (u))_\ell + \beta^\ell(u)(\sigma^*(\xi(u)) \text{ grad }
   f(\xi(u))_k] du. 
 $$

 All\pageoriginale except the second term on the right clearly tend to
 zero as $|| 
 \beta(.) ||^0_T\break \rightarrow 0$. Moreover, the second term is covered
 by (\ref{chap2:lem4.18}) with $\alpha = 2$. Thus 
 $$
 \lim_{\delta \downarrow 0} P(||
 \int\limits_0^. f(\xi(u))d(\beta^k\beta^\ell(u)) ||^0_T \ge
 \varepsilon  ||\beta(.)||^0_T < \delta ) = 0 
 $$
 for all $\varepsilon > 0$ and $1 \le k$, $\ell \le d$. This proves our
 first assertion upon taking $k = \ell $. 
 
 To prove the second assertion, note that
{\fontsize{10pt}{12pt}\selectfont 
$$
 \int\limits_0^t f(\xi(u))\beta^k(u)d\beta^\ell(u) = \frac{1}{2} \int
 f(\xi(u))d (\beta^k(u)\beta^\ell(u)) + \frac{1}{2}\int\limits_0^t
 \theta(u) |\beta(u)| dL_{k,\ell}(u), 
 $$}\relax
 where $L_{k,\ell}(.)$ is as in (\ref{chap2:lem4.14}) and
$$
\theta(.) = \frac{(\beta^k(.)^2 +
  \beta^{\ell}(.)^2)^{1/2}}{|\beta(.)|} f(\xi(u)). 
$$

Thus the first term is of the sort just treated and the second one is
covered by (\ref{chap2:lem4.14}). 
\end{proof}

\setcounter{theorem}{19}
\begin{theorem}\label{chap2:thm4.20}%theorem 4.20
Let $(\beta(t),F_t,P)$ be a $d-$dimensional Brownian motion and let
$\xi(.,x)$ be given by (\ref{chap2:eq4.5}). Given $\psi \in
C^2_0((0,\infty);R^d)$, define $\phi(.)$ by (\ref{chap2:eq4.3}). Then
for each $\varepsilon > 0$ and $T > 0$:  
$$
\lim_{\delta \downarrow 0} P(\Sup_{0 \le t \le T} | \xi(t,x) -\phi(t) 
| \ge \varepsilon \Sup_{0 \le t \le T} || \beta(t) - \psi(t) | \le
\delta) = 1. 
$$

In particular, if $P_x \sim  L$ at $x$, where $L$ is defined by
(\ref{chap2:eq4.1}), then  
$$
\text{ supp } (P_x) = \overline{\mathscr{S}(x; \sigma ,b)}. 
$$
\end{theorem}

\begin{proof}
We\pageoriginale first note that it suffices to handle the case when
$\psi \equiv 0$ but $b(.)$ may depend on $t$ as well as $x$. Indeed,
the general case reduces to this one by considering the probability
measure $Q$ defined by $dQ = R_\psi dP$, where  
$$
R_\psi  = \exp(\int\limits^\infty_0 < \dot{\psi}(s), d\beta(s) . -
\frac{1}{2}\int\limits_0^{\infty} | \dot{\psi}(s)|^2 ds). 
$$

By Lemma (\ref{chap2:lem3.5}), $(\beta_{\psi}(t), F_t, Q)$ is a
$d$-dimensional Brownian 
motion, where $\beta_{\psi}(t)=\beta(t)-\psi(t)$, $t \geq 0$ and clearly 
$$
\xi(t, x)=x+\int\limits_0^t \sigma(\xi (s,x))d
\beta_{\psi}(s)+\int\limits_0^t \tilde{b}_{\psi}(s, \xi(s, x)), t\geq
0, 
$$
(a.s., Q), where $\tilde{b}_{\psi}(t,
x)=\tilde{b}(x)+\sigma(x)\dot{\psi}(t)$. Thus, assuming the case when
$\psi \equiv 0$, we have 
$$
\lim_{\delta \downarrow 0} Q(|| \xi(., x)-\phi(.)||^0_T \geq \varepsilon
\; || \beta(.)- \psi(.)||^0_T \leq \delta )=1 
$$
and so
\begin{align*}
& \varlimsup_{\delta \downarrow 0} P(||\xi(., x)- \phi(.)||^0_T \geq
\varepsilon \;\; ||\beta (.)-\psi(.)||^0_T \leq \delta ) \\
=\lim_{\delta \downarrow 0} \qquad &
\frac{E^P[\chi_{\varepsilon, \infty}(||\xi 
    (,.x)-\phi(.)||_T^0)\chi_{[0,
        \delta}](||\beta(.)-\psi(.)||^0_T)]}{E^P[R_{\psi}(T)\chi_{[\varepsilon,
        \infty]}(||\xi(., x)-\phi(.)||^0_T)\chi_{[0,
        \delta]}||\beta(.)-\psi(.)||^0_T)]} \times\\
& \times \frac{E^P[R_{\psi}(T)\chi_{[o,
        \delta]}(||\beta(.)-\psi(.)||^0_T)]}{E^P[\chi_{0,
      \delta}(||\beta(.)-\psi(.)||^0_T)]} 
\end{align*}
where
\begin{align*}
R_{\psi}(T) &=\exp (\int\limits_0^T < \dot{\psi}(s),d\beta(s) > -
\frac{1}{2}\int\limits_0^T | \dot{\psi}(s)|^2 ds]\\ 
&=\exp (< \psi (T), \beta(T)> -\int\limits_0^T < \beta(s), \ddot{\psi}(s) >
ds - \frac{1}{2}\int\limits_0^T|\dot{\psi}(s)|^2 ds). 
\end{align*}\pageoriginale

Observe that the ratio  in the first factor tends to 
$$
\exp(- \dot{\psi} (T) |^2 + \int\limits_{0}^t < \psi(s), \ddot{\psi} (s) > ds + 
\frac{1}{2} \int\limits_{0}^T| \dot{\psi}(s)|^2 ds) \text { as
} \delta \downarrow 0 
$$
while the ratio in the second factor tends to  
$$
\exp( |\dot{\psi} (T) |^2 -\int\limits_{0}^t < \psi(s),
\ddots{\psi}(s), > ds - 
\frac{1}{2} \int\limits_{0}^T| \dot{\psi}(s)|^2 ds; 
$$
and so the products tends to 1. 

To handle the case when $\psi \equiv 0$ and $b$ depends on $(t,x)$,
let $\xi(t) =\xi(t,x)$. Then  
\begin{align*}
\xi^i (t) & = x^i + \sigma^i_\ell (\xi(t)) \beta^\ell
(t)-\int\limits_{0}^t  \beta^\ell(s) \sigma^i_{\ell, j}(\xi(s))
\sigma^j_k (\xi(s))d \beta^k(s) \\ 
& \quad - \int \limits^t_0 \beta^\ell(s) (L \sigma^i_\ell)(\xi(s)) ds
- \frac{1}{2} \int\limits_{0}^t \sigma^i_{\ell,j}(\xi(t))
\sigma^j_\ell(\xi(s)) ds \\ 
& \qquad \qquad + \int\limits_{0}^t b^i(s, \xi(s)) ds \\ 
& = x^i + \int\limits_{0}^{t} b^i (s,\xi(s)) ds - \Delta^i(t), 
\end{align*}
where
\begin{align*}
\Delta^i (t) &= \sigma_\ell^i(\xi(t)) \beta^\ell (t)- \sum_{k \neq
  \ell} \int\limits_{0}^t \sigma_{\ell,j}^i, \sigma^j_k(\xi(s))
\beta^\ell(s) d \beta^k (s)\\ 
&- \frac{1}{2} \int\limits_{0}^t \sigma_{k,j}^i, \sigma^i_k(\xi(s))
d (\beta^k (s))^2- \int\limits_{0}^t \beta^\ell(s) (L
\sigma^i) (\xi(s)) ds.   
\end{align*}

We\pageoriginale have used in these expressions the convention that
repeated indices are summed and the notation  
$$
\sigma_{k,j}^i = \frac{\partial \sigma_k^i}{\partial x_j} 
$$
By Lemma (\ref{chap2:lem4.19}), 
$$
P( || \Delta(.) ||^0_T \ge \varepsilon || \beta)(.) ||^0_T \le
\delta) \to 0 \text{ as } \delta \downarrow 0 
$$ 
for each $\varepsilon > 0$ and $T >0$. Hence 
$$
\lim\limits_{\delta \downarrow 0} )P(|| \xi(.) - x-
\int\limits_{0}^. b(s, \xi(s))  ds ||^o_T \ge \varepsilon || \beta(.)
||^0_T \le \delta =0 
$$
But, since $| b (t,x) -b(t,y)| \le c|x-y|$, it is easily seen from
this that  
$$ 
\lim\limits_{\delta \downarrow 0} )P(|| \xi(.) - \phi(.) ||^o_T \ge
\varepsilon \;\; ||\beta(.) ||^0_T \le \delta) =0, 
$$
where
$$
\phi(t) = x+ \int\limits_{0}^t b(s, \phi(s)) ds, \;\; t \ge 0. 
$$
\end{proof}

\setcounter{coro}{20}
\begin{coro} % cor 4.21
Let $G$ be an open set in $R \times R^n$ and suppose that $u \in
C^{1,2}(G)$ satisfies 
$$
\frac{\partial u}{\partial t} + Lu \ge 0
$$
in $G$, where $L$ is given by (\ref{chap2:eq4.1}). If $(t_0, x^o) \in G$ and $u
(t_0, x^0) = \max_{G} u$, then $u(t,x) =u (t_0, x^o)$ at all points
$(t,x) \in G_L (t_0, x^o)$, where $G_L(t_0,x^o)$ is the closure in $G$
of the points $(t_1,   \phi(t_1-t_0)), t_1 \ge t_0$, where $\phi \in
\mathscr{S}(x^o; \sigma, b)$ satisfies $(t, \phi(t- t_0)) \in G$ for $t_0 \le t
\le t_1$. In fact, if   
$$
u(t_0, x)= \max_{G_L(t_0,x^o)} u,
$$\pageoriginale
then $u \equiv u(t_0,x)$ on $G_L(t_0, x^o)$.
\end{coro}

\begin{proof}
Given (\ref{chap2:thm4.20}), the proof is precisely the same as that
of (\ref{chap2:coro3.11}).  
\end{proof}


\setcounter{remark}{21}
\begin{remark} % rem 4.22
When $G= R \times R^n$, it is easy to show that if $(t_1, x^1) \notin
G_L(t_0,x^o)$ then there is a $u \in C^{1,2}(G)$ whose maximum is achieved at
$(t_0,x^o)$ and yet $u(t_1 , x^1) < u(t_0, x^o)$. If $t_1 < t_0$, this
is easy: simply take 
\begin{equation*}
u(t,x)=
\begin{cases}
 - \exp (-\frac{1}{t_0 -t}) &\text{ if } t < t_0\\[4pt]
 0 & \text{ if } t \ge t_0
\end{cases}
\end{equation*}

If $t_1 \ge t_0$, choose an open set $U \ni(t_1, x^1)$ so that  $U(
\cap G_L(t_1, x^1)= \phi$ and let $f \in  C^\infty_0(U)$ be a
non-positive function such that $f(t_1, x^1) < 0$. Set 
$$
u (t,x)= E [ \int\limits_{0}^\infty f(s, \xi(s-t,x)) ds]. 
$$

Using (I, \ref{chap1:eq4.6}), one can easily show that $u \in  C_b(R
\times R^n)$ and that   
$$
\frac{\partial u}{\partial t} +Lu =-f \ge 0. 
$$ 
Moreover, by (\ref{chap2:thm4.20}), $u(t_0, x^o)=0$. Finally, $u(t_1, 
x^1)< o$.    

In order to show that $G_L(t_0, x^o)$ is \textit{maximal} when $G \neq
R \times R^n$, one must extend the notion of ``$\partial u / \partial
t + Lu \ge 0$'' to functions $u$ which are not necessarily
smooth. This is done in the paper [Degen, Diff, s]. In that same
paper, it is also shown how\pageoriginale to extend the
characterization of supp $(P_x)$ to $P_x \sim L$ at $x$ when $L$
cannot be written in the form (\ref{chap2:eq4.1}).    
\end{remark}


\section{The ``Most Probable path'' of a brownian motion with drift}

Let $(\beta(t), F_t, P)$ be a $d-$dimensional Brownian motion and
let $b: R^d \to R^d$ be a $C^\infty$-vector field with bounded first
derivatives. Define $\xi(.)$ by   
\begin{equation*}
\xi(t) = \beta(t) = \beta(t) + \int\limits_{0}^t b(\xi(s)) ds, t \ge
0. \tag{5.1}\label{chap2:eq5.1} 
\end{equation*}

We already know that foe any $\phi \in C^2 ([0, \infty), R^d)$ with
  $\phi(0) =0$, 
$$
P(\Sup_{0 \le t \le T} || \xi(t) - \phi(t) || < \varepsilon)> 0 
$$
for all $\varepsilon > 0$ and $T >0$. We now want to get an asymptotic
estimate on this probability as $\varepsilon \downarrow 0$. 


\setcounter{lemma}{1}
\begin{lemma}\label{chap2:lem5.2}%%% 5.2
There is an orthonormal real basis  
$$
\{ \phi_n\}^\infty_0 \subseteq C^\infty_b(B(0,1)) \text{ of } L^2
(B(0,1)) 
$$
such that 
$$
\lim_{|x| \uparrow 1} \phi_n (x) =0 
$$
and 
$$
- \frac{1}{2}\Delta \phi_n = \lambda_n \phi_n, n \ge 0 
$$
where $0  < \lambda_0 < \lambda_1 \le \lambda_2 < \cdots \ge \lambda_n
\le \cdots \uparrow \infty$. Furthermore, $\phi_0$ never vanishes in
$B(0.1)$. Finally, there is an $N=N(d)$ such that  
$$
\sum_0^{\infty} \frac{1}{\lambda_n^N} \phi_n (x) \phi_n(y)
$$
converges absolutely and uniformly in $\overline{B(0,1)} \times
\overline{B(0,1)}$ and if  $f \in C_b\break (B(0,1))$ then  
\begin{equation*}
E[f(x+ \beta(t)), \tau_x > t ]= \sum e^{-\lambda_n t}(f,
\phi_n)_{L^2(B(0,1))} \phi_n(x), \tag{5.3}\label{chap2:eq5.3} 
\end{equation*}\pageoriginale
where
$$
\tau_x=  \inf\{ t \ge 1: |x+ \beta(t)| \ge 1 \}. 
$$
In particular,
$$
P(\Sup_{0 \le t \le T}|\beta(t)| < \varepsilon)\sim C e^{-\lambda T/
  \varepsilon^2} \text{ as } T/ \varepsilon^2 \to \infty, 
$$
where $ c=  (\phi_0, 1)_{L^2 (B(0,1))} \phi_0(0) $ and $\lambda=
\lambda_0$. 
\end{lemma}


\begin{proof}
The spectral properties of $- \dfrac{1}{2} \Delta$ with Dirichlet
boundary conditions in $(B(0,1))$ are well-known. In particular, the
facts that the spectrum is completely discrete and positive and that
$\lambda_0$ is simple can be found in elementary books on Partial
Differential Equations. The absolute and uniform convergence of    
$$
\sum_{0}^\infty \frac{1}{\lambda_n^N} \phi_n(x) \phi_n(y)
$$
for some $N$ is a consequence of Mercer's theorem applied to the
$N$-th iterate of the Green's functions. From this it is easy to check
that if $u(t,x)$ is given by the right hand side of (\ref{chap2:eq5.3}) with $f
\in C^\infty_0(B(0.1))$ then   
\begin{align*}
&\frac{\partial u}{\partial t}= \frac{1}{2} \Delta u \text{ to } (0,
  \infty) \times B(0,1),\\ 
&\lim_{|x| \uparrow 1} u(t,x) =0 \text{ for } t \ge 0, \text{ and }\\ 
& \lim_{t \downarrow 0}u(t,x) = f(x), x \in B(0,1).
\end{align*}

Hence by It$\hat{o}'s$ formula, (\ref{chap2:eq5.3}) holds for $f \in
C^\infty_0(B(0,1))$. The general case in then proved by
approximation. 

Once\pageoriginale one has (\ref{chap2:eq5.3}), it is not hard to show
that $\phi_0$ 
never vanishes. Indeed, from (\ref{chap2:eq5.3}), it is clear that    
$$
0 \le e^{\lambda_0 t} P(\tau_x >t)= ( \phi_0, 1) \phi_0(x) +0(1)
\text{ as } t \uparrow \infty. 
$$

Thus if $(\phi_0, 1) > 0(<0)$, then $\phi_0 \ge 0( \le 0)$. Form
$\dfrac{1}{2} \Delta \phi_0=- \lambda_0 \phi_0$ and the strong maximum
principle, it follows that $\phi_0 >0 (0 < 0)$. On the other hand, if
$(\phi_0,1)=0$, then from (\ref{chap2:eq5.3}) with $f= || \phi_0 ||_u
+ \phi_0$:  
$$
0 \le e^{\lambda_0 t} E[f(x+ \beta(t)), \tau_x >t]= \phi_0(x)+ \mathcal{O}(1) , 
$$
which obviously contradicts $(\phi_0, 1)=0$. 

Finally, as we saw in (\ref{chap2:lem4.8}), 
$$
P(\Sup_{0 \le t \le T}|\beta(t)| < \varepsilon) = P(\tau_0 >
T/\varepsilon^2) 
$$
and clearly the above considerations prove that  
$$
P(\tau_0 >t)\sim C e^{-\lambda_0 t}  
$$
with 
$$
C=  (\phi_0,1) \phi_0(0) >0.  
$$
\end{proof}

\setcounter{exercise}{3}
\begin{exercise}% 5.4
Let $G$ be a bounded, connected open set in $R^d$. Given $x \in G$,
let $\tau_x = \inf \{ t \ge 0: (x + \beta(t))\notin G\}$. Show that  
$$
P(\tau_x >t) \sim  C(x) e^{-\lambda t} \text{ as } t \uparrow \infty,
$$
where $C(x) > 0$ and $\lambda > 0$ does not depend on $x$. 
\end{exercise}

Now let $\phi \in C^2_0([0, \infty), R^d)$ and $T > 0$ be given and
  define  
\begin{align*}
\psi(t) & = \phi(t) - \int\limits_{0}^t b(\phi(s)) ds, t \ge 0
\tag{5.5}\label{chap2:eq5.5}\\ 
b(t,x) & = b(x + \phi(t))-b (\phi(t)), (t,x) \in [0, \infty) \times
  R^d , \tag{5.6}\label{chap2:eq5.6}\\ 
\xi_\phi(t) &= \xi(t)- \phi(t), t \ge 0, \tag{5.7}\label{chap2:eq5.7}\\ 
\beta_\psi(t) &= \beta(t)- \psi(t \Lambda T), t \ge 0
\tag{5.8}\label{chap2:eq5.8}  
\end{align*}\pageoriginale
and 
\begin{equation*}
R_\psi= \exp [\int\limits_{0}^T < \psi(s), d \beta(s) > - \frac{1}{2}
  \int\limits_{0}^T | \psi(s)|^2 ds]. \tag{5.9}\label{chap2:eq5.9} 
\end{equation*}
Observe that 
\begin{equation*}
\xi_\phi(t)= \beta_\psi(t) + \int\limits_{0}^t b(s, \phi(s)) ds, \;\; t \ge 
0; \tag{5.10}\label{chap2:eq5.10} 
\end{equation*}
and that if $d Q_\psi= R_\psi dP$, then  $(\beta_\psi(t), F_t,
Q_\psi)$ as a $d-$dimensional Brownian motion (cf. Lemma
\ref{chap2:lem3.5}). Also  
\begin{align*}
R_\psi &= \exp [< \dot{\psi}(T),\beta(T) >- \int\limits_{0}^T <
  \ddot{\psi}(t), \beta(t) >dt - \frac{1}{2} \int\limits_{0}^T
  |\dot{\psi}(t) |^2 dt]\\ 
&= \exp < \dot{\psi}(T),(T) >- \int\limits_{0}^T < \ddot{\psi}(t),
\psi(t) > dt - \frac{1}{2} \int |\dot{\psi}(t) |^2 dt]\\ 
& \qquad \quad \times \exp [< \dot{\psi}(T), \beta_\psi(T) >-
  \int\limits_{0}^T < \ddot{\psi} (t), \beta_\psi(t) > dt ]\\ 
&\qquad \quad \to \exp [\frac{1}{2} \int\limits_{0}^T |\dot{\psi}(t)
  |^2 dt ] 
\end{align*}
uniformly and boundedly as $|| \beta_\psi(.) ||_T \to 0$. 

\noindent
Since
$$
|| \beta_\psi(.) ||^0_T < (1 + LT) || \xi_\phi(.) ||^0_T, 
$$
where $L = || \text{ grad b} ||_u$, we now have 
$$
\lim_{\varepsilon \downarrow 0} \frac{P( || \xi_\phi(.) ||^0_T <
  \varepsilon )}{Q_\psi ( || \xi_\phi(.) ||^0_T < \varepsilon )} =
\exp [- \frac{1}{2} \int\limits_{0}^T | \phi(t) - b(\phi(t))|^2 dt] 
$$\pageoriginale
We now to the study of $Q_\psi(|| \xi_\phi(.) ||^o_T < \varepsilon
)$. 

\setcounter{lemma}{11}
\begin{lemma} % lem 5.12
Referring to the notation in (\ref{chap2:eq5.5}) -
(\ref{chap2:eq5.9}), we have   
$$
Q_\psi (|| \xi_\Phi (.) ||^o_T < \varepsilon ) =  E [R(T) \;\; || \beta (.)
  ||^0_T < \varepsilon], 
$$
where
$$
R(T)= \exp[ \int\limits_{0}^T < b, \beta(s)), d \beta(s) >-
  \frac{1}{2} \int\limits_{0}^T|b(s, \beta(s)) |^2 ds] 
$$
\end{lemma}

\begin{proof}
First note that without loss of generality we may assume that $b \in
C([0, \infty) \times R^n)$, since if this is not the case we can we
  can replace $b(s,x)$ by such a function $b'$ which coincides with
  $b$ on 
$$
\overline{[0;T]\times B(0,|| \phi(.) ||^0_T +
    \varepsilon)}. 
$$
Next observe that since $(\beta_\psi(.), F_t,
  Q_\psi)$ is a Brownian motion and $\xi_\phi(.)$ is given by
  (\ref{chap2:eq5.10}), the distribution of $\xi_\phi(.)$ under
  $Q_\psi$ is the   same as the distribution of $\eta(.)$ under $P$,
  where   
$$
\eta(t) =\beta(t) + \int\limits_{0}^t b(s, \eta(s)) ds , t > 0. 
$$ 

But, if $dQ=R(T) dp$, then $\eta(. \Lambda T)$ has the same
distribution under $P$ as $\beta(. \Lambda T)$ has under $Q$. Hence  
\begin{align*}
Q_\psi( || \xi_\psi(.) ||^o_T  < \varepsilon) &= P(|| \eta (.) ||^o_T
< \varepsilon )\\ 
&= E^P[R(T) || \beta (.) ||_T < \varepsilon ].  \hspace{2cm}{\text{Q.E.D.}}
\end{align*}

Since we already know the asymptotic of $P( || \beta ( . ) ||_T <
\varepsilon)$ as $\varepsilon \downarrow 0$, it remains only to
compute  
$$
\lim_{\varepsilon \downarrow 0} E^P[R(T) || \beta (.) ||_T <
  \varepsilon ]. 
$$
To\pageoriginale this end, note that 
{\fontsize{10pt}{12pt}\selectfont
$$
E^p[R(T), || \beta(.)  || ^0_T < \varepsilon]= e^{0 (\varepsilon)}E^P
[\exp[\int\limits_{0}^T < b (s, \beta(s)), d \beta(s) > || \beta (.)
    ||^0_T < \varepsilon]] 
$$}\relax
Next
{\fontsize{10pt}{12pt}\selectfont
\begin{gather*}
\int\limits_{0}^T< b(s, \beta(s)), d \beta (s) > = < \beta (T), b (T,
\beta(T)) >  - \int\limits_{0}^T( \frac{\partial}{\partial s} +
\frac{1}{2} \Delta )b(s, \beta(s)) ds\\ 
- \sum_{k, \ell} \int\limits_{0}^T \beta_k(s) b^k_{'\ell}(\beta(s) +
\phi(s)) d \beta^\ell(s) 
\end{gather*}}\relax
where
$$
b^k_{'\ell} (x) \equiv \frac{\partial b^k}{\partial x_\ell}(x). 
$$
Finally,
\begin{align*}
\sum_\ell & \int\limits_{0}^T \beta_k (s) b_{'\ell}^k (\beta(s) +
\phi(s)) d \beta^\ell (s) \\ 
& =\frac{1}{2} \int\limits_{0}^T \text{ div } b \beta(s) + \phi(s)) ds -
\frac{1}{2} \sum_{k} \int\limits_{0}^T b_{'k}^k (\beta(s) + \phi(s)) d
(\beta^k (s))^2\\ 
& \qquad - \sum_{k \neq \ell} \int\limits_{0}^T b_{'\ell}^k (\beta(s) +
\phi(s)) \beta^k(s) d \beta^\ell(s). 
\end{align*}
Combining all these, we arrive at 
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
 E^p & [R(T) || \beta (.)||^0_T < \varepsilon]\\
& = e^{o(\varepsilon)} \exp( \frac{1}{2} \int\limits_{0}^T \text{ div }
 b(\phi(s)) ds) \times E^p \exp (\sum_{k} \Delta_{k} (T) +
 \sum_{k\neq, \ell}(T)) ||  \beta (.) ||^0_T < \varepsilon]
\end{align*}}\relax
where
$$
\Delta_k (T) =- \frac{1}{2} \int\limits_{0}^T b_{'k}^k ( \beta(s) +
\phi(s)) d( \beta^k(s))^2 
$$
and
$$
\Delta _{k, \ell}(T)= \int\limits_{0}^T b ^k _{, \ell}( \beta (s) +
\phi (s)) \beta^k (s) d \beta ^{\ell}(s). 
$$\pageoriginale

To complete our programme, we must prove that 
$$
\lim_{ \varepsilon \downarrow 0} E^p[\exp ( \sum _{k} \Delta _{k} (T)
  + \sum_{ k \neq \ell} \Delta _{k, \ell}(T)) || \beta (.) ||_T^0 <
  \varepsilon  ) = 1 . 
$$
\end{proof}

\setcounter{lemma}{12}
\begin{lemma}\label{chap2:lem5.13} % lem  5.13
Let $f \in C^{\infty}_{b} ([0, \infty ) \otimes R^d ;
  R^1)$. Then for $1 \leq k \leq d$ or $1 \leq k \neq  \ell \leq d $ 
$$
\Sup _{ 0 \leq \varepsilon \leq 1} E^p [ \exp ( \int_0 ^T f(u, \beta
  (u))d (\beta ^k (u))^2) || \beta (.) ||_T^o < \varepsilon ] < \infty 
$$
and 
$$
\Sup_{0 \leq \varepsilon \leq 1}E^p [\exp ( \int _{o}^{T}f(u,\beta(u))
  \beta^k (u) d \beta^{\ell}(u)) || \beta (.) ||_T^o < \varepsilon ]<
\infty . 
$$
\end{lemma}

\begin{proof}
Let $1 \leq k , \ell \leq d$. Then 
\begin{gather*}
\int\limits_{o}^{T} f(u,\beta(u))d(\beta ^k \beta {\ell}(u)) = \beta^k
\beta^{\ell} (T) f(T,\beta(T)) \\
- \int\limits_0^{T}\beta^k \beta^{\ell} (t) < grad_{x} f(s,\beta (s)), 
d \beta (s) >\\ 
- \int\limits_o^T [ \beta ^k \beta^{\ell}(s)
  (\frac{\partial}{\partial s}+ \frac{1}{2}\Delta) f+\beta ^k (s)
  \frac{\partial f}{\partial x_k} + \beta^{\ell}(s) \frac{\partial
    f}{\partial x_{\ell}}](s, \beta (s)) ds . 
\end{gather*}
Thus for $0 \leq \varepsilon \leq 1$: 
\begin{align*}
E^P & [ \exp( \int\limits^T_o f(s,\beta(s)) d(\beta^k
  (u)\beta^{\ell}(u)) \;\; || \beta(.) ||^0_T < \varepsilon ]\\
& \leq CE^P [ \exp(- \int\limits_O^T \beta ^k \beta^{\ell}(s) < grad_{x}
  f(s, \beta(s)),d \beta(s)> ) \;\; || \beta(.)||_T^o < \varepsilon ] . 
\end{align*}
But, as in the proof of (I. \ref{chap1:lem4.3})		
\begin{gather*}
P( - \int_{0}^{T} \beta ^k \beta^{\ell} (s) , grad_{x} f(s,\beta (s)),
d \beta(s) > \ge R |  \;\; || \beta (.) ||_t^o < \varepsilon ) \\
\leq \exp(- \frac{R^2}{2T \varepsilon^4 || grad_{x} f|| ^2 _u }), R >
0. 
\end{gather*}\pageoriginale
Thus for any  $1 \leq  k$, $\ell \leq d$: 
 $$
 \Sup_{0 \leq \varepsilon \leq1} E^P [\exp(\int\limits_o^T f(s, 
   \beta(s)) d(\beta ^k \beta^{\ell})(s)))  \quad|| \beta(.) ||_T^o <
   \varepsilon], < \infty
 $$
since 
$$
P(|| \beta (.) ||_T^o  < \varepsilon ) \ge A e^{-BT/ \varepsilon ^2}.
$$
This clearly proves the first assertion.	

If $1 \leq  k \neq \ell \leq d$, then 
\begin{gather*}
\int\limits_0^T f(s, \xi (s)) \beta^k (s) d \beta^{\ell}(s) =
\frac{1}{2} \int\limits_{0}^{T} f (s, \xi (s)) d( \beta^k \beta
^{\ell}(s))\\
 + \frac{1}{2} \int\limits_{0}^{T}\theta (s) |\beta (s) |
dL_{k, \ell }(s) , 
\end{gather*}
where $L_{k, \ell}(.) $ is as in (\ref{chap2:lem4.14}) and 
$$
\theta (.) = \frac{ ( \beta^k (.)^2 + \beta^{\ell}(.)^2 )^{1/2}}{
  | \beta (.)|} f(.,\xi (.)). 
$$
By what we have just seen
$$
\Sup_{0 \leq \varepsilon \leq 1}E^P [\exp (\lambda \int\limits_o^T
  f(s, \xi (s)) d ( \beta^k \beta^{\ell}(s)) \quad || \beta (.)
  ||_T^o < \varepsilon ] < \infty 
$$
for any $ \lambda > 0$. On the other  hand, by (\ref{chap2:eq4.15}); 
\begin{gather*}
P( || \int\limits_o^. \theta (s)|\beta(s) | dL_{k, \ell }(s) ||^o _T
\ge R \quad || \beta (.)| _T^o < \varepsilon )\\
 < 2 \exp \left(-\frac{R^2}{ 2T || \theta ||_u ^2 \varepsilon^2} \right) 
\end{gather*}
Thus\pageoriginale
$$ 
 \Sup_{0 < \varepsilon \leq1}E[ \exp(\lambda
   \int\limits_{0}^{T}\theta (s) | \beta (s)| dL_{k, \ell}(s)) \quad 
   || \beta(.) ||_T ^o < \varepsilon ] < \infty 
$$
for any $\lambda > 0$. Clearly this completes the proof.  
\end{proof}

\setcounter{theorem} {13}
\begin{theorem}\label{chap2:thm5.14}%theorem 5.14
 Let $(\beta (t), F_t , P )$ be a $d-$dimensional Brownian motion and
 suppose that $\xi (.)$ is given  by (\ref{chap2:eq5.1}), where $b:R^d
 \rightarrow 
 R^d $ is $ C^{\infty}$  and has bounded first derivatives. Then for $
 \phi \in C^2 ( [0, \infty),R^d)$ satisfying $\phi (0) =0$: 
 \begin{gather*}
  \lim_{\varepsilon \downarrow 0} P ( || \xi (.) - \phi (.) ||_T < \varepsilon)
  \sim C_0 e^{-\lambda_0 T/ \varepsilon ^2}
  \exp(\frac{1}{2}\int\limits_0^T |\phi (t) -b(\phi(t)) |^2 dt\\
 +  \frac{1}{2} \int\limits_0^T \text{ div } b ( \phi (t))dt) 
 \end{gather*}
 as $\varepsilon \uparrow 0$, where $C_0$ and $\lambda_0$ are the
 numbers described in (\ref{chap2:lem5.2}). 
   \end{theorem}   

   \begin{proof}
We have seen that 
\begin{align*}
& P( || \xi (.) - \phi (.) ||_T^O <\varepsilon \;\; || \beta (.) ||_T^O < 
\varepsilon )\\ 
& \quad = e^{O(\varepsilon )} \exp (-\frac{1}{2} \int\limits^T_0 |
\dot{\phi}(t) - b(\phi(t)) |^2 + \frac{1}{2} \int\limits^T_0 
\text{ div } b (\phi (t)) \, dt ) \times   \\ 
& \qquad \qquad \times  E^p \exp (\sum_k \Delta_k (T) +\sum_{k \neq \ell}
\Delta_{k, \ell}(T))|\; ||\beta (.)||^O_T < \varepsilon ]. 
\end{align*}
 By (\ref{chap2:lem4.19}) (with $x = 0$) and $\xi (t, x) = \beta (t)$,
 we know   
 $$
 P (|\Delta_k (T)| \geq \alpha |\; || \beta(.)||^0_T < \varepsilon )
 \text{ and }  P (|\Delta_{k, \ell} (T) | \geq \alpha ||
 \beta(.)||^0_T < \varepsilon)  
 $$
tend to 0 as $ \varepsilon \downarrow 0$ for each $ \alpha > 0$. By
(\ref{chap2:lem5.13}), we know that for any $\lambda > 0$.  
$$
\varlimsup_{\varepsilon \downarrow 0} E^p [\exp [\lambda (\sum_k
    \Delta_k (T) + \sum\limits_{k\neq \ell} (T))] || \beta (.)||^0_T <
  \varepsilon]   
$$
is\pageoriginale finite. Hence  
$$
\lim_{\varepsilon \downarrow 0} E^p [\exp [\lambda (\sum_k \Delta_k(T)
    + \sum_{k\neq \ell} (T)) ||  \beta (.)||^0_T < \varepsilon = 1.  
$$
Therefore
\begin{gather*}
\lim_{\varepsilon \downarrow 0} P( || \xi (.) - \phi (.)||^0_T <
\varepsilon || \beta (.) ||^0_T <\varepsilon \\ 
= \exp (- \frac{1}{2}\int\limits^{T}_0 | \dot{\phi} (t) - b (\phi)(t) |^2
dt +\frac{1}{2} \int\limits^T_0\text{ div } b (\phi(t))dt). 
\end{gather*}

The desired result therefore follows from (\ref{chap2:lem5.2}). 
 \end{proof}

\setcounter{remark}{14}
\begin{remark}% rem 5.15
The result in (\ref{chap2:thm5.14}) makes it possible to discuss the notion of the
``most probable path''  of the process $\xi (.)$. Indeed, from
(\ref{chap2:thm5.14}), 
one sees that most likely route followed by $\xi (.)$ in going from $
0 \to x \in R^d$ in time $T$ is the path $\phi_0(.)$ which
minimizes.  
$$
\frac{1}{2} \int\limits_0^T |\dot{\phi} (t) - b (\phi (t))|^2 = \frac{1}{2}
\int\limits_0^T \text{ div } b (\phi (t)) \, dt 
$$
subjects to $\phi (0)= 0$ and $ \phi (T) = x$. Using the usual
techniques from the calculus of variations, it is easy to develop the
Euler equation for this problem and thereby get and idea about what
$\phi _0 (.)$ looks like. In particular, when $b \equiv 0$, it is
cleat that  
$$
\phi_0 (t) = \frac{t}{T}x, 0 \leq t \leq T.
$$

Next\pageoriginale suppose that $d = 1$. By the calculus variations : 
$$
\ddot{\phi}_0(t) - (b'. b) = \phi_0 (t) + \frac{1}{2}b''  \cdot \phi_0
(t) = 0. 
$$

In general, this equation of course cannot be solved explicitly.
However, after multiplying through by $\dot{\phi}_0$, one sees that  
$$
\phi^2_0 (t) - b^2 \cdot \phi_0(t) + b \cdot \phi_0(t) = \text{
  constant }.  
$$

Thus one can get some idea how $\phi_0 (.)$ looks by using the ``phase
plane method''. That is, if $p = \dot{\phi_0}(t)$ and $q = 
\phi_0(t)$, then  
$$
p^2 - b^2 (q) + b' (q) = \text{ constant. } 
$$

In the special case when $b(x) = \alpha x$ , one can solve for $\phi
_0(.)$ explicitly:  
$$
\ddot{\phi}_0- \alpha_2\phi_0 = 0 
$$
and so 
$$
\phi_0(t) = x \frac{\sinh \alpha t}{\sinh \alpha T} 
$$
In this case 
$$
P (|| \xi (.) - \phi_0 (.)||^0_T < \varepsilon) \sim C e^{-\lambda T
  /\varepsilon^2}\exp (- \frac{\alpha x^2}{2}\frac{e^{- 2 \alpha
    T}}{l-e^{- 2  \alpha T}} + \frac{\alpha}{2} T). 
$$
\end{remark}

\begin{thebibliography}{99}
\bibitem{key1} [S \& V] :\pageoriginale {STOROOCK D.W and S.R.S. VARADHAN},
  ``Multidimensional diffusion process'', Springer-Verlag, Grudhlehren
  $\sharp$ 233. (1979)  

\bibitem{key2} [Berk , Symp] : STOROOCK D.W and S.R.S. VARADHAN,  ``On the
  support of Diffusion Process, with applications to the strong
  maximum principle''. Proc. 6th Berkeley symp. On Math. Stat. and
  Prob., Vol.III (1970), pp.333-360.   

\bibitem{key3} [Deg. Diff's] : STROCK , D.W and S.R.S VARADHAN, ``On 
  degenerate elliptic-parabolic operators of second order and their
  associated diffusions'', Comm, Pure Appl. Math. XXV (1972), pp.651-774. 
\end{thebibliography}



