\setcounter{chapter}{-1}

\chapter{Introduction}\label{chap0}

Let\pageoriginale us consider an ordinary differential equation on $\mathbb{R}^d$
\begin{equation*}
  \frac{dx}{dt} = f (x,t), \tag{0.1}\label{c0:eq0.1}
\end{equation*}
where $f(x,t)$ is continuous in $(x,t)$ and is Lipschitz continuous in
$x$. Denote by $\phi_{s,t}(x)$ the solution of the equation~\eqref{c0:eq0.1}
starting from $x$ at time s, i.e., with initial condition $x(s) =
x$. It is a well known fact that $\phi_{s,t}(x)$ satisfies the
following properties: 

\begin{enumerate}[(a)]
\item $\phi_{s,t}(x)$ is continuous in $s,t,x$,
\item $\phi_{t,u} (\phi_{s,t}(x)) = \phi_{s,u}(x)$ for any $s,t,u$ and any $x$,
\item $\phi_{s,s}(x) = x$ for any $s$,
\item the map $\phi_{s,t}: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is a
  homeomorphism for any $s,t$. 
\end{enumerate}

The map $\phi_{s,t}$ with above properties is called a flow of homeomorphisms.

A stochastic flow of homeomorphisms is an $\mathbb{R}^d$ valued random
field $\phi_{s,t}(x, \omega)$, $o \le s \le t \le T$, $x  \in
\mathbb{R}^d$ defined on a probability space ($\Omega F, P$) such that,
for almost all $\omega$, it has the above mentioned properties $(a)
\sim (d)$. In particular, if $\phi_{t_i, t_{i+1}}$, $i = 0, 1, \ldots, 
n - 1$ are independent for any $0 \le t_0 < t_1 < \ldots < t_n \le T$,
it is called a Brownian flow.  An important class of Brownian flows is
constructed by solving Ito's stochastic differential equation 
\begin{equation*}
  dx (t) = \sum^\tau_{k = 1} F_k (x,t) 
d B_k(t) + F_0(x,t)dt, \tag{0.2}\label{c0:eq0.2}
\end{equation*}
where $F_o(x,t),F_1(x,t),\ldots,  F_r(x,t)$ are continuous in ($x,t$)
and Lipschitz continuous in $x$, and $(B_1(t), \cdots,  B_r(t))$ is a
standard Brownian motion. Let $\phi_{s,t}(x, \omega)$ be the solution
of the equation under the initial condition $x(s) = x$. Then, taking a
suitable modification,\pageoriginale it defines a Brownian flow. This fact has been
established by many authors, e.g. Elworthy \cite{7}, Malliavin \cite{8},
Bismut \cite{3}, Ikeda - Watanabe \cite{13}, Kunita \cite{18} . However, not all
stochastic flows can be constructed by the above method. In fact, we
need an infinite number of independent Brownian motions $B_1(t),
B_2(t), \ldots$ and functions $F_O(x,t), F_1(x,t), \ldots, $ or
equivalently a Brownian with values in $C = C (\mathbb{R}^d ;
\mathbb{R}^d)$, i.e., a $C$- Brownian motion. Here, a continuous
$\mathbb{R}^d$ valued random field $X(s,t), x \in \mathbb{R}^d,  0 \le
t \le T$, is called a C- Brownian motion if $X(x,t_{i+1}) -
X(x,t_{i}), i = 0, 1, \ldots,  n - 1 $ are independent for any $0 \le
t_0 < t_1 \ldots < t_n \le T$. Then under a mild condition any
Brownian flow can be obtained by a stochastic differential equation of
the form  
\begin{equation*}
DC(t) = X(x(t), dt). \tag{0.3}\label{c0:eq0.3}
\end{equation*} 
 
  This fact is due to Le Jan \cite{23} and Baxendale \cite{1}. See also
  Fujiwara - Kunita \cite{8}.  
 
 The first part of these lectures will be devoted to the study of the
 basic properties of stochastic flows including the above facts. In
 Chapter~\ref{chap1} we shall characterize the Brownian flown by means of its
 local characteristics: 
 \begin{align*}
   b(x,t) & = \lim \frac{1}{h} (E [\phi_{t,t +h}(x)] - x)  \tag{0.4}\label{c0:eq0.4}\\
   a(x,y,t) & = \lim \frac{1}{h} (E [\phi_{t,t +h}(x) -x) (\phi_{t,t
       +h}(y) -y)^* ], 
 \end{align*}  
 where $*$ is the transpose of the column vector $(.). b(x,t)$ and
 $a(x,y,t)$ will be referred to as infinitesimal mean and
 infinitesimal covariance respectively. It will be shown that these
 two objects determine the law of the Brownian flow. 
 
 In Chapter~\ref{chap2}, we shall consider the stochastic differential equation
 based on C - valued Brownian motion or more generally C-valued
 continuous semimartingale $X(x,t)$, given in the form \eqref{c0:eq0.3}. It will
 be shown that the equation has a unique solution if its local
 characteristics are Lipschitz continuous and that the solution
 defines\pageoriginale a stochastic flow. Conversely, under some conditions the flow
 can be expressed as a solution of a suitable stochastic differential
 equation. 
 
 The second part of these notes will be devoted to various limit
 theorems concerning stochastic flows. We  shall consider three types
 of limit theorems. The first is the approximation theorem for a
 stochastic differential equation given by \eqref{c0:eq0.2}. Let 
 $v^\varepsilon (t) = (v^\varepsilon_1 (t), \dots,  v^\varepsilon_r(t))$, 
 $\varepsilon > 0$ be a piecewise smooth r dimensional process such that
 $B^\varepsilon (t) = \int^t_0 v^\varepsilon (r) dr, \varepsilon > 0$
 converges to a Brownian motion $B(t)$ as $\varepsilon \rightarrow
 0$. Consider the stochastic ordinary differential equation 
 \begin{equation*}
   \frac{dx}{dt} = \sum\limits^r_{k=1} F_k(x,t) v^\varepsilon_k (t) +
   F_o(x,t). \tag{0.5}\label{c0:eq0.5} 
 \end{equation*}
  
 Let $\phi^\varepsilon_{s,t}(x)$ be the solution of \eqref{c0:eq0.5} starting
 from $x$ at time s. The question is whether the family of stochastic
 flows $\phi^\varepsilon,  \varepsilon > 0$ converges weakly or
 strongly to a Brownian flow determined by the stochastic differential
 equation \eqref{c0:eq0.2} ( with some correction term). The problem has been
 considered by many authors in several special cases of $v^\varepsilon
 (t)$, e.g., Bismut \cite{3},  Dowell \cite{6}, Ikeda - Watanabe \cite{14}, Kunita
 \cite{15}, Malliavin \cite{25}. 
 
 The second limit theorem we shall be concerned with is that by
 Papanicolaou Stroock-Varadhan \cite{29}. Consider the following system
 of stochastic differential equations 
  \begin{align*}
    dx^\varepsilon(t) & = \frac{1}{\varepsilon} F(x^\varepsilon(t),
    z^\varepsilon (t))dt + G(x^\varepsilon (t), z^\varepsilon(t))dt +
    \sum^p_{j = 1} \sigma_j (x^\varepsilon (t), z^\varepsilon(t))d
    \beta_j (t) \\ 
    dz^\in(t) & =\frac{1}{\varepsilon^2} \tilde{F} (x^\varepsilon(t),
    z^\varepsilon (t))dt + \frac{1}{\varepsilon} \sum^q_{j = 1}
    \sigma_j (x^\varepsilon (t), z^\varepsilon(t))d \tilde{\beta}_j
    (t),  
 \end{align*} 
 where $( \beta_1(t), \ldots,  \beta_{p}(t))$ and $(
 \tilde{\beta}_1(t), \ldots,  \tilde{\beta}_{q}(t))$ are independent
 Brownian motions, $F, G, \tilde{F}, \sigma,  \tilde{\sigma}$ are
 bounded smooth functions with bounded derivatives. The processes
 $z^\varepsilon (t), \varepsilon > 0$(called the driving processes) do
 not converge. However, under\pageoriginale suitable conditions on the
 coefficients, the processes $x^\varepsilon (t), \varepsilon > 0$(the
 driven processes) can converge weakly to a diffusion process. 
 
 The third one is concerned with the stochastic ordinary differential
 equation  
 $$
 \frac{dx}{dt} = \varepsilon f(x,t,\omega) + \varepsilon^2 g(x,t,\omega),
 $$
 where $f,g$ are random velocity fields satisfying suitable strong
 mixing conditions and $E[f] = 0$. The stochastic flows
 $\phi^\varepsilon_{s,t}$ determined by the above equation converge to
 the trivial flow $\phi^0_{s,t} (x) \equiv x$. Khasminskii
 \cite{16}, Papanicolaou - Kohlet \cite{28}, Borodin \cite{5}, Kesten- Papanicolaou
 \cite{15} et al have shown that after changing the scale of time, the
 processes $\psi^\varepsilon_t =\phi^\varepsilon_{0,t/\varepsilon^2},
 \varepsilon > 0$ converge weakly to a diffusion process. 
 
 In Chapter~\ref{chap3} we shall present a general limit 
 theorem so that the above mentioned cases can be 
 handled together. 
 


\chapter{Brownian Flows}\label{chap1}
  
  This\pageoriginale chapter consists of four sections. In Section~\ref{chap1:sec1.1} 
  we define
  stochastic flows of measurable maps. If a stochastic flow of
  measurable maps continuous in $t$, has independent increments, we
  call it a Brownian flow. We then describe its N- point motion and
  show that it is a Markov process. Finally we show that given a
  consistent family of transition probabilities, we can construct a
  stochastic flow with independent increments whose N-point motion
  will have the same family of transition probabilities. In 
  Section~\ref{chap1:sec1.2}, we introduce the notion of local 
  characteristics of a
  Brownian flow which are essentially the infinitesimal mean and
  covariance of the flow. We then show the existence of a Brownian
  flow with prescribed local characteristics. In Section~\ref{chap1:sec1.3}, we
  study Brownian flow of homeomorphisms. We show that if the local
  characteristics of a Brownian flow satisfy certain Lipschitz
  conditions, then it becomes a flow of homeomorphisms. In 
  Section~\ref{chap1:sec1.4} we establish the diffeomorphism 
  property of a Brownian flow under some smoothness 
  Assumptions on its local characteristics. 
  
\section{Stochastic Flows with Independent Increments}\label{chap1:sec1.1}
  
  Let $(\Omega,  F, P)$ be a probability space. Let $T > 0$ be
  fixed. For $0 \le s \le t \le T$,  $x \in \mathbb{R}^d$, let
  $\phi_{s,t}(x, \omega)$ be an $\mathbb{R}^d$ valued random field
  such that for each fixed $s, t, s \le t, \phi_{s,t}(., \omega)$ is a
  measurable map from $\mathbb{R}^d$ into $\mathbb{R}^d$. Let $M$
  denote the totality of measurable maps from $\mathbb{R}^d$ into
  $\mathbb{R}^d$. Then $\phi_{s,t}$ can be regarded as an M - valued
  process. 

  \begin{definition}\label{c1:def1.1.1} % def 1.1.1
    $\phi_{s,t}$ is called a stochastic flow of measurable maps if 
    \begin{enumerate}[(i)]
    \item $\phi_{s,t}(x,.)$ is continuous in probability w.r.t $(s,t,x)$,
    \item $\phi_{s,s}$ = identity map a.s.for each $S$,
    \item $\phi_{s,u} = \phi_{t,u} o \phi_{s,t}$ a.s. for each $s < t < u$.
    \end{enumerate}
  \end{definition}  
  where\pageoriginale $o$ stands for the composition of maps. By a stochastic flow
  we will always mean a stochastic flow of measurable maps. Let $0 \le
  t_0 < t_1 < \ldots < t_n \le T$, $x_i \in \mathbb{R}^d, 0 \le
  i \le n-1$ be arbitrary. If $\phi_{t_i,t_{i+1}}(x_i), i = 0, 1,
  \ldots,  n-1$ are independent random variables for any such $\{
  t_i, x_i \}$, then $\phi_{s,t}$ is called a stochastic flow with
  independent increments. Further, if $\phi_{s,t}(x, \omega)$ is
  continuous in $t$ a.s. for each $s,x$ then $\phi_{s,t}$ is called a
  Brownian flow (of measurable maps). 
  
  Let $\phi_{s,t}$ be a stochastic flow with independent
  increments. Let $x^{(N)} = (x_1, x_2, \ldots,  x_N) \in
  \mathbb{R}^{Nd}$ where each $x_i \in \mathbb{R}d$. Set
  $\phi_{s,t}(x^{(N)}) = (\phi_{s,t}(x_1),\break \phi_{s,t}(x_2), \ldots,
  \phi_{s,t}(x_N))$. For fixed $s, x^{(N)}, \phi_{s,t}(x^{(N)})$ is an
  $\mathbb{R}^{Nd}$ valued process. We claim that it is a Markov
  process with transition probability 
  \begin{equation*}
    P^{(N)}_{s,t} (\underline{x}^{(N)}, E) = P
    (\phi_{s,t}(\underline{x}^{(N)}) \in E). \tag{1.1.1}\label{c1:eq1.1.1} 
  \end{equation*}  
  Indeed, let
  \begin{equation*}
    T^{(N)}_{s,t} f (\underline{x}^{(N)}) = \int f(
    \underline{y}^{(N)}) P^{(N)}_{s,t} (\underline{x}^{(N)},
    d\underline{y}^{(N)}) \tag{1.1.2}\label{c1:eq1.1.2} 
  \end{equation*}  
  where $f: \mathbb{R}^{Nd} \rightarrow \mathbb{R}$ is a bounded
  measurable map. Let $F_{s,t} = \sigma (\phi_{u,v}(x):  s \le u \le v
  \le t, x \in \mathbb{R}^d)$. Note that $\phi_{s,u}
  = \phi_{t,u} o \phi_{s,t} $ and the independent increment property of
  $\phi_{s,t}$ implies that $\phi_{t,u}$ is independent of
  $F_{s,t}$. Now  
  \begin{align*}
    E\left[f\left(\phi_{s,u}(\underline{x}^{(N)}) |F_{s,t}\right)\right] & =
    E\left[f\left(\phi_{t,u}(\underline{y}^{(N)})\right)\right]_{y^{(N)}}
    = \phi_{s,t}(\underline{x}^{(N)}) \\ 
    & = T^{(N)}_{t,u} f(\phi_{s,t}(\underline{x}^{(N)}).
  \end{align*}  
  
  From\pageoriginale the above property, we see that 
  \begin{equation*}
    T^{(N)}_{s,t} f = T^{(N)}_{s,t} 
    o T^{(N)}_{t,u} f. \tag{1.1.3}\label{c1:eq1.1.3}
  \end{equation*}  
  
  
  If $f$ is bounded and continuous, then $T^{(N)}_{s,t}
  f((\underline{x}^{(N)})$ is a continuous function of $s,t,
  (\underline{x}^{(N)}) $. 

 \setcounter{remark}{1}
 \begin{remark}\label{c1:rem1.1.2} %rem 1.1.2
   The family $\left\{ P^{(N)}_{s,t} (\underline{x}^{(N)},. )\right\}_{N=1,2,
     \ldots}$ defined by \eqref{c1:eq1.1.1} of transition probabilities is
   consistent in the following sense. Suppose $M$ and $N$ are two
   positive integers and $N > M$. Let $1 \le i_1 < i_2 < \cdots \le N$
   be positive integers, $\{x_{i_l} \cdots,  x_{i_M} \}$ a subset of
   $\{x_1, \ldots,  x_N \}$ and $E_1, \ldots,  E_N$ Bo rel sets in
   $\mathbb{R}^d$ such that $E_k = \mathbb{R}^d$ if $K^M \notin \{i_1,
   \ldots,  i_M \}$. Then  
   \begin{equation*}
     P^{(N)}_{s,t} (x_1, \ldots,  x_N, E_1  \, x \ldots x E_N) =
     P^{(M)}_{s,t} (x_{i_1}, \ldots,  x_{i_M}, E_{i_1} x \ldots
     x E_{i_M}). \tag{1.1.4}\label{c1:eq1.1.4} 
   \end{equation*}
 \end{remark} 

\setcounter{proposition}{2} 
 \begin{proposition}\label{c1:prop1.1.3} % pro 1.1.3
   Let $\left\{ P^{(N)}_{s,t} (\underline{x}^{(N)}, .  ) N = 1,2,  \ldots
   \right\}$ be a family of transition probabilities satisfying the
   consistency condition \eqref{c1:eq1.1.4}. Assume that the corresponding
   $T^{(N)}_{s,t} f(\underline{x}^{(N)} )$ is continuous in $(s,t,
   \underline{x}^{(N)})$ for any bounded continuous function $f$. Then
   there is a stochastic flow of measurable maps with independent
   increment whose N-point process has the transition probability
   $\left\{P^{(N)}_{s,t} (\underline{x}^{(N)},. \right)\}$. 
 \end{proposition} 


 \begin{proof}
   Let $0 \le t_1 < t_2 < \cdots < t_n \le T$. Then by Kolmogorov
   consistency theorem, there exist $n$ independent random fields
   $\xi_i,  i = 1,2, \cdots n$ s.t. the law of $(\xi_i (x_1)), \ldots,
   \xi_i (x_N))$ coincides with $P^{(N)}_{t_i, t_{i+1}}
   (\underline{x}^{(N)},.)$ for any $\underline{x}^{(N)} \in
   \mathbb{R}^{Nd}$.  Since $T^{(1)}_{s,t} f(x)$ is continuous $w.r.t$ 
   $x$ for all bounded continuous $f$, we can pick a version of $\{
   \xi_i (x)\}$ s.t $x \rightarrow \xi_i (x) $ is measurable. For $t_i
   \le t_j$, define  
   \begin{equation*}
     \xi_{t_i., t_j} =
     \begin{cases}
       \text{identity if}~~ i = j \\ 
       \xi_{j-1}o \xi_{j-2}o \cdots  o\xi_{i} ~\text{if}~ i < j. 
     \end{cases}
   \end{equation*}

   Denote the law of $\xi_{t_i, t_j}, t_i \le t_j$ by $\{Q_{t_1\cdots,
   t_n}$. Then $\{Q_{t_1, \ldots, t_n} (x)\}$ is a consistent family
   of probability measures as the parameters vary over $0 \le t_1 <
   t_2 < \cdots < t_n \le T, x \in \mathbb{R}^d$.\pageoriginale Therefore,
   by Kolmogorov consistency theorem, there exists a random field
   $\phi_{s,t}(x, \omega)$ such that the joint law of $\{ \phi_{t_i,
     t_j}(x_k),  1 \le i \le j \le n, k=1, \ldots,  N\}$ coincides
   with $\{ \xi_{t_i,  t_j}(x_k),  1 \le i \le j \le n, k=1,2, \ldots,
   N\}$. Again the continuity of $T^{(1)}_{s,t}f(x)$ $w.r.t. s,t,x$
   allows us to pick a version such the map $s,t,x \rightarrow
   \phi_{s,t}(x, \omega)$ is measurable a.s This $\phi_{s,t}(x,
   \omega)$ clearly satisfies (i) and (ii) of Defn. $1.1.1$ (iii)
   being an easy consequence of \eqref{c1:eq1.1.3} above. 
   Thus it is the required  stochastic flow. 
\end{proof} 

\setcounter{remark}{3}
\begin{remark}\label{c1:rem1.1.4} % 1.1.4
   In view of the above propositions there is a one to one
   correspondence between stochastic flows with independent increments
   and a consistent family of transition probabilities whose
   corresponding family of semigroups satisfies a certain continuity
   criterion. 
\end{remark}

\section[Local Characteristics. Generator of N-Point Motion]
{Local Characteristics. Generator of N-Point\hfill\break Motion}\label{chap1:sec1.2}
  
Let $\phi_{s,t}(s, \omega)$ be a Brownian flow. We make the following
assumptions: 	 


\begin{Assumption}\label{c1:asm1} % assum 1
  $\phi_{s,t}(x,. )$ is square integrable and the following limits exist:  
\begin{enumerate}[(i)]
\item $\lim\limits_{h \rightarrow 0} \dfrac{1}{h} E [\phi_{t,t+h} (x)-x]$

  Denote the above limit by $b(x,t)$. Then $b(x,t) = (b^1(x,t), \ldots,\break
  b^d(x,t))$ is an $\mathbb{R}^d$ vector. 

\item $\lim\limits_{h \rightarrow 0} \dfrac{1}{h} E [(\phi_{t,t+h}
  (x)-x) (\phi_{t,t+h} (y)-y)^*]$ 

  where $x^*$ stands for the transpose of $x \in
  \mathbb{R}^d$. Denote the above limit by $a(x,y,t)$. Then $a(x,y,t)
  = (a_{ij}(x,y,t))$ is a $d \times d$ matrix. 
\end{enumerate}

  The pair ($a,b$) is called the local characteristics of the flow
  $\phi_{s,t}$. Clearly the matrix $a(x,y,t) = (a_{ij}(x,y,t))$
  satisfies the following properties:  
\begin{enumerate}[(i)]
\item Symmetry:\pageoriginale  $a_{ij}(x,y,t) = a_{ij}(y,x,t)$ for any $x, y,t$

\item Nonnegative definiteness:  $\sum\limits_{i,j,p,q}
  a_{ij}(x_p,x_q, t) \xi^i_p xj^i_q \ge 0$ for any $(x_1$, $\ldots,  x_N)$
  and $\xi_p = (\xi^1_p, \ldots \xi^d_p), \, p = 1,2, \ldots, N$. 
\end{enumerate}  
\end{Assumption}

\begin{Assumption}\label{c1:asm2} % assum 2
  There exists a constant $K$ independent of $x,s,t$ such that 
  \begin{align*}
    |E[\phi_{s,t}(x) & - x] | \le K (1+ |x|) (t-s),  \tag{1.2.1}\label{c1:eq1.2.1}\\
    |E[\phi_{s,t}(x) & - x)(\phi_{s,t}(y) - y)^* ] | \le K (1+|x|)
    |(1+|y|) (t-s).\tag{1.2.2}\label{c1:eq1.2.2} 
  \end{align*}
\end{Assumption}

\begin{remark}\label{c1:rem1.2.1} % rem 1.2.1
  Assumption $2$ is technical in nature but it is not very
  restrictive. It will naturally be satisfied in most of the
  interesting cases.  
\end{remark}

\begin{remark}\label{c1:rem1.2.2} % 1.2.2
  It follows from $(A1)$ and $(A2)$ that
  \begin{align*}
    |b(x,t)| & \le K(1+|x|), \tag{1.2.3}\label{c1:eq1.2.3}\\
    |a(x,y,t)| & \le K(1+|x|)(1+|y|). \tag{1.2.4}\label{c1:eq1.2.4}
  \end{align*}
\end{remark}

Note that the first norm is a usual vector norm and the second one is
a matrix norm. 

Let $M_{s,t}(x)$ be defined as follows: 
\begin{equation*}
  M_{s,t}(x) = \phi_{s,t}(x) - x - 
\int^t_s b(\phi_{s,r}(x),r) dr. \tag{1.2.5}\label{c1:eq1.2.5}
\end{equation*}

\setcounter{Lemma}{2}
\begin{Lemma}\label{c1:lem1.2.3} % lem 1.2.3
  For each $s,x, M_{s,t}(x), t \in [s,T]$ is a continuous
  $L^2$-martingale and  
  \begin{equation*}
    < M^i_{s,t}(x), M^j_{s,t}(y) > = \int^t_s a_{ij}(\phi_{s,r}(x),
    \phi_{s,r}(y),r)dr \tag{1.2.6}\label{c1:eq1.2.6} 
  \end{equation*} 
  where $<.,.>$ stand for quadratic variation process.
\end{Lemma}

\begin{proof}
  Set 
  \begin{equation*}
    m_{s,t}(x) = E[\phi_{s,t}(x)]. 
\tag{1.2.7}\label{c1:eq1.2.7}
  \end{equation*}
\end{proof}

Then
\begin{align*}
  \frac{\partial}{\partial t} m_{s,t}(x) & = \lim_{h \rightarrow 0}
  \frac{1}{h}  [(m_{s,t+h} (x)m_{s,t}(x)] \\ 
  & = \lim_{h \rightarrow 0} \frac{1}{h} E [(m_{t,t+h}(\phi_{s,t}(x))
    -  \phi_{s,t}(x))]. \tag{1.2.8}\label{c1:eq1.2.8} 
\end{align*}\pageoriginale

Now 
$$
\frac{1}{h}|m_{t,t+h}(\phi_{s,t}(x)) - \phi_{s,t}(x)| \le K (1 +
|\phi_{s,t}(x)|) 
$$
Since $ K (1 + |\phi_{s,t}(x)|)$ is integrable, we can change the
order of lim and $E$ in~\eqref{c1:eq1.2.8}. Hence we get 
$$
\frac{\partial}{\partial t}|m_{s,t}(x) = E [b(\phi_{s,t}(x), t)]
\text{for all} t \ge s. 
$$

Therefore
\begin{equation*}
m_{s,t}(x) - x = \int^t_s E [b(\phi_{s,r}(x),r)]dr 
\tag{1.2.9}\label{c1:eq1.2.9}
\end{equation*}

Hence
$$
E [M_{s,t}(x)] = 0.
$$

Note that for $s < t < u$,
$$
M_{s,u}(x) = M_{s,t}(x) + M_{t, u}(x) (\phi_{s,t}(x)).
$$

Therefore
\begin{align*}
  E[M_{s,u}(x) | F_{s,t}] & = M_{s,t}(x) + 
E[M_{t,u}(y)]_{y = \phi_{s,t}}(x) \\
  & = M_{s,t}(x)
\end{align*}
This proves the first assertion. We will now establish \eqref{c1:eq1.2.6}. Define
\begin{equation*}
  V_{s,t}(x,y) = E[M_{s,t}(x) (M_{s,t}(y))^\ast]. 
\tag{1.2.10}\label{c1:eq1.2.10}
\end{equation*}

Then\pageoriginale
$$
V_{s,t+h}(x,y) - V_{s,t}(x,y) = E[(M_{s,t+h}(x) - M_{s,t}(x))
  (M_{s,t+h}(y) - M_{s,t}(y))^*]. 
$$

Therefore 
\begin{align*}
  \frac{1}{h} [V_{s,t+h}(x,y) - V_{s,t}(x,y)] & = \frac{1}{h}
  E[(M_{t,t+h}(\phi_{s,t}(x)) (M_{t,t+h}(\phi_{s,t}(y)))^* ] \\ 
  & = \frac{1}{h}
  E[(V_{t,t+h}(\phi_{s,t}(x),\phi_{s,t}(y))]. 
\tag{1.2.11}\label{c1:eq1.2.11} 
\end{align*}

Letting $h \rightarrow 0$ in \eqref{c1:eq1.2.11}, we get
$$
\frac{\partial}{\partial t} V_{s,t}(x,y) = E[a(\phi_{s,t}(x),\phi_{s,t}(y),t) ].
$$
 
Therefore
\begin{equation*}
  V_{s,t}(x,y) = \int^t_s E[a(\phi_{s,r}(x),\phi_{s,r}(y),r)]
  dr. \tag{1.2.12}\label{c1:eq1.2.12} 
\end{equation*}

Set
\begin{equation*}
  N_{s,t}(x,y) = M_{s,t}(x) M_{s,t}(y)^* - \int^t_s
  a(\phi_{s,r}(x),\phi_{s,r}(y),r) \, 
dr. \tag{1.2.13}\label{c1:eq1.2.13} 
\end{equation*}	

Then \eqref{c1:eq1.2.12} implies
$$
E[N_{s,t}(x,y)] = 0.
$$

Let $s < t < u$. Then a simple computation yields
\begin{align*}
N_{s,u}(x,y) & = N_{s,t}(x,y) + N_{t,u}(\phi_{s,t}(x), \phi_{s,t}(y)) \\
& + M_{s,t}(x) M_{t,u} (\phi_{s,t}(y)) + M_{s,t}(y)M_{t,u}(\phi_{s,t}(x)).
\end{align*}

Hence
$$
E[N_{s,u}(x,y) |F_{s,t}] = N_{s,t}(x,y).
$$

Thus $N_{s,t}(x,y)$ is a martingale. This completes the proof of the
lemma.

\setcounter{remark}{3}
\begin{remark}\pageoriginale % rem 1.2.4
  $b(x,t)$ and $a(x,y,t)$ are often referred to as the infinitesimal
  mean and covariance respectively of the flow.
\end{remark}

Let $x_k \in \mathbb{R}^d, k = 1,2, \ldots,  N, x_k = (x^1_k,
\ldots,  x^d_k)$. We define a differential operator $L^{(N)}_t$ as
follows:  
\begin{multline*}
  L^{(N)}_t f(\underline{x}^{(N)}) = \frac{1}{2} \sum_{i,j,k,\ell}
  a_{ij}(x_k,  x_\ell, t) \frac{\partial^2 f}{\partial x^i_k \partial
    x^j_\ell} (\underline{x}^{(N)})\\ 
  + \sum_{i,k} b^i(x_k, t) \frac{\partial f}{\partial x^i_k}
  (\underline{x}^{(N)}). \tag{1.2.14}\label{c1:eq1.2.14} 
\end{multline*}
$L^{(N)}_t$ is elliptic operator which may be degenerate. It is the
infinitesimal generator of $T^{(N)}_{s,t}$, which is the semigroup of
N-point process. This fact will follow from the following theorem. 
\setcounter{theorem}{4}
\begin{theorem}\label{c1:thm1.2.5} % them 1.2.5
  Let $f$ be a $C^2$ function on $\mathbb{R}^{Nd}$ such that $f$ and its
  derivatives have polynomial growth. Then the following holds for any
  $s,t, \underline{x}^{(N)}$:   
  \begin{equation*}
    T^{(N)}_{s,t} f(\underline{x}^{(N)}) - f(\underline{x}^{(N)}) =
    \int^t_s T^{(N)}_{s,r} L^{(N)}_{r}
    f(\underline{x}^{(N)})dr. \tag{1.2.15}\label{c1:eq1.2.15} 
  \end{equation*}
\end{theorem}

In particular
\begin{equation*}
  \lim_{h \rightarrow 0} \frac{1}{h} \left(T^{(N)}_{t,t+h} f- f\right) =
  L^{(N)}_{t} f. \tag{1.2.16}\label{c1:eq1.2.16}   
\end{equation*}

\begin{proof}
  The proof is essentially based on Ito's formula. It follows from
  Lemma~\ref{c1:lem1.2.3} that for fixed ($s,x$), $\phi_{s,t}(x)$ is a
  continuous semimartingale with the following decomposition:  
  \begin{equation*}
    \phi_{s,t}(x) = x+M_{s,t}(x) + \int^t_s 
b(\phi_{s,r}(x),r)dr. \tag{1.2.17}\label{c1:eq1.2.17}
  \end{equation*}
\end{proof}

By Ito's formula, we have 
\begin{align*}
  f(\phi_{s,t}(\underline{x}^{(N)} )) & - f(\underline{x}^{(N)}) -
  \int^t_s L^{(N)}_r f(\phi_{s,r}(\underline{x}^{(N)} )) \, dr \\ 
  & = \sum_{k,i} \int^t_s \frac{\partial f}{\partial x^i_k}
  (\phi_{s,r}(\underline{x}^{(N)} )) dM^i_{s,r} (x_k). 
\tag{1.2.18}\label{c1:eq1.2.18} 
\end{align*}

Claim:\pageoriginale  $\phi_{s,t}(x)$ has finite moments of all orders. Granting the
claim, the above is a zero mean martingale. Then taking expectation,
we get 
$$
T^{(N)}_{s,t}  f(\underline{x}^{(N)}) -f(\underline{x}^{(N)})  - 
\int^t_s T^{(N)}_{s,r} L^{(N)}_{s,r} f(\underline{x}^{(N)})dr = 0, 
$$
which proves the theorem. So it remains to substantiate the claim
which we do in the following lemma. 


\setcounter{Lemma}{5}
\begin{Lemma} % lem 1.2.6
  $\phi_{s,t}(x)$ has finite moments of any order.  Further, for any
  $p$ real and $\varepsilon > 0$, there is a positive constant $C =
  C(p, \varepsilon)$ such that  
  \begin{equation*} 
    E(\varepsilon + |\phi_{s,t}(x)|^2 )^p \le C(\varepsilon + |x|^2)^p
    \tag{1.2.19}\label{c1:eq1.2.19} 
  \end{equation*}
  for any $s,t,x$.
\end{Lemma}


\noindent\textit{Proof. }
Define $g(x) = \varepsilon + |x|^2$ and $f(x) = g(x)^P$. Let $L_r =
L^{(1)}_r$. We shall apply Ito's formula to the $1-$ point process. We
have  
\begin{multline*}
  L_r f = 2pg(x)^{p-1} \sum_i b^i(x,t)x_i + pg(x)^{p-1}\sum_i
  a_{ii}(x,x,t)\\ 
  + 2p(p-1)g(x)^{p-2} \sum_{i,j} a_{ij}(x,x,t)x_i x_j. \tag*{$\Box$}
\end{multline*}

Using the estimates \eqref{c1:eq1.2.3} and \eqref{c1:eq1.2.4}, 
we have $|L_r f(x)| \le
C'f(x)$, where $C'$ is a constant independent of $x,r$. Now for any
$n,$ define the stopping time 
\begin{equation*}
  \tau_n = \tau_n(x,s) =
  \begin{cases}
    \inf \{t > s: |\phi_{s,t}(x)| \ge n \} \\ 
    \infty ~\text{if the above set is empty}.
\end{cases}
\end{equation*}

Let the stopped process $\phi_{s,t \Lambda \tau_n}$ be denoted by
$\tilde{\phi}_{s,t}$. Then $f(\tilde{\phi}_{s,t}(x)) - \int^t_s L_r f
\tilde{\phi}_{s,r}dr$ is a martingale. Therefore 
\begin{align*}
  E [f \tilde{(\phi}_{s,t}(x))] & = f(x) + \int^t_S E[L_r f
    (\tilde{\phi}_{s,r}(x))] dr \\ 
  & \le f(x) + C' \int^t_S E[f (\tilde{\phi}_{s,r}(x))] dr.
\end{align*}\pageoriginale

Hence by Gronwall's inequality, we have
$$
E[f (\tilde{\phi}_{s,t}(x))] \le f(x) e^{C' (t-s)}.
$$

In the above inequality, the right hand side is independent of $n$; so
letting $n \uparrow \infty$, we get 
$$
E[f ({\phi}_{s,t}(x))] \le f(x) e^{C' (t-s)}.
$$

Thus 
$$
E[\varepsilon +| \phi_{s,t}(x) |^2 ]^p \le C(\varepsilon + |x|^2)^P.
$$

\setcounter{remark}{6}
\begin{remark} % rem 1.27
(i) For each positive integer $k, 1 \le k \le N$, define
  \begin{equation*}
    L^k_t = \frac{1}{2} \sum_{i,j} a_{ij} (x_k,  x_k,t) \frac{\partial
      ^2}{\partial x^i_k \partial x^j_k} + \sum_i b^i(x_k,t)
    \frac{\partial}{\partial x^i_k}. \tag{1.2.20}\label{c1:eq1.2.20} 
  \end{equation*}
  $L^k_t$ is the generator of $1-$point process. Then the following is
  easily verified:  
  \begin{equation*} 
    L^{(N)}_t f = \sum^N_{k=1} L^{(N)}_t f + \sum_{k \neq \ell}
    \frac{1}{2} \sum_{i,j} a_{ij}(x_k,  x_\ell, t) \frac{\partial
      ^2}{\partial x^i_k \partial x^j_\ell}. \tag{1.2.21}\label{c1:eq1.2.21}  
\end{equation*}

The second term in \eqref{c1:eq1.2.21} could be regarded as the interaction
between $\phi_{s,t}(x_k)$ and $\phi_{s,t}(x_\ell)$. Thus the generator
of the $N-$point is the sum of the generators of the $1-$ point
motions together with the cross - interaction. If no interaction exists
then the second term will cease to exist and $\phi_{s,t}(x_1),
\phi_{s,t}(x_2), \ldots$ would move independently. 

(ii) The family of operators $\{ L^{(N)}_t,  N = 1,2, \ldots \}$ is
consistent in the following sense:\pageoriginale  Let $M, N$ be two positive
integers and $N > M$. Let $1\le i_i <i_2 < \ldots< i_M \le N,
\{x_{1_1},\ldots, x_{i_M}\}$ a subset of $\{x_1, \ldots,
x_N\}$. Let $f$ be a function of $\mathbb{R}^{Nd}$ depending only on
$(x_{i_1},\ldots, x_{i_M})$. Then  
\begin{equation*}
  L_t^{(N)} f(x_1, \ldots,x_N) = L_t^{(M)} f(x_{1{_1}},\ldots
  x_{i{_M}}), \tag{1.2.22}\label{c1:1.2.22} 
\end{equation*}
which is an obvious consequence of the Remark~\ref{c1:rem1.1.2}.
We conclude this section by showing the existence of a Brownian flow
with given local characteristics.  
\end{remark}


\setcounter{theorem}{7}
\begin{theorem}\label{c1:thm1.2.8}%the 1.2.8
  Let $b(x,t)$ be an $\mathbb{R}^d$-valued bounded continuous function
  and $a (x,y,t)$ and $d \times d$-matrix valued bounded continuous
  function which is nonnegative definite and symmetric
  $(a_{ij}(x,y,t))=(a_{ij}(y,x,t))$. Suppose $a$ and $b$ are twice
  spatially differentiable and their derivatives are bounded. Then
  there is a Brownian flow with local characteristics
  $(a,b)$. Further, the law of the flow unique.  
\end{theorem}

\noindent \textit{Proof.}
  We define $L^{(N)}_t$ as before. By Oleinik's theorem
  (Stroock - Varadhan~\cite{31}) there exists a unique $T^{(N)}_{s, t}$ such
  that 
  \begin{equation*}
  T^{(N)}_{s,t} f(\underline{x}^{(N)})= f(\underline{x}^{(N)}) +
  \int\limits_s^t T^{(N)}_{s,t} 
L^{(N)}_r  f(\underline{x}^{(N)})dr. \tag*{$\Box$}
  \end{equation*}

  The consistency of the family $\left\{ T^{(N)}_{s,t}, N =
  1,2,\cdots\right\}$ follows from that of $\{ L^{(N)}_t, N =
  1,2,\dots\}$. By virtue of Proposition~\ref{c1:prop1.1.3} there exists a
  stochastic flow with independent increments $\phi_{s,t}^{(x)}$ whose
  N-point process has the same semigroup as $T^{(N)}_{s,t}$. Since
  $T^{(N)}_{s,t}$ defines a diffusion semigroup, it follows that
  $\phi_{s,t}(x)$ is continuous in $t$ a.s for each $s,x$. Hence
  $\phi _{s,t}$ is a Brownian flow. 

\setcounter{remark}{8}
\begin{remark}\label{c1:rem1.2.9}%rema 1.2.9
  Under the condition of the above theorem, the law of the Brownian
  flow is determined by the $2-$points motions. Indeed, let  $\left\{
  T^{(N)}_{s,t}, N = 1,2,\cdots\right\}$ and\pageoriginale $\left\{ \tilde{T}^{(N)}_{s,t}, N =
  1,2,\cdots\right\}$ be consistent families of semigroups with generators
  $\left\{ L^{(N)}_t, N = 1,2,\dots\right\}$ and $\left\{ \tilde{L}^{(N)}_t, N =
  1,2,\dots\right\}$ respectively. If $T^{(2)}_{s,t} = \tilde{T}^{(2)}_{s,t}
  $ then $T^{(N)}_{s,t}$, $\tilde{T}^{(N)}_{s,t} $ for any $N \ge
  2$. This follows from the above theorem because the generator
  depends only on the local characteristics and $L^{(2)}_t =
  \tilde{L}^{(2)}_t$ implies  $L^{(N)}_t = \tilde{L}^{(N)}_t, N= 1,2,
  \dots$ 
\end{remark}

\section{Brownian Flow of Homeomorphisms}\label{chap1:sec1.3}%sec 1.3

\begin{definition}\label{c1:def1.3.1}%defin 1.3.1
  Let $\phi_{s,t} (x)$ be a Braownian flow of measurable maps. Then
  $\phi_{s,t}{(x)}$ is said to be a Brownian flow of homeomorphisms if 
\end{definition}

\begin{enumerate}[(i)]
\item $\phi_{s,t} {(x)}$ is continuous in $(s,t,x)$ a.s

\item $\phi_{s,t} (., \omega):\mathbb{R}^d \to \mathbb{R}^d$ is a
  homeomorphism for any $s<t$ a.s. Further if 

\item $\phi_{s,t} (., \omega):\mathbb{R}^d \to \mathbb{R}^d$  is a
  $C^k$-diffeomorphism for any $s < t$ a.s., then $\phi_{s,t}$ is
  called a Brownian flow of $C^k$-diffeomorphisms. 
\end{enumerate}

Apart from assumptions~\ref{c1:asm1} and \ref{c1:asm2}, we will impose a Lipschitz
condition on the local characteristics $(a, b)$. 


%\setcounter{assumption}{}
\begin{Assumption}\label{c1:asm3}%assum 3
  There exists a constant $L$ such that 
  \begin{align*}
      &|b(x,t) - b(y,t) \le L|x-y| \tag{1.3.1},\label{c1:eq1.3.1}\\
      &|a(x,x,t)-2a(x,y,t) + a(y,y,t)|\le L|x-y|.^2 \tag{1.3.2}\label{c1:eq1.3.2}
    \end{align*}
\end{Assumption}

We will see in the next theorem that Assumption~\ref{c1:asm3} makes a Brownian
flow a flow of homeomorphisms. 

\setcounter{theorem}{1}
\begin{theorem}\label{c1:thm1.3.2}%thm 1.3.2
  Let $\phi_{s,t}$ be a Brownian flow satisfying $(A1) \sim(A3)$. Then
  it is a Brownian flow of homeomorphisms. More precisely there exists a
  version of $\phi_{s,t}$ which is a Brownian flow of homeomorphisms. 
\end{theorem}

The\pageoriginale proof of the above theorem is based on several estimates which we
will derive in the following lemmas. In these lemmas $(A1), (A2),
(A3)$ will be assumed. 


\setcounter{Lemma}{2}
\begin{Lemma}\label{c1:lem1.3.3}%lem 1.3.3
  For  any real $p$, there is a positive constant $C=C(p)$ such that
  for any $\varepsilon >0,x,y,\in \mathbb{R}^d$ 
  \begin{equation*} 
    E(\varepsilon+|\phi_{s,t} (x) - \phi_{s,t} (y)|^2)^p \le C
    (\varepsilon + |x-y|^2)^P. \tag{1.3.3}\label{c1:eq1.3.3} 
\end{equation*}
\end{Lemma}

\noindent \textit{Proof.}
Set $g(x,y) = \varepsilon +|x-y|^2$ and $f(x,y) =
g(x,y)^p$. A simple computation yields 
\begin{multline*}
  L^{(2)}_t f(x,y) = 2pg(x,y)^{p-1} \left\{\sum_i (b^i (x,t) -b^i(y, t))
  (x_i - y_i) \right\}\\  
  + pg(x,y)^{p-2} \left\{ \sum_{i,j} (g(x,y)) \delta_{ij} + 2(p-1)
  (x_i-y_i) (x_j-y_j))\right.\\ 
  \left.\vphantom{\sum_{i,j}} \times (a_{ij} (x,x,t) - 2a_{ij} (x,y,t) + a_{ij}
  (y,y,t))\right\}.\tag*{$\Box$} 
\end{multline*}

Using the estimates \eqref{c1:eq1.2.3}, \eqref{c1:eq1.2.4} and 
\eqref{c1:eq1.3.1}, \eqref{c1:eq1.3.2}, we can
find a constsnt $C'$ such that  
$$
|L^{(2)}_t f (x,y)| \le C' f(x,y).
$$


Now, by Ito's formula,
\begin{align*}
  E[f(\phi_{s,t} (x),\phi_{s,t} (y))] &= f(x,y) + \int\limits_s^t
  E[L^{(2)}_rf(\phi_{s,t} (x),\phi_{s,t} (y))]dr \\ 
  & \le f(x,y) + C' \int\limits_s^t E[f(\phi_{s,r} (x),\phi_{s,r} (y))]dr. 
\end{align*}

By Gronwall's inequality, we have
$$
 E[f(\phi_{s,t} (x),\phi_{s,t} (y))] \le e^{C'(t-s)}_{f(x,y)}. 
$$

Hence there exists a constant $C$ such that
$$
E(\varepsilon +|\phi_{s,t} (x) - \phi_{s,t} (y)|^2)^p \le C
(\varepsilon + |x-y|^2)^p. 
$$

\begin{Lemma}\label{c1:lem1.3.4}\pageoriginale%lemm 1.3.4
  For any positive integer $p$ there exists a constant $C=C(p)$ such
  that for any $x_o \in \mathbb{R}^d$, we have 
\begin{equation*} 
E |\phi_{s,t} ( x_o ) - x_o|^{2p} \le C|t-s|^p 
(1+|x_o|)^{2p}. \tag{1.3.4}\label{c1:eq1.3.4}
\end{equation*}
\end{Lemma}

\noindent\textit{Proof.}
Fix a point $x_o \in \mathbb{R}^d$ and set $g (x)=
|x-x_o|^2,f(x)=g(x)^p$. Then as before, using
\eqref{c1:eq1.2.3}, \eqref{c1:eq1.2.4}, 
\eqref{c1:eq1.3.1} and \eqref{c1:eq1.3.2} we can find constants $C_1,C_2,C_3$
such that  
\begin{equation*}
|L_t f(x)| \le C_1 f(x) + C_2 g(x)^{p- 1/2} (1+|x_o|) + C_3 g(x)^{p-1}
(1+|x_o|)^2. \tag*{$\Box$}
\end{equation*}

Therefore using Ito's formula, we have
\begin{align*}
E[f(\phi_{s,t} (x_o))] & \le f(x_o) + C_1 \int\limits_s^t E[f
(\phi_{s,r}(x_o))]dr\\ 
&\quad + C_2 \int\limits_s^t E[g(\phi_{s,t} (x_o))^{p-1/2}]dr\, (1+|x_o|)\\
&\qquad + C_3 \int\limits_s^t E[g(\phi_{s,t} (x_o))^{p-1}]dr \,(1+|x_o|)^2.
\end{align*}

Applying Gronwall's inequality, we have
\begin{multline*}
  E[f(\phi_{s,t} (x_o))] \le C_4 (1+ |x_o|) \int\limits_s^t
  E[g(\phi_{s,r} (x_o))^{p-1/2}]dr + C_5 (1+|x_o|)^2\\ 
  \int\limits_s^t
  E[g(\phi_{s,r} (x_o))^{p-1}]dr. \tag{1.3.5}\label{c1:eq1.3.5} 
\end{multline*}

Now for $p = \dfrac{1}{2},1$, the estimate \eqref{c1:eq1.3.4} follows from
$(A2)$. Using~\eqref{c1:eq1.3.5}, the same estimate follows for $p =1
\dfrac{1}{2}$. Again using~\eqref{c1:eq1.3.5}, we get \eqref{c1:eq1.3.4} for
$p=2$. Proceeding inductively, we conclude the result for any $p$.  

\setcounter{remark}{4}
\begin{remark}\label{c1:rem1.3.5}%rema 1.3.5
  If $p$ is a positive integer there exists a constant $C = C(p)$ such
  that for any $x_o,y_o \in \mathbb{R}^d$ 
  \begin{equation*}
    E|\phi_{s,t} (x_o)-x_o-(\phi_{s,t} (y_o)-y_o)|^{2p} \le C
    |t-s|^p|x_o-y_o|^{2p}. \tag{1.3.6} \label{c1:eq1.3.6}
  \end{equation*}
\end{remark}

Set\pageoriginale $g (x,y) = |(x,x_o)-(y-y_o)|^2, f(x,y) = g(x,y)^p$. 
Then we can show that
$$
|L^{(2)}_t f|\le C_1 f + C_{2} g^{p -\frac{1}{2}} |x_o-y_o|+C_3g^{p-1}
|x_o-y_o|^2. 
$$

The rest to the proof is similar to that of lamma~\ref{c1:lem1.3.4}.

\setcounter{Lemma}{5}
\begin{Lemma}\label{c1:lem1.3.6}%lemm 1.3.6
  Let $p$ be a positive integer. Then there exists a constant $C=C(p)$
  such that 
  \begin{multline*}
    E|\phi_{s,t}{(x)}- \phi_{s',t'}{{(x')}}|^{2p}\\ 
    \le C \left\{|x-x'|^{2p} +
    (1+|x|+|x'|)^{2p} (|t-t'|^p + |s-s'|^p)\right\} \tag{1.3.7}\label{c1:eq1.3.7} 
  \end{multline*}
  holds for any $s \le t$, $s' \le t'$ and $x,x' \varepsilon
  \mathbb{R}^d$. 
\end{Lemma}

\begin{proof}
  We consider the case $s' \le s \le t \le t'$ only. The other case
  will follow similarly.  
\end{proof}

We split the proof in various steps
\begin{enumerate}[(a)]
\item $s = s'$  and $x = x'$.
  \begin{align*}
    E |\phi_{s,t}{{(x)}} - \phi_{s,t}{{(x)}}|^{2p} & = E
    |\phi_{s,t}{{(x)}} - \phi_{t,t'}, \phi_{s,t}{{(x))}}|^{2p}\\ 
    & = \int E [|y -\phi_{t,t'}{{(y)}}|^{2p}] P(\phi_{s,t}{{(x)}}
    \varepsilon dy) \\ 
    & \le C_1 |t-t'|^p \int (1+ |y|)^{2p} P(\phi_{s,t}{{(x)}}
    \varepsilon dy) \\
     & \tag*{[using Lemma 1.3.4]}\\ 
    & = C_1 |t-t'|^p E(1+\phi_{s,t}{^{s,t}}|)^{2p}\\
    & \le  C_2 |t-t'|^p (1+|x|)^{2p}
  \end{align*}

\item $s = s', x \neq x'$.
  \begin{align*}
    E|\phi_{s,t}{{(x)}} - \phi_{s,t}{{(x')}}|^{2p} & \le 2^{2p}\\ 
    & \left\{
    E|\phi_{s,t}{{(x)}} - \phi_{s,t'}{{(x')}}|^{2p} +
    E|\phi_{s,t'}{{(x)}} - \phi_{s,t'}{{(x')}}|^{2p}\right\}\\ 
    & \le C_3 \left\{ |t-t'|^p (1+|x|)^{2p} + |x-x'|^{2p}\right\}.\\
    & \tag*{[using lemma~\ref{c1:lem1.3.3}]}
\end{align*}

\item $E|\phi_{s,t}{{(x)}}-  \phi_{s',t'}{{(x')}}|^{2p} =
  E|\phi_{s,t}{{(x)}} - \phi_{s,t'}(\phi_{s',s}(x'))|^{2p}$ 
  \begin{align*}
    &= \int E|\phi_{s,t}{{(x)}}- \phi_{s,t'}{{(y)}}|^{2p}) P
    (\phi_{s', s}{{(x')}} \varepsilon dy)\\ 
    & \le C_3 \int \left\{ |t-t'|^p (1+|x|)^{2p} + |x-y|^{2p}\right\} P
    (\phi_{s',s}{{(x')}} \varepsilon dy)\\ 
    & = C_3 \left\{ |t-t'|^p (1+|x|)^{2p} + E|x-\phi_{s',s}{{x'}}|^{2p}\right\}\\
    & \le C_3 \left\{ |t-t'|^p (1+|x|)^{2p}\right\} + C_4 |x-x'|^{2p} + C_5
    |s-s'|^p (1+|x'|)^{2p}\\ 
    & \le C_6 \left\{ (|t-t'|^p + |s-s'|^p) (1+|x|+|x'|)^{2p} +
    |x-x'|^{2p}\right\}. 
\end{align*}\pageoriginale
\end{enumerate}

We shall now state without proof a criterion for the continuity of
random fields which is a generalization of the wellknown Kolmogorov's
criterion for the continuity of stochastic processes. 

\setcounter{theorem}{6}
\begin{theorem}[Kolmogorov -Totoki]\label{c1:thm1.3.7} % the 1.3.7
  Let $\{ x (\lambda), \lambda \in \Lambda\}$ be a random
  field where $\Lambda$ is bounded rectangular in
  $\mathbb{R}^n$. Suppose there exist positive constants $\alpha,
  \beta, K$ such that 
  \begin{equation*}
    E |X(\lambda) - X (\mu)|^\alpha \le 
K |x-y|^{n + \beta}, \tag{1.3.8}\label{c1:eq1.3.8}
  \end{equation*}
  then $\{ X(\lambda), \lambda \in, \Lambda\}$ has a
  continuous modification.  
\end{theorem}

For the proof of the above theorem, see Kunita~\cite{18}.

\setcounter{proofoftheorem}{1}
\begin{proofoftheorem}\label{c1:prfthm1.3.2}%the 1.3.2
  Let $p> 2(d+2)$. Then by Theorem~\ref{c1:thm1.3.7}, $\phi_{s,t}{{(x)}}$ has a
  continuous modification. Therefore $\phi_{s,t} (., \omega):
  \mathbb{R}^d \to \mathbb{R}^d$ is a continuous map for any $s < t$
  a.s. For negative $p$ we have  
  \begin{align*}
    E[|\phi_{s,t}{{(x)}} - \phi_{s,t}{{(x)}}|^{2p}] & \le C |x-y|^{2p}\\
    E[1+|\phi_{s,t}{{(x)}}|^{2p}] & \le C (1+|x|)^{2p}. 
  \end{align*}
  These\pageoriginale two will imply that $\phi_{s,t}(., \omega)$ is a
  homeomorphism. The proof is exactly the same as in Kunita \cite{18}. We
  omit the details. 
\end{proofoftheorem}


\section{Stochastic Flow of Diffeomorphisms}\label{chap1:sec1.4}%sec 1.4

In this section, we will see that if the local characteristics $(a,b)$
of a Brownian flow are smooth and their derivatives bounded then the
flow becomes a flow of diffeomorphisms. We will make this precise in
the next theorem. Before that, we will add a few words about
notations. For a multiindex $\alpha= (\alpha_1, \alpha_2, \ldots,
\alpha_d)$, where $\alpha_i \in \mathbb{N},i = 1,2, \ldots, d, 
|\alpha|=\alpha_1+ \alpha_2 + \cdots+ \alpha_d$ 
$$
D^\alpha = D^\alpha_x = (\frac{\partial}{\partial x_1})^{\alpha_1}
\dots \dots (\frac{\partial}{\partial x_d})^{\alpha_d}. 
$$

\begin{theorem}\label{c1:thm1.4.1}%theo 1.4.1
  Let $\phi_{s,t}$ be a Brownian flow with local characteristics
  $(a,b)$ satisfying $(A1),(A2),(A3)$. Assume further $a (x,y,t)$ and
  $b(x,t)$ are k-times continuously differentiable in $x$ and $y$ and
  $D^\alpha_x D^\beta_y a (x,y,t),  D^\alpha_x$ $b (x,t), |\alpha| \le
  k, |\beta| \le k$ are bounded. Then $\phi_{s,t}(.,\omega):
  \mathbb{R}^d \to \mathbb{R}^d$ is $C^{k-1}$ diffeomorphism for any
  $s < t$ a.s.  
\end{theorem}

We will prove the above theorem for $k=2$. For higher $k$, the proof
if similar. The proof is based on the following lemmas. 

\setcounter{Lemma}{1}
\begin{Lemma}\label{c1:lem1.4.2}%lemm 1.4.2
  Let $e$ be a unit vector in $\mathbb{R}, y (\neq 0)$ a real number. Set
  $$
  \eta _{s,t}{{(x,y)}} = \frac{1}{y} (\phi_{s,t} (x+ye) -\phi_{s,t}{{(x)}}).
  $$ 
\end{Lemma}

Then, for any positive integer $p$, there exists a constant $C = C
(p)$ such that 
\begin{multline*}
  E|\eta_{s,t}{{(x,y)}} -\eta_{s',t'}{{(x',y')}}|^{2p} \le C
  \left\{|x-x'|^{2p} + |y-y'|^{2p}\right.\\ 
  \left. \vphantom{|^{2p}} + (1+|x|+|x'|+|y|+|y'|)^{2p} \times
  (|t-t'|^p+|s-s'|^p)\right\}. \tag{1.4.1}\label{c1:eq1.4.1} 
\end{multline*}

\begin{Lemma}\label{c1:lem1.4.3}\pageoriginale%lemm 1.4.3
  For $(x_1,x_2,x_3,x_4) \in \mathbb{R}^{4d}$, set 
  $$
  g (x_1, x_2,x_3,x_4)= \frac{1}{y}(x_1 - x_2) - \frac{1}{y'}, (x_3 -x_4)
  $$
  where $y,y'$ are fixed nonzero real number. Set $f = |g|^{2p}$,
  where $p$ is a positive integer. Then there exist constants $C_i =
  C_i(p), \, i = 1,2$ such that 
  \begin{multline*}
    |L^{(4)}_t f(x_1,x_2,x_3,x_4)| \le C_1 f(x_1,x_2,x_3,x_4)\\
    + C_2(|x_1-x_3|+|x_2-x_4|)^{2p} \left(1+|\frac{1}{y'},
    (x_3-x_4)|^{2p}\right). \tag{1.4.2}\label{c1:eq1.4.2} 
  \end{multline*}
\end{Lemma}

\setcounter{proofofsublemma}{2}
\begin{proofofsublemma}\label{c1:prflem1.4.3}% pr 1.4.3
  We shall consider the case $d=1$ for simplicity. The time $t$ will
  be dropped from $a (x,y,t)$ and $b(x,t)$ since it is fixed. A simple
  computation yields 
\begin{align*}
  L^{(f)}_t & f(x_1,x_2,x_3, x_4)  = 2p \left\{ \frac{1}{y} (b (x_1) -
  b(x_2))- \frac{1}{y'},(b (x_3) - b(x_4))\right\}\\ 
  &\hspace{2cm} |g(x_1,x_2,x_3, x_4)|^{2p-1} x ~\text{sign}~ g(x_1,x_2,x_3, x_4)\\ 
 & + p(2p-1) \left[\frac{1}{y^2} \{ a (x_1,x_2) - 2a  (x_1,x_2) + a
    (x_2,x_2)\}\right] \\
 & - \frac{2}{yy'}, \{ a (x_1,x_3) -a(x_1,x_4)+a(x_2,x_3)-a(x_2,x_4) \}\\
 & +  \frac{2}{y'^2} \{ a (x_3,x_3) -2a(x_3,x_4)+a(x_4,x_4)\}] g
   (x_1,x_2,x_3, x_4)|^{2p-2}\\ 
  &  = I_1 + I_2, ~\text{say}~  \tag{1.4.3}\label{c1:eq1.4.3}
\end{align*}
\end{proofofsublemma}


Using\pageoriginale mean value theorem the first tern is written as 
\begin{align*}
  \frac{1}{2p}I_I & = \left\{ \left(\int\limits_0^1 b (x_1 +  \theta (x_2-x_1))
  d \theta\right) \frac{1}{y} (x_1-x_2)\right.\\
  &\qquad \left.-\left(\left(\int\limits_0^1 b'(x_3 + \theta
  (x_4-x_3)) d \theta\right) \frac{1}{y'} (x_3-x_4)\right)\right\} 
  \times  |g|^{2p-1} ~\text{sign}~ g\\
  & = \left(\int\limits_0^1 b'(x_1 + \theta (x_2-x_1)) d \theta\right)
  |g|^{2p} + \int\limits_0^1 \left\{ b'(x_1 + \theta
  (x_2-x_1))\right.\\ 
  &\qquad \left.-b'(x_3, \theta
  (x_4-x_3))\right\} d \theta x \frac{1}{y'} (x_3-x_4) |g|^{2p-1}
  ~\text{sign}~g.  
\end{align*}

Using mean value theorem once again, we can find positive constants
$C_3, C_4$ such that  
$$
|i_1| \le C_3 |g|^{2p} + C_4 (|x_1-x_3|+|x_2-x_4|)|\frac{1}{y'}
(x_3,x_4)||g|^{2p-1}. 
$$

Using the inequality $ab \le \dfrac{a^\alpha}{\alpha} +
\dfrac{b^\beta}{\beta}$ where $\alpha, \beta \ge 1, \alpha^{-1} +
\beta^{-1} = 1$, we get  
\begin{equation*}
  |I_1| \le C_5 |g|^{2p} + C_6 (|x_1-x_3|+|x_2-x_4|)^{2p}|\frac{1}{y'}
  (x_3,x_4)|^{2p}. \tag{1.4.4}\label{c1:lem1.4.4} 
\end{equation*}

We next estimate $I_2$. Note the relation
\begin{multline*}
  a(x_i,x_k) -a(x_i,x_m) +a(x_j,x_k)-a(x_j,x_m)\\
  = \int \int a''(x_i+\theta(a(x_j - x_i), x_k + \tau (x_m - x_k)) \theta
   \mathscr{O} d \tau . (x_i - x_j) (x_k - x_m). 
\end{multline*}
where $a'' (x,y)= \dfrac{\partial^2}{\partial x \partial y} a (x,y)$.

Set
\begin{align*}
\xi_{ik}{{(\theta, \tau)}} &= a'' ((x_i+\theta(x_j -x_i), x_k + \tau
(x_m -x_k)),\\ 
W_1 & =\frac{1}{y} (x_1-x_2), W_3 = \frac{1}{y'} (x_3-x_4).
\end{align*}


Then we have
{\fontsize{10}{12}\selectfont
\begin{align*}
  \frac{I_2}{p(2p-1)} & = \left\{ \int\limits_0^1 \int\limits_0^1
  (\xi_{11}{{(\theta, \tau)}}) W^2_1-2  \xi_{13}{{(\theta,\tau)}} W_1
  W_3\right.\\ 
  & \hspace{4cm}\left. \vphantom{\int\limits_0^1} +
  \xi_{33}{{(\theta,\tau)}} W^2_3)  d \theta d \tau \right \} |g|^{2p-2}\\  
  & =\left\{ \int\limits_0^1 \int\limits_0^1 \xi_{11}{{(\theta, \tau)}}
  d \theta d \tau \right\} |g|^{2p} + \left\{ \int\limits_0^1
  \int\limits_0^1 (\xi_{11}{{(\theta, \tau)}})\right.\\ 
  & \hspace{1cm}\left. \vphantom{\int\limits_0^1} -\xi_{13}{{(\theta,\tau)}} -
  \xi_{31}{{(\theta,\tau)}}-\xi_{33}{{(\theta,\tau)}}) d \theta d \tau
  \right\} |g|^{2p-2}|W_3|^2\\ 
  & + \left\{ \int\limits_0^1 \int\limits_0^1 (2 \xi_{11}{{(\theta,
      \tau)}}) -\xi_{13}{{(\theta,\tau)}} - \xi_{31}{{(\theta,\tau)}} d
  \theta d \tau \right\} |g|^{2p-2}|gW_3|^2. 
 \end{align*}}\relax
  
 Here, the relation $\int \int \xi_{13}(\theta, \tau) d \theta d \tau
 = \int \int \xi_{13}(\theta, \tau) d \theta d \tau$ in used. The
 first term in the\pageoriginale above is bounded by $C_9 |g|^{2p}$. Again by using
 mean value theorem, the second term is bounded by $C_{10}
 (|x_1-x_3|+|x_2-x_4|)^2 |g|^{2p-2}|W_3|^2$. The third term  is
 bounded  by  $C_{11} (|x_1-x_3|+|x_2-x_4|)^2
 |g|^{2p-2}|W_3|$.Therefore, we have 
 \begin{align*}
   |I_2| \le C_9 f & + C_{10}(|x_1-x_3|+|x_2-x_4|)^2 |g|^{2p-2}\\
   & + C_{11}(|x_1-x_3|+|x_2-x_4|) |g|^{2p-1}| \frac{1}{y'} (x_3-x_4)|\\
   \le C_{12} f & + C_{13}(|x_1-x_3|+|x_2-x_4|)^{2p}\\
   & +C_{14} (|x_1-x_3|+|x_2-x_4|)^{2p} |\frac{1}{y'} 
(x_3-x_4)|^{2p}. \tag{1.4.5}\label{c1:eq1.4.5}
 \end{align*} 
  
 Finally, using the estimates~\eqref{c1:eq1.1.4} and \eqref{c1:eq1.4.5} 
in \eqref{c1:eq1.4.3}, we obtain the desired result. 

\setcounter{proofofsublemma}{1}
\begin{proofofsublemma}\label{c1:prflem1.4.2} %  p of lem 1.4.2
In view of the notation of Lemma~\ref{c1:lem1.4.3}
\begin{multline*}
  f(\phi_{s,t} {{(x + ye)}},\phi_{s,t}{{(x)}}, \phi_{s',t}{{(x'+y'e)}},
  \phi_{s',t}{{(x')}})\\ 
  =|\eta_{s,t}{{(x,y)}} - \eta_{s',t'}{{(x',y')}}|^{2p}.
\end{multline*}
 \end{proofofsublemma} 
 
We split the proof in two steps.

(a) $t = t'$, $s'< s$.

  Applying Ito's formula, we get
  \begin{multline*}
    E [f(\phi_{s,t} {{(x + ye)}},\phi_{s,t}{{(x)}},
      \phi_{s',t}{{(x'+y'e)}}, \phi_{s',t}{{(x)}}))] \\
    = E [f((x+ye,x,\phi_{s',s}{{(x'+y'e)}}, \phi_{s,'s}{{(x')}} ))]\\
    + \, E \left[\int\limits_0^t L^{(4)}_r  f(\phi_{s,r}
      {{(x,ye)}},\phi_{s,r}{{(x)}}, \phi_{s',r}{{(x'+y'e)}},
      \phi_{s,r}{{(x')}}) dr\right]. 
  \end{multline*}

Therefore\pageoriginale using Lemma~\ref{c1:lem1.4.3},
{\fontsize{10}{12}\selectfont
\begin{align*}
  E [|\eta _{s,t}{{(x,y)}} - \eta _{s',t}{{(x',y')}}|^{2p}]  & \le
  E[|e-\eta_{s',s}{{(x',y')}}|^{2p}]\\ 
  & + C_1 \int\limits_s^t E[\eta _{s,r}{{(x,y)}} - \eta _{s',r}{{(x',y')}}|^{2p}]dr\\
    & + C_2 \int\limits_s^t E[(|\phi_{s,r}{{(x+ye)}} -  \phi_{s',r}{{(x',y'e)}}|\\
    & +| \phi_{s,r}{{(x)}}-
    \phi_{s',r} {{(x')}}|)^{2p}. (1+|\eta_{s',r}{{x',y'}})|)^{2p}]dr.  
\end{align*}}\relax

Consider the first term on the right hand side. Since
$$
\eta_{s,'s}{{(x',y')}}-e = \frac{1}{y'} \{ \phi_{s',s}{{(x'+y'e)}}
-\phi_{s',s}{{(x')}}-y'e\}, 
$$
using (1.3.6), we get  
$$
E | \eta_{s',s}{{(x',y')}}-e|^{2p} \le C | s-s'|^p.
$$

Applying Schwarz's inequality and \eqref{c1:eq1.3.7}, the third term can be
dominated by $C'\{ |x-x|^{2p} + |y-y|^{2p} +
(1+|x|+|x'|+|y+|y'|)^{2p}|s-s'|^p\}$. We then apply Gronwall's
inequality to get the desired estimate. 

(b) $s' < s < t' < t$.

Using the flow property, we have
\begin{align*}
  & E| \eta_{s,t}(x,y)- \eta_{s,t}(x,y)|^{2p} ]
    =\int E \left[|  \frac{1}{y}( \phi_{t',t}(z_1)- \phi_{t',t}(z_2) - z_1+z_2)
      |^{2p}\right]\\
    & \hspace{5cm} \times  P(\phi_{s,t'} (x+ye) \varepsilon dz_1',  \phi_{s,t'}
    (x) \varepsilon dz_2)\\ 
    &\quad  \le C|t-t'|^p \frac{1}{|y|^{2p}} \int |z_1- z_2| ^{2p}
    P(\phi_{s,t'} (x+ye)  \varepsilon dz_1, \phi_{s, t} (x)
    \varepsilon dz_2)\\ 
    & \quad = C|t-t'|^p \frac{1}{|y|^{2p}} E| \phi_{s,t'} (x + ye) -
    \phi_{s,t'} (x)|^{2p}\\ 
    &\quad  \le C'|t-t'|^P.
\end{align*}

Now\pageoriginale combining this estimate and the estimate in $(a)$, we get the
required inequality. 

\begin{proofoftheorem}\label{c1:prfthm1.4.1}%theo 1.4.1
  Applying Theorem~\ref{c1:thm1.3.7}, $\eta_{s,t}(x,y)$ has a continuous 
  extension at $y=0$, i.e 
  $$
  \lim_{y\to 0} \frac{1}{y} (\phi_{s,t}{{(x + ye_i)}} - \phi_{s,t}{{(x)}}) 
  = \frac{\partial}{\partial x_i} \phi_{s,t}{{(x)}} 
  $$ 
  exists for all $s,t,x$ and for each $i=1,2, \ldots$, where
  $e_i=(0,\ldots,1,0,\ldots,\break 0) 1$, being at the $i$th place, and is
  continuous in $(s,t,x)$. Hence $\phi_{s,t}{{(x)}}$ is continuously
  differentiable.  
\end{proofoftheorem}

Claim: $\phi_{s,t}$is a diffeomorphism.

In view of the fact that $\phi_{s,t}$ is a homeomorphism, it suffices
to show that the Jacobain matrix $\partial\phi_{s,t}{{(x)}}$ is
nonsingular. Consider the map $\mathbb{R}^d \times
\mathbb{R}{^d{^{^2}}} \to \mathbb{R}^d \times \mathbb{R}{^d{^{^2}}}$ 
\begin{equation*} 
  (x,z_1, \ldots,z_d) \to (\phi_{s,t}(x),
  \partial\phi_{s,t}(x) z_1, \ldots, \partial\phi_{s,t}(x)
  z_d). \tag{1.4.6}\label{c1:eq1.4.6}  
\end{equation*}

We claim that the above is a Brownian flow. Indeed for $s < t < u$
$$
\partial\phi_{s,u}(x) = \partial\phi_{t,u} (\phi_{s,t}(x)) 
\partial\phi_{s,t}(x), 
$$
whence the flow property follows. Therefore \eqref{c1:eq1.4.6} defines a
Brownian flow of homeomorphisms. Thus the map
$\partial\phi_{s,t}{{(x)}}$ is 1-1 and therefore
$\partial\phi_{s,t}{{(x)}}$ is nonsingular. 


\chapter{Stochastic Flows and Stochastic 
Differential Equations}\label{chap2}%chap 2

This\pageoriginale Chapter deals with the interplay between stochastic flows and
stoc\-hastic differential equation. In section~\ref{chap2:sec2.1} we study
non-Brownian stoc\-hastic flows.Under certain assumptions we establish
the homeomor\-phi\-sm and diffeomorphism properties of such flows. In
section~\ref{chap3:sec3.2} we define $C = C (\mathbb(R)^d ; \mathbb{R}^d)$- valued
semimartingales and their local characteristics. At the end of the
this section we  obtain a representation result for a C-valued Brownian
motion. In section~\ref{chap2:sec2.3} we define the stochastic integrals of
progressively measurable processes with respect to C-semimartingales,
which is essentially a generalization of the usual stochastic
integrals. In section~\ref{chap2:sec2.4} we introduce the concept of the solution
of a stochastic differential equation in this setup and then show that
the solution defines a stochastic flow. In the next section we take up
the converse problem and obtain the representation of a stochastic
flow by an $SDE$. In section~\ref{chap2:sec2.6} we discuss the inverse flows and
backward infinitesimal generators. The Chapter ends with an appendix
where we describe generalized Ito formula, Stratonovich integrals and
Stratonovich stochastic differential equations. 


\section{Non-Brownian Stochastic Flows}\label{chap2:sec2.1}%sec 2.1

Let $(\Omega, F)$ be a standard measurable space and $P$ a probability
measure defined on it. Let $\phi_{s,t}(x,\omega)$ be a stochastic
flow of measurable maps which is continuous in $t$. Let $F_t$ denote
the (right) continuous natural filtration of the flow i.e $F_t =
\bigcap_{\varepsilon >0}   \sigma (\phi_{u,v}(.): 0 \le  u \le v\le t +
\varepsilon)$. As in Chapter~\ref{chap1} we will make  three 
assumptions on the flow. 
\setcounter{Assumption}{0}
\begin{Assumption}\label{chap2:asm1}%asum 1
  $\phi_{s,t}{{x,.}}$ is square integrable and the following limits
  \break exit:
  \begin{align*}
    & \lim_{h \to 0} \frac{1}{h} E [\phi_{t,t+h}{{(x)-x)}}| F_t]
    (\omega) = b (x,t,\omega), \text{say, a,s for each t}\\ 
    & \lim_{h \to 0} \frac{1}{h} E [(\phi_{t,t+h}(x)-x)
      (\phi_{t,t+h}(y)-y)^* | F_t] \omega = a (x,y,t,\omega) \\ 
    & \qquad \text{ say, a.s. for each $t$}.
  \end{align*}\pageoriginale
\end{Assumption}

In the above expressions the conditional expectation is computed with
respect to a version of the regular conditional distribution which
does exist by the stipulation on the measurable space. We further
assume that $b (x,t,\omega)$ and $a (x,y,t,\omega)$ are jointly
measurable and progressively measurable for each $x$ (respectively $x$
and $y$). 

\begin{definition}\label{c2:def2.1.1}%defi 2.1.1
  The pair $(a, b)$ is called the local characteristics of the flow
  $\phi_{s, t}$. 
\end{definition}

\setcounter{remark}{1}
\begin{remark}\label{c2:rem2.1.2}%rem 2.1.2
  If $\phi_{s, t} $ is a Brownian flow then $\phi_{t,t+h} (x)$ is
  independent of $F_t$ and therefore $a(x, y, t)$ and $b(x, t)$ do not
  depend on $\omega$. So in that case a and $b$ coincide with the
  local characteristics of the Brownian flow. We show later that if
  the local characteristics $a, b$ do not depend on $\omega$ then
  $\phi_{s, t} $ is a Brownian flow.  
\end{remark}

\begin{Assumption}\label{c2:asm2}%assum 2
There exists a positive constant $K$ (independent of $\omega$) such that 
\begin{align*}
  & | E [ \phi_s,t (x) - x |F_s ] | \le K(|+|x|) |t-s|, \tag{2.1.1}\\
  & | E [(\phi_{s,t}(x) - x) (\phi_{s,t} (y) - y)^\ast |F_s ]| \le K(1+|x|)
  (1+|y|) |t-s|. \tag{2.1.2}\label{c2:eq2.1.2} 
\end{align*}
\end{Assumption}

These two inequalities imply
\begin{align*}
  & | b(x,t, \omega) | \le K(1 + |x|), \tag{2.1.3}\label{c2:eq2.1.3}\\
  & | a(x, y, t, \omega) | \le K (1+ |x|) (1+|y|). \tag{2.1.4}\label{c2:eq2.1.4}
\end{align*}

\setcounter{Lemma}{2}
\begin{Lemma}\label{c2:lem2.1.3}%lemm 2.1.3
  For each $s, x$
  \begin{equation*}
    M_{s,t}(x) = \phi_{s, t} (x) - x - \int_s^t b(\phi_{s, r} (x), r)
    dr, t \ge s \tag {2.1.5}\label{c2:eq2.1.5} 
  \end{equation*}
  is\pageoriginale an $L^2$-martingale and 
  \begin{equation*}
    < M^i_{s,t}(x), M^j_{s,t}(y) > = \int\limits_s^t
    a_{ij}(\phi_{s,r}(x), \phi_{s,r}(y),r)dr. \tag{2.1.6}\label{c2:eq2.1.6} 
  \end{equation*}
\end{Lemma}

\noindent \textit{Proof.}
The proof is the same as that of lemma~\ref{c1:lem1.2.3}. Set
\begin{equation*}
  m_{s,t}{{x,\omega}} = E [\phi_{s,t}(x)|F_s](\omega).\tag*{$\Box$}
\end{equation*}

For $s < t < u$, we have
\begin{align*}
  E [\phi_{s,t}(x)|F_s] & = \int \phi_{s,u}{{(x,\omega')}} p_t
  (\omega, d \omega')\\ 
  & =  \int \phi_{t,u} (\phi_{s,t} (x,\omega'),\omega') p_t (\omega, d \omega')
\end{align*}
where $p_t (\omega, d \omega')$ is a regular conditional distribution
given $F_t$. Now for fixed $s,t,\omega$ 
$$
p_t (\omega, \{ \omega':  \phi_{s,t}{{(x,\omega')}} = 
\phi_{s,t} (x,\omega) \}) = 1  \quad a.a.\omega. 
$$

Therefore
\begin{align*}
  E [\phi_{s,u}{{(x)}}|F_t] & = \int
  \phi_{t,u}{{(\phi_{s,t}{{(x,\omega)}})}} p_t (\omega, d \omega')\\ 
  & = m_{t,u}{{(\phi_{s,t}{{(x,\omega),\omega}})}}. 
\end{align*}


Hence
$$
m_{s,u}{{(x,\omega)}} = E [m_{t,u} {{(\phi_{s,t}{{(x)}})}}|F_s]
$$

So
$$
\frac{1}{h} [m_{s,t+h} {{(x,\omega)}}- m_{s,t}{{(x, \omega)}}] = 
\frac{1}{h} E [m_{t,t+h} {{(\phi_{s,t}{{(x)}})}} - \phi_{s,t} (x) | F_s]. 
$$

Letting $h \to 0$, we get
$$
\frac{\partial}{\partial t} m_{s,t}{{(x,\omega)}} = 
E[b(\phi_{s,t}(x),t)|F_s]. 
$$

Thus\pageoriginale
$$
m_{s,t}{{(x)}}-x = E \left[\int\limits_s^t b (\phi_{s,r}(x), 
        r)dr|F_s\right]. 
$$

Then proceeding as in Lemma~\ref{c1:lem1.2.3} we conclude that $M_{s,t}$ is a
martingale. For the second assertion set 
$$
V_{s,t}{{(x,y,\omega)}} = E [M_{s,t} {{(x)}} M_{s,t}{{(y)^*}}|F_s].
$$

We can show similarly that 
$$
V_{s,t}{{(x,y,\omega)}} = E\left[\int\limits_s^t a
  (\phi_{s,r}{{(x)}}, \phi_{s,r}(y),r) dr|F_2\right] (\omega).  
$$

Again proceeding the same way we prove that 
 $$
 M_{s,t}{{(x)}}  M_{s,t}{{(y)^*}}  - \int\limits_s^t a
 (\phi_{s,r}{{(x)}}, \phi_{s,r}(y),r)dr 
 $$
 is a martingale. The proof is complete.
$$
\text{Let} \quad \underline{x}^{(N)} = (x_1,\ldots,x_N) \in
\mathbb{R}^{Nd}. 
$$
Define 
\begin{multline*}
  (L_t^{(N)} f ) (\underline{x}^{(N)},\omega) = \frac{1}{2}
  \sum\limits_{i,j,k,\ell} a_{ij}(x_k,x_\ell,t,\omega) \frac{\partial^2
    f}{\partial x^i_k \partial x^j_\ell} (\underline{x}^{(N)})\\ 
  +  \sum_{k,i} b^i (x_k,t,\omega) \frac{\partial f}{\partial x^i_k}
  (\underline{x}^{(N)}). \tag{2.1.7}\label{c2:eq2.1.7} 
\end{multline*}
in this case $L_t^{(N)}$ is a random differential operator.

\setcounter{theorem}{3}
\begin{theorem}%theo 2.1.4
$\phi_{s,t}{{(x)}}$ has finite moments of any order and if $f$ is a
  $C^2$-function, $f$ and its derivatives are of polynomial growth,
  then 
  \begin{equation*}
    f (\phi_{s,t}{{(x_1)}}, \ldots,\phi_{s,t}{{(x_N)}})
    -\int\limits_s^t (L_r^{(N)} f) (\phi_{s,r}{{(x_1)}},  \ldots,
    \phi_{s,r}{{(x_N)}}) dr \tag{2.1.8}\label{c2:eq2.1.8} 
  \end{equation*}
  is a martingale.
\end{theorem}

The\pageoriginale proof is similar to that of Theorem~\ref{c1:thm1.2.5} and hence it is omitted.

Now we make our assumption which is essentially a Lipschitz condition
on the local characteristics. 

\begin{Assumption}\label{c2:asm3}%assum 3
  There exists a constant $L$ (independent of $t, \omega$) such that
  \begin{align*}
    &|b (x, t, \omega) - b (y, t, \omega)| \le L|x-y|, \tag{2.1.8}\\
    &|a (x, x, t, \omega) - 2a ( x, y, t, \omega)| +a(y,y,t,\omega) |
    < L|x-y|^2. \tag{2.1.9}\label{c2:eq2.1.9} 
  \end{align*}
\end{Assumption}


\begin{theorem}\label{c2:thm2.1.5}%the 2.1.5
  Let $\phi_{s,t}$ be a continuous stochastic flow of measurable maps
  satisfying $(A1) \sim (A3)$. Then it admits a modification which is a
  stochastic flow of homeomorphisms.  
\end{theorem}

\begin{proof}
  The proof of this theorem also goes along the lines of 
Theorem~\ref{c1:thm1.3.2}. Using Ito's formula and Gronwall's inequality the following
  estimates can be derived: 
  \begin{enumerate}[(i)]
  \item For any real $p$ and $\varepsilon > 0$ there exists a constant
    $C = C(p) > 0$ such that for any $t,x$ 
    \begin{equation*} 
      E [(\varepsilon + | \phi_{s,t}{{(x)}})|^2)^p|F_s] \ge C
      (\varepsilon + |x|^2)^p a.s. \tag{2.1.10}\label{c2:eq2.1.10} 
    \end{equation*}

  \item For real $p$ and $\varepsilon > 0$ there exists $C =C(p)$ such that
    \begin{equation*}
      E [(\varepsilon + | \phi_{s,t}{{(x)}}- \phi_{s,t}{{(y)}}|^2)^p
        |F_s ] \ge C (\varepsilon + |x-y|^2)^p \tag{2.1.11}\label{c2:eq2.1.11} 
    \end{equation*}
    holds for all $t,x$ a.s
  \item For any positive integer $p$ there exists $C = C(p)$ such that  
    \begin{equation*}
      E[|\phi_{s,t}{{(x_o)}} - x_o|^{2p}| F_s] \le C |t-s|^p
      (1+|x_o|^2)^p \tag{2.1.12}\label{c2:eq2.1.12} 
    \end{equation*}
    holds for any $x_o \in \mathbb{R}$ a.s
  \end{enumerate}
  
  Using\pageoriginale \eqref{c2:eq2.1.10}, \eqref{c2:eq2.1.11}, \eqref{c2:eq2.1.12} we can show that
  \begin{equation*}
    E[ | \phi _{s,t}(x)-\phi_{s',t'}(x')|^{2p}]\leq
    C(|x-x'|^{2p}+(1+|x|+|x'|)^{2p}.(|t-t'|^p+|s-s'|^p)) \tag{2.1.13}\label{c2:eq2.1.13} 
  \end{equation*} 

  Indeed, consider the case $s=s', x=x', t<t'$.
  \begin{align*}
    E\left[ | \phi _{s,t}(x)-\phi_{s,t'}(x)|^{2p}\right] & = E\left[|\phi
      _{s,t'}(x)-\phi_{t,t'}(\phi_{s,t}(x))|^{2p}\right]\\ 
    & = E\left\{E[|\phi_{s,t}(x)- \phi_{t,t'} (\phi_{s,t}(x))|^{2p}|F_t]\right\}\\
    & = E\left\{ E[|y-\phi_{t,t'}(y)|^{2p}|F_t]_{y= \phi_{s,t}(x)}\right\}\\
    & \leq C|t-t'|^{p}E \left\{ 1+|\phi_{s,t}(x)|^{2})^p\right\}\\
    & \leq C'|t-t'|^{p}(1+|x|^{2})^p.
  \end{align*}
  The rest of the proof is exactly similar to that of 
Theorem~\ref{c1:thm1.3.2}. We therefore omit the details. 
\end{proof}


\setcounter{remark}{5}
\begin{remark}%Remk 2.1.6
  Assuming suitable smoothness conditions on $(a, b)$ and boundedness
  of their derivatives we can establish the diffeomorphism property of
  the flow exactly the same way as we did in Chapter~\ref{chap1}. 
\end{remark}

\section{Vector Valued Semimartingales}\label{chap2:sec2.2}%Sec 2.2

Let $(\Omega, F)$ be a standard measurable space and $P$ a probability
measure defined on it. Let there be given a filtration $\{F_t\}, o
\leq t \leq T$.  Let $X(x, t \omega), x \in \mathbb{R}^d, t
\in [O, T]$ be a sample continuous $\mathbb{R}^d$-valued
random field. We assume that for each $x, t$ it is
$F_t$-measurable. Let $C=C(\mathbb{R}^d;\mathbb{R}^d)$ endowed with
compact uniform topology. 

\begin{definition}\label{c2:def2.2.1}%Def 2.2.1
  $X(x, t)$ is called a $C$-valued martingale if it is an
  $\mathbb{R}^d$-valued martingale for each $x$. It is called a
  $C^k$-valued martingale if $D^\alpha_x X(x, t)$ is a $C$-valued
  martingale\pageoriginale for any $|\alpha|\leq k$. Let $X(x,t)$ be a continuous
  random field which admits the following decomposition. 
\end{definition}
$$
X(x,t)=X(x,o)+Y(x,t)+V(x,t)
$$
where $Y(x, t)$ is a $C$-valued martingale, $V(x,t)$ a process of
bounded variation for each $x$ and $Y(x, o)=0, V(x,0)=0$. Then $X(x,
t)$ is called a $C$-valued semimartingale. If $Y(x, t)$ is a
$C^k$-martingale and $D^\alpha_x V(x, t), |\alpha | \leq k$ is of
bounded variation then it is called a $C^k$-semimartingale. 

Next we will define the local characteristics of an
$\mathbb{R}^d$-valued semimartingale.  Here we shall dispense with the
continuity condition. Let $X(x, t)$ be an $\mathbb{R}^d$-valued
semimartingale with parameter $x \in \mathbb{R}^d$ such that 
$$
X(x,t)= Y(x,t)+V(x,t)
$$
where for each $x,Y(x, t)$ is a martingale and $V(x, t)$ a process of
bounded variation. Assume that there exist $d$-vector valued process
$\beta(x, t, \omega)$ with parameter $x$ and $d \times d$-matrix valued
process $\alpha(x,y,t,\omega)$ with parameters $x,y$ such that $\beta$
and $\alpha$ are progressively measurable w.r.t. $\{F_t\}$ and  
\begin{align*}
V(x,t) & = \int^t_o\beta (x,r)dr,\\
< Y(x,t), Y(y,t)^* > & = \int ^t _o \alpha(x,y,r)dr.
\end{align*}

Then the pair $(\alpha, \beta)$ is called the local characteristics of
the semimartingale $X(x,t)$. 

\setcounter{remark}{1}

\begin{remark}\label{c2:rem2.2.2}%Remk 2.2.2
  The above definition of local characteristics differs from that of a
  flow. Indeed, if $\phi_{s,t}$ is a stochastic flow as described in
  section~\ref{chap2:sec2.1}, then 
  $$
  \phi_{s,t}(x)= x+M_{s,t}(x)+ \int^t_s b(\phi_{s,r}(x),r)dr.
  $$
\end{remark} 
 
 Here\pageoriginale $\phi_{s,t}(x)$ is a semimartingale, $M_{s, t}(x)$ its
 martingale part and\break $\int^t_s b(\phi_{s, r}(x),r)dr$ is the bounded
 variation part. In this case 
 \begin{align*}
   \beta(x,t) & = b(\phi_{s,t}(x),t),\\
   \alpha(x,y,t) & = a(\phi_{s,t}(x),\phi_{s,t}(y),t).
 \end{align*} 
 
 We shall always assume that $\alpha$ and $\beta$ are integrable,
 $$
 \displaylines{\text{i.e., } \hfill E[\int^t_o | \alpha(x,y,r)|dr] <
   \infty, \quad E[\int^t_o | \beta (x,r) |dr ]< \infty.\hfill } 
 $$
 
 Thus $Y$ becomes an $L^2$- martingale.
 
\setcounter{proposition}{2}
\begin{proposition}\label{c2:prop2.2.3}%Prop 2.2.3
  Let $\{Y(x, t),x \in \mathbb{R}^d\}$ be a family of
  continuous  $\mathbb{R}^d$- valued semimartingales with local
  characteristics $(\alpha, \beta)$. Suppose that $ \alpha $ and
  $\beta$ are Lipschitz continuous and $| \alpha (| (0, 0, t)|, |
  \beta (0, t)|$ are bounded, then $Y(x, t)$ has a modification as
  continuous $C$-semimartingale. Further if $\alpha$ and $\beta$ are
  $k$-times differentiable in each $x, y$ and there exists $K$ such
  that  
  $$
  |D^\alpha_x D^\beta_y \alpha(x,y,t)|\leq K,|D^\beta_x(x,t)| \leq K,
  |\alpha | \leq k, |\beta | \leq k, 
  $$ 
  then $Y(x,t)$ has a modification as $C^{k-1}$-semimartingale.
 \end{proposition}
 
\begin{proof}
  Using Burkholder's inequality, one can show that
  $$
  E[|Y(x,t)-Y(x',t')|^p] \leq C(|t-t'|^{p/2}+ |x-x'|^p).
  $$ 
  Now using Kolmogorov's theorem one can complete the proof.
\end{proof}

\setcounter{definition}{3}
\begin{definition}\label{c2:def2.2.4}%Def 2.2.4
  Let $X(x, t)$ be a $C$-valued process continuous in $t$. It is
  called a $C$-Brownian motion if for $0 \leq t_0 < t_1 < \cdots < t_n
  \leq T, X_0, X_{t_{i+1}} - X_{t_i}= 0,1,\ldots, n-1,$ are
  independent. 
\end{definition} 
 
\setcounter{remark}{4}
\begin{remark}\label{c2:rem2.2.5}\pageoriginale%Remk 2.2.5
  For $(x_1, \ldots, x_N) \in \mathbb{R}^{Nd},
  (X_t(x_1),\ldots, X_t(x_\mathbb{N}))$ is an $\mathbb{R}^{ND}$-valued
  Brownian motion in the usual sense, i.e.,  it is a Gaussian process
  with independent increments. 
\end{remark}  
 
\setcounter{proposition}{5}
\begin{proposition}\label{c2:prop2.2.6}%Prop 2.2.6
  Let $X(x, t)$ be a continuous $C$-valued semimartingale with local
  characteristics  $(\alpha, \beta)$. If $\alpha, \beta$ do not depend
  on $\omega$ then $X(x, t)$ is a $C$-Brownian motion 
\end{proposition}  
 
\begin{proof}
  This is a straightforward implication of Levy's characterization of
  Brownian motion. 
\end{proof} 
 
\setcounter{example}{6}
\begin{example}\label{c2:exam2.2.7}%Exmple 2.2.7
  Examples of $C$-Brownian motions.
\end{example} 

 Let $(B^1_t,\ldots, B^r_t)$ be an $r$-dimensional standard B.M. Let
 $F_0 (x, t)$, $F_1(x, t)$, $\ldots,  F_r(x, t)$ be $\mathbb{R}^d$-valued
 functions Lipschitz continuous in $x$. Let 
\begin{equation*}
   X(x,t)= \int^t_o F_o(x,s)ds + \sum^{r}_{k=1}\int^t_o
   F_k(x,s)dB^k_s. \tag{2.2.1}\label{c2:eq2.2.1} 
\end{equation*} 

Then $X(x, t)$ is a $C-B.m$. with $\beta(x, t)=F_0(x,t)$ and $\alpha
(x,y,t) = \sum \limits^{r}_{k=1} F_k (x, t)F_k(y, t)^*$. 
 Next consider an infinite sequence of independent one-dimensional
 $B.m's\{B^k_t\}^\infty_{k=1}$. Let $\{ F_k(x,t)\}^\infty_{k=o}$ be a
 sequence of $\mathbb{R}^d$-valued functions such that there exists $L
 > 0$ satisfying 
 \begin{align*}
   \sum^{\infty}_{k=o}|F_k(x,t) &-F_k(y,t)|^2 \leq L|x-y|^2,
   \sum^{\infty}_{k=o}|F_k(x,t)|^2  \leq  L(1+|x|^2.
 \end{align*} 
 Then
 \begin{equation*}
   X(x,t)=\int^t_o F_o(x,r)dr+ \sum^\infty_{k=1}\int^t_o
   F_k(x,r)dB^k_r \tag{2.2.2}\label{c2:eq2.2.2} 
 \end{equation*} 
 converges and is a $C-B.m$. In this case $\beta(x, t)$ and $\alpha(x,
 y, t)= \sum \limits^{\infty}_{k=1} F_k(x,t)$ $F_k(y,t)^*$. 
 
 We\pageoriginale will see in the next proposition that if the local characteristics
 of a C-B.m. satisfy suitable condition then it is of the form
 \eqref{c2:eq2.2.2}. 
 
\setcounter{proposition}{7}
\begin{proposition}\label{c2:prop2.2.8}%Prop 2.2.8
  Let $X(x, t)$ be a C-B.m. with local characteristics $(\alpha,
  \beta)$ and $X(x, 0)=0$. Assume $\alpha, \beta$ are Lipschitz
  continuous. Then there exist an infinite sequence of independent
  standard B.m.'s $\{B^k_t\}_{k=1}$ and functions $F_k(x,t),k=0,1,2,
  \ldots $ such that 
  $$
  X(x,t)=\int^t_o F_o(x,r)dr+\sum ^{\infty}_{k=1}\int^t_o F_k(x,r)dB^k_r.
  $$
\end{proposition}

\noindent \textit{Proof.}
  We will only consider the homogeneous case, i.e.,  when the local
  characteristics do not depend on $t$. Let $F_0(x)=E[X(x, 1)]$, then
  $E[X(x, t)]=t F_0(x)$. Set 
  \begin{equation*}
  Y(x,t) = X(x,t)-t F_o(x)\tag*{$\Box$}
  \end{equation*}

 Then $Y(x, t)$ is a zero-mean $C-B.m$. Let $\{x_n\}$ be a dense
 subset of $\mathbb{R}^d$. Consider the sequence
 ${\{Y^i(x_k,t)\}}_{\substack{k=1,2,\ldots\\ i=1,2,\ldots, d}}$. By
 Schmidt's orthogonalization procedure we can find a sequence
 $\{B^k_t\}_{k=1,2, \ldots}$ of orthogonal (=Independent) Brownian
 motions such that the linear span of $\{B^k_t, k=1,2,\ldots\}$ is
 equal to that of $\{Y^i (x_k,t),k=1,2,\ldots,i=1, \ldots,d\}$. Set
 $F_k(x)=E[Y(x,1)B^k_1]$. 
 \noindent  Then 
 \begin{align*}
   F_k(x)t & =E[Y(x,t)B^k_t],\\
   Y(x,t)& =F_o(x)t+ \sum^{\infty}_{k=1}F_k(x)B^k_t.
 \end{align*} 
 Also it is easily seen that $\alpha(x,y)= \sum
 \limits^{\infty}_{k=1}F_k(x)F_k(y)^*$. 

 \section{Stochastic Integrals}\label{chap2:sec2.3}\pageoriginale%Sec 2.3
 
 Let $Y(x, t)$ be a $C$-martimgale such that the characteristic
 $\alpha (x, y, t, \omega) $ is integrable and continuous in 
$(x, y)$. Further we make the following assumption:  


\begin{Assumption}\label{c2:asm4} % asum 4
   $   \int^t_{o}\sup_{|x|,|y| \leq K}| \alpha(x,y,t,\omega)|dt <
  \infty$ for any $K$ and $t$. 
 
   Let $f_t(\omega)$ be a progressively measurable
   $\mathbb{R}^d$-valued process such that  
   \begin{equation*}
     \int^t_{o}|\alpha(f_{s,}f_s,s)|ds < \infty   ~a.s. \text{ for
       each }t. \tag{2.3.1}\label{c2:eq2.3.1} 
   \end{equation*} 
 Our endeavour here is to define the stochastic integral $\int^t_{0}
 Y(f_r, dr)$. This would be a natural generalization of the usual
 stochastic integral in the sense that if $Y(x, t)=xY_t,Y_t$ an
 $L^2$-martingale then $\int ^t_{o}Y(f_r,dr)=\int^t_{o}f_r dY_r$. 
\end{Assumption}


\medskip
\noindent{\textbf{\textit{Case a $f_t$ is continuous in $t$.}}}

 For a positive integer $N$, define the following stopping time.
 \begin{equation*}
    \tau_N(\omega)=
    \begin{cases}
&     \inf \left\{t \in [0,T]:\sup\limits_{0 \leq r \leq t}|
     Y(f_r,t)|> N \quad \text{or} \right.\\
& \hspace{2cm} \left.\int ^t_{o} \sup \limits_{0 \leq r \leq
       s}| \alpha(f_r,f_r,s)|ds >N\right\}\\ 
 &    \infty, \text{if the above set is empty.}
   \end{cases}
 \end{equation*}
 
 
 Then $\tau_N \uparrow \infty $  as $N \uparrow \infty$. Set $Y_N(x,
 t)=Y(x, t \Lambda \tau _N)$. Then $Y_N(x, t)$ is a $C$-martingale
 with local characteristic $\alpha_N(x, y, t)=\alpha(x,y,t)I_{\{\tau_N
   >t\}}$.  Let $\Delta = \{0 =t_0 < t_1 < \cdots < \tau_n =T\}$ be a
 partition of the interval [0,T]. For any $t \in [0,T]$ define 
 \begin{equation*}
   L^{N \Delta}_t(f)=\sum^{n-1}_{i=0}\left\{Y_N(f_{t_i \Lambda
     t},t_{i+1}\Lambda t)-Y_N(f_{t_i \Lambda t},t_i \Lambda
   t)\right\}. \tag{2.3.2}\label{c2:eq2.3.2}  
 \end{equation*} 
 Then $L^{N \Lambda}_t(f)$ is an $\mathbb{R}^d$-valued
 $L^2$-martingale, since for $t=t_k$, $s=t_i$, $k>i$, 
 \begin{multline*}
 E\left[L^{N \Delta}_t (f)-L^{N \Delta}_s(f)|F_s\right]\\ 
 = \sum^{k-1}_{j=i}E
 \left[E\left[Y_N(f_{t_j},t_{j+1})-Y_N(f_{t_j},t_{j})|F_{t_j}\right]|F_s
   \right]=0.     
 \end{multline*}\pageoriginale
 Also
 \begin{align*}
   E &\left[L^{N \Delta}_t (f)-L^{N \Delta}_s(f)(L_t^{N \Delta}(f)-L^{N
       \Delta}_s(f))^*|F_s\right]\\ 
   & = \sum^
   {k-1}_{j=i}E[E[(Y_N(f_{t_j},t_{j+1}) - Y^1_N(f_{t_j},t_{j}))
       (Y_N(f_{t_j}, t_{j+1})\\
  &\qquad-Y_N(f_{t_j},t_j))^\ast|F_{t_j}]|F_s]\\  
   & = \sum^ {k-1}_{j=i}E[E[\int ^{t_{j+1}}_{t_j}\alpha_N(
       f_{t_j},f_{t_j},r)dr|F_{t_j}]|F_s]\\ 
   & = E[\int^t_s \alpha_N(f^\Delta_r,f^\Delta_r,r)dr|F_s],
 \end{align*} 
 where $f^\Delta_r =f_{t_k}$ if $t_k \leq r < t_{k+1}$. Therefore
 \begin{equation*}
   < L^{N \Delta}_t(f), L^{N \Delta}_t(f)^* > = \int^t_o
   \alpha_N(f^\Delta_r, f^\Delta_r,r)dr. \tag{2.3.3}\label{c2:eq2.3.3} 
 \end{equation*}  
 
 Now let $\{\Delta_n,n=1,2, \ldots\}$ be a sequence of partitions of
 [$0,T$] such that $|\Delta_n| \to o$. 
 Consider the corresponding sequence of $L^2$-martingales $\{L^{N
   \Delta_n}_{t} (f), n=1,2,\ldots\}$. As before it can be verified
 that 
 $$ 
 < L^{N \Delta_n}_{t} (f), L^{N \Delta_m}_{t}(f)^*> = \int^t_o
 \alpha_N (f^{\Delta_n}_r, f^{\Delta_m}, r)dr. 
 $$
 Therefore 
\begin{multline*}
  < L^{N \Delta_n}_{t} (f), -L^{N \Delta_m}_{t}(f),( L^{N \Delta_n}_{t}
  (f), -L^{N \Delta_m}_{t}(f))^*>\\ 
  = \int^t_o \left[\alpha_N \left(f^{\Delta_n}_r,f^{\Delta_n}_r,r
    \right)-\alpha_N\left(f^{\Delta_n}_r,f^{\Delta_m}_r, r\right)
    - \alpha_N\left(f^{\Delta_m}_r,f^{\Delta_n}_r,r\right)\right.\\ 
    \left.+\alpha_N\left(f^{\Delta_m}_r,f^{\Delta_m}_r,r\right)\right]dr
  \to 0 ~\text{as}~ m,n \to \infty ~\text{a.s.}\\
\end{multline*}
  
 Now\pageoriginale in view of (A4), the above also converges in $L^1$-sense. We
 then define  
 \begin{equation*}
   \lim_{n \to \infty}L^{N \Delta_n}_t (f)=L^N_t (f)=\int^t_o
   Y_N(f_r,dr).\tag{2.3.4}\label{c2:eq2.3.4} 
 \end{equation*} 
 Thus $L^N_t(f)$ is an $L^2$-martingale. For $N>M$ it is easy to see
 that $L^N_t(f)$ = $L^M_t(f)$ if $t < \tau_M(\leq \tau_N)$. Define 
 \begin{equation*}
   L_t(f)=L^N_t(f) ~\text{if}~ t<\tau_N. \tag{2.3.5}\label{c2:eq2.3.5}
 \end{equation*} 
 Then $L_t(f)$ is a continuous local martingale. We write
 $$
 L_t(f)=\int^t_o Y(f_s,ds).
 $$

\medskip
\noindent{\textbf{\textit{Case b. $f_t$ is progressively measurable
      and bounded.}}}

 In this case set $f^N_t = \dfrac{1}{N}\int^t_{t-1/N} f_s ds$. Then
 $\{f^N_t\}$ is uniformly bounded continuous process and it converges
 to $f_t$ a.s. w.r.t. $dt \otimes dP$. Now since 
 \begin{multline*}
   < L_t\left(f^N\right)-L_t\left(f^M\right),
   \left(L_t\left(f^N\right)  -L_t\left(f^M\right)\right)^*>\\ 
   =\int^t_o \left[\alpha\left(f^N_\nu,f^N_\nu,\nu\right) 
     -\alpha\left(f^N_\nu,f^M_\nu,\nu\right)
     -\alpha\left(f^M_\nu,f^N_\nu,\nu\right) \right.\\  
      \left.+\alpha\left(f^M_\nu,f^M_\nu,\nu\right)\right]d \nu \to 0
   ~\text{a.s.} 
 \end{multline*}
 therefore $\{L_t (f^N),N=1,2,\ldots\}$converges uniformly in $t$ in
 probability (see Kunita~\cite{18}, Thm~3.1). Hence we set 
 $$
 L_t(f)=\lim_{N \to \infty}L_t\left(f^N\right)=\int^t_o
 Y\left(f_r,dr\right). 
 $$

\smallskip 
\noindent{\textbf{\textit{Case c.}}}

   In the general case when $f_t$ is only progressively measurable,
   write $f^N=(f \Lambda N)V(-N)$. then $\{L_t(f^N),N=1,2,\ldots\}$
   converges uniformly in $t$ in probability. Set 
   $$
   L_t(f)= \lim_{N \to \infty}L_t(f^N) = \int^t_o Y(f_r,dr).
   $$
 

 \begin{proposition}\label{c2:prop2.3.1}\pageoriginale%Prop 2.3.1
   Let $f_t$ and $g_t$ be progressively measurable processes
   satisfying (2.3.1). Then 
   \begin{equation*} 
     < \int^t_o Y(f_r,dr),\int^t_o Y(g_r,dr)=\int^t_o
     \alpha(f_s,g_s,s)ds. \tag{2.3.6}\label{c2:eq2.3.6} 
   \end{equation*}
 \end{proposition} 
 

 \begin{proof} 
It is straightforward. 

   Let $Y(x,t)$ and $Z(x,t)$ be continuous $C$-martingales with local
   characteristics $\alpha^Y$ and $\alpha^Z$ respectively satisfying
   $(A4)$. Let $f_t$ and $g_t$ be progressively measurable processes
   such that $\int^t_o| \alpha^Y(f_s,f_s,s)|ds < \infty$,
   $\int^t_o|\alpha^Z(g_s,g_s,s)|ds$ $< \infty$ for all $t$ a.s. Then
   $\int\limits^t_o Y (f_r, dr)$ and $\int\limits^t_o Z (g_r, dr)$
   make sense. We are interested in computing 
   $$< \int^t_o
   Y(f_r,dr),(\int^t_s Z(g_r,dr))^*>.$$ 
 \end{proof} 


\setcounter{Lemma}{1}
 \begin{Lemma}\label{c2:lem2.3.2}%Lem 2.3.2
   There exists a random field $\alpha^{YZ}(y, z, t, \omega)$ which is
   measurable and is continuous in $y,z$ such that 
   \begin{equation*} 
     <Y(y,t),Z(z,t)^* > = \int^t_o 
\alpha^{YZ}(y,z,r,\omega)dr. \tag{2.3.7}\label{c2:eq2.3.7}
   \end{equation*}
   Further
   \begin{equation*}
     |\alpha^{YZ}_{ij}(y,z)|\leq
     \alpha^Y_{ii}(y)^{1/2},\alpha^Z_{jj} (z)^{1/2}, \alpha^Y_{ii}(y)=
     \alpha^Y_{ii}(y,y), \tag{2.3.8}\label{c2:eq2.3.8} 
   \end{equation*}
   and
   \begin{multline*}
     |\alpha^{YZ}_{ij}(y,z)-\alpha^{YZ}_{ij}(y',z')|\leq 
     \alpha^Y_{ii}(y)^{1/2}\left\{\alpha^Z_{jj}(z)-2
     \alpha^Z_{jj}(z,z')+\alpha^Z_{jj}(z')\right\}^{1/2}\\ 
     +  \alpha^Z_{jj}(z')^{1/2}\left\{\alpha^Y_{ii}(y)-2
     \alpha^Y_{ii}(y,y')+\alpha^Y_{ii}(y')\right\}^{1/2}.\tag{2.3.9}\label{c2:eq2.3.9} 
   \end{multline*}
 \end{Lemma}
 
\setcounter{definition}{2}
 \begin{definition}%Def 2.3.3
   $\alpha^{YZ}(y,z,t)$ is called the joint local charcteristic of $Y$
   and $Z$. 
 \end{definition}

\setcounter{proofofsublemma}{1}
\begin{proofofsublemma}\label{c2:prflem2.3.2}%Prf of lem 2.3.2
  It is well known that for any fixed $y$, $z < Y$ $(y, t)$,
  $Z(z,t)^*>$ has\pageoriginale a 
  density function $\alpha^{YZ}(y,z,t)$ w.r.t. $dt$. Further 
  \begin{multline*}
    |<Y^i(y,t),Z^j(z,t)>-<Y^i(y,s),Z^j(z,s) >|\\
    \leq(|<Y^i(y,t)>-<Y^i(y,s)>)^{1/2}(<Z^j(z,t) >-<Z^j(z,s)>)^{1/2}.
  \end{multline*}
\end{proofofsublemma}

\noindent Therefore 
  $$
  \frac{1}{t-s}|\int^t_s \alpha^{YZ}_{ij}(y,z,r)dr| \leq
  \frac{1}{t-s} \left(\int^t_s \alpha^Y_{ii}(y,r)dr\right)^{1/2}\left(\int^t_s
  \alpha^Z_{jj}(z,r)dr\right)^{1/2}.  
  $$
  Letting $t \to s$, we get
  $$
  |\alpha^{YZ}_{ij}(y,z,s)|\leq
  \alpha^Y_{ii}(y,s)^{1/2}\alpha^Z_{jj}(z,s)^{1/2} ~\text{for
    all}~ y, z, ~\text{a.a.s.} 
  $$
  The second inequality can be proved similarly. Also \eqref{c2:eq2.3.9}
  implies that $\alpha^{YZ}(y,z,t)$ is continuous in $y,z$. 
 
\setcounter{theorem}{3}
\begin{theorem}\label{c2:thm2.3.4}%Thm 2.3.4
   We have
   \begin{equation*}
     < \int^t_sY(f_r,dr),\left(\int^t_s Z(g_r,dr)\right)^*>=\int^t_s
     \alpha^{YZ}(f_r,g_r,r)dr. \tag{2.3.11}\label{c2:eq2.3.11} 
   \end{equation*}
   The proof is simple.
\end{theorem}   

\section[Construction of stochastic......]{Construction of stochastic Flows by Stochastic Differential
   Equations}\label{chap2:sec2.4}%Sec 2.4
 
 Let
\begin{equation*}
  X(x,t)=Y(x,t)+ \int^t_o \beta(x,r)dr \tag{2.4.1}\label{c2:eq2.4.1}
\end{equation*}
be a continuous $C$-semimartingale whose local characteristics
$\alpha, \beta$ are continuous in $x,y$ and satisfy 
\begin{equation*}
  \int^t_o \sup_{|x|,|y|\leq K} |\alpha (x,y,r,\omega)|dr < \infty,
  \int^t_o \sup_{|x|\leq K}|\beta(x,r)|dr < \infty\tag*{$(A4)'$} 
\end{equation*} 
for\pageoriginale any $K > 0$ and any $t$.


Let $f_t$ be a progressively measurable process satisfing \eqref{c2:eq2.3.1}
and $\int^t_o | \beta(f_r,r)|dr<\infty$. We define 
\begin{equation*}
  \int^t_o X(f_r,dr)=\int^t_o \beta(f_r,r)dr+
\int^t_o Y(f_r,dr). \tag{2.4.2}\label{c2:eq2.4.2}
\end{equation*}


\begin{definition}\label{c2:def2.4.1}%Def 2.4.1
  A continuous $F_t$-adapted $\mathbb{R}^d$-valued process $\phi_t$ is
  said to be a solution of the stochastic differential equation 
  \begin{equation*}
    d\phi_t =X(\phi_t,dt) \tag{2.4.3}\label{c2:eq2.4.3}
  \end{equation*}
  starting from $x$ at time $s(t \geq s)$ if 
  \begin{equation*}
    \phi_t =x+\int^t_s X(\phi_tr,dr) \quad \text{for all} \quad t \geq
    s. \tag{2.4.4}\label{c2:eq2.4.4} 
  \end{equation*}
\end{definition}

\setcounter{example}{1}
\begin{example}%Exmp 2.4.2
Let us consider the Example~\ref{c2:exam2.2.7}, i.e., 
$$
X(x,t)= \int^t_o F_o(x,s)ds+ \sum^{r}_{k=1}\int^t_o F_k(x,s)dB^k_s.
$$
In this case the $SDE$ reduces to the usual one, viz.
$$
d\phi_t =F_o (\phi_t,t)dt+\sum^{r}_{k=1}F_k(\phi_t,t)dB^k_t.
$$
\end{example}

\setcounter{theorem}{2}
\begin{theorem}\label{c2:thm2.4.3}%Thm 2.4.3
  Assume that the local characteristics $\alpha, \beta$ of the
  semimartingale $X(x, t)$ satisfy Lipschitz continuity and have
  linear growth, then the $SDE(2.4.3)$ has a unique solution for any
  $x$ and $s$. Further if $\phi_{s,t}(x)$ is the solution starting
  from $x$ at time $s$ then it has a modification which is a
  stochastic flow of homeomorphisms. 
\end{theorem}

\begin{proof}
  The proof is based on the method of successive approximations. The
  steps are similar to those used in proving the existence and
  uniqueness of the solution of a usual $SDE$. Set for $t \geq s$ 
  \begin{align*}
    & \phi^0=x\\ 
    & \phi^1_t = x+\int^t_s X(\phi^o_r,dr)\\
    & \cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots
    \cdots\cdots\cdots\cdots\cdots\cdots\cdots\\  
    & \cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots
    \cdots\cdots\cdots\cdots\cdots\cdots\cdots\\ 
    & \phi^n_t = x+\int^t_s X(\phi^{n-1}_r,dr)=x+\int^t_s
    \beta(\phi^{n-1}_r,dr)+\int^t_s Y(\phi^{n-1}_r,dr) 
  \end{align*}\pageoriginale 
\end{proof}

Therefore, we have
\begin{align*}
  E & \left\{\sup _{s \leq r \leq t}|\phi^{n+1}_r-\phi^{n}_r|^2\right\}\\
  \leq & 2 \left\{E \left[\sup_{s < r \leq
      t}|\int^t_s(\beta(\phi^n_r, r) -
    \beta(\phi^{n-1}_r,r))dr|^2\right]\right. \\ 
  &\left. + 4E\left[|\int^t_s Y(\phi^n_r,dr)-\int^t_s
    Y(\phi^{n-1}_r,dr)|^2\right]\right\}\\
  \leq & 2 \left\{E \left[\sup_{s \leq  r \leq
      t}|\int^t_s(\beta(\phi^{n}_{r},r)-\beta(\phi^{n-1}_r,r))dr|^2\right]
  \right.\\ 
  & \left.+4E \left[\int^t_s Tr\left\{\alpha(\phi^n_r,\phi^n_r)-2
    \alpha(\phi^n_r,\phi^{n-1}_r)+\alpha(\phi^{n-1}_r,\phi^{n-1}_r)
    \right\}dr\right]\right\}\\   
  \leq & 2\left\{L^2 TE\left[\sup_{s \leq r \leq t}\int^t_s|\phi^n_r -
    \phi^{n-1}_r|^2 dr\right]+4LE \left[\int^t_s|\phi^n_r - \phi^{n-1}_r|^2
    dr\right] \right\} 
\end{align*}
(where $L$ is the Lipschitz constant associated with the local
characteristics) 
\begin{align*}
&   \leq 2L(LT,4)\int^t_s E\left[\sup_{s \leq r' \leq
      r}|\phi^n_{r'}-\phi^{n-1}_{r'}\right]dr\\ 
 & \leq \{2L(L**4)\}^n \frac{1}{n!}(t-s)^n L(1+|x|)^2, \text{by
    induction}. 
\end{align*}
Hence
$$
\sum^{\infty}_{n=1}E\left\{\sup_{s \leq r \leq
  t}|\phi^{n+1}_{r}-\phi^{n}_{r}|^2\right\}^{1/2}< \infty. 
$$


Therefore $\{\phi_t\}$ converges uniformly in $t$ in $L^2$-sense. Let
$$
\phi_t = \lim_{n \to \infty} \phi^n_t.
$$
Then\pageoriginale $\phi $ is a solution of the equation \eqref{c2:eq2.4.3}. The uniqueness
result is similar. We denote the solution by $\phi_{s,t}(x)$. We shall
now establish the homeomorphism property of the solution. Note that 
$$
M_{s,t}(x)=\phi_{s,t}(x)-x-\int^t_s \beta(\phi_{s,r}(x),r)dr=\int^t_s
Y(\phi_{s,r}(x),dr) 
$$
is an $L^2$-martingale and
$$
< M_{s,t}(x),M_{s,t}(y)^*> = \int^t_s \alpha(\phi_{s,r}(x),\phi_{s,r}(y),r)dr).
$$
Therefore we get
$$
E| \phi_{s,t}(x)-\phi_{s,t}(y)|^p \leq C|x-y|^p,
$$
whence it follows that $\phi_{s,t}(x)$ is continuous in $x$. Next define
\begin{equation*}
  \tilde{\phi}_{s,u}(x)=
  \begin{cases}
    \phi_{s,u}(x) \quad \text{if} \quad s <u <t\\
    \phi_{t,u}(\phi_{s,t}(x)) \quad \text{if} \quad u > t.
  \end{cases}
\end{equation*}
Then $\tilde{\phi}_{s,u}(x)$ is also a solution of \eqref{c2:eq2.4.3} starting
from $x$ at time $s$. Therefore $\tilde{\phi}_{s,u}(x)=\phi_{s,u}(x)$
a.s. Hence $\phi_{s,t}$ has the flow property. Therefore by 
Theorem~\ref{c2:thm2.1.5} $\phi_{s,t}$ is a stochastic flow of homeomorphisms. 

\setcounter{definition}{3}
\begin{definition}\label{c2:def2.4.4}%Def 2.4.4
  $F_{s,t}= \sigma(X(x,u)-X(x,v):s \leq u, v \leq t)$. Then clearly
  $\phi_{s,t}(x)$ is $F_{s,t}$-measurable. 
\end{definition}

\setcounter{coro}{4}
\begin{coro}\label{c2:cor2.4.5}%Corlry 2.4.5
  If $X(x,t)$ is a $C$-valued B.M. then $\phi_{s,t}$ is a Brownian flow.
\end{coro}

\begin{proof}
  For any $0 \leq t_0 < t_1 < \ldots t_n \leq T,F_{t_i,t_{i+1}},i=0,1,
  \ldots,n-1 $ are independent. 
  Hence $\phi_{t_i,t_{i+1}},i=0,1, \ldots,n-1$ are independent. Thus
  $\phi_{s,t}$ is a Brownian flow. 
\end{proof}

\section{Representation of Stochastic Flows by SDES}\label{chap2:sec2.5}%Sec 2.5

In the previous section we have seen that the solution of an $SDE$
defines a stochastic flow of homeomorphisms. Here we discuss the
converse problem, $i.e$. given a\pageoriginale stochastic flow can it be
represented as the stochastic integral $w.r.t$. some semimartingale?
To make it precise let $\phi_{s,t}$ be a stochastic flow satisfying
$(A1) \sim (A3)$ with local characteristics $(a,b)$. 


\medskip
\noindent{\textbf{Problem}}
  To find a continuous $C$-semimartingale $X(x,t)$ such that 
  \begin{equation*}
    \phi_{s,t}(r)=x+ \int^t_s 
X(\phi_{s,r}(x),dr). \tag{2.5.1}\label{c2:eq2.5.1}
  \end{equation*}
  The solution to this problem is very when $a \equiv 0$. Indeed,
  $$
  M_{s,t}(x)=\phi_{s,t}(x)-x-\int^t_s b(\phi_{s,r}(x),r)dr=0.
  $$
  Therefore $\phi_{s,t}$ is the solution of the stochastic ordinary
  differential equation 
  \begin{align*}
    \frac{dx}{dt} & =b(x,t,\omega)\\
    x(s) & = x.
  \end{align*}
  The general case is dealt with in the following theorem.


\begin{theorem}\label{c2:thm2.5.1}%Thm 2.5.1
Let $\phi_{s,t}$ be a stochastic flow satisfying $(A1)\sim (A3)$ with
local characteristics $(a,b)$. Then there exists a unique continuous
$C$-semimartingale $X(x,t)$ satisfying $(A4)'$ such that the
representation \eqref{c2:eq2.5.1} holds. Furthermore the local 
characteristics of $X(x,t)$ coincide with $a,b$.  
\end{theorem}

\setcounter{definition}{1}
\begin{definition}\label{c2:def2.5.2}%Def 2.5.2
  The semimartingale $X(x, t)$ associated with $\phi_{s, t}(x)$ is
  called the infinitesimal generator (or stochastic velocity field) of
  the flow $\phi_{s,t}$. 
\end{definition}


\begin{proofoftheorem}\label{c2:prfthm2.5.1}%Prf of thm 2.5.1
  Set $M(x,t)=M_{0,t}(x)$. Then $M(x,t)$ is a continuous
  $C$-martingale with local characteristic $a
  (\phi_{0,t}(x),\phi_{0,t}(y),t)$,i.e.,  
  \begin{equation*} 
    <M(x,t),M(y,t)^*> = \int^t_o a(\phi_{o,r}(x),\phi_{o,r}(y),r)dr
    \tag{2.5.2}\label{c2:eq2.5.2} 
  \end{equation*}
\end{proofoftheorem}

Next\pageoriginale define
\begin{equation*}
Y(x,t)= \int^t_o M(\phi^{-1}_{o,r}(x),dr). 
\tag{2.5.3}\label{c2:eq2.5.3}
\end{equation*}
Then using Theorem~\ref{c2:thm2.3.4}, we have 
\begin{equation*}
< Y(x,t),Y(y,t)^* >=  \int^t_o a(x,y,r)dr. 
\tag{2.5.4}\label{c2:eq2.5.4}
\end{equation*}
We will show that 
\begin{equation*}
< X(x,t)=Y(x,t) + \int^t_o b(x,r) \, dr. 
\tag{2.5.5}\label{c2:eq2.5.5}
\end{equation*}
is the required semimartingale. Define
\begin{equation*}
  \tilde{M}_{s,t}(x)=  \int^t_s Y( \phi_{s,r}(x),dr). 
\tag{2.5.6}\label{c2:eq2.5.6}
\end{equation*}
Then by Theorem~\ref{c2:thm2.3.4}, we have 
\begin{equation*}
< \tilde{M}_{s,t}(x),\tilde{M}_{s,t}(y)^* > =  \int^t_s a(
\phi_{s,r}(x),\phi_{s,r}(y),r)dr). \tag{2.5.7}\label{c2:eq2.5.7} 
\end{equation*}
Also
\begin{equation*}
< Y(x,t)-Y(x,s),M_{s,t}(y)^* > =  \int^t_s a(x,
\phi_{s,r}(y),r)dr. \tag{2.5.8}\label{c2:eq2.5.8} 
\end{equation*}
Combining \eqref{c2:eq2.5.4} and \eqref{c2:eq2.5.8} we get
\begin{equation*}
  < \tilde{M}_{s,t}(x),M_{s,t}(y)^* > =  \int^t_s a(
  \phi_{s,r}(x),\phi_{s,r}(y),r)dr). \tag{2.5.9} \label{c2:eq2.5.9}
\end{equation*}
Therefore
$$
<M_{s,t}(x)-\tilde{M}_{s,t}(x),(M_{s,t}(x)-\tilde{M}_{s,t}(x))^* >=0. 
$$
Hence
$$
M_{s,t}(x)=\tilde{M}_{s,t}(x).
$$
Thus\pageoriginale 
$$
\phi_{s,t}(x)-x-\int^t_s b(\phi_{s,r}(x),r)dr = \int^t_s Y(\phi_{s,r}(x),dr).
$$
This completes the existence part. Now we will show the uniqueness of
the representation. 

Suppose there exists another continuous $C$-semimartingale $X'(x,t)$
satisfying $(A4)'$ such that 
$$
\phi_{s,t}(x)=x+\int^t_{s-} X'(\phi_{s,r}(x),dr) =x+\int^t_s
X(\phi_{s,r}(x),dr). 
$$
Claim:$X=X'$

Indeed, we can write
$$
X'(x,t)=\int^t_o b'(x,r)dr+Y'(x,t)
$$
where $Y$ is a continuous $C$-martingale. By the uniqueness of
Doob-Mayer decomposition, we have 
$$
\displaylines{\hfill
  \int^t_s b(\phi_{s,r}(x),r)dr = \int^t_s b'(\phi_{s,r}(x),r)dr,\hfill
  \cr 
  \text{and}\hfill 
  \int^t_s Y(\phi_{s,r}(x),dr) = \int^t_s Y'(\phi_{s,r}(x),dr).\hfill}
$$

Let $\alpha^Y, \alpha^{Y'}$ and $\alpha{YY'}$ denote the local
characteristics of $Y,Y'$ and the joint local characteristic of
$(Y,Y')$ respectively. We have for any $s < t, x \in
\mathbb{R}^d$ 
\begin{multline*}
  < \left(\int^t_s Y(\phi_{s,r}(x),dr)-\int^t_s
  Y'(\phi_{s,r}(x),dr)\right),\\
  \left(\int^t_s Y(\phi_{s,r}(x),dr)-\int^t_s
  Y'(\phi_{s,r}(x),dr)\right)^*> \\ 
  =\int^t_s\left[\alpha^Y(\phi_{s,r}(x),\phi_{s,r}(x),r)-\alpha^{YY'}
    (\phi_{s,r}(x),\phi_{s,r}(x),r)\right.  \\
    \left. -\alpha^{YY'}(\phi_{s,r}(x),\phi_{s,r}(x),r)+\alpha^{Y'}
    (\phi_{s,r}(x),\phi_{s,r}(x),r)\right]dr. 
\end{multline*}
This\pageoriginale implies
\begin{multline*}
  \alpha^Y(\phi_{s,t}(x),\phi_{s,t}(x),t)-\alpha^{YY'}
  (\phi_{s,t}(x),\phi_{s,t}(x),t)\\
  -\alpha^{Y'Y}(\phi_{s,t}(x),\phi_{s,t}(x),t)+\alpha^{Y'}
  (\phi_{s,t}(x),\phi_{s,t}(x),t)=0  
\end{multline*}
for almost all $t$. Putting $x = \phi^{-1}_{s,t}(x)$, we get
$$
\alpha^Y(x,x,t)-\alpha^{YY'}(x,x,t)-\alpha^{Y'Y}(x,x,t)+\alpha^{Y'}(x,x,t)=0
~\text{for a.a.t}. 
$$
Hence 
$$
<Y(x,t)-Y'(x,t),(Y(x,t)-Y'(x,t))^* >=0.
$$
Therefore
$$
Y(x,t)=Y'(x,t).
$$

\setcounter{coro}{2}
\begin{coro}\label{c2:coro2.5.3}%Corlry 2.5.3
  If $\phi_{s,t}$ is a Brownian flow then $X(x,t)$ is a C-B.M.
\end{coro}

\begin{proof}
  If $\phi_{s, t}$  is a Brownian flow then the local characteristics
  $a, b$ are deterministic which in turn implies that the local
  characteristic of $Y$ is deterministic. Thus $Y(x,t)$ is a
  $C.B.m$. with local characteristic a. Hence $X(x,t)$ is a C-B.m. 
\end{proof}

In the light of representation of stochastic flows by $SDES$ we
reconsider the existence of a Brownian flow with given local
characteristics $(a, b)$. It was assumed in Theorem~\ref{c1:thm1.2.8} that $a$
and $b$ were twice spatially differentiable and their derivatives
bounded. In the next theorem we will drop these conditions and still
show the existence of a Brownian flow using Theorem~\ref{c2:thm2.5.1}. 

\setcounter{theorem}{3}
\begin{theorem}\label{c2:thm2.5.4}%Thm 2.5.4
  Suppose we are given a pair of functions $(a(x,y,t),\break b(x,t))$ such that
  \begin{enumerate}[\rm (i)]
  \item $a(x,y,t)=(a_{ij}(x,y,t))_{i,j=1,\ldots,d}$ is
    symmetric and nonnegative definite and is continuous in $(x,y,t)$
    and Lipschitz continuous in $x,y$. 
  \item\pageoriginale $b(x,t)=(b^i(x,t))_{i=1,\ldots,d}$ is continuous in
    $x,t$ and Lipschitz continuous in $x$. 
  \end{enumerate}
  Then there exists a unique Brownian flow with local characteristics
  $(a,b)$ satisfying $(A1)\sim(A3)$(of Chapter~\ref{chap1}). 
\end{theorem}

\begin{proof}
  We can find a Gaussian random field $X(x,t, \omega)$with independent
  increments such that the mean of $X(x,t,\omega)$is $\int^t_o
  b(x,r)dr$ and\break $Cov(X(x,t)$, $X(y,s))=\int^{t \Lambda
    s}_{o}a(x,y,r)dr$. 
  Also $X(x,t)$ has no fixed discontinuity. Therefore $X(x,t)$ is a
  Brownian motion for each $x$. Since $a(x,y,t)$ is Lipschitz continuous
  in $x,y,X(x,t)$ has a modification such that it is continuous in
  $(x,t)$. Therefore $X(x,t)$ is a $C-B.m$. Now consider the $SDE$ based
  on $X(x,t)$ 
  $$
  d \phi_t =X(\phi_t,dt),\phi_s =x, t \geq s.
  $$
  The solution of the $SDE$ $\phi_{s,t}(x)$ is a Brownian flow of
  homeomorphisms with local characteristics $(a,b)$. This completes the
  existence part. 
\end{proof}

\medskip
\noindent{\textbf{uniqueness.}}
  Let $\phi_{s,t}$ be a Brownian flow with local characteristics
  $(a,b)$ satisfying $(A1)\sim (A3)$. Then there exists an
  infinitesimal generator of $\phi_{s,t}$, say $X(x,t)$, which is a
  $C-B.m$. with local characteristics $(a,b)$. The law of $X(x,t)$ is
  uniquely determined by $(a,b)$. Hence the law $\phi_{s,t}$ is
  unique. 
 
\setcounter{remark}{4}
\begin{remark}\label{c2:rem2.5.5}%Remk 2.5.5
  The law of a non-Brownian flow is not in general determined by its
  local characteristics. Similarly the law of a $C$- semimartingale is
  not determined by its local characteristics.  To justify this we
  produce a counterexample.  Let $B^1_t$, $B^2_t$ be two independent
  one-dimensional Brownian motions.  Set 
  $$
  Y_t = \int^t_o B^1_s dB^1_s, Z_t = \int^t_o B^1_s dB^2_s.
  $$
  Here $Y_t$ and $Z_t$ have the same local characteristics,
  viz. $(B^1_t)^2$, but the laws of $Y_t$ and $Z_t$ are different. 
\end{remark} 


\begin{remark}\label{c2:rem2.5.6}\pageoriginale%Remk 2.5.6
  We know that 2-point process determines the law of a Brownian
  flow.  But 1-point process does not determine it.  In fact one can
  find several Brownian flows with the same 1-point process.  Here
  we give an example which is due to Harris \cite{9}. 
\end{remark}


\begin{example*}%Exmple
  Let $x \in \mathbb{R}^1$ and $c(x)$ a real, nonnegative
  definite function of class $C^2_b$ and $c(0) = 1$.  Set $a(x,y) =
  c(x-y)$.  Then $a(x,y)$ is symmetric and nonnegative definite and is
  Lipschitz continuous.  Therefore there exists a stochastic flow of
  homeomorphisms $\phi_{s,t}$ with local characteristic $a$.  Consider
  the one-point process $\phi_{s,t}(x)$ $s,x$ fixed). This is a
  diffusion with infinitesimal generator
  $L=\dfrac{1}{2}\dfrac{d^2}{dx^2}$. Therefore $\phi_{s,t}(x)t
  \in[s,T]$, is a Brownian motion for any $x$.  There may be
  many such $c(x)$. Simplest Brownian flow would be $\phi_{s,t}(x) =
  x+B_t (B_t: $ one-dimensional B.M). In this case $c(x) \equiv 1$. 
\end{example*}

To end this section it would perhaps be of some relevance to discuss
the following problem.  We know that the infinitesimal generator of
1-point motion of a Brownian flow is an elliptic operator.  Now
given an elliptic operator 
$$
L_t= \frac{1}{2}\sum^d_{i,j=1}a_{ij}(x,t)\frac{\partial^2}{\partial
  x_j\partial x_j}+ \sum^d_{i=1}b^i (x,t)\frac{\partial}{\partial x_i} 
$$
(where $(a_{ij})$ is nonnegative definite, symmetric and is continuous
in $x$), does there exists a Brownian flow whose 1-point motion has
the infinitesimal generator $L_t$? The problem is reduced to finding
$(a_{ij}(x,y,t))$ which should
be nonnegative definite and symmetric with some smoothness condition
such that $a_{ij}(x,x,t)=a_{ij}(x,t)$. 

If $a_{ij}$ is independent of $t$ and $a_{ij}(x)$ is $C^2_b$ then we
can find such $a(x,y)$ (see Ikeda-Watanabe [13]). Indeed there exists a
square root of $a =(a_{ij})$ i.e., $a(x)=\sigma(x)\sigma(x)^*$ where
$\sigma(x)$ is Lipschitz continuous. We then define $a(x,y) = \sigma(x)
\sigma(y)$ which\pageoriginale is symmetric and nonnegative definite. Finally the
solution $\phi_{s,t}$ of 
$$
d \phi_t = b(\phi_t)dt+\sigma(\phi_t)dB_t
$$
is a Brownian flow with local characteristic $a(x,y)$. The solution to
the problem in the general case is not known. 

\section{Inverse Flows and Infinitesimal Generator}\label{chap2:sec2.6}%Sec 2.6

Let $\phi_{s,t}$ be a stochastic flow of homeomorphisms generated by a
continuous $C$-semimartingale $X(x,t)$ with local characteristics $a,b$
which are Lipschitz continuous and have linear growth. In other words
$\phi_{s,t}$ satisfies the following $SDE$ 
\begin{equation*}
  d\phi_{s,t}(x)=X(\phi_{s,t}(x),dt),
\phi_{s,s}(x)=x. \tag{2.6.1}\label{c2:eq2.6.1}
\end{equation*}
Let $\Psi _{s,t} \equiv \phi^{-1}_{s,t}$, the inverse of
$\phi_{s,t}$. Then obviously $\Psi_{s,t} o \Psi_{t,u} \equiv \Psi_{s,u}$
for $s < t < u$. $\Psi_{s,t}$ is called a backward flow. In this
section, we take up the following problem.  



\medskip
\noindent{\textbf{Problem}}
  To find the backward infinitesimal generator of  $\Psi_{s,t}$.
 
 In other words we want to represent $\Psi_{s,t}$ in the form
 \eqref{c2:eq2.6.1}. To accomplish this we have to define backward
 semimartingales and backward integrals. 
 
 Let $X(x,t)$ be a continuous $C$-semimartingale given by 
 \begin{equation*}
X(x,t)=Y(x,t)+\int^t_o b(x,r)dr \tag{2.6.2}\label{c2:eq2.6.2}
 \end{equation*} 
where $Y(x,t)$ is a continuous $C$-martingale. Set
$$
G_{s,t}=\sigma(Y(., u)-Y(.,v):s \leq u, v \leq t).
$$
For fixed $s$, $X(x,t)-X(x,s)$, $t \in [s,T]$ is a
$G_{s,t}$-semimartingale but for fixed $t$ $X(x,s)-X(x,t)$, $s \in
[0,t]$\pageoriginale is adapted to $G_{s,t}$ but need not be a semimartingale. So
here we make such an assumption. 

(A5)~ For any fixed $t$, $Y(., s)-Y(., t)$, $s \in[0,t]$, is a
backward martingale adapted to $G_{s,t},i.e$. for $s< u < t$ 
$$
E[Y(x,s)-Y(x,t)|G_{u,t}]=Y(x,u)-Y(x,t).
$$

\begin{remark}\label{c2:rem2.6.1}%Remk 2.6.1
  Under (A.5) $X(x,t)$ has the same local characteristics $(a,b)$.
\end{remark}

\setcounter{example}{1}
\begin{example}\label{c2:exam2.6.2}%Exmp 2.6.2
  \begin{enumerate}[(1)]
  \item Let $Y(x,t)$ be a $C-B.m$. and $b(x,t)$ independent of $Y$. Then
    $$
    X(x,t)=Y(x,t)+\int^t_o b(x,r)dr
    $$
    is a backward semimartingale.

  \item Let $Y(x,z,t)$ be a $C-B.m$. with parameter $z \in S$
    and $z(t)~S$-valued stochastic process independent of $Y$. Then  
    $$
    \tilde{Y}(x,t)\equiv \int^t_o Y(x,z(r),dr)
    $$
    is a backward integral as follows;
    \begin{equation*}
      \int^t_s X(f_r,d \hat{r}) = \lim\limits_{|\Delta|\to
        0} \sum\limits^{n-1}_{k=0} \{X(f_{t_{k+1}}, t_{k+1}) -
      X(f_{t_{k+1}}, t_k)\}\tag{2.6.3}\label{c2:eq2.6.3} 
    \end{equation*}
    where $\Delta=\{s=t_0 < t_1 < \ldots < t_n =t\}$
  \end{enumerate}
\end{example}


\setcounter{theorem}{2}
\begin{theorem}\label{c2:thm2.6.3}%Thm 2.6.3
  Let $\phi_{s,t}$ be a stochastic flow of homeomorphisms generated by
  a continuous $C$-semimartingale $X(x,t)$ satisfying (A.5) with local
  characteristics $a,b$ satisfying the Lipschitz continuity and linear
  growth properties. Suppose that 
  $$
  d(x, t) = \sum_j \frac{\partial}{\partial x_j} a_j (x, y, t)|_{y=x}
  $$\pageoriginale
  exists and is of linear growth, then $\psi_{s, t} = \Phi_{s,t}^{-1}$
  is a continuous backward stochastic flow generated by $-X (x, t) +
  \int ^t_0 d(x, r) dr$, i.e.,  
  \begin{equation*}
    \psi_{s,t}(y) = y - \int_s^t X (\psi_{r,t}(y), d\hat{r}) + \int^t_s
    d(\psi_{r,t}(y),r)dr. \tag{2.6.4}\label{c2:eq2.6.4} 
  \end{equation*}
\end{theorem}

\begin{proof}%proof
  We have
  $$
  \Phi_{s,t}(x) = x + \int_s^t X (\Phi_{s,r}(x), dr). 
  $$
\end{proof}

Putting $x = \psi_{s, t}(y)$, we have
\begin{equation*}
  y = \psi_{s, t}(y) + \int_s^t X(\Phi_{s, r}(x), dr)|_{x= \psi_{s,t}
    (y)}. \tag{2.6.5}\label{c2:eq2.6.5} 
\end{equation*}

We shall now compute $\int_s^t X(\Phi_{s, r}(x), dr)|_{x =
  \psi_{s,t}(y)}$. Let $\Delta = \{ s = t_0, <t_1 < \cdots < t_n = t
\}$. Then  
\begin{align*}
  \int_s^t X & (\Phi_{s, r}(x), dr)|_{x = \psi_{s, t}(y)}\\ 
  & =
  \lim_{|\Delta|\rightarrow 0} \sum_{k = 0}^{n-1} \left\{X(\Phi_{s,t_k}(x),
  t_{k+1}) - X(\Phi_{s, t_k}(x), t_k)\right\}|_{x = \psi_{s, t}(y)}\\ 
  & = \lim_{|\Delta| \rightarrow 0} \sum_{k =0}^{n-1} \left\{X(\psi_{t_k,
    t} (y), t_{k+1} -X(\psi_{t_k, t}
(y), t_k))\right\}\tag{2.6.6}\label{c2:eq2.6.6}
\end{align*}

Now
\begin{align*}
  \sum_{k=0}^{n-1} & \left\{X(\psi_{t_k,t}(y), t_{k+1})  
  -X(\psi_{t_k,t}(y), t_k)\right\}\\ 
  & =\sum_{k=0}^{n-1} \left[X(\psi_{t_{k+1}, t}(y),
    t_{k+1})-X(\psi_{t_{k+1},t}(y), t_k)\right] \\
  &\quad -\sum_{k=0}^{n-1} \left[\left\{X(  \psi_{t_{k+1},t}(y),
    t_{k+1})-X(\psi_{t_{k},t}(y), t_{k+1})\right\}\right.\\ 
   & \qquad \qquad-\left. \left\{  X(\psi_{t_{k+1},t}(y),
    t_{k})-X(\psi_{t_{k},t}(y), t_{k})\right\}\right]\\ 
    &= I_1^{\Delta} + I_2^{\Delta}, \quad say.
\end{align*}

By\pageoriginale definition
$$
I_1^{\Delta} \rightarrow \int_s^t X(\psi_{r, t}(y), \hat{dr}).
$$

Using mean value theorem, we have
\begin{align*}
  I_2^{\Delta} = I_2^{\Delta}(y) & = - \sum_k \sum_i \left\{
  \frac{\partial}{\partial x_i} X(\psi_{t_k, t}(y), t_{k+1})
  -\frac{\partial}{\partial x_i} X(\psi_{t_k,  t}(y), t_k) \right\}\\ 
  & \hspace{5cm} \times (\psi^i_{t_{k+1}, t}(y)- \psi^i_{t_k, t}(y))\\
  & - \frac{1}{2} \sum_{i, j, k} \xi^{\Delta}_{i, j,
    k}(\psi^i_{t_{k+1}, t}(y) - \psi^i_{t_k, t}(y)) (\psi^j_{t_{k+1},
    t}(y) - \psi^j_{t_k, t}(y))\\ 
  & = J^{\Delta}_1 + J^{\Delta}_2, \quad \text{say}.
\end{align*}
where $\xi^{\Delta}_{i, j, k}$ is a random variable given by
$$
\xi^{\Delta}_{i, j, k} = \frac{\partial^2}{\partial x_j \partial x_j} 
X(\eta_k, t_{k+1}) - \frac{\partial^2}{\partial x_j \partial x_j}
X(\zeta_k, t_K) 
$$
where $|\eta_k - \psi_{t_k, t}| \le |\psi_{t_{k+1, t}} - \psi_{t_k, t}|,
|\zeta_k - \psi_{t_{k, t}}|\le |\psi_{t_{k+1, t}} - \psi_{t_{k, t}}|$.
Hence \quad  $\sup\limits_k |\xi^{\Delta}_{i, j, k}| \rightarrow 0$
\quad as \quad $|\Delta|\rightarrow 0$. \quad  Thus \quad
$J^{\Delta}_2 \rightarrow 0$ \quad as $\quad   |\Delta|\rightarrow 0
$\quad and 
\begin{align*}
   J^{\Delta}_1 (\Phi_{s, t} (x)) &= - \sum _i \sum_k
   \left\{\frac{\partial} {\partial x_i} X(\Phi_{s, t_k}(x), t_{k+1}) -
  \frac{\partial}{\partial x_i} X(\Phi_{s, t_k} (x), t_k) \right\}. \\  
  & \times (\Phi^i_{s, t_{k+1}} (x) - \Phi^i_{s, t_{k}}(x))\\
  &\xrightarrow[|\Delta| \rightarrow 0]{} - \sum_i <\int_s^t
  \frac{\partial}{\partial x_i} X(\Phi_{s, r}(x), dr), \Phi^i_{s,
    t}(x)- x^i>\\ 
  & = - \sum_i <\int_s^t \frac{\partial}{\partial x_i} X(\Phi_{s,
    r}(x), dr), \int_s^t X^i (\Phi_{s, r}(x), dr)>\\ 
  & = - \sum_i \frac{\partial}{\partial x_i} \int_s^t a_i(\Phi_{s,
    r}(x), \Phi_{s, r}(x), r) dr\\ 
  & = - \int_s^t d(\Phi_{s, r} (x), r) dr.
\end{align*}

Therefore\pageoriginale
\begin{multline*}
  J^{\Delta}_1 (y){ \xrightarrow[|\Delta| \rightarrow 0]{}} 0-
  \int_s^t d(\Phi_{s,r}(x), r)dr |_{x = \psi_{s, t}(y)} 
  = - \int_s^t d(\psi_{r, t} (y), r)dr.
\end{multline*}

Hence
$$
I_2^{\Delta} \xrightarrow[|\Delta| \rightarrow 0]{} - \int_s^t
d(\psi_{r, t} (y), r) dr. 
$$

Combining all these results, we get
$$
y = \psi_{s,t}(y)+ \int_s^t X(\psi_{r,t}(y), \hat{dr} ) - \int_s^t 
d(\psi_{r, t}(y), r)dr. 
$$




\section{Appendix}\label{chap2:app2.7}%sec 2.7

\section*{Generalized ITO Formula, Stratonovich Integral\hfil\break and
  Stratonovich Stochastic Differential Equations}

Let $X(x, t)$ be a one-dimensional continuous random field. It is said
to be a continuous $C^k$-process if it is $k$-times continuously
differentiable in x a.s. and $D^{\alpha} X (x,t)$ is continuous in $(x,
t) $ a.s. for 
$|\alpha| \le k$. It is called a continuous $C^k$-martingale if
$D^{\alpha}X(x, t)$  is martingale for any $x, |\alpha|\le k$ and a
continuous $C^k$-process of bounded variation if  $D^{\alpha}X(x, t)$
is a process bounded variation for each $x$ and $|\alpha| \le
k$. Finally $X(x, t)$ is said to be a continuous $C^k$-semimartingale
if $X(x, t) = Y(x, t) + V(x, t)$, where $Y(x, t)$ is a continuous
$C^k$-martingale and $V(x,t)$ a continuous $C^k$-process of bounded
variation. Let $(a(x, y, t), b (x, t))$ be the local characteristics
of $X(x, t)$. We make the following assumptions. $(A4)'_k$ $a(x, y, t)$
and $b(x, t)$ are k-times continuously differentiable in $x$ and 
$y$\pageoriginale
(respectively $x$), and for $|\alpha|, |\beta| \le k$ and for any $K >
0$ 
\begin{align*}
  & \int_o^t  \sup_{|x|, |y| \le K} |D^{\alpha}_x D^{\beta}_y a(x, y,
  r)| dr < \infty & a.s.\\ 
  &  \int_o^t  \sup_{|x| \le K} |D^{\alpha}_x b(x, r)| dr < \infty & a.s.
\end{align*}
We shall now present a differential rule for the composition of two
processes, which is a generalization of the well known Ito formula. 

\begin{theorem}[Generalized Ito Formula I]\label{c2:thm2.7.1}%thm 2.7.1
  Let $F(x, t)$ be a one-dimen\-sional $C^2$- process and a
  $C^1$-semimartingale with local characteristics satisfying $(A4)_1$
  and $X_t$ an $\mathbb{R}^d$-valued continuous semimartingale. Then 
  \begin{align*} 
    F(X_t, t)  = & F(X_o, 0) + \int_o^t F(X_t, dr) + \sum_{i = 1}^d
    \int_o^t \left(\frac{\partial}{\partial x_i} F\right) (X_r, r) dX_r^i\\ 
    & + \sum_{i = 1}^d <\int_o^t \left(\frac{\partial}{\partial x_i}
    F\right)(X_r, dr), X_t^i>\\ 
    & +  \frac{1}{2} \sum_{i, j=1}^d \int_o^t \frac{\partial^2
      F}{\partial x_i \partial x_j} (X_r, r)d <X^i_r,
    X^j_r>. \tag{2.7.1}\label{c2:eq2.7.1} 
  \end{align*} 
\end{theorem}


\begin{proof}%proof
  For a partition $\Delta = \{ 0 = t_o < t_1 < \cdots < t_n = t \}$, set
  \begin{align*}
    F(X_t, t) - F(X_o, 0) & = \sum_{k=0}^{n-1} \left\{ F(X_{t_k}, t_{k+1})
    - F(X_{t_K}, t_k) \right\}\\ 
    &\hspace{1cm} +  \sum_{k=0}^{n-1} \left\{ F(X_{t_{k+}}, t_{k+1}) - F(X_{t_K},
    t_{k+}) \right\}\\
    &= I_1^{\Delta} + I_2^{\Delta}, say.
\end{align*}
\end{proof}

We have by the definition of the stochastic integral 
$$
I_1^{\Delta}\xrightarrow[|\Delta| \rightarrow0] {} \int_o^t F(X_r, dr).
$$
The\pageoriginale second term is written as 
\begin{align*}
  I_2^{\Delta} = &\sum_{k=0}^{n-1} \sum_{i=1}^{d} \left\{
  \frac{\partial}{\partial x_i} F(X_{t_k}, t_{k+1}) -
  \frac{\partial}{\partial x_i} F(X_{t_k}, t_k)  \right\} (X^i_{t_{k+1}} -
  X^i_{t_k})\\ 
  &\quad + \sum_{k=0}^{n-1}  \sum_{i=1}^{d}   \frac{\partial}{\partial x_i}
  F(X_{t_k}, t_k)   (X^i_{t_{k+1}} - X^i_{t_k})\\ 
  & \quad + \frac{1}{2} \sum_{i,j=1}^{d}  \sum_{k=0}^{n-1}
  \frac{\partial^2}{\partial x_i \partial x_j} F(\xi_k, t_{k+1})
  (X^i_{t_{k+1}} - X^i_{t_k})  (X^j_{t_{k+1}} - X^j_{t_k})\\ 
  & =J_1^{\Delta} + J_2^{\Delta} + J_3^{\Delta},
\end{align*}
where $|\xi_k - X_{t_k}| \le |X_{t_{k+1}} - X_{t_k}|$. Set
$$
\displaylines{\hfill
  L_s^{\Delta} = \sum_{k=0}^{n-1} \left\{ \frac{\partial}{\partial x_i}
  F(X_{t_k \Lambda s}, t_{k+1} \Lambda s)- \frac{\partial}{\partial x_i}
  F(X_{t_k} \Lambda s) \right\} \hfill \cr
  \text{and}\hfill 
  L_s = \int_o^s \frac{\partial}{\partial x_i} F(X_r, dr).\hfill }
$$

Then $L_s^{\Delta}$ converge to $L_s$ uniformly in $s$ in probability
as $|\Delta|\to 0$. On the other hand, 
$$
\sum_{k=0}^{n-1} (L_{t_{k+1}} - L_{t_k}) (X^i_{t_{k+1}}-  X^i_{t_K}) 
\xrightarrow[|\Delta| \rightarrow0] {} <L, X^i_t> \text{in
  probability}. 
$$

These two facts imply
$$
\sum_{k=0}^{n-1} (L_{t_{k+1}}^{\Delta} - L_{t_k}^{\Delta})
(X^i_{t_{k+1}} - X^i_{t_k}) \xrightarrow[|\Delta| \rightarrow0] {} <L,
X^i>_t \text{in probability}. 
$$

Hence
$$
J_1^{\Delta} \xrightarrow[|\Delta| \rightarrow0] {}   \sum_{i=1}^d
<\int_o^t \frac{\partial}{\partial x_i} F(X_r, dr), X^i_t>. 
$$

Also
$$
J_2^{\Delta} \xrightarrow[|\Delta| \rightarrow 0] {} \sum_{i=1}^d
\int_o^t \frac{\partial}{\partial x_i} F(X_r, dr),dX^i_r. 
$$

Set\pageoriginale
$$
\tilde{J}_3^{\Delta} = \frac{1}{2} \sum_{i,j = 1}^d  \sum_{k=0}^{n-1}
\frac{\partial^2}{\partial x_i \partial x_j} F(X_{t_k}, t_k)
(X^i_{t_{k+1}}  - X^i_{t_{k}} ) (X^j_{t_{k+1}}  - X^j_{t_{k}} ). 
$$

Then
$$
\tilde{J}_3^{\Delta} \xrightarrow[|\Delta| \rightarrow 0]{}
\frac{1}{2} \sum_{i,j=1}^d <\int_o^t \frac{\partial^2}{\partial x_i
  \partial x_j} F(X_r, r),dX^i_r, X^j_t>.  
$$

Now set
$$
\xi^{\Delta}_{ijk} =  \frac{\partial^2}{\partial x_i \partial x_j}
F(\xi_k, t_{k+1}) - \frac{\partial^2}{\partial x_i \partial x_j}
F(X_{t_k}, t_k).  
$$

Then
$$
\sup_{i,j,k} |\xi_{ijk}^{\Delta}| \to 0 \quad as \quad |\Delta| \to 0. 
$$

Note that
$$ 
|J_3^{\Delta}- \tilde{J}_3^{\Delta}| \le \frac{1}{2} \sum_{i,j}
\sup_{i,j,k} |\xi_{ijk}^{\Delta}| \left\{ \sum_k (X^i_{t_{k+1}} -
X^i_{t_k})^2 \right\}^{\frac{1}{2}} \times  \left\{ \sum_k (X^i_{t_{k+1}} -
X^i_{t_{k}})^2 \right\}^{\frac{1}{2}}. 
$$

Since $\sum_k (X^{\ell}_{t_{k+1}} - X^{\ell}_{t_k} )^2$ tends to a
finite value in probability as $|\Delta| \to 0$, the above converges
to zero in probability. Hence 
$$
J_3^{\Delta} \xrightarrow[|\Delta| \rightarrow 0]{} \frac{1}{2} 
\sum_{i,j} \int_o^t \frac{\partial^2}{\partial x_i \partial x_j}
F(X_r, r) \, d<X^i, X^j>_r. 
$$

Combining all these results, the desired formula \eqref{c2:eq2.7.1} follows.

\setcounter{remark}{1}
\begin{remark}\label{c2:rem2.7.2}%rem 2.7.2
  If $F(x, t)$ is a deterministic function twice continuously
  differentiable in $x$ and once continuously differentiable in $t$,
  then the last but one term in the right hand side of \eqref{c2:eq2.7.1}
  vanishes. This corresponds to the well known Ito formula. 
\end{remark}


\section*{Stratonovich Integral}\pageoriginale

Let $X(x, t)$ be a continuous $C^2$-valued process and a continuous
$C^1$-semimartingale. Let $f_t$ be an $\mathbb{R}^d$-valued continuous
semimartingale. We shall define the Stratonovich Integral of $f_t$
based on $X(x, t)$. For a partition $\Delta = \{0 =t_o < t_1 < \cdots <
t_n = t\}$ set
\begin{equation*}
  K_t^{\Delta} = \sum_{k=0}^{n-1} \frac{1}{2} \left\{ (X(f_{t_{k+1}},
  t_{k+1}) - X(f_{t_{k+1}}, t_k)) + (X(f_{t_k}, t_{k+1})- X(f_{t_k},
  t_k)) \right\}\tag{2.7.2}\label{c2:eq2.7.2} 
\end{equation*}

\setcounter{Lemma}{2}
\begin{Lemma}\label{c2:lem2.7.3}%lemma 2.7.3
  Suppose that the local characteristics of $X$ and
  $\dfrac{\partial}{\partial x_i}X$ satisfy $(A4)'_1$. Then $\lim
  \limits_{|\Delta|\to 0} K_t^{\Delta}$ exists and  
  \begin{equation*}
    \lim_{|\Delta|\to 0} K_t^{\Delta} = \int_o^t X(f_r, dr) +
    \frac{1}{2} \sum_j <\int_o^t \left( \frac{\partial}{\partial
      x_j}X\right)(f_r, dr), f^j_t>. \tag{2.7.3}\label{c2:eq2.7.3} 
  \end{equation*}
\end{Lemma}

\noindent \textit{Proof.}%proof
Rewrite $K_t^{\Delta}$ as 
\begin{align*}
  K_t^{\Delta} & = \sum_{k=0}^{n-1} \left\{ X(f_k, t_{k+1})- X(f_{t_k}, t_k)
  \right\}\\ 
  & + \frac{1}{2} \sum_{k=0}^{n-1} \left\{ ( X(f_{t_{k+1}}, t_{k+1})-
  X(f_{t_k}, t_{k+1})) -  (X(f_{t_{k+1}}, t_{k})- X(f_{t_k}, t_k))\right\}
  \\ 
  & = L_t^{\Delta} + M_t^{\Delta}, \quad \text{say}\tag*{$\Box$}
\end{align*}

We have 
$$
L_t^{\Delta} \xrightarrow[|\Delta| \rightarrow 0]{} \int_o^t X(f_r, dr). 
$$

By the mean value theorem,
\begin{multline*}
  M_t^{\Delta} = \frac{1}{2} \sum_j \sum_k  \left\{ \frac{\partial}{\partial
    x_j} X(f_{t_k}, t_{k+1}) - \frac{\partial}{\partial x_j} X(f_{t_k},
  t_k) \right\}\\ 
  \left(f^j_{t_k+1} - f^j _{t_k}\right) + \frac {1}{4} \sum_{i,j,k}
  \xi_{ijk}^{\Delta} \left(f^i_{t_k+1} - f^i _{t_k}\right) \left(f^j_{t_k+1} - ^j
  _{t_k}\right). 
\end{multline*}
where\pageoriginale $\sup \limits_k |\xi_{ijk}^{\Delta}| \to 0$ as $|\Delta| \to
0$. Then as in the proof of generalized Ito formula we get 
$$
M_t^{\Delta} \xrightarrow[|\Delta| \rightarrow 0]{} \frac{1}{2}\sum_j
<\int_o^t   \left(\frac{\partial}{\partial x_j} X\right)(f_r, dr), f_t^j>.  
$$

The lemma is thus proved.

\setcounter{definition}{3}
\begin{definition}\label{c2:def2.7.4}%def 2.7.4
  The limit of $K_t^{\Delta}$ as $|\Delta| \to 0$ is called the
  Stratonovich Integral of $f_t$ based on $X(x,t)$ and is written as
  $\int_o^t X(f_r, odr)$. 
\end{definition}

\setcounter{proposition}{4}
\begin{proposition}\label{c2:prop2.7.5}%proposition 2.7.5
  Let $X(x, t)$ be a continuous $C^2$-process and a $C^1$-
  semimartingale. Let the local characteristics of $X$ and
  $\dfrac{\partial}{\partial x_j} X$ satisfy $(A4)'_1$. Let $f_t$ be
  an $\mathbb{R}^d$-valued continuous semimartingale. Then the Stratonovich
  integral is well defined and is related to Ito integral by  
  \begin{equation*}
    \int_o^t X(f_r, odr) = \int_o^t X(f_r, dr) + \frac{1}{2}
    \sum_{j=1}^d <\int_o^t \left(\frac{\partial}{\partial x_j} X\right) (f_r,
    dr), f_t^j>. \tag{2.7.4}\label{c2:eq2.7.4} 
  \end{equation*}
\end{proposition}

\setcounter{theorem}{5}
\begin{theorem}[Generalized Ito Formula II]\label{c2:thm2.7.6}% thm 2.7.6
  Let $ F(x, t)$ be a continuous $C^3$-process and
  $C^2$-semimartingale with local characteristics $(\alpha,
  \beta)$. Suppose that these are twice continuously differentiable
  and both $\alpha, \beta$ and their derivatives satisfy
  $(A4)'_1$. Let $X_t$ be a continuous semimartingale. Then we have 
  \begin{equation*}
    F(X_t, t) - F(X_o, 0) = \int_o^t F(X_r, odr) + \sum_i \int_o^t
    \left(\frac{\partial}{\partial x_j} 
F\right) (X_r, r) odX_r^j. \tag{2.7.5}\label{c2:eq2.7.5} 
  \end{equation*}
\end{theorem}

\begin{proof}%proof
  Rewrite the right hand side of \ref{c2:eq2.7.5} using Ito integral. By the
  generalized Ito formula I, we have 
  \begin{align*}
    \frac{\partial}{\partial x_j} F(X_t, t) & - \frac{\partial}{\partial
      x_j} F(X_o, 0)\\ 
     & = \int_o^t \frac{\partial}{\partial x_j}  F(X_r, dr) + \sum_i
    \int_o^t \frac{\partial^2}{\partial x_i \partial x_j} F(X_r, r)
    dX_r^j\\ 
    &\quad  + \sum_i  <\int_o^t \frac{\partial^2}{\partial x_i \partial x_j}
    F(X_r, dr), \,  X_t^i>\\ 
    & \qquad + \frac{1}{2} \sum_{i,k} \frac{\partial^3}{\partial x_i \partial x_j
      \partial x_k} F(X_r, r) d<X_r^i, X_r^k>. 
  \end{align*}\pageoriginale
  Therefore
  \begin{align*}
    \int_o^t \left(\frac{\partial}{\partial x_j}F\right) &(X_r, r)
    odX_r^j = \int_o^t \frac{\partial}{\partial x_j} F(X_r, r) dX_r^j +
    \frac{1}{2}<\frac{\partial}{\partial x_j}F(X_t, t), X_t^j >\\ 
    & = \int_o^t \frac{\partial}{\partial x_j}F(X_r,  r) dX_r^j +
    \frac{1}{2} <\int_o^t \frac{\partial}{\partial x_j} F(X_r, dr),
    X_t^j >\\   
    & \qquad + \frac{1}{2} \sum_i \int_o^t \frac{\partial^2}{\partial
      x_i \partial x_j} F(X_r, r)d <X_r^i, X_r^j>. \tag{2.7.6}\label{c2:eq2.7.6}   
  \end{align*}
  On other hand
  \begin{equation*}
    \int_o^t F(X_r, odr) = \int_o^t F(X_r, dr) + \frac{1}{2} \sum_i
    <\int_o^t \left(\frac{\partial}{\partial x_i}F\right) (X_r, dr),
    X_t^i>. \tag{2.7.7}\label{c2:eq2.7.7} 
  \end{equation*}
  Finally combining \eqref{c2:eq2.7.1}, \eqref{c2:eq2.7.6} and 
\eqref{c2:eq2.7.7} we get \eqref{c2:eq2.7.5}.
\end{proof}

\section*{Stratonovich Stochastic Differential Equation}

Let $X(x,t)$ be a continuous $C^2$-process and a $C^1$-semimartingale
with local characteristics $(a,b)$ satisfying $(A4)'_1$. A continuous
$\mathbb{R}^D$-valued semimartingale $\Phi_t$ is called a solution of
the Stratonovich Stochastic Differential equation 
\begin{equation*}
  d \phi_t = X(\Phi_t, odt) \tag{2.7.8}\label{c2:eq2.7.8}
\end{equation*}
starting from $x$ at time  $s$ if it satisfies
\begin{multline*}
 \phi_t = X+ \int_s^t X(\Phi_r, odr) \tag{2.7.9}\label{c2:eq2.7.9}\\
 = x + \int^t_s ~ X(\phi_r,dr) + \frac{1}{2} ~ \sum_j < \int^t_s ~
 \frac{\partial}{\partial ~ x_j} ~ X (\phi_r,dr),\phi^j_t >. 
\end{multline*}\pageoriginale

Note that 
\begin{align*}
  \sum_j \frac{1}{2} & < \int^t_s \frac{\partial}{\partial x_j} X^i
  (\phi_r,dr),\phi^j_t >\\ 
  & = \sum_j \frac{1}{2} < \int^t_s
  \frac{\partial}{\partial x_j}  X^i (\phi_r,dr),\int^t_s X^j
  (\phi_r,dr) >\\ 
  & = \frac{1}{2} \sum_j \int^t_s \frac{\partial}{\partial x_j} a_{ij}
  (x,y,r) |_{x=y=\phi_r} ~ dr. 
\end{align*}

Therefore, setting
\begin{equation*}
  d^i(x,t) = \sum_j \frac{\partial}{\partial x_j} ~
  a_{ij}(x,y,t)|_{y=x},\tag{2.7.10}\label{c2:eq2.7.10} 
\end{equation*} 
we see that if a solution $\phi_t$ of \eqref{c2:eq2.7.8} exists then it satisfies
\begin{equation*}
  \phi_t = x + \int^t_s ~ X(\phi_r,dr) + \frac{1}{2} \int^t_s ~
  d(\phi_r,r) ~ dr.\tag{2.7.11}\label{c2:eq2.7.11} 
\end{equation*}

Consequently we have the following theorem.


\begin{theorem}\label{c2:thm2.7.7}% theorem 2.7.7
  Let $X(x,t)$ be a continuous $C^2$-process and a
  $C^1$-semi\-martingale with local characteristics $a,b$ which are
  continuously differentiable in $x,y$ and their derivatives are
  bounded. Suppose further that $d(x,t)$ defined in \eqref{c2:eq2.7.10} is
  Lipschitz continuous. Then the Stratonovich SDE~\eqref{c2:eq2.7.8} has a
  unique solution and it defines a stochastic flow of
  homeomorphisms. Furthermore if $a,b,d$ are k-times continuously
  differentiable and their derivatives bounded then the solution
  defines a stochastic flow of $C^{k-1}$ diffeomorphisms. 
\end{theorem}



\chapter{Limit Theorems for Stochastic Flows}\label{chap3}%chap 3

This\pageoriginale Chapter is devoted to the study of limit theorems for stochastic
flows. In section~\ref{chap3:sec3.1}, we introduce the notion of weak and strong
convergence of stochastic flows. In section~\ref{chap3:sec3.2} we discuss the
convergence of random ordinary differential equations to a diffusion
process. We state a theorem in this regard and elucidate it with
various examples. In section~\ref{chap3:sec3.3} we state the main limit
theorem. The proof of the theorem is very long. We develop it in the
subsequent sections. In section~\ref{chap3:sec3.4} we discuss the tightness of
$(N+M)$-point processes and in the next section the weak convergence
of $(N+M)$-point process is dealt with. In section~\ref{chap3:sec3.6} we describe
the tightness of Sobolev space-valued processes. We conclude the proof
of the main limit theorem in section~\ref{chap3:sec3.7}. In section~\ref{chap3:sec3.8} we
complete the proof of the approximation theorem stated in 
section~\ref{chap3:sec3.2}. In the next two sections we treat the ergodic and mixing
cases. Finally, conclude the chapter with tightness and weak
convergence of inverse flows. 


\section[Weak and Strong Convergence of...]{Weak and Strong Convergence of Stochastic\hfill\break Flows}\label{chap3:sec3.1} % sec 3.1

Suppose we are given a family of filtrations $\{
F_t^\varepsilon\}_{\varepsilon > 0}$ and $X^\varepsilon_t =
X^\varepsilon (x,t)$ a continuous C-semimartingale adapted to
$F^\varepsilon_t$ with local characteristics having ``nice"
properties. Let $\phi^\varepsilon_{s,t}$ be the stochastic flow
generated by $X^\varepsilon_t$, i.e.,  
\begin{align*} 
  d ~ \phi_{s,t}(x) & = X^\varepsilon ~ (\phi_{s,t}(x),dt),t ~ \ge ~s\\
  \phi_{s,s}(x) & = x.
\end{align*}


We write $\phi_t^\varepsilon = \phi_{o,t}^\varepsilon (x)$. Our aim is
to study the convergence of $(\phi_t^\varepsilon,X^\varepsilon_t)$ as
stochastic flows. We will introduce three notions of convergence,
viz, strong convergence, weak convergence\pageoriginale and convergence as diffusion
processes. Of these, the weak convergence plays the most importance
role and we discuss it in detail. Before giving the definitions of
various convergence  we shall introduce some function spaces. 

Let $C^m = c^m(\mathbb{R}^d; \mathbb{R}^d)$. Let $f ~ \in ~ C^m$ and $N$ a
positive integer. Then 
$$
|| f ||_{m,N} = \sum_{|\alpha | \le ~ m} \sup_{|x|\le N} ~ |D^\alpha ~
f(x) |, N = 1,2,\ldots \ldots \ldots 
$$
defines a family of seminorms and with this family of seminorms $C^m$
becomes a complete separable space. Let $W_m = C([0,t]; C^m)$ be the
set of all continuous maps from [$0,T$] to $C^m$. For $\phi, X ~
\in ~ W_m$ let $\phi_t, X_t$ denote their values at $t ~
\in ~ [0,T]$. Define 
$$
||| \phi |||_{m,N} = \sup_{t ~ \in ~ [0,T]} ~ || \phi_t
||_{m,N}, N = 1,2, \ldots \ldots.   
$$

The above family of seminorms makes $W_m$ a complete separable
space. Let $W^2_m = W_m \times W_m$ and let $B(W^2_m)$ denote its
topological Borel $\sigma$-field. 

Assume that the local characteristics $a^\in,  b^\in $
of the flow are $(m+1)$-times continuously spatially differentiable
and the derivatives are boun\-ded. Then in view of Remark $2.1.6 ~
(\phi^\varepsilon (.,  \omega),X^\varepsilon ~ (.,\omega)) ~
\in ~W^2_m ~a.s$  In other words, it is a $W^2_m$-valued
random variable. Let 
$$
P^{(\varepsilon)} ~(A) = P(\omega:  (\phi^\varepsilon_. ~ 
(\omega),X^\varepsilon_. (\omega)) ~ \in ~ A), A ~ \in
~ B(W^2_m)). 
$$


\begin{definition}\label{c3:def3.1.1}% definition 3.1.1
  Let $P^{(0)}$ be a probability measure on $W^2_m$. The family
  $(\phi^\varepsilon_t,X^\varepsilon_t), \varepsilon > 0$ is said to
  converge weakly to $P^{(0)}$ as stochastic flows if the family $\{
  P^{(\varepsilon)}, \varepsilon > 0 \}$ converges to $P^{(0)}$
  weakly. 
\end{definition}

\begin{definition}\label{c3:def3.1.2}\pageoriginale%Definition 3.1.2
  Let $(\phi_t,X_t)$ be pair of stochastic flow of
  $C^m$-diffeomor\-phism and continuous $C^m$-semimartingale. Then
  $(\phi^\varepsilon_t,X^\varepsilon_t)$ is said to converge strongly
  to $(\phi_t,X_t)$ as stochastic flows if $||| \phi^\varepsilon -
  \phi |||_{m,N}$ and $||| X^{\varepsilon} - X |||_{m,N}$ converge to
  0 in probability for any $N =   1,2,\ldots \ldots$. 
\end{definition}

\setcounter{remark}{2}
\begin{remark}\label{c3:rem3.1.3}%remark 3.1.3
  We shall show later that if $(\phi^\varepsilon_t, X^\varepsilon_t)$
  converges weakly to $(\phi_t,X_t)$, i.e.,  $P^{(\varepsilon)}$
  converges weakly to the joint law of $(\phi_t, X_t), |||
  X^\varepsilon - X |||_{m,N} \rightarrow 0$ in probability and some
  other conditions are satisfied then $||| \phi^\varepsilon - \phi
  |||_{m,N}\rightarrow 0$ and therefore
  $(\phi^\varepsilon_t,X^\varepsilon_t)$ converges to $(\phi_t,X_t)$
  strongly. 
\end{remark}

\setcounter{definition}{3}
\begin{definition}\label{c3:def3.1.4}%definition 3.1.4
  Let $x^{(N)} = (x_1, \ldots, x_N) ~ \in ~ \mathbb{R}^{Nd},
  v^{(M)} = (y_1,\ldots,y_M) ~ \in \break
  \mathbb{R}^{Md}$. Consider the $(N+M)$-point process
  $(\phi_t^\varepsilon ~ (x^{(N)}), X^\varepsilon_t (y^{(M)}))$. Let
  $V_N = C([0,T]; \mathbb{R}^{Nd})$, $V_M =
  C([0,T];\mathbb{R}^{Md})$. On the measurable space $(V_N  X  V_M$,
  $B(V_N ~ X ~ V_M))$ we define the law of the $(N+M)$-point process as
  follows: 
$$
Q^{(\varepsilon)}_{(x^{(N)},y^{(M)})} ~ (A) = P \{ \omega: 
(\phi^\varepsilon_t (x^{(N)}) ; X^\varepsilon_t (y^{(M)}) \in
A \}, A ~ \in ~ B(V_N ~ X ~ V_M). 
$$
\end{definition}

If the law of every $(N+M)$-point process converges weakly, then we
say that the flow converges as diffusion process. Obviously, if
$(\phi^\varepsilon_t,X^\varepsilon_t)$ converges weakly as stochastic
flows, then the law of $(N+M)$-point process converges weakly. 

\setcounter{proposition}{4}
\begin{proposition}\label{c3:prop3.1.5}%Proposition 3.1.5
  The family of laws $\{ P^{(\varepsilon)} \}_{\varepsilon > 0} $ on
  $(W^2_m,B(W^2_m))$ converges weakly if we following two conditions
  are satisfied: 
  \begin{enumerate}[\rm (i)]
  \item $\left\{ P^{(\varepsilon)}\right\}_{\varepsilon > 0} $ is tight, i.e.,
    for any $\delta > 0$ there exists a compact subset $K_\delta$ of
    $W^2_m$ such that $P^{(\varepsilon)}(K_\delta) > 1 - \delta$ for
    any $\varepsilon > 0$. 

  \item $\left\{ Q^{(\varepsilon)}_{(x^{(N)}, \, y^{(M)})} \right\}_{\varepsilon >
    0}$ converges weakly for any $x^{(N)},y^{(M)},M,N =\break
    1,2,\ldots$.  
  \end{enumerate}
  The\pageoriginale proof is based on standard arguments.  See
    Billingsley \cite{2}. 
\end{proposition}

Sometimes it is convenient to consider
$(\phi^\varepsilon_t,X^\varepsilon_t)$ as a process with values in
Sobolev spaces. So we will now introduce a few Sobolev spaces.  For a
positive integer $N$ let $B_N$ denote the ball in $\mathbb{R}^d$ with
centre at the origin and of radius $N$. Let $p > 1$. Let $f:
\mathbb{R}^d \rightarrow \mathbb{R}^d$ be a function such that
$D^\alpha ~ f ~ \in ~ L^p (B_N)$ for all $\alpha$ such that
$|\alpha | \le m$. For such functions we define the following seminorm 
$$
|| f ||_{m,p,N} = \left(\sum_{|\alpha | \le m} \int_{B_N} | D^\alpha ~
f(x) |^p ~ dx\right)^{1/p}, 
$$
where the derivatives are in the sense of distributions. We define
$$
H^{loc}_{m,p} = \left\{ f:  \mathbb{R}^d \rightarrow \mathbb{R}^d | ~ || f
||_{m,p,N}  <   \infty ~\text{for any}~ N\right\}. 
$$

The family of seminorms $\{ || \cdot ||_{m,p,N}, N = 1,2,\ldots \}$
makes $H^{loc}_{m,p}$ a complete separable space. Let $W_{m,p} =
C([0,T]; H^{loc}_{m,p})$. For $\phi ~ \in ~ W_{m,p}$ define 
$$
||| \phi |||_{m,p,N} = \sup_{t ~ \in ~ [0,T]} ~ || \phi
||_{m,p,N}, N = 1,2,\ldots \ldots.  
$$

With this family of seminorms, $W_{m,p}$ is a complete separable
space. For $p = \infty$ we write $W_m$, instead of $W_{m,\infty}$. Let
$W^2_{m,p} = W_{m,p} ~ X ~ W_{m,p}$. We usually suppose $p > d$. In
such cases we have 
$$
 W^2_{m+1,p} \subset W^2_m \subset W^2_{m,p} \subset
 W^2_{m-1}. 
$$

These inclusions are consequences of the well known Sobolev\break imbedding
theorem. Indeed, $H^{loc}_{m+1,p} \subset C^m \subset
H^{loc}_{m,p}$. We shall now define the weak topology of $W_{m,p}$. 

\setcounter{definition}{5}
\begin{definition}\label{c3:def3.1.6}%definition 3.1.6
  Let $\langle., . \rangle_N$ denote the canonical bilinear form on
  $H^{loc}_{m,p}$ restricted to $B_N$. We say that $\{ \phi^n, n =
  1,2,\ldots \} \in W_{m,p}$ converges weakly to $\phi ~
  \in ~ W_{m,p}$ if $\langle \phi^n_t, f \rangle_N$ converges
  to $\langle \phi_t,f \rangle_N $ uniformly in $t$ for any $f ~
  \in ~ (H^{loc}_{m,p})$. The space $W_{m,p}$ equipped with
  the weak topology is a complete separable space. 
\end{definition} 

\setcounter{remark}{6}
\begin{remark}\label{c3:rem3.1.7}\pageoriginale%remark 3.1.7
  Let $A$ be a bounded subset of $H^{loc}_{m,p}$, i.e.,  for any
  positive integer $N$ there exists a constant $K_N$ such that
  $\sup_{f ~\in ~ A} || f ||_{m,p,N} ~ \le ~ K_N$. Then $A$ is
  relatively compact in $H^{loc}_{m,p}$ w.r.t. the weak topology. 
\end{remark}

We are now in a position to describe a criterion under which the
family of measures $\{P^\varepsilon \}_{\varepsilon > 0}$ is tight. 

\setcounter{proposition}{7}
\begin{proposition}\label{c3:prop3.1.8}%Proposition 3.1.8
  Let $\{ P^{(\varepsilon)} \}_{\varepsilon > 0}$ be a family of
  probability measures on $W^2_{m,p}$. Suppose for any positive
  integer $N$ there exist positive integer $N$ there exist positive
  numbers $\alpha, \beta, K$ such that 
  \begin{enumerate}[\rm (i)] 
  \item $E^{(\varepsilon)} [ || \phi_t ||^\alpha_{m,P,N} + || X_t
    ||^\alpha_{m,P,N}] \le K$, 
  \item $E^{(\varepsilon)} [ || \phi_t - \phi_s ||^\alpha_{m,P,N} + ||
    X_t - X_s ||^\alpha_{m,P,N}] \le K |t-s |^{1+\beta}$ 
  \end{enumerate}
  hold for any $t,s ~\in [0,T]$ and for any $\varepsilon >
  0$. Then $\{P^{(\varepsilon)} \}_{\varepsilon > 0}$ is tight in
  $W^2_{m,p}$ w.r.t. the weak topology. 
\end{proposition}
The proof is similar to that of Kolmogorov's criteria for tightness
and is therefore omitted. 

\setcounter{remark}{8}
\begin{remark}\label{c3:rem3.1.9}%remark 3.1.9
  The above Proposition is not true for $p = \infty$, ie.
  $$ 
  \displaylines{\hfill 
  E^{(\varepsilon)} \left[ || \phi_t ||^\alpha_{m,N} + || X_t
    ||^\alpha_{m,N}\right] \le K ~\text{for all}~ t ~ \in ~ [0,T]
  \hfill \cr
  \text{and}\hfill 
  E^{(\varepsilon)} \left[ || \phi_t - \phi_s ||^\alpha_{m,N} + || X_t -
    X_s ||^\alpha_{m,N}\right] \le K|t-s|^{1+\beta}, \hfill}
  $$
  for all $t,s ~ \in [0,T]$, do not imply the tightness of
  $\{P^{(\varepsilon)}\}_{\varepsilon > 0}$ in $W^2_m$. 
\end{remark}

If $\{P^{(\varepsilon)}\}$ is defined as a family of probability
measures on $W^2_m$, then it can be extended to $W^2_{m,p}$ as follows:
Consider the class of sets 
$$
\left\{ A \cap W^2_m:  A ~ \in ~ B(W^2_{m,p}) \right\} = B(W^2_{m,p})
|_{W^2_{m}} \subset B(W^2_m). 
$$

Define\pageoriginale 
$$
P^{\varepsilon}_{m,P}(A)= P_m^{(\varepsilon)}(A \cap W^2_m), A
\in B(W^2_{m,P}). 
$$
Similarly $P_m^{\varepsilon}$ can be extended to
$P^{\varepsilon}_{m-1}$ on $W^2_{m-1}$. 

\setcounter{proposition}{9}
\begin{proposition}\label{c3:prop3.1.10}%Proposition 1.1.10
  Suppose $m >1, p >d$. Then $\{P^{(\varepsilon)}_{m-1}\}_{\varepsilon
    > 0}$ is tight in $W^2_{m-1}$ if $\{P^\varepsilon_{m,p}\}_{
    \varepsilon > 0}$ is tight in $W^2_{m,p} w.r.t$. the weak
  topology. 
\end{proposition}


\begin{proof}
  The proof follows from  Kondraseev's theorem which states that any
  relatively compact set in $H^{loc}_{m,P}$ $w.r.t$. the weak topology
  is embedded in $C^{m-1}$ as a relatively compact set. See Sobolev
  [30]. 
\end{proof}



\section{Approximation of Stochastic Differential Equations}\label{chap3:sec3.2}%sec 3.2

Let $v^{\varepsilon}(t,\omega) = (v^\varepsilon_{1}(t,\omega), \ldots
\ldots,  v^\varepsilon_r (t,\omega))$ be an $r$-dimensional piecewise
continuous stochastic process such that $E[v_i^\varepsilon(t)]=0$ for
all $i$. Let $F_k(x,t)$, $k=0, 1, \ldots,  r$ be continuous functions,
$C^\infty$ in $x$ and the derivatives bounded.  

Consider the following stochastic ordinary differential equation 
\begin{equation*}
  \frac{dx}{dt} = \sum_{\ell=1}^r F_\ell(x,t) v^\varepsilon_\ell (t) +
  F_o(x,t). \tag{3.2.1}\label{c3:eq3.2.1} 
\end{equation*}

Let $\phi^\varepsilon_{s,t}(x)$ denote the solution starting from $x$
at time  $s$. We now consider the following problem. 

\smallskip
\noindent{\textbf{Problem.}}  If $\{v^\varepsilon(t)\}_{\varepsilon >
  0}$ tends to a white noise 
  or more precisely, if $B^\varepsilon (t)= \int\limits^{t}_o
  v^\varepsilon (s)ds$ tends to a Brownian motion $B_t= (B_1(t),
  \ldots,  B_r(t))$ weakly or strongly then $\phi^\varepsilon_t(=
  \phi^\varepsilon_{o,t})$ tends to a Brownian flow $\phi_t$ weakly or
  strongly and the limiting flow $\phi_t$ satisfies the following
  stochastic differential equation  
  \begin{equation*} 
    d \phi_t= \sum^{r}_{\ell=1} F_\ell(\phi_t, t) odB_{\ell}(t)
    +F_o(\phi_t,t) dt. \tag{3.2.2}\label{c3:eq3.2.2} 
  \end{equation*}

  The\pageoriginale solution of the above problem is not always in affirmative. In fact, we
  need some additional term in the right hand side of \eqref{c3:eq3.2.2}. To
  solve the problem in concrete terms we make the following
  assumptions: Let $G^\varepsilon_t = \sigma(v^\varepsilon (s): 0 \le s
  \le t)$. 
  \begin{enumerate}[\rm (a)]
    \renewcommand{\theenumi}{\alph{enumi}}
    \renewcommand{\labelenumi}{\rm (\theenumi)}
  \item[{\rm (A1)(a)}] $\int\limits^t_s|E[v_i^\varepsilon (r) |G^\varepsilon_s ]| dr
    \to 0$ uniformly in $s,t$ in $L^2$-sense.

   \setcounter{enumi}{1}
  \item $E \left[ \int\limits_{s^t} v_i^\varepsilon (\tau ) d \tau
    \int\limits_{j}^\tau v^{\varepsilon}_j (\sigma) d \sigma|
    G^\varepsilon_s \right] \to 
    \int\limits_{s}^{t} \nu_{ij}(r)dr$ unifomly in $s,t$, where
    $\nu_{ij}$ is a deterministic measurable function. 

  \item there exist $\gamma > 1$, $K > 0$ such that  
    $$
    E\left[| \int\limits_{s}^t |E [v^s_i (r) |G^\varepsilon_s]|dr
      |^\gamma | v^\varepsilon_j (s)|^\gamma \right] \le K. 
    $$
  \end{enumerate}



\begin{remarks}\label{c3:rem3.2.1}%rem 3.2.1
  \begin{enumerate}[(i)]
  \item  $(a)$ and $(b)$ roughly show that $\{B^\varepsilon (t)\}$
    converges to a zeromean, martingale with quadratic variation
    $\int\limits_{s}^t(\nu_{ij}(r) + \nu_{ji} (r))\break dr$. Hence $\{ B^\varepsilon
    (t)\}_{\varepsilon > 0}$ converges to a Brownian motion with mean
    $0$ and covariance $\int\limits_{s}^t(\nu_{ij}(r) + \nu_{ji} (r))
    \, dr$.   

  \item $(C')$ ensures the tightness of the law pf $(B^\varepsilon(t),
    \phi^\varepsilon_T(x))$. Since  $V^\varepsilon(t)$ converges to a
    white noise its moment tends to infinity. The condition $(c)$ shows
    that the rates of divergence of the moment of $v^\varepsilon (t)$
    and convergence of $\int\limits^t_s |E[v^\varepsilon (r)
      |G^\varepsilon_s] | dr$ are balanced. In fact, in all examples
    given later the fourth moment of $v^\varepsilon (t)=
    0(1/\varepsilon^2)$ and the fourth moment of $\int\limits^t_s
    |E[v^\varepsilon (r) | G^\varepsilon_s] |dr=0(\varepsilon^2)$. 
  \end{enumerate}  
\end{remarks}

We shall now that state the main result of this section. Let $W_m=
C([0,T];  C^m), V^r= C([0,T]; \mathbb{R}^r)$. Let $P^{(\varepsilon)}_m$
Senate the law of $(\phi^\varepsilon, B^\varepsilon)$ defined on $W_m
\times  V^r$. 

\setcounter{theorem}{1}
\begin{theorem}\label{c3:thm3.2.2}%the 3.2.2
  Assume (A1). Then $\{P_m^{(\varepsilon)}\}_{\varepsilon > 0}$
  converges weakly for any $m \ge 0$. Further the limit $P_m^{(0)}$
  has the following properties: $(i)$ $B(t)$ is an $r$-dimensional
  Brownian motion with zero mean and covariance
  $\int\limits_{o}^t(\nu_{ij}(r) + \nu_{ji}(r)) dr$. $(ii) \quad
  \phi_t$ satisfies 
  \begin{align*}
    d \phi_t & = \sum_{\ell=1}^r F_\ell (\phi_t, t) odB_\ell (t) +F_o
    (\phi_t, t) \, dt \\
& \qquad \qquad 
    + \sum_{1 \le \ell \le m \le r} S_{\ell, m} [F_\ell, F_m](\phi_t,
    t) dt. \tag{3.2.3}\label{c3:eq3.2.3} 
  \end{align*}\pageoriginale
  where $odB$ stands for Stratonovich integral and $S_{\ell,m}=
  \dfrac{1}{2}(\nu_{\ell m}- \nu_{m \ell})$,  
  $$
    [F_\ell, F_m]^k (x,t) = \sum^{d}_{i=1} F^i_\ell(x,t)
    \frac{\partial}{\partial x_i} F^k_m (x,t)  - \sum^{d}_{i=1}
    F^i_m(x,t)  \frac{\partial}{\partial x_i} F^k_\ell(x,t). 
  $$ 

    Further, if $\{B^\varepsilon(t)\}$ converges to $B(t)$ strongly
    then $\{\phi^\varepsilon _t\}$ converges to $\phi_t$ strongly. We
    do not give the proof here. We will do it later in a more general
    setup. Approximation  theorems for the solution of stochastic
    differential equations have been discussed by several authors:
    McShane \cite{27}. Wong-Zakai \cite{34}, Stroock-Varadhan \cite{32}, Kunita
    \cite{19}, \cite{20}, \cite{21}. Ikeda-Nakao-Yamato \cite{12}. Malliavin  \cite{25}, Bismut
    \cite{3}. Dowell \cite{6}. We shall wlucidate the theorem with the help of a
    few examples.     
\end{theorem} 

\setcounter{example}{2}
\begin{example}\label{c3:exam3.2.3}%exe 3.2.3
  \textbf{Polygonal approximation  of Brownian motion}

  Let $B(t)=(B_1(t), \ldots,  B_r(t))$ be an $r$-dimensional Brownian
  motion.Set  
  $$
  \displaylines{\hfill \qquad 
  v^\varepsilon_\ell(t) = \frac{1}{\varepsilon} \delta^\varepsilon_k
  B_\ell \quad \text{if} \quad \in k \le t < \varepsilon(k+1) \hfill\cr
  \text{where}\hfill
  \Delta^\varepsilon_k B_\ell =B_\ell (\varepsilon (k+1))=
  B_\ell(\varepsilon k).\qquad \hfill }
  $$
 
  Then 
$$
B^\varepsilon (t) = \int\limits_{o}^t v^\varepsilon (s) ds \to B(t) 
~\text{ uniformly in}~ t. 
$$
\end{example}

All we have to do is to verify the assumptions in  $(A1)$. Since
$v^\varepsilon (t)$ and $v^\varepsilon (s)$ are independent if $|t-s|>
\varepsilon$, we have  
$$
| \int\limits_{s}^t |E [v^\varepsilon_\ell(u) |G^\varepsilon_s]| du|
\le | \Delta^\varepsilon_k B_\ell| \quad \text{if} \quad \varepsilon k \le s <
\varepsilon (k+1). 
$$

Also\pageoriginale 
$$
\text{variance}~ (\Delta^\varepsilon_k B_\ell) =\varepsilon \to 0.
$$

Therefore  $(a)$ is satisfied. Now $var(v^\varepsilon_j (s)) =
\dfrac{1}{\varepsilon}$, therefore 
$$
E \left[| \int\limits_s^t |E[v^\varepsilon_i (u) |G^\varepsilon_s]|du|^2
  |v^\varepsilon_j(s)|^2\right] \le \left(3 \varepsilon^2. 3
\frac{1}{\varepsilon^2}\right)^{1/2}=3, 
$$
which verifies $(c)$. Also it is easy to verify $(b)$ with
$\nu_{ij}(u) =\dfrac{1}{2} \delta_{ij}$. Hence $S_{\ell,m}=0$. 


\begin{example}\label{c3:exam3.2.4}%exe 3.2.4
  \textbf{McShane} Here we consider the approximation of two
  dimensional Brownian motion by continuously differentiable
  function. For $t \in[0,1]$, let $\phi_1(t)$ and $\phi_2(t)$
  be continuously differentiable functions such that  $\phi_1(0)=
  \phi_2 (0) =0$ and $\phi_1(1)= \phi_2(1) =1$. Let $B(t)=(B_1 (t),
  B_2(t))$ be a two dimensional standard Brownian motion. Set  
  \begin{equation*} 
    v_i^\varepsilon (t) =
    \begin{cases}
      \frac{1}{\varepsilon} \dot{\phi}_i ((t-k \varepsilon)/
      \varepsilon )\Delta^\varepsilon_k B_i &\text{if} \quad 
      \Delta^\varepsilon_k B_1 \Delta^\varepsilon_k  B_2 \ge 0,\\ 
      \frac{1}{\varepsilon} \dot{\phi}_{3-i} ((t-k \varepsilon)/
      \varepsilon )\Delta^\varepsilon_k B_i &\text{if} \quad \Delta^\varepsilon_k
      B_1 \Delta^\varepsilon_k  B_2 < 0, 
    \end{cases}
  \end{equation*}
  if $k \varepsilon < t < (k+1) \varepsilon, k=0,1, \ldots $, Then
  $$
  B^\varepsilon(t) = \int\limits_{o}^t v^\varepsilon (s) ds \to B(t)
  ~\text{uniformly in}~ t. 
  $$
\end{example}

As in the previous example, we can verify $(a)$ and $(b)$. Also it can
be shown that  
$$
E \left[ \int\limits_{s}^t v^\varepsilon_i (\tau) d \tau 
  \int\limits_{s}^\tau v ^\varepsilon_j(\sigma) d
  \sigma| G^\varepsilon_s\right] \to \frac{1}{2} \delta_{ij} + \frac{1}{\pi}
\int\limits_{o}^t [\phi_i (s) \dot{\phi}_j (s)- \dot{\phi}_j(s) \phi_i
  (s)]ds. 
$$

\begin{example}\label{c3:exam3.2.5}%exe 3.2.5
  \textbf{Mollifiers approximation (Malliavin)}
  
  Let $B(t) = (B_1(t), \ldots,  B_r(t))$ be an $r$-dimensional standard
  Brownian motion. Let $\phi$ be a nonnegative $C^\infty -$ function
  whose support is contained in [0,1] and $\int\limits_{o}^1 \phi(s)
  ds =1$. Set 
  $$
  \displaylines{\hfill 
  \phi_\varepsilon (t) = \frac{1}{\varepsilon} \phi(t / \varepsilon),
  \;   \varepsilon > 0 \hfill \cr
  \text{and}\hfill   
  B^\varepsilon(t) =  \int\limits_{o}^{\infty}\phi_\varepsilon
  (s-t)B(s) ds=  \int\limits_{o}^\varepsilon \phi_\varepsilon(s)
  B(s+t)ds. \hfill }
  $$\pageoriginale 
\end{example}

Then 
$$
B^\varepsilon(t) \to B(t) ~\text{uniformly in}~ t. 
$$

Set
$$
v^\varepsilon(t) =-  \int\limits_{o}^{\infty} \dot{\phi}_\varepsilon
(s-t) B(s) ds. 
$$

Then
$$
 \int\limits_{s}^t |E[ v^\varepsilon(u) |G^\varepsilon_s] |du =
 \int\limits_{s}^{s+\varepsilon}|E[v^\varepsilon(u) |G^\varepsilon_s]|
 du, 
$$
since $v^\varepsilon (u)$ is independent of $G^\varepsilon_s$ if $u >
s+ \varepsilon$. Also  
$$
var(v^\varepsilon(u)) = \frac{1}{3} \int\limits_{o}^1 \phi(s)^2 ds.
$$

Therefore
$$
var( \int\limits_{s}^{s+ \varepsilon} v^\varepsilon(u)du) \le \varepsilon.
$$

Now it is easy to verify $(a)$, $(b)$ and $(c)$. For $(b), \nu_{ij}=
\dfrac{1}{2} \delta_{ij}$. 


\begin{example}\label{c3:exam3.2.6}%exe 3.2.6
\textbf{Approximation by Ornstein-Uhlenbech processes\break (Dowell)}

Let $B(t)$ be an $r$-dimensional standard Brownian motion. Let
$\{v^\varepsilon (t)\}$ ne given by  
\begin{tabbing}
  \quad $dv^\varepsilon (t)$ \= $=- \frac{1}{\varepsilon} v^\varepsilon (t) dt +
  \frac{1}{\varepsilon} dB(t)$\\ 
  \quad $v^\varepsilon (0)$\> $=a$ Gaussian random variable with mean zero and
  covariance\\ 
  \> \qquad $\dfrac{1}{2 \varepsilon}(\delta_{ij})$. 
\end{tabbing}
\end{example}

Then $\{v^\varepsilon(t)\}$ is a stationary Gaussian  process with mean
zero and covariance $\dfrac{1}{2\varepsilon} (\delta_{(ij)})$. Also
$v^\varepsilon (t)$ is given by  
$$
v^\varepsilon (t)= e^{-\frac{1}{\varepsilon}(t-s)} v^\varepsilon(s) + 
\frac{1}{\varepsilon} \int\limits_{s}^t e ^{-\frac{1}{\varepsilon}
  (t-u)} dB(u). 
$$

Therefore\pageoriginale
$$
B^\varepsilon(t) - B^\varepsilon (s) = \varepsilon(1-e^{-
  \frac{1}{\varepsilon}(t-s)}) v^\varepsilon (s) +
\int\limits_{s}^t(1-e^{- \frac{1}{\varepsilon}(t-u)})dB(u).
$$
Since the variance of the first term in the $r.h.s$. is
$0(\varepsilon)$ and the integrand tends to $1$ as $\varepsilon \to
0$, we have  
$$
B^\varepsilon(t) - B^\varepsilon(s) \to B(t) -B(s).
$$

The assumptions in $(A1)$ can be verified.
\begin{align*}
  \int\limits_{s}^t|E[v^\varepsilon_i (r) dr |G^\varepsilon_s ]| dr &=
  \int\limits_{s}^t (e^{- \frac{1}{\varepsilon}(r-s)}dr)
  |v_i^\varepsilon (s) |\\ 
  &= \varepsilon (1-e^{- \frac{1}{\varepsilon}(t-s)}) |v_i^\varepsilon
  (s)| \to 0 \; \text{ as } \;  \varepsilon \to 0. 
\end{align*} 

Also
\begin{align*}
E & \left[\int\limits_{s}^t v^\varepsilon_i (\tau) d \tau
  \int\limits_{s}^\tau v^\varepsilon_j(\sigma)  d \sigma |
  G^\varepsilon_s \right] 
  =E \left[\int\limits_{s}^t v^\varepsilon_j (\sigma) d \sigma
  \int\limits_{\sigma}^t v^\varepsilon_i(\tau)  d \tau |
  G^\varepsilon_s \right]\\ 
  &=E \left[\int\limits_{s}^t v^\varepsilon_j (\sigma) d \sigma
    \left\{\varepsilon \left(1-e^{-\frac{1}{\varepsilon}(t -
      \sigma)}\right)  v_i^\varepsilon(\sigma) \right\}
    |G^\varepsilon_s \right]\\  
  &=\varepsilon \int\limits_{s}^t \left(1-e^{-\frac{1}{\varepsilon}(t
    - \sigma )}\right) 
  E\left[v^\varepsilon_i (\sigma)  v^\varepsilon_j (\sigma )|
    G^\varepsilon_s \right]d \sigma\\ 
  &= \varepsilon \int\limits_{s}^t \left(1-e^{-\frac{1}{\varepsilon}(t
    - \sigma )}\right) 
  \left[e^{- \frac{2}{\varepsilon}(\sigma-s )}v^\varepsilon_i (s)
    v_j^\varepsilon (s)\right] 
    + \frac{1}{2 \varepsilon} \left[\left(1-e^{-
        \frac{2}{\varepsilon}(\sigma-s)} \right)\delta_{ij} \right] d
    \sigma\\
    & \to \frac{1}{2}(t-s) \delta_{ij}.
\end{align*}



\section{The Main Limit Theorem}\label{chap3:sec3.3}%sec 3.3

 In this section, we will state the main  limit theorem for stochastic
 flows and the proof of the theorem will be developed in the
 subsequent sections. 
  
 Let\pageoriginale for each $\varepsilon > 0$ there be given a continuous $C^{k-1}$-
 semimartingale $X^\varepsilon(x,t)$ such that $X^\varepsilon (x,0)=0$
 and it is adapted to $F^\varepsilon_t$. Let ($a^\varepsilon
 (x,y,t)$, $b^\varepsilon(x,t)$) be the local charactersistics of
 $X^\varepsilon (x,t)$. Suppose that $a^\varepsilon$ and
 $b^\varepsilon$ are Lipschitz continuous and $a^\varepsilon$ is
 $k$-times continuously differentiable in $x,t,b^\varepsilon$ is
 $(k+2)$- times continuously diffrentiable in $x$. Also assume that
 $b^\varepsilon (x,t)$ is $F^\varepsilon_o$- measurable. We can write
 $X^\varepsilon (x,t)$ as  
 \begin{equation*} 
   X^\varepsilon (x,t) = Y^\varepsilon(x,t) + \int_o^t b^\varepsilon
   (x,t) \, dt \tag{3.3.1}\label{c3:eq3.3.1}  
 \end{equation*} 
 where $Y^\varepsilon (x,t)$ is a continuous $C^{k-1}$ -martingale. Set
 $$
 G^\varepsilon_t= \sigma(Y^\varepsilon(x, u), b^\varepsilon (x,u): 0 
 \le u \le t, x \in \mathbb{R}^d). 
 $$
  
 Then $X^\varepsilon (x,t)$ is $G_t^\varepsilon$-adapted contonuous
 $C^{k-1}$-semimartingale. Now we introduce the following quantities: 
 \begin{align*}
   \bar{b}^\varepsilon (x,t) & = E[b^\varepsilon (x,t)],
   \tilde{b}^\varepsilon (x,t) = b^\varepsilon (x,t) -
   \bar{b}^\varepsilon (x,t),\\ 
   A^\varepsilon_{ij}(x,y,t,s) &= E \left[ \int^t_s \tilde{b}_i^\varepsilon
     (x,r) dr |G^\varepsilon_s \right] b^\varepsilon_j (y,s),\\ 
   c_i^\varepsilon (x,t,s) &= \sum_{j} \frac{\partial}{\partial x_j}
   A^\varepsilon_{ij} (x,y,t,s)|_{y=x} ,\\ 
   d^\varepsilon_{ijk}(x,y,t,s) &= E \left[ \int\limits_s^t
     \tilde{b}^\varepsilon_i (x,r) dr| G^\varepsilon_s
   \right]a^\varepsilon_{jk}(y,y,s). 
 \end{align*} 
  
 We next introduce two sets of assumptions. The first is concerned
 with the convergence and the second with  moments 
 essentially needed for tightness.
  
 $(A2)_k$ There exist continuous functions $a=(a_{ij}(x,y,t))$,
 $\bar{b}=\break (\bar{b}^i (x,t))$,  $c=(c^i(x,t))$, $A = (A_{ij}(x,y,t))$,
 which are $k$-times continuously differentiable in $x,y$ or $x$, as the
 case may be, and satisfy the following properties: 
 \begin{enumerate}[(1)] 
 \item\pageoriginale $E \left[ \sup\limits_{|x| \le K} |E \left[ \int\limits_{s}^t
     a^\varepsilon_{ij} (x,y,r) dr |G^\varepsilon_s \right] -
   \int\limits_{s}^t a_{ij} (x,y,r) dr | \right] \to 0 \text{ as } \varepsilon
   \to 0$,  
 
\item $\sup\limits_{|x| \le K} | \int\limits_{s}^t \bar{b}^\varepsilon
   (x,r) dr-  \bar{b}\int\limits_{s}^t (x,r) dr | \to 0$, 

 \item $E \left[ \sup\limits_{|x| \le K} |E \left[ \int\limits_{s}^t D^\alpha_x
     \tilde{b}^\varepsilon (x,r) dr |G^\varepsilon_s \right]|\right]
   \to 0 ~\text{for}~ | \alpha | \le k$, 

 \item $E \left[ \sup\limits_{|x| \le K} |E \left[ \int\limits_{s}^t
     A^\varepsilon_{ij} (x,y,t,r) dr |G^\varepsilon_s \right] -
   \int\limits_{s}^t A_{ij} (x,y,r) dr | \right] \to 0$, 

 \item $E \left[ \sup\limits_{|x| \le K} |E \left[ \int\limits_{s}^t
     c^\varepsilon (x,t,r) dr |G^\varepsilon_s \right] - \int\limits_{s}^t c
   (x,r) dr | \right] \to 0$.   

   All the above convergences are uniform in $t,s$ for any $K > 0$.
 
   $ (A3)_k$ For any $K >0$ there exist constants $\gamma > 1$ and $L
   >0$ such that for any $x,y,t,s$ 

 \item $E \left[\sup\limits_{|x| \le K, |y| \le K} |D^\alpha_x D^\beta_y
   a^\varepsilon_{ij} (x,y,t)|^\gamma \right] \le L $ $\forall \varepsilon >
   0$, $|\alpha | \le  k, |\beta| \le k$, 

 \item $\sup\limits_{|x| \le K} |D^\alpha_x \bar{b}^\varepsilon(x,t)
   | \le L$  for all $\varepsilon > 0,  | \alpha | \le k$ 

 \item $E \left[\sup\limits_{|x| \le K, |y| \le K} |D^\alpha_x D^\beta_y
   A^\varepsilon_{ij} (x,y,t,s)|^\gamma \right] \le L $ $\forall \varepsilon
   > 0$, $|\alpha | \le  k+1, |\beta| \le k+1$,

 \item $E \left[\sup\limits_{|x| \le K, \; |y| \le K} |D^\alpha_x D^\beta_y
   d^\varepsilon_{ijk} (x,y,t,s)|^\gamma\right] \le L $ $\forall \varepsilon
   > 0$, $|\alpha | \le  k+2, |\beta| \le k$. 
 \end{enumerate}  
  
  Let $\phi^\varepsilon_t(x)= \phi^\varepsilon_{o,t}(x)$ be the
  stochastic flow generated by $X^\varepsilon(x,t)$. Let
  $P^{(\varepsilon)}_m$, $m \le k-1$, be the law of
  $(\phi^\varepsilon_t,  X^\varepsilon_t)$ defined on $W^2_m$. 
  

\begin{theorem}\label{c3:thm3.3.1}%the 3.3.1
  Assume $(A2)_k$ and $(A3)_k$ for some $k \ge 2$. Then
  $\{P^{(\varepsilon)}_{k-2}\}_{\varepsilon > 0}$ converges weakly
  to $P^{(o)}$ as stochastic flows. The limits measure $P^{(o)}$ satisfy
  the following properties: 
  \begin{enumerate}[\rm (a)]
  \item $X (x,t)$ is a $C^{k-1}-B.m$. with local characteristics
    $(a+\bar{a}, \bar{b})$ where $\bar{a}_{ij}(x,y,t)= A_{ij}(x,y,t)
    + A_{ij}(y,x,t)$, 

  \item\pageoriginale $\phi_t$ is a Brownian flow of $C^{k-1}$-diffeomorphisms
    genereted by 
    $$X(x,y) + \int\limits_{o}^t c(x,r) dr.$$  
    
    Further if $X^\varepsilon _t$ converges strongly then
    $\phi^\varepsilon_t$ also converge strongly. 
  \end{enumerate}
\end{theorem}  

  In the next section, we will prove the tightness of $(N+M)$-point
  processes under an additional assumption $(A4)$. In section  3.5,
  we will first  prove the weak convergence assuming $(A4)$ and then
  we will drop the assumption  $(A4)$ and prove the same in the general
  case. In section~\ref{chap3:sec3.6},  we prove the tightness of Sobolev space
  valued processes and in section~\ref{chap3:sec3.7} we conclude the proof of the
  main theorem. 
  
\section{Tightness of (N+M)-Point Processes}\label{chap3:sec3.4}%sec 3.4   
  
  Let $\underline{x}^{(N)}=  (x_1, \ldots,  x_N) \in
  \mathbb{R}^{Nd}, \underline{y}^{(M)}=  (y_1, \ldots, y_{M})
  \in \mathbb{R}^{Md}$. Consider the of the $(N+M)$-point
  process $(\phi^{\varepsilon}_{t} (( \underbar{x}^{(N)})),
  X^{\varepsilon}(\underbar{y}^{(M)}, t))$. As before we denote the
  law of the $(N+M)$- point process by $Q^{(\varepsilon)}$ $_{(
    \underline{x}^{(N)}, \underline{y}^{(M)})}$ which is defined on
  $V_N \times  V_M$. For fixed $\underline{x}^{(N)}$ and
  $\underline{y}^{(M)}$ we shall drop the subscript from
  $Q^{(\varepsilon)}$ $_{( \underline{x}^{(N)}, \underline{y}^{(M)})}$
  and write simply $Q^{(\varepsilon)}$. We shall  show the tightness
  of these laws $\{Q^{\varepsilon}\}_{\varepsilon > 0}$ under the
  following condition: 
  
\noindent
  (A4) There exists a $K > 0$ such that 
  \begin{align*}
    a^\varepsilon (x,y,t,\omega) = 0 & \quad \text{if} \quad  |x| \ge
    K, |y| \ge K,\\ 
    b^\varepsilon (x,t,\omega) = 0 & \quad \text{if} \quad  |x| \ge K.
  \end{align*}  
  
  Under the  above condition  $X^\varepsilon(x,t, \omega)= 0$ if
  $|x| > K$ and the associated flow $\phi^\varepsilon_t (x)$ satisfies
  $|\phi^\varepsilon_t (x) | \le K$ if $|x| \le K$ and
  $\phi^\varepsilon_t (x)=x$ if $|x| > K$. Let $\gamma = \gamma(K)$ be
  the positive constant as in $(A3)_k$.    


\begin{Lemma}\label{c3:lem3.4.1}%lem 3.4.1
  For any $p \in [2,2 \gamma]$ there exists a positive
  constant $C = C(p)$ such that  
  \begin{equation*}
    E[ |X^\varepsilon_t(x)- X^\varepsilon_s (x) |^p] \le C|t-s|^{2-
      \frac{2}{P}} ~\text{for all}~ \varepsilon > 0 ~\text{and for
      all } x. \tag{3.4.1}\label{c3:eq3.4.1} 
  \end{equation*}
\end{Lemma}  

\noindent \textit{Proof.}
  We\pageoriginale will consider the case for $d=1$ only for implicity. We suppress
  $x$ from $X^\varepsilon_t (x), a^\varepsilon (x,x,t)$ etc. For fixed
  $x$. We have  
  \begin{align*} 
  E&\left[|Y^\varepsilon_s- T^\varepsilon_s|^p\right] = \frac{1}{p}
  p(p-1) E \left[ \int\limits_{s}^a a^\varepsilon (r) |Y^\varepsilon_r-
    Y^\varepsilon_s|^{p-2} dr\right]\\ 
    &\le \frac{1}{2} p(p-1) L^{\frac{1}{\gamma}} \int\limits_{s}^t E \left[|
      Y^\varepsilon_r - Y^\varepsilon_s |^{(p-2)
        \frac{\gamma}{\gamma-1}}\right]^{\frac{\gamma-1}{\gamma}} dr \tag*{(by
      (A.3)(6))}\\ 
    &\le \frac{1}{2} p(p-1) L^{\frac{1}{\gamma}} \int\limits_{s}^t E \left[|
      Y^\varepsilon_r - Y^\varepsilon_s |^p\right]^{\frac{P-2}{P}} dr
    \tag{3.4.2}\label{c3:eq3.4.2}  
  \end{align*}
  since $(p-2) \dfrac{\gamma}{\gamma-1}<p$. Now for any $a>0$ we have
  $a^{\dfrac{p-2}{p}} < 1+a$. Therefore 
  \begin{equation*}
    E \left[|Y^\varepsilon_t - Y^\varepsilon_s|^p\right] \le
    \frac{1}{2} p(p-1)L^{\frac{1}{\gamma}} \left\{(t-s) +
    \int\limits_{s}^t E \left[|Y^\varepsilon_r- 
      Y^\varepsilon_s |^p\right] \, dr \right\}. \tag*{$\Box$} 
  \end{equation*}
  
 By Gronwall's inequality, we get $E[| Y^\varepsilon_t-
   Y^\varepsilon_S |^p] \le c|t-s|$. Substituting this back to
 $(3.4.2)$ we have  
 \begin{equation*}
   E[| Y^\varepsilon_t - Y^\varepsilon_S |^P]  \le c \int\limits_{s}^t
   |r-s|^{\frac{p-2}{p}} dr 
   \le c'|t-s|^{2-\frac{2}{p}}. \tag{3.4.3}\label{c3:eq3.4.3} 
  \end{equation*}
   
Next, note that  
\begin{align*}
  |\int\limits_{s}^t b^\varepsilon (r) dr |^p &= p
  \int\limits_{s}^t \bar{b}^\varepsilon(\tau)| \int\limits_{s}^t
  b^\varepsilon (r) dr|^{p-1} ~\text{sign}~ (\tau) d \tau\\
  & \quad + p(p_1)
  \int\limits_{s}^t \tilde{b}^\varepsilon (\tau ) \int\limits_{s}^\tau
  b^\varepsilon (\sigma) | \int\limits_{s}^\sigma b^\varepsilon (r) dr
  |^{p-2} d \sigma\\ 
  &=I^\varepsilon_1 + I^\varepsilon_2,  \text{say}
\end{align*} 
 where sign $(\tau)= sign \int\limits_{s}^\tau b^\varepsilon (r) dr$. We have
 \begin{align*}
   E[|I^\varepsilon_1 |] &\le pL \int\limits_{s}^t E\left[|
     \int\limits_{s}^\tau b^\varepsilon (r) dr |^{p-1}\right] d \tau
   \tag*{(by(A.3)(7))}\\ 
   E[I^\varepsilon_2 ]| &\le p(p-1)| \int\limits_{s}^t
   E\left[A^\varepsilon(t, \sigma) |\int\limits_{s}^\sigma b^\varepsilon
     (r) dr |^{p-2}\right] d \sigma |\\ 
   &\le p(p_1)L^{\frac{1}{\gamma}} \int\limits_{s}^t E\left[|
     \int\limits_{s}^\tau b^\varepsilon (r) dr| ^{(p-2)
       \frac{\gamma}{\gamma-1}}\right]^{\frac{\gamma-1}{\gamma}} d \sigma 
 \end{align*} 
 by\pageoriginale (A.3) (8). These two imply
{\fontsize{10}{12}\selectfont 
$$
 E\left[| \int\limits_{s}^t b^\varepsilon (r) dr|^p\right] \le c
 \left\{\int\limits_{s}^t \left(E\left[| \int\limits_{s}^\sigma b^\varepsilon (r)
   dr|^{p-1}\right] +E \left[| \int\limits_{s}^\sigma b^\varepsilon  (r)
   dr|^p\right]^{\frac{p-2}{p}}\right) d \sigma \right\}. 
 $$}\relax
  
 Then we obtain similarly as the above
 $$
 E\left[| \int\limits_{s}^t b^\varepsilon (r)  dr |^p \right] \le
 C'|t-s|^{2- \frac{2}{p}}. 
 $$

\setcounter{Lemma}{1}
\begin{Lemma}\label{c3:lem3.4.2}%lem 3.4.2
  For any $p > 3(2- \dfrac{1}{r})$ there exists a positve constant
  $C=C(p)$ such that 
  \begin{equation*} 
    E[| \phi^\varepsilon_t (x) - \phi^\varepsilon_s(x) |^p ] \le
    C|t-s|^{2- \frac{1}{\gamma}}, \tag{3.4.4}\label{c3:eq3.4.4} 
  \end{equation*}
  for all $\varepsilon > 0$ and for all $x$.
\end{Lemma} 

\noindent \textit{Proof.}
  We prove the case for $d=1$ only. In view of $(A4)$ \eqref{c3:eq3.4.4} is
  obvious if $|x| >K$. So we assume $|x| \le K$. We suppress $x$ from
  $\phi^\varepsilon_t (x)$. We have 
  \begin{equation*}
    \phi^\varepsilon_t - \phi^\varepsilon_s= \int\limits_{s}^t
    \bar{b}^\varepsilon(\phi^\varepsilon_r,r) dr + \int\limits_{s}^t
    \tilde{b}^\varepsilon (\phi^\varepsilon_r,r) dr +
    \int\limits_{s}^t Y^\varepsilon (\phi^\varepsilon_r, dr).
    \tag{3.4.5}\label{c3:eq3.4.5}\hfill\Box  
  \end{equation*}  
 
  Using Ito's  formula for $F(x) = |x|^P$ and writing sign $r=
  ~\text{sign}~(\phi^\varepsilon_r- \phi^\varepsilon_s)$, we get  
\begin{align*}
  |\phi^\varepsilon_t-\phi^\varepsilon_s|^p & = p \int\limits_{s}^t
  \bar{b}^\varepsilon(\phi^\varepsilon_r,r)| \phi^\varepsilon_r-
  \phi^\varepsilon_s|^{p-1} sign(r) dr \\
  & \qquad + p \int\limits^t_s  \tilde{b}^\varepsilon (\phi^\varepsilon_r,r)|
  \phi^\varepsilon_r- \phi^\varepsilon_s|^{p-1} ~\text{sign}~(r) dr\\ 
  &\qquad + p \int\limits^t_s | \phi^\varepsilon_r- \phi^\varepsilon_s |^{p-1}
  sign(r) \gamma^\varepsilon ( \phi^\varepsilon_r, dr)\\ 
  & \qquad + \frac{1}{2} p(p-1) \int\limits^{t}_s a^\varepsilon (
  \phi^\varepsilon_r,\phi^\varepsilon_r,r)| \phi^\varepsilon_r
  -\phi^\varepsilon_s|^{p-2} dr\\ 
  & = I^\varepsilon_1 +I^\varepsilon_2 + I^\varepsilon_3
  +I^\varepsilon_4, \text{say}. \tag{3.4.6}\label{c3:eq3.4.6} 
\end{align*}
 
 Note that $|\phi^\varepsilon_r|\le K$. Then from  $(A3)_k$
 \begin{equation*}
   |E[I^\varepsilon_1 ]| \le pL \int\limits_{s}^t E
   [|\phi^\varepsilon_r-
\phi^\varepsilon_s|^{P-1} ] dr. \tag{3.4.7}\label{c3:eq3.4.7} 
 \end{equation*}
  
We\pageoriginale have 
\begin{equation*}
  E[I^{\varepsilon}_3] = 0. \tag{3.4.8}.\label{c3:eq3.4.8}
\end{equation*}

By $(A3)_k$, we have 
\begin{equation*}
  E[I^{\varepsilon}_4] \leq C \int \limits^t_s E[
    \phi^{\varepsilon}_r-\phi^{\varepsilon}_s|^{(p-2)
      \frac{\gamma}{\gamma-1}}] ^{\frac{\gamma-1}{\gamma}}
  dr. \tag{3.4.9}\label{c3:eq3.4.9} 
\end{equation*}

We shall now calculate $I^{\varepsilon}_2$. By Ito's formula 
\begin{align*}
  &  \tilde{b}^{\varepsilon}(\phi^{\varepsilon}_r,r)|
  \phi^{\varepsilon}_r - \phi^{\varepsilon}_s|^{p-1}~\text{sign}~(r)\\  
  & = \int \limits^t_s \left\{ \frac{\partial}{\partial x}
  b^{\varepsilon}(\phi^{\varepsilon}_{\sigma},r) b^{\varepsilon}
  (\phi^{\varepsilon}_{\sigma},\sigma)  
  + \frac{1}{2} \frac{\partial^2}{\partial x^2} \tilde{b}^{\varepsilon}
  (\phi^{\varepsilon}_{\sigma},r) a^{\varepsilon}
  (\phi^{\varepsilon}_{\sigma}, \phi^{\varepsilon}_{\sigma},
  \sigma) \right\} \\
  & \hspace{6cm}\times |\phi^{\varepsilon}_{\sigma}
  -\phi^{\varepsilon}_{s} |^{p-1} \text{sign} (\sigma) d \sigma \\ 
  & + (p-1) \int\limits_{s}^{r} \left[\tilde{b}^{\varepsilon}
    (\phi^{\varepsilon}_{\sigma},r) b^{\varepsilon} (\phi
    ^{\varepsilon}_{\sigma}, \sigma ) + \frac{\partial}{\partial x}
    (\tilde{b}^{\varepsilon} \phi^{\varepsilon}_{\sigma},r)
    a^{\varepsilon} (\phi^{\varepsilon}_{\sigma},
    \phi^{\varepsilon}_{\sigma}, \sigma )\right]\\ 
  & \hspace{6cm}\times
  |\phi_{\sigma}^{\varepsilon} - \phi_{s}^{\varepsilon}|^{p-2} d
  \sigma\\ 
  & + \frac{1}{2}(p-1) (p-2) \int \limits^t_s
  \tilde{b}^{\varepsilon}(\phi^{\varepsilon}_{\sigma},r)
  a^{\varepsilon}(\phi^{\varepsilon}_{\sigma},\phi^{\varepsilon}_{\sigma},
  \sigma)|\phi^{\varepsilon}_{\sigma}-\phi^{\varepsilon}_s|^{p-3}
  \text{sign}(\sigma) d \sigma \\ 
  & + \text{a martingale with zero mean}. 
\tag{3.4.10}\label{c3:eq3.4.10}
\end{align*}

Therefore 
{\fontsize{10}{12}\selectfont
\begin{align*}
  E &\left[I^{\varepsilon}_2\right]  = p \int\limits^t_s\\ 
  E &\left[(c^{\varepsilon}(\phi^{\varepsilon}_{\sigma},t, \sigma)
    + \frac{1}{2} \frac{\partial^2}{\partial x^2}
    d^{\varepsilon}(\phi^{\varepsilon}_{\sigma},\phi^{\varepsilon}_{\sigma},
    t, \sigma))|\phi^{\varepsilon}_{\sigma}-\phi^{\varepsilon}_{s}|^{p-1}
     \text{sign}(\sigma)\right]d \sigma  \\
  & + p (p-1) \int\limits^t_sE[\{A^{\varepsilon}
    (\phi^{\varepsilon}_{\sigma},\phi^{\varepsilon}_{\sigma}, t,
    \sigma) + \frac{\partial}{\partial x} d^{\varepsilon}
    (\phi^{\varepsilon}_{\sigma}, \phi^{\varepsilon}_{\sigma}, t,
    \sigma)\} |\phi^{\varepsilon}_{\sigma} 
    -\phi^{\varepsilon}_{s}|^{p-2}]d \sigma \\
  & +\frac{1}{2} p(p-1)(p-2) 
  \int
  \limits^t_sE\left[d^{\varepsilon}(\phi^{\varepsilon}_{\sigma},
    \phi^{\varepsilon}_{\sigma}, t \sigma)|\phi^{\varepsilon}_{\sigma}
    -\phi^{\varepsilon}_{s}|^{p-3} \text{sign} (\sigma)\right]d \sigma. 
\end{align*}}\relax

Applying\pageoriginale Holder inequality and using the tightness assumption, we have 
\begin{align*}
  E\left[I^{\varepsilon}_2\right]| & \leq 2pL^{1 / \gamma} \int\limits^t_s
 E\left[|\phi^{\varepsilon}_{\sigma} -\phi^{\varepsilon}_{s}|^{(p-1)
    \frac{\gamma}{\gamma -1}} \right]^{\frac{\gamma -1}{\gamma}} d \sigma\\ 
   & + 2p(p-1) L^{1 / \gamma} \int\limits^t_s
    E\left[|\phi^{\varepsilon}_{\sigma} -\phi^{\varepsilon}_{s} |^{(p-2)
      \frac{\gamma}{\gamma -1}} \right]^{\frac{\gamma -1}{\gamma}} d \sigma\\ 
      & + p(p-1)(p-2) L^{1 / \gamma} \int\limits^t_s
     E\left[|\phi^{\varepsilon}_{\sigma} -\phi^{\varepsilon}_{s}|^{(p-3)
        \frac{\gamma}{\gamma -1}} \right]^{\frac{\gamma -1}{\gamma}} 
     d \sigma   \tag{3.4.11}\label{c3:eq3.4.11} 
\end{align*}

Summing up these estimates, we obtain 
\begin{multline*}
  E \left[|\phi^{\varepsilon}_s -\phi^{\varepsilon}_{s}|^P \right]  \leq C \int
  \limits^t_s \left\{E\left[| \phi^{\varepsilon}_r
    -\phi^{\varepsilon}_{s}|^{p-1}\right] + E \left[|\phi^{\varepsilon}_s
    -\phi^{\varepsilon}_{s}|^{(p-1) \frac{\gamma}{\gamma -1}}
    \right]^{\frac{\gamma -1}{\gamma}}\right. \\  
  +\left. E\left[|\phi^{\varepsilon}_r -\phi^{\varepsilon}_{s}|^{(p-2)
      \frac{\gamma}{\gamma -1}} \right]^{\frac{\gamma -1}{\gamma}}  +
  E\left[|\phi^{\varepsilon}_r -\phi^{\varepsilon}_{s}|^{(p-3)
      \frac{\gamma}{\gamma -1}} \right]^{\frac{\gamma -1}{\gamma}}\right\} dr.
  \tag{3.4.12}\label{c3:eq3.4.12} 
\end{multline*}

Since $|\phi^{\varepsilon}_t -\phi^{\varepsilon}_{s}| \leq 2K,
E[|\phi^{\varepsilon}_t -\phi^{\varepsilon}_{s}|^p] \leq C|
t-s|$. Substituting this in~\eqref{c3:eq3.4.12}, we obtain 
$$
E\left[|\phi^{\varepsilon}_t -\phi^{\varepsilon}_{s}|^p\right] \leq C \int
\limits^t_s \left\{ r-s|+|r-s|^{\frac{\gamma-1}{\gamma}} \right\} dr. 
$$
Hence 
$$
E[|\phi^{\varepsilon}_t -\phi^{\varepsilon}_{s}|^p] \leq C|t-s|^{2 -
  \frac{1}{\gamma}}. 
$$

The following proposition is clear from Lemmas~\ref{c3:lem3.4.1}, 
\ref{c3:lem3.4.2} and Kolmogorov's theorem 

\setcounter{proposition}{2}
\begin{proposition}\label{c3:prop3.4.3}%prop 3.4.3
  The family of measures $ \left\{ Q^{(\varepsilon)}_{(\underbar{x}^{(N)}
    \underbar{y}^{(M)}} \right\}_{\varepsilon > 0}$ is tight for any
  $\underbar{x}^{(N)}$ and $\underbar{y}^{(M)}$. 
\end{proposition}



\section{Weak Convergence of (N+M)-Point processes}\label{chap3:sec3.5}%sec 3.5

Let $\underbar{x}^{(N)} = (x_1, x_2, \ldots,x_N) \in
\mathbb{R}^{Nd}$ and $\underbar{y}^{(M)}= (y_1, \ldots, y_M)
\in \mathbb{R}^{Md}$. As before let $
Q^{(\varepsilon)}_{(\underbar{x}^{(N)} \underbar{y}^{(M)})}$ be the law
of $\phi^{\varepsilon}_t (\underbar{x}^{(N)}, X^{\varepsilon}_t
(\underbar{y}^{M}))$ defined on $V_N \times V_M$. In this section we
discuss the weak convergence of $ \left\{
Q^{(\varepsilon)}_{(\underbar{x}^{(N)}, \,  \underbar{y}^{(M)})}
\right\}_{\varepsilon > 0}$. 


\begin{theorem}\label{c3:thm3.5.1}\pageoriginale%the 3.5.1
  Assume $(A_2)_k,(A3)_k$ for some $k \geq 2$ and $(A4)$. Then for
  each $\underbar{x}^{(N)}$ and $\underbar{y}^{(M)}$ the
  family \footnote{1) We suppress
    $(\underbar{x}^{(N)}, \, \underbar{y}^{(M)})$
    from $Q^{(\varepsilon)}_{(\underbar{x}^{(N)}, \,  \underbar{y}^{(M)})}
    $.} $\left\{ Q^{(\varepsilon)}_{(\underbar{x}^{(N)}, \,  \underbar{y}^{(M)})}
  \right\}_{\varepsilon > 0}$ converges weakly to a probalility measure
  $Q^{(0)}_{(\underbar{x}^{(N)}, \,
    \underbar{y}^{(M)})}$. Further let $P^{(0)}= P^{(0)}_{k-1}$ be a
  probability measure on $W_{k-1}$ satisfying $(a)$ and $(b)$ of
  Theorem~\ref{c3:thm3.3.1}. Then the law of $(\phi_t
  (\underbar{x}^{(N)}), X_t(\underbar{y}^{(M)}),  P^{(0)})$ coincides
  with $Q^{(0)}_{(\underbar{x}^{(N)}, \,  \underbar{y}^{(M)})}$ for
  any $\underbar{x}^{(N)}, \, \underbar{y}^{(M)}$.  
\end{theorem}

The proof of the above theorem will be developed through several
Lemmas. By Proposition~\ref{c3:prop3.4.3} the family $ \{
Q^{(\varepsilon)}_{(\underbar{x}^{(N)}, \,  \underbar{y}^{(M)})}
\}_{\varepsilon > 0}$ is tight. Let $Q^{(0)}$ be an accumulation point
of $\{ Q^{\varepsilon} \}^{1)}_{\varepsilon > 0}$ as $\varepsilon \to
0$. Therefore $\{ Q^{\varepsilon} \}$ converges along a subsequence
$\varepsilon_n \downarrow 0$. Let $h$ be a bounded continuous function
on $\mathbb{R}^{(N+M) \ell d}$, where $\ell$ is a positive
integer. Let $(\phi,X) \in V_N \times V_M$. For $0 \leq s_1 <
s_2 < \cdots <s_{\ell} \leq s$, define  
\begin{equation*}
  \phi(\phi,X) = h(\phi_{s1}, \ldots \phi_{s \ell}, X_{s1}, \ldots,
  X_{s \ell}). \tag{3.5.1}\label{c3:eq3.5.1} 
\end{equation*}


Then $\phi$ is a bouded continuous function on $V_N \times V_M$. Also set 
\begin{equation*}
  \phi^{\varepsilon}(\omega) =
  h(\phi^{\varepsilon}_{s1}(\underbar{x}^{(N)}, \omega), \ldots,
  \phi^{\varepsilon}_{s \ell}(\underbar{x}^{(N)}, \omega),
  X^{\varepsilon}_{s1}(\underbar{y}^{(M)}, \omega) \ldots,
  X^{\varepsilon}_{s \ell}(\underbar{y}^{(M))}, 
\omega )) \tag{3.5.2}\label{c3:eq3.5.2}  
\end{equation*}

$\phi^{\varepsilon}$ is a measurable function defined on $(\Omega,
F,P)$, the basic probability space. 

We have 
\begin{equation*}
  E\left[\left( \int \limits^t_s
    f(\phi^{\varepsilon}_r,r)dr\right)^{\phi^\varepsilon}\right] =
  E_{Q^{(\varepsilon)}}\left[\left( \int 
    \limits^t_s f(\phi_r, r) dr 
\right)^{\phi}\right] \tag{3.5.3}\label{c3:eq3.5.3} 
\end{equation*}
for any function $f$ for which the above makes sense.

\setcounter{Lemma}{1}
\begin{Lemma}\label{c3:lem3.5.2}\pageoriginale%lem 3.5.2
  Let $\{ g^{\varepsilon}(x,t,\omega) \}_{\varepsilon > 0}$ be a
  family of $C^1$ valued processes satisfying  
  \begin{enumerate}[\rm (a)]
  \item for any $K > 0$ there exist $\gamma > 1$ and $L > 0$ such that 
    \begin{equation*}
      E\left[\sup_{|x| \leq K} |D^{\alpha}g^{\varepsilon}(x,t)|^{\gamma}\right]
      \leq L, \quad |\alpha| \leq 1, \tag{3.5.4}\label{c3:eq3.5.4} 
    \end{equation*}

  \item there exists a deterministic function $g(x,t)$ such that for
    any $K> 0$ 
    \begin{equation*}
      E\left[\sup_{|x| \leq K}  |E[ \int \limits^t_s g^{\varepsilon}(x,r)
          dr |G^{\varepsilon}_s] - \int \limits^t_s g(x,r) dr |\right] \to
      \text{as} \varepsilon\to 0.\tag{3.5.5}\label{c3:eq3.5.5} 
    \end{equation*}
  \end{enumerate}
\end{Lemma}

Then 
\begin{equation*}
  E\left[\left(\int \limits^t_s g^{\varepsilon_n}(\phi^{\varepsilon_n}_r,r)dr\right)
    \phi^{\varepsilon_\eta}\right] \xrightarrow[\varepsilon_n \to 0]{}
    E_{Q^{(0)}} \left[ \left( \int \limits^t_s g(\phi_r,r ) dr
      \right)\phi\right], \tag{3.5.6}\label{c3:eq3.5.6} 
\end{equation*}
where $\phi^{\varepsilon_n}_r = \phi^{\varepsilon_n}_r(\underbar{x}^{(N)})$.

\noindent \textit{Proof.}
  By the tigtness of $\{ \phi^{\varepsilon}_t \}_{\varepsilon > 0}$,
  for any $\theta, \eta > 0$ there exists $\zeta >0$ such that the set  
\begin{equation*}
    A^{\varepsilon}(\zeta, \eta) = \left\{ \omega \sup_{|t-s| \leq \zeta}
    |\phi^{\varepsilon}_r-\phi^{\varepsilon}_s| < \eta \right\}
    \tag{3.5.7}\label{c3:eq3.5.7}
    \end{equation*}
    \text{satisfies}
\begin{equation*}
P(A^{\varepsilon}(\zeta, \eta))> 1 - \theta. 
\tag{3.5.8}\label{c3:eq3.5.8}\hfill\Box
\end{equation*}

This is a consequence of the well known Arzela-Ascoli theorem. By
$(a)$, for any $\delta > 0$ jthere exists $\eta >0$ such that  
\begin{equation*}
  E\left[ \sup_{\substack{|x-y|< \eta \\ |x|\leq K,| y | \leq K}}
    |g^{\varepsilon}(x,t)-g^{\varepsilon}(y,t)|\right] \leq
  \delta. \tag{3.5.9}\label{c3:eq3.5.9} 
\end{equation*}


This follows from mean value theorem. We fix $\eta$. Let $\Delta$ be
a partition of $[s,t]$ given by $\Delta = \{ s = t_o < t_1 \cdots <
t_n = t \}, |\Delta| < \zeta$. We have  
\begin{align*}
   |E & \left(\left(\int^t_s g^\varepsilon (\phi^\varepsilon_r,  r)dr
   - \sum^{n-1}_{k=0} 
  \int^{t_{k+1}}_{t_k} g^\varepsilon (\phi^\varepsilon_{t_k},  r)dr\right)
  \phi^\varepsilon \right)| \\ 
  & \le | E \left( \left(\int^t_s g^\varepsilon (\phi^\varepsilon_r,  r)dr -
  \sum^{n-1}_{k=0} \int^{t_{k+1}}_{t_k} g^\varepsilon
  (\phi^\varepsilon_{t_k},  r)dr\right) \phi^\varepsilon \right):  (A^\varepsilon
  (\zeta,  \eta))| \\ 
  & +  E\left(\left(\int^t_s g^\varepsilon (\phi^\varepsilon_r,  r)dr -
  \sum^{n-1}_{k=0} \int^{t_{k+1}}_{t_k} g^\varepsilon
  (\phi^\varepsilon_{t_k},  r)dr\right) \phi^\varepsilon \right):  (A^\varepsilon
  (\zeta,  \eta)^c)| \\  
  & \le \delta(t - s) || \phi || + 2L^{\frac{1}{\gamma}}(t-s) || \phi
  || \theta \tag{3.5.10}\label{c3:eq3.5.10} 
\end{align*}\pageoriginale

Again e have
\begin{align*}
  & \left|E \left[\left(  \sum^{n-1}_{k=0} \int^{t_{k+1}}_{t_k} g^\varepsilon
    (\phi^\varepsilon_{t_k},  r)dr\right) \phi^\varepsilon \right] -
  E_{Q^{(0)}} \left[ \left( 
    \sum^{n-1}_{k=0} \int^{t_{k+1}}_{t_k} g^\varepsilon
    (\phi^\varepsilon_{t_k},  r)dr\right) \phi \right]\right| \\ 
  & \le \left| \sum^{n-1}_{k=0} \left\{ E \left[\left(E \left[\int^{t_{k+1}}_{t_k}
      g^\varepsilon(y,r)dr |G^\varepsilon_{t_K}\right]|_{y =
      \phi^\varepsilon_{t_k}} \right) \phi^\varepsilon
    \right]\right.\right.\\  
  & \hspace{5cm}\left.\left.-  E \left[\left( \int^{t_{k+1}}_{t_k}
    g(\phi^\varepsilon_{t_k},  r)dr\right) 
      \phi^\varepsilon \right] \right\} \right| \\ 
    & + \left| \sum^{n-1}_{k=0} \left\{ E \left[\left( \int^{t_{k+1}}_{t_k}
      g(\phi^\varepsilon_{t_k},  r)dr \right) \phi^{\varepsilon} \right]  -
    E_{Q^{(0)}} \left[\left( \int^{t_{k+1}}_{t_k} g(\phi_{t_k},  r)dr \right)
      \phi \right]  \right\} \right| \\ 
  & = I^\varepsilon_1 + I^\varepsilon_2,  
\quad \text{ say.} \tag{3.5.11}\label{c3:eq3.5.11}
\end{align*}

Now $I^\varepsilon_1 \rightarrow 0$ as $\varepsilon \to 0$ by (b) and
$I^\varepsilon_2 \rightarrow 0$ along a subsequence by the weak conver
of $Q^{(\varepsilon)}$. Therefore  
\begin{multline*}
  \varlimsup_{\varepsilon_n  \rightarrow 0} \left|E \left[ \left( \int^t_s
    g^{\varepsilon_n} (\phi^{\varepsilon_n}_r,\,  r)dr\right)
    \phi^{\varepsilon_n} \right] - E_{Q^{(0)}} 
    \left[\left(\sum^{n-1}_{k=0} \int^{t_{k+1}}_{t_k}
              g(\phi_{t_k},  r)dr\right) \phi \right]\right|\\ 
    \le \delta
            (t-s) || \phi || + 2 L^{\frac{1}{\gamma}} (t-s) || \phi ||
            \theta^{\frac{\gamma - 1}{\gamma}}.  
\end{multline*}

Since $\delta, \theta$ are arbitary, we  let $\theta, \delta
\rightarrow 0$ and conclude the assertion of the lemma. 

We make the following convention as a definition.

\setcounter{definition}{2}
\begin{definition}\label{c3:def3.5.3}\pageoriginale %def 3.5.3
  If $E [f^{\varepsilon_n} \phi^{\varepsilon_n} ] \rightarrow E_{Q^{(0)}}[f
    \phi]$ as $\varepsilon_n \rightarrow 0$ then we say that
  $f^{\varepsilon_n} \rightarrow f$ weakly. 
\end{definition}

\setcounter{Lemma}{3}
\begin{Lemma}\label{c3:lem3.5.4}%lem 3.5.4
  The following are $L^2$ martingales with respect to $Q^{(0)}$
\begin{equation*}
  M_t(x) = \phi_t(x) - \int^t_0 (\bar{b} (\phi_r(x),r) +
  c(\phi_r(x),r)) dr \tag{3.5.12}\label{c3:eq3.5.12} 
\end{equation*}
if $x \in \{x_1, \ldots,  x_N \}$,
\begin{equation*}
  Y_t(y) = X_t(y) - \int^t_0 \bar{b}
(y,r)dr \tag{3.5.13}\label{c3:eq3.5.13}
\end{equation*}
if $y \in \{y_1, \ldots,  y_M \}$,
\end{Lemma}

\begin{proof}
  We will consider the case $d=1$ only. Take a subsequence
  $\varepsilon_n \downarrow 0$ such that $Q^{(\varepsilon_n)}
  \rightarrow Q^{(0)}$ weakly. We have 
  \begin{equation*}
    E\left[\left( \phi^{\varepsilon_n}_t -\phi^{\varepsilon_n}_s - \int^t_s
      \bar{b}^{\varepsilon_n} (\phi^{\varepsilon_n}_r, r)dr - \int^t_s
      \tilde{b}^{\varepsilon_n}_r (\phi^{\varepsilon_n}_r, r)dr\right)
      \phi^{\varepsilon_n}\right] = 0. \tag{3.5.14}\label{c3:eq3.5.14} 
  \end{equation*}
\end{proof}

Now $\phi^{\varepsilon_n}_t - \phi^{\varepsilon_n}_s \rightarrow
\phi_t - \phi_s$ weakly. By ($A2)_k$ and the previous lemma 
\begin{equation*}
 \int^t_s \bar{b}^{\varepsilon_n}_r (\phi^{\varepsilon_n}_r, r)dr
 \rightarrow \int^t_s \bar{b}(\phi_r,  r)dr \; 
 \text{ weakly}. \tag{3.5.15}\label{c3:eq3.5.15}
\end{equation*}

Using Ito's formula we have
\begin{align*}
  \int^t_s \tilde{b}^{\varepsilon_n} (\phi^{\varepsilon_n}_r, r)dr & =
  \int^t_s \tilde{b}^{\varepsilon_n} (\phi^{\varepsilon_n}_s, r)dr +
  \int^t_s  dr\\ 
  & \hspace{2.5cm}\left(\int^r_s  \frac{\partial}{\partial x}
  \tilde{b}^{\varepsilon_n} (\phi^{\varepsilon_n}_\sigma, r)
  \tilde{b}^{\varepsilon_n} (\phi^{\varepsilon_n}_\sigma, \sigma)d
  \sigma\right) \\ 
  & + \frac{1}{2} \int^t_s dr \left(\int^r_s \frac{\partial^2}{\partial
    x^2} \tilde{b}^{\varepsilon_n} (\phi^{\varepsilon_n}_\sigma, r)
  a^{\varepsilon_n} (\phi^{\varepsilon_n}_\sigma,
  \phi^{\varepsilon_n}_\sigma,  \sigma ) d \sigma \right) \\ 
  & \qquad +   \text{a martingale}. \\
  & = I^{\varepsilon_n}_1 + I^{\varepsilon_n}_2 + I^{\varepsilon_n}_3
  + I^{\varepsilon_n}_4,  \text{say}. \tag{3.5.16}\label{c3:eq3.5.16} 
\end{align*}

Since $E \left[\int^t_s \tilde{b}^{\varepsilon_n}(z,r) dr |
  {G}^{\varepsilon_n}_s\right]_{z = \phi^{\varepsilon_n}_s} \rightarrow 0$,
we have $I^{\varepsilon_n}_1 \rightarrow 0$ weakly by 
lemma~\ref{c3:lem3.5.2}. Using $(A2)_k$, we obtain 
$$
E \left[I^{\varepsilon_n}_2 | G^{\varepsilon_n}_s\right] = E \left[ \int^t_s
  c^{\varepsilon_n} (\phi^{\varepsilon_n}_\sigma,  t, \sigma) d\sigma
  | G^{\varepsilon_n}_s\right] \rightarrow \int^t_s C(\phi_r,  r)dr
~\text{weakly}.  
$$\pageoriginale
$$E\left[ I^{\varepsilon_n}_3 | G^{\varepsilon_n}_s\right] = E \left[
  \int^t_s \dfrac{\partial^2}{\partial x^2} d^{\varepsilon_n}
  (\phi^{\varepsilon_n}_\sigma,  \phi^{\varepsilon_n}_\sigma,  t, \sigma)
 d \sigma | G^{\varepsilon_n}_s\right] \rightarrow 0$$ 
weakly since
$\dfrac{\partial^2}{\partial x^2} d^{\varepsilon_n} \rightarrow 0
$. Also $I^{\varepsilon_n}_4 \rightarrow 0$ weakly since it is a
martingale. Combining all these result it follows that $M_t(s)$ is a
martingale. The same procedure matatis mutandis shows that $Y_t(y)$ is
a martingale. 

In the next lemma, we shall compute the quadratic variations of these
martigales. 

\setcounter{Lemma}{4}
\begin{Lemma}\label{c3:lem3.5.5} %lem 3.5.5
  With respect to $Q^{(0)}$ we have
\begin{align*}
&{\rm (i) }  
    <M_t(x),  M_t(y)^*>  =~ \int^t_0 (a + \tilde{a})
    (\phi_r(x), \phi_r (y), r)dr,  \tag{3.5.17}\label{c3:eq3.5.17}\\
&{\rm (ii) }< M_t(x),  Y_t(y)^*>  =~ \int^t_0 (a + \tilde{a})
 (\phi_r(x),y,r) dr, \tag{3.5.18}\label{c3:eq3.5.18}\\ 
&{\rm (iii) } <Y_t(x),  Y_t(y)^*>  =~ \int^t_0 (a + \tilde{a})
    (x,y,r)dr. \tag{3.5.19}\label{c3:eq3.5.19} 
\end{align*}
\end{Lemma}


\begin{proof}
  As before, we consider the case $d = 1$ only. Using Ito's formula, we have 
  \begin{align*}
    \phi^\varepsilon_t (x)\phi^\varepsilon_t(y) -
    \phi^\varepsilon_s(y) & = \int^t_s \phi^\varepsilon_r (x)
    \bar{b}^{\varepsilon}(\phi^\varepsilon_r(y),r)dr + \int^t_s
    \phi^\varepsilon_r(y) \bar{b}^{\varepsilon}(\phi^\varepsilon_r(x),r)dr \\ 
    & + \int^t_s \phi^{\varepsilon}_r(x)
    \tilde{b}^{\varepsilon}(\phi^\varepsilon_r(y),r)dr + \int^t_s
    \phi^\varepsilon_r (x)
    \tilde{b}^{\varepsilon}(\phi^\varepsilon_r(x),r)dr \\ 
    & +\int^t_s a^{\varepsilon}
    (\phi^{\varepsilon}_r(x),\phi^\varepsilon_r(x),r)dr +
    \text{a martingale}. \tag{3.5.20}\label{c3:eq3.5.20} 
\end{align*}
\end{proof}

Let $\varepsilon_n \downarrow 0$ be the same sequence as in Lemma~\ref{c3:lem3.5.4} then
\begin{gather*}
  \phi^{\varepsilon_n}_t (x)\phi^{\varepsilon_n}_t(y) -
  \phi^{\varepsilon_n}_s(x)\phi^{\varepsilon_n}_s(y) \rightarrow
  \phi_t(x)\phi_t(y) - \phi_s(x)\phi_s(y)  ~\text{weakly}, \\ 
  \int^t_s
  \phi^{\varepsilon_n}_r(x)\bar{b}^{\varepsilon_n}(\phi^{\varepsilon_n}_r(y),r)dr
  \rightarrow \int^t_s \phi_r(x)\bar{b}(\phi_r(y),y)dr ~\text{weakly},
  \\ 
  \int^t_s a^{\varepsilon_n}(\phi^{\varepsilon_n}_r(x),
  \phi^{\varepsilon_n}_r(y),r) dr \rightarrow \int^t_s
  a(\phi_r(x),\phi_r(y), r)dr  ~\text{weakly}. 
\end{gather*}


We\pageoriginale next consider the third term in the right hand side of \eqref{c3:eq3.5.2}. We have 
\begin{align*}
  \phi^{\varepsilon_n}_r(x)\tilde{b}^{\varepsilon_n}& (\phi^{\varepsilon_n}_r(y),r)
    =
  \phi^{\varepsilon_n}_s(x)\tilde{b}^{\varepsilon_n}(\phi^{\varepsilon_n}_s(y),r)\\
  & + \int^r_s \phi^{\varepsilon_n}_\sigma(x) \frac{\partial}{\partial
    x}\tilde{b}^{\varepsilon_n} (\phi^{\varepsilon_n}_\sigma(y),r)
  b^{\varepsilon_n}(\phi^{\varepsilon_n}_\sigma(y), \sigma) d \sigma
  \\ 
  & + \int^r_s b^{\varepsilon_n} \phi^{\varepsilon_n}_{\sigma}(x), \sigma
  \tilde{b}^{\varepsilon_n}(\phi^{\varepsilon_n}_\sigma(y),r) d \sigma
  \\ 
  & + \int^r_s \frac{\partial}{\partial x}
  \tilde{b}^{\varepsilon_n} (\phi^{\varepsilon_n} (y), r )
  a^{\varepsilon_n} (\phi^{\varepsilon_n}_\sigma (x),
  \phi^{\varepsilon_n}_\sigma (y), \sigma ) d \sigma \\  
  & + \frac{1}{2} \int^r_s \phi^{\varepsilon_n}_\sigma (x)
  \frac{\partial^2}{\partial x^2}
  \tilde{b}^{\varepsilon_n} (\phi^{\varepsilon_n}_\sigma (y), r)
  a^{\varepsilon_n} (\phi^{\varepsilon_n}_\sigma (y),
  \phi^{\varepsilon_n}_{\sigma} (y), \sigma ) d \sigma\\
  &\qquad + \text{ a martingale}\\
  & = I^{\varepsilon_n}_1(r) + I^{\varepsilon_n}_2 (r)+
  I^{\varepsilon_n}_3(r)+ I^{\varepsilon_n}_4(r) +
  I^{\varepsilon_n}_5(r)+ I^{\varepsilon_n}_6(r). \tag{3.5.21}\label{c3:eq3.5.21} 
\end{align*}

Now $\int^t_s I^{\varepsilon_n}_1(r)dr \rightarrow 0$ weakly since
$\phi^{\varepsilon_n}_s$ is $G^{\varepsilon_n}_s$- measurable. Since  
$$
\displaylines{\hfill
  E\left[ \int^t_s I^{\varepsilon_n}_2(r) dr | G^{\varepsilon_n}_s\right] = E\left[
    \int^t_s \phi^{\varepsilon_n}_\sigma(x) c^{\varepsilon_n}_\sigma
    (\phi^{\varepsilon_n}_\sigma(y),t, \sigma ) d \sigma |
    G^{\varepsilon_n}_s\right] \hfill \cr
  \text{therefore }
  \hfill \int^t_s I^{\varepsilon_n}_3(r)dr \rightarrow \int^t_s \phi_r(x)
  c(\phi_r(y), r) dr  ~\text{weakly}. \hfill }
$$

Also
$$
\displaylines{\hfill 
  \int^t_s I^{\varepsilon_n}_3(r)dr \rightarrow \int^t_s
  A(\phi_\sigma(x), \phi_\sigma(y), \sigma) d \sigma
  ~\text{ weakly},\hfill \cr 
  \text{and} \hfill 
  \int^t_s I^{\varepsilon_n}_k(r)dr \rightarrow 0  \text{ weakly for }~ k =
  4,5,6. \hfill } 
$$

To sum up we have
\begin{align*}
  (1) \phi_t(x) \phi_t(y) & - \phi_s(x) \phi_s (y)  - \int^t_s \phi_r
  (x) \bar{b} (\phi_r(y), r) dr \\ 
  & -  \int^t_s \phi_r (y) \bar{b} (\phi_r(x), r) dr -\int^t_s
  a(\phi_r(x), \phi_r(y), r) dr \\ 
  & -  \int^t_s \phi_r (x) c (\phi_r(y), r) dr -\int^t_s \phi_r(y)c
  (\phi_r(x), r) dr \\ 
  & - \int^t_s \tilde{a} (\phi_r (x), \phi_r(y), r) dr
\end{align*}
is\pageoriginale a martingale with respect to $Q^{(0)}$, i.e
\begin{align*}
  \phi_t(x) \phi_t(y) - \phi_s(x) \phi_s(y) & - \int^t_s \phi_r(x)
  (\bar{b}(\phi_r(y), r) + c (\phi_r(y), r))dr \\ 
  & - \int^t_s \phi_r(y)(\bar{b}(\phi_r(x), r) + c (\phi_r(x), r))dr \\
  & -\int^t_s (a + \tilde{a}) (\phi_r(x),  \phi_r(y), r) dr
\end{align*}
is a $Q^{(0)}$-martingale.

On the other hand by Ito's formula we have

(2) $\phi_t(x) \phi_t(y) = \phi_s(x) \phi_s(y) +
\int^t_s \phi_r(x)d \phi_r(y) + \int^t_s \phi_r (y) d \phi_r(x) + <
\phi_t(x),  \phi_t(y) >$. 

Obviously
$$
< \phi_t(x), \phi_t(y) > = <M_t(x), M_t(y)>.
$$ 
Substituting (2) in (1) we find
\begin{multline*}
  \int \phi_r(x) dM_r(y) + \int^t_s \phi_r(y) d M_r(x)\\ 
  - \int^t_s (a +
  \tilde{a}) ( \phi_r(x), \phi_r(y), r) dr + <M_t(x), M_t(y)> 
\end{multline*}
is a martingale. Hence 
$$
<M_t(x), M_t(y)^*>  = \int^t_0 (a + \tilde{a}) (\phi_r(x),  \phi_r(y), r) dr.
$$
This proves $(i)$. The relations (ii) and (iii) can be proved similarly.


\smallskip
\noindent{\textbf{Proof of the Theorem 3.5.1 under (A4)}}~~
Let $\underline{x}^{(N)}_0 = (x_10, \ldots,
  x_N 0) \in \mathbb{R}^{Nd}$, $\underline{y}^{(M)}_0 =(y_10,
  \ldots,  x_M 0) \in \mathbb{R}^{Md}$ be two fixed
  points. For  $\underline{x} = (x_1, \ldots,  x_N ) \varepsilon$
  $\mathbb{R}^{Nd}$ and  $\underline{y} = (y_1, \ldots,  x_M )
  \in \mathbb{R}^{Md}$, define the following differential
  operator 
  \begin{align*}
    L^{(N,M)}_{s,\underline{y}_o^{(M)}} f(\underline{x},
    \underline{y}) & = \frac{1}{2} \sum^N_{p,q=1}\sum^d_{i,j=1} (a +
    \tilde{a})_{ij} (x_p,  x_q, s) \frac{\partial^2 f (\underline{x},
      \underline{y})}{\partial x^i_p \partial x^j_q} \\ 
    & + \sum_{i,p} \left\{ \bar{b}^i (x_p,  s) + c^i(x_p, s) \right\}
    \frac{\partial f}{\partial x^i_p} (x,y) \\ 
    & + \frac{1}{2} \sum_{p,q} \sum_{i,j} (a + \tilde{a})_{ij}
    (y_{p^0}, y_{q^0},s ) \frac{\partial^2 i (\underline{x},
      \underline{y})}{\partial y^i_p \partial y^j_q} \\ 
    & + \sum_{i,p} \bar{b}^1 (y_{p^0},s) \frac{\partial
      f(\underline{x}, \underline{y})}{\partial y^i_p} \\ 
    & +\frac{1}{2} \sum_{p,q} \sum_{i,j} (a,  \tilde{a})_{ij} (x_p,
    y_{q^0},s) \frac{\partial^2 f (\underline{x},
      \underline{y})}{\partial x^i_p \partial y^j_q} 
\tag{3.5.22}\label{c3:eq3.5.22}
\end{align*}

 By\pageoriginale Ito's formula, if is a $C^2$-function with bounded derivatives,
 then using Lemmas~\ref{c3:lem3.5.5} and 3.5.6, we find that 
the following is  a martingale with respect to $Q^{(0)}$ 
 \begin{equation*}
   f (\phi_t, X_t) - \int^t_0 L^{(N,M)}_{s,\underline{y}_o^{(M)}}
   f(\phi_s,  X_s) ds. \tag{3.5.23}\label{c3:eq3.5.23} 
 \end{equation*}

Therefore $Q^{())}$ is the solution of the martingale problem for\break
$L^{(N,M)}_{s,y^{(M)}_0}$ and hence it is unique. Now
$\left\{Q^{(0)}_{(\underline{x}_0^{N}, \underline{y}_0^{M})
}\right\}_{(\underline{x}_0^{M}, \underline{y}_0^{M})}$ is a consistent
  family of 
measures. Therefore there exists a unique probability measure $P^{(0)}$ on
$W^2_{k-1}$ such that the law of $(\phi_t (\underline{x}_0^{(N)}, X_t
(\underline{y}_0^{M}))$ with respect to $P^{(0)}$ is
$Q^{(0)}_{(\underline{x}_0^{N}, \underline{y}_0^{M})}$.  Also
\eqref{c3:eq3.5.17}--\eqref{c3:eq3.5.19} with $x \in \{ x_1 0, \ldots,  x_N 0
\}$,  $ y \in \{ y_1 0, \ldots, Y_M 0 \}$ is satisfied with
respect to $P^{(0)}$. Hence $X_t(x)$ is a $C^{k-1}-$ valued Brownian
motion with local characteristics $(a + \tilde{a}, b)$. We claim that
$\phi_t$ is generated $X_t (x) + \int^t_0 c(x,r)dr$. Set  
$$
\tilde{M}_t(x) =  \int^t_0 Y(\Phi_s, ds).
$$
 
Then 
$$
< \tilde{M}_t (x), \tilde{M}_t (y)^* > = \int^t_0 (a +  \tilde{a}) 
(\phi_r (x), \phi_r (y), r) dr. 
$$

Therefore\pageoriginale 
$$
< M_t (x)- \tilde{M}_t (x), (M_t(y) - \tilde{M}_t(y))^* > = 0.
$$
This implies $M_t(x) = \tilde{M}_t (x)$. Thus  $\phi_t$ is generated
by $X_t(x) + \int\limits_{o}^t c(x,r) dr$.  


\smallskip
\noindent{\textbf{Proof of the Theorem 3.5.1 (without (A4))}}~~
 Let $P^{(0)}$ be a probability measure on
  $W^2_{k-1}$ satisfying $(a)$ and $(b)$ of the theorem. Then  the law
  of $(\phi_t(\underline{x}_o^{(N)}),
  X_t(\underline{y}_o^{(M)}) , P^{(0)})$ is
  $Q^{(0)}_{(\underline{x}_o^{(N)}, \, \underline{y}_o^{(M)})}$. We claim
  that $Q^{(\varepsilon)} \xrightarrow[\varepsilon \to 0]{} Q^{(0)}$
  weakly.   

\begin{enumerate}
\item[{\bf Step 1.}] We consider the truncated process for $K >
  0$. Let $\psi_K:\mathbb{R}^d \to \mathbb{R}$ be a smooth function
  such that  
  \begin{equation*}
    \psi_K(x) =
    \begin{cases}
      1 & \text{if} \quad  |x| \le / 2, \\
      0 & \text{if} \quad  |x| > K,
    \end{cases}
  \end{equation*}
  and $0 \le \psi_K\le 1$. Set $X^{\varepsilon, K}(x,t)=
  X^\varepsilon(x,t) \psi_K(x)$. Then the local characteristics of
  $X^{\varepsilon, K}$ are $a^\varepsilon(x,y,t) \psi_K(x) \phi_K(y)$ and
  $b^\varepsilon(x,t) \psi_K(x)$ which obviously satisfy (A4). Let
  $\phi^{\varepsilon, K}_t$ be the flow generated by
  $X^{\varepsilon,K}$. Denote the law of $(\phi^{\varepsilon,K}_t
  (\underline{x}_o^{(N)}), X^{\varepsilon,K}_t
  (\underline{y}_o^{(M)}))$ by  $Q^{\varepsilon, K}$. Then
  $Q^{(\varepsilon, K)} \xrightarrow[\varepsilon \to 0]{} Q^{0,K}$
  weakly. Let us compare $Q^{(0,K)}$ and $Q^{(0)}$. We may assume that
  $|\underline{x}_o^{(N)} | \le \dfrac{K}{2},| \underline{y}_o^{(M)}|
  \le \dfrac{K}{2}$. Let $A \in B(V_N \times  V_M)$.  

Then clearly
\begin{equation*}
  Q^{(0)} \left(A \cap \left\{ \phi: || \phi || <
  \frac{K}{2}\right\}\right) = Q^{(0,K)} \left(A 
  \cap \left\{\phi  || \phi < \frac{K}{2} \right\}\right). 
\tag{3.5.24}\label{c3:eq3.5.24} 
\end{equation*}


\item[{\bf Step 2.}]
In order to show the weak convergence of $\{
Q^{(\varepsilon)}\}_{\varepsilon < 0}$, it suffices to show that for
any  closed subset $S$ of $V_N \times  V_M$  
$$
\varlimsup_{\varepsilon \to 0} Q^{(\varepsilon)}(S) \le Q^{(0)} (S). 
$$

For\pageoriginale any $\delta > 0 $, there exists a $K > 0$ such that
$Q^{(0)}{_{(G_K)}} > 1 - \delta $, where $G_K= \{ \phi:  ||\phi || <
K/2\}$.Then 
\begin{equation*}
  Q^{(0,k)}(G_K) = Q^{(0)}(G_K) > 1 -\delta, ~\text{by }\eqref{c3:eq3.5.24}
\end{equation*}

Since $ Q^{(\varepsilon,K)} \to Q^{(0,K)} $ weakly, therefore
$$\varliminf\limits_{\varepsilon \to 0} Q^{(\varepsilon,K)}(G_K) \ge
Q^{(0)} (G_K).$$ 
Hence there exists an $\varepsilon_o > 0$ such that for any
$\varepsilon < \varepsilon_o$, we have $Q^{(\varepsilon,K)} (G_K) >
1-2 \delta$. Therefore   
\begin{align*}
  Q^{(\varepsilon)}(S) & = Q^{(\varepsilon)} (S \cap G_K) + Q
  ^{(\varepsilon)} (S \cap G^c_K) \\ 
  & \le Q^{(\varepsilon,K)} (S \cap G_K) + Q ^{(\varepsilon,K)} (G_K^c)\\
  & \le Q ^{(\varepsilon,K)} (S \cap \bar{G}_K)+2 \delta.
\end{align*}

Thus
\begin{align*}
  \varlimsup_{\varepsilon \to 0} Q^{(\varepsilon)}{_{(S)}} & \le
  \varlimsup_{\varepsilon \to 0} Q^{(\varepsilon,K)} (S \cap
  \bar{G}_K) + 2 \delta\\ 
  & \le Q^{(0,K)} (S \cap \bar{G}_K) + 2 \delta\\
  & \le Q^{(0)} (S) + 2 \delta
\end{align*}

Since $\delta$ is arbitrary
$$
\varlimsup_{\varepsilon \to 0} Q^{(\varepsilon)} (S) \le Q^{(0)} (S) 
$$
\end{enumerate}


\section{Tightness of Sobolev Space-valued Processes}\label{chap3:sec3.6}

Let $P_{m,p}^{(\varepsilon)}$ denote the law of $(\phi^\varepsilon_t,
X_t^\varepsilon)$ on $W^2_m = (C([0,T]; H^{loc}_{m,p}))^2$ for $m \ge
k-1$. We shall discuss the tightness of $\{
P^{(\varepsilon)}_{m,p}\}_{\varepsilon >0}$. We begin by considering
the case $m=0, W^2_{0,p} = (C([0,T]: L_p^{loc}))^2 $. For some $K > 0$
consider a truncated process $(\phi^{\varepsilon, K}_t,  X^{\varepsilon,
  K}_t)$. Let the law of the truncated process be denoted by
$P^{(\varepsilon,K)}_{0,p}$. Let $\tilde{\psi}_K:  \mathbb{R}^d \to
\mathbb{R}^d$ be a smooth function such that  
$$
\tilde{\psi}_K^{(x)} =
\begin{cases}
  x & \text{if}~  |x| < K/2\\
  0 & \text{if}~  |x| > K
\end{cases}
$$\pageoriginale

Set $\tilde{X}_t^{\varepsilon,K} = \tilde{\psi}_K
(X^{\varepsilon,K}_t)$. Let the law of $(\phi^{\varepsilon,K}_t,
\tilde{X}^{\varepsilon,K}_t)$ be denoted by
$\tilde{P}^{(\varepsilon,K)}_{o,p}$. We have the following result
about the tightness of
$\left\{\tilde{P}^{(\varepsilon,K)}_{0,p}\right\}$. 

\begin{Lemma}\label{c3:lem3.6.1} %lem 3.6.1
  $\left\{ \tilde{P}^{(\varepsilon,K)}_{0,p}\right\}_{\varepsilon >
    0}$ is tight with respect to the weak topology of $W^2_{0,p}$ for
  any $K > 0$ and $p > d V3 \left(2-\dfrac{1}{\gamma}\right)$ 
\end{Lemma}

\noindent \textit{Proof.}
  We have form Lemma~\ref{c3:lem3.4.2}
  $$ 
  \displaylines{\hfill 
  E\left[| \phi^{\varepsilon,K}_t (x) - \phi^{\varepsilon,K} _s (x) |^p\right] \le
  L|t-s|^{2- \frac{1}{\gamma}} ~\text{ for all }~ x \in
  \mathbb{R}^d\hfill \cr 
  \text{and} \hfill 
  E\left[| \phi^{\varepsilon,K}_t (x)|^p\right] \le L ~\text{ for all }~ x
  \in \mathbb{R}^d \hfill \Box} 
  $$ 

Integrating the above relations with respect to the Lebesgue measure
on the ball $B_n = \{ x:  |x| \le n\}$, we get 
\begin{align*}
  & E \left[|| \phi^{\varepsilon,K}_t - \phi^{\varepsilon,K}_s ||
    ^p_{0,p,n}\right] \le L ~\text{vol}~ (B_n) |t-s| ^{2 -\frac{1}{\gamma}}\\ 
  & E \left[|| \phi^{\varepsilon,K}_t || ^p_{0,p,n}\right] \le L ~\text{Vol}~ (B_n).
\end{align*}

We can find similar estimates for $\tilde{X}^{\varepsilon,K}_t$. These
estimates imply the tightness of $\{ \tilde{P^{(\varepsilon,K)}} _{0,p}\}
_{\varepsilon > 0}$ in the weak topology. 

Let $p^{(0,K)}_{k-1}$ be a probability measure on $W^2_{k-1}$, with
local characteristics $a (x,y,t) \psi_K (x) \psi_K (y)$ and $b
(x,t)\psi_K (x)$, where $\psi_K: \mathbb{R}^d \to \mathbb{R}^1$ is the
smooth function described in section~\ref{chap3:sec3.5}. For $0 \le t_1 < t_2 \dots
< t_N$ define 
\begin{multline*}
  \tilde{P}^{(0,K)}_{k-1} (X_{t_{_1}} \in A_1, \ldots, X_{t_{N}}
  \in A_N, \phi_{_t{_1}}  \in   B_1,\ldots,
  \phi_{_t{_N}}  \in  B_N)\\ 
  = P^{(0,K)}_{k-1} (\psi_K (X_{t_1}) \in A_1, \ldots,
  \psi_K (X{_t}_{_N}) \in B_N, \phi_{t{_1}} \in B_1
  \ldots, \phi_{t{_N}} \in B_N ) 
\end{multline*}\pageoriginale

The measure $\tilde{P}^{(0,K)}_{k-1}$ on $W^2_{k-1}$ can be extended
to a measure $\tilde{P}^{(0,K)}_{0,p}$ on $W^2_{0,p}$. 



\begin{Lemma}\label{c3:lem3.6.2} % 3.6.2
  $\{\tilde{P}^{(\varepsilon,K)}_{0, p}\}_{\varepsilon > 0}$ converges
  weakly to $\tilde{P}^{(0,K)}_{0, p}$ with respect to the weak
  topology of $W^2_{0, p}$. 
\end{Lemma}

\begin{proof}
  Fix $N$, take $t_1 < t_2 <  \dots < t_N$. It follows from the weak
  convergence of the $(N+M)$- point process that  
  \begin{equation*}
    \lim_{\varepsilon \to 0} E^{(\varepsilon,K)}_{k-1} \left[\phi{_t{_i}}(
      x_1) \dots \phi{_t{_N}}( x_N) \right]  E^{(0,K)}_{k-1} \left[\phi{_t{_i}}(
      x_1) \dots \phi{_t{_N}}( x_N) \right]. \tag{3.6.1}\label{c3:eq3.6.1} 
  \end{equation*}

Let $\eta_1( x ),\ldots, \eta_N(x) \in L^q (B_n)$, where
$\dfrac{1}{p} + \dfrac{1}{q} = 1$. Multiply the above to the both
sides of \eqref{c3:eq3.6.1}) and integrate over $B_n$. Then we get  
\begin{multline*}
  \lim_{\varepsilon \to 0} E^{(\varepsilon,K)}_{k-1} \left[(\phi_{t_1},
    \eta_1)  (\phi_{t_2}, \eta_2) \dots (\phi_{t_N} \eta_N)\right]\\ 
  = E^{(0,K)}_{k-1}  \left[(\phi_{t_1} \eta_1) \dots (\phi_{t_N}
  \eta_N)\right]. \tag{3.6.2}\label{c3:eq3.6.2} 
\end{multline*}

Similarly we can show that for $\alpha_1, \ldots,\alpha_N > 0$, $\beta_1$,
$\ldots$, $\beta_M > 0$, $\zeta_1 \ldots,\zeta_M  \in  L^q (B_n)$  
\begin{align*}
  \lim_{\varepsilon \to 0}  E^{(\varepsilon,K)}_{k-1} &
  \left[(\phi_{t_1}, \eta_1)^{\alpha_1} \dots (\phi_{t_N} ,  \eta_N)^{\alpha_N}
    (\tilde{\psi}_K (X_{t_1}), \zeta_1)^{\beta_1} \ldots
(\tilde{\psi}_K (X_{t_M}) .  \zeta_M)^{\beta_M}\right]\\ 
& = E^{(0,K)}_{k-1} [(\phi_{t_1} , \eta_1)^{\alpha_1} \ldots
    (\phi_{t_N}, \eta_N)^{\alpha_N} (\tilde{\phi}_K (X_{t_1}),
    \zeta_1)^{\beta_1} \ldots\\
    &\qquad (\tilde{\psi}_K (X_{t_M}),
    \zeta_M)^{\beta_M}]\\[4pt]
& = E^{(0,K)}_{k-1} [(\phi_{t_1}, \eta_1)^{\alpha_1} \ldots
    (\phi_{t_N}, \eta_N)^{\alpha_N} (X_{t_1}, \zeta_1)^{\beta_1}
    \ldots (X_{t_M}, \zeta_M)^{\beta_M}]
\end{align*}

We can replace $\tilde{P}^{(\varepsilon,K)}_{k-1}$ etc. by
$\tilde{P}^{(\varepsilon,K)}_{0,p}$ etc. and hence
$\tilde{P}^{(\varepsilon,K)}_{0,P} \to \tilde{P}^{(0,K)}_{0, p}$ with
respect to the weak topology. 
\end{proof}

\setcounter{proposition}{2}
\begin{proposition}\label{c3:prop3.6.3}\pageoriginale % 3.6.3
  $\{ P_{0,p}{^{(\varepsilon)}}\}_{\varepsilon > 0}$ converges
  weakly.The proof is similar to that of the weak convergence of
  $\{Q{^{(\varepsilon)}}\}$ and is therefore omitted. 
\end{proposition}

\setcounter{theorem}{3}
\begin{theorem}\label{c3:thm3.6.4} % 3.6.4
  Assume $(A2)_k, (A3)_k, k \ge 2$. Then $\{
  P_{k-2,p}{^{(\varepsilon)}}\}_{\varepsilon > 0}$ is tight with
  respect to weak topology of $W^2_{k-2,p}$ 
\end{theorem}

\begin{proof}
  Consider the system of equations for $D^\alpha \phi^\varepsilon_t, |
  \alpha | \le k - 2$. Set $D^\alpha \phi^\varepsilon_t = {}^\alpha
  \phi^\varepsilon_t$ and consider $\underline{\phi}^\varepsilon_t =
  ({}^\alpha \phi^\varepsilon_t |\alpha| \le k-2 )$, 
  \begin{align*}
    d \phi^\varepsilon_t & = X^\varepsilon (\phi^\varepsilon_t, dt)\\
    d^\alpha \phi^\varepsilon_t & = \sum^d_{i=1}
    \frac{\partial}{\partial x_i} X^\varepsilon (\phi^\varepsilon_t,
    dt ) ({}^\alpha \phi^\varepsilon_t)^i, |\alpha| = 1 
  \end{align*}
  etc. In vector notation
  $$
  d \phi^\varepsilon_t = \underline{X}^\varepsilon
  (\underline{\phi}^\varepsilon_t, dt) 
  $$
  where $\underline{X}^\varepsilon (x,t) = (X^\varepsilon (x,t),
  \sum\limits_i \frac{\partial}{\partial x_i} X^\varepsilon (x,t)
  x^i_\beta, \ldots), \underline{x} = (x,x_\beta, \ldots)$. Then the
  law of $(\underline{\phi}^\varepsilon_t,
  \underline{X}^\varepsilon_t),
  \text{viz}. \underline{P}_{0,p}{^{(\varepsilon)}}$ with respect to
  the weak topology which is equivalent to  the tightness of $\{
  P^{(\varepsilon)}_{k-2, p}\}$. 
\end{proof}

\setcounter{remark}{4}
\begin{remark}\label{c3:rem3.6.5} % 3.6.5
  In view of Proposition 3.1.10, the family of measures\-$\{
  P_{k-3}{^{(\varepsilon)}}\}_{\varepsilon >0}$ in tight in $W^2_{k-3}$ 
\end{remark}


\section{Proof of the Main Theorem}\label{chap3:sec3.7}
The weak convergence of $\{P_m{^{(\varepsilon)}}\}_{\varepsilon > 0}
(m \le k-3)$ has already been proved. Here we shall prove the strong
convergence of $\phi^\varepsilon_t$ under the assumption that
$X^\varepsilon_t$ converges strongly to $X^o_t$. We shall prove that
$\phi^\varepsilon_t \to \phi^\circ_t$ strongly. Consider the
stochastic differential equation  
\begin{equation*}
  d \phi^o_t = X^o (\phi^o_t,dt) +c(\phi^o_t)dt, 
\phi^o_o = x \tag{3.7.1}\label{c3:eq3.7.1}
\end{equation*}\pageoriginale

The solution of \eqref{c3:eq3.7.1} is denoted by 
$\phi^o_t(x)$. For $m \le k-3$,
let $\tilde{P}_m^{(\varepsilon)} $ denoted the law of
$(\phi^\varepsilon_t, X^\varepsilon_t,\phi^o_t,X^o_t)$ defined on
$W^2_m \times \tilde{W}^2_m$ (where $\tilde{W}^2_m$) is a replica of
$ W^2_m)$. A typical element of $\tilde{W}^2_m$ will be denoted as
$(\tilde{\phi}, \tilde{X})$. We show that
$\{\tilde{P}_m{^{(\varepsilon)}}\}_{\varepsilon > 0}$ converges weakly
to some $\tilde{P}_m{^{(0)}}$. Since
$\{\tilde{P}_m{^{(\varepsilon)}}\}_{\varepsilon > 0}$ is tight, let
$\tilde{P}_m{^{(0)}}$ be any limits point of
$\{\tilde{P}_m{^{(\varepsilon)}}\}$. Then
$\tilde{P}_m{^{(0)}}|_{W^2_m} = P_m{^{(0)}}$ ($\equiv$ limit of
$P_m{^{(\varepsilon)}}$). By the first assertion of the theorem
$\phi_t$ is generated by $X_t + \int\limits_o^t c (x,r)dr$. Since $X_t
= \tilde{X}_t$ a.s. $\tilde{P}_m{^{(0)}}, \phi_t = \tilde{\phi}_t$ a.s
$\tilde{P}_m{^{(0)}}$ by the uniqueness of the solution  i.e.,
$\tilde{P}_m{^{(0)}}$ is supported in the diagonal set $\{ (\phi, X,
\tilde{\phi}, \tilde{X}): \phi = \tilde{\phi}, X = \tilde{X}\}$. Let
$\rho_m$ be the metric on $W^2_m$, i.e., for $(\phi, \tilde{\phi})
\in C ([0,T]$; $C^m) \times  C ([0,T]; C^m)$,  
$$
\rho_m (\phi, \psi) = \sum_N \frac{1}{2^N} \frac{|||\phi-
  \psi|||_{m,N}}{1 + |||\phi- \psi|||_{m,N}} 
$$
which is a bounded continuous function on $W^2_m\times
\tilde{W}^2_m$. Therefore  
$$
E\left[\rho_m (\phi^\varepsilon, \phi^o)\right] = \tilde{E}_m{^{(\varepsilon)}}
\left[\rho_m (\phi, \tilde{\phi})\right] \to E{^{(0)}}_m
\left[\rho_m(\phi, \tilde{\phi})\right] = 0,  
$$
since $\rho_m (\phi, \tilde{\phi}) = 0$ a.s. Hence $\phi^\varepsilon
\to \phi^o$ strongly. 



\section{Proof of the Approximation Theorem}\label{chap3:sec3.8}
In this section, we shall discuss the proof of the approximation
theorem for stochastic ordinary differential equations described in
section~\ref{chap3:sec3.2} Recall  
\begin{equation*}
  \frac{d \phi^\varepsilon_t}{d t} = \sum_{k=1}^r F_k
  (\phi^\varepsilon_t, t) v^\varepsilon_k (t) + F_o
  (\phi^\varepsilon_t,t). \tag{3.8.1}\label{c3:eq3.8.1} 
\end{equation*}

We assume (A1). To prove the theorem we need the following lemma.


\begin{Lemma}\label{c3:lem3.8.1} % 3.8.1
  Assume $(A1)$. Then for any continuous functions $f(x,t)$ and $g (x,
  t )$ on $\mathbb{R}^d \times [0,T]$ 
  \begin{equation*}
    E \left[\int\limits_s^t f(x,\tau) v^\varepsilon_i (\tau) d\tau
      \int\limits_s^t g (y,\sigma) v^{\varepsilon}_j (\sigma) d
      \sigma|G^\varepsilon_s \right]\to 
    \int\limits^t_s f(x,r) g (y,r) \nu_{ij}(r) \, dr 
\tag{3.8.2}\label{c3:eq3.8.2} 
\end{equation*}\pageoriginale
  uniformly on compact sets.
\end{Lemma}

\noindent 
\textit{Proof.}
  Let $f(t)$ and $g(t)$ be bounded measurable functions.Then
  \begin{equation*}
    E \left[\int\limits_s^t f(\tau) v^\varepsilon_i (\tau) d\tau
      \int\limits_s^t g (\sigma)  v^\varepsilon_j (\sigma)d \sigma
      |G^\varepsilon_s\right] |\le nK ||f|| 
\; ||g|| \;  |t-s| \tag{3.8.3}\label{c3:eq3.8.3} 
  \end{equation*}
  Indeed, the left hand side \eqref{c3:eq3.8.3} is equal to 
  \begin{multline*}
  \left| E \left[\int\limits_s^t \int\limits^t_\sigma f(\tau) E [v^\varepsilon_i (\tau)
      |G^\varepsilon_\sigma] d \tau) g (\sigma) v^\varepsilon_i (\sigma)
    d \sigma | G^\varepsilon_s\right]\right|\\ 
  \le K^{\frac{1}{\gamma}} ||f|| \; ||g|| \; |t-s|, \; [\text{from (A1
      (c))}].\tag*{$\Box$}   
  \end{multline*}

So in view of~\eqref{c3:eq3.8.3} it is enough to prove the case when $f (x,t),\break g
(x,t)$ are step functions of $t$. Therefore assume that 
$$
f (x,t) = f(x, t_i), g(x,t) = g (x,t_i) ~\text{for}~ t_i \le t < t _{i+1}
$$

Then
\begin{align*}
  E & \left[\int\limits^t_s f(x,\tau) v^\varepsilon_i (\tau) d \tau
    \int\limits^\tau_s g (y,\sigma) v^\varepsilon_j (\sigma) d \sigma|
    G^\varepsilon_s\right]\\ 
  & = \sum_k f(x,t_k) g (y,t_k) E \left[\int^{t_{k +1}}_{t_{k}}
    v^\varepsilon_i (\tau) d \tau \int^\tau_{t_k} v^\varepsilon_j
    (\sigma) d \sigma | G^\varepsilon_s\right]\\ 
  & + \sum_k f(x,t_k) E \left[\int^{t_{k+1}}_{t_k}
    v^\varepsilon_i (\tau) d \tau \int^{t_k}_s  g (y, \sigma)
    v^\varepsilon_j (\sigma) d \sigma | G^\varepsilon_s\right]\\ 
  & = I^\varepsilon_1 + I^\varepsilon_2, \; \text{ say}.
\end{align*}

Now
$$
\displaylines{\hfill 
  I^\varepsilon_1 \xrightarrow[\varepsilon \to 0]{} \sum_k f (x,t_k)g
  (y,t_k) \int\limits^{t_{k-1}}_{t_k} \nu_{ij} (r) \, dr 
  = \int\limits_s^t f (x,t) g (y,r) \nu_{ij} (r) dr.\hfill \cr
  \hfill I^\varepsilon_2 = \sum_k f(x,t_k) E \left[E\left[
      \int\limits^{t_{k+1}}_{t_{k}} v^\varepsilon_i (\tau) d \tau |G
      t^\varepsilon_k\right] \int\limits^{t_{k}}_s g (y \sigma) v
    ^\varepsilon_j (\sigma) d \sigma |G^\varepsilon_s\right]
  \xrightarrow[\varepsilon \to 0]{} 0,\hfill \cr
  \text{since}\hfill
  E \left[\int\limits^{t_{k+1}}_{t_{k}} v^\varepsilon_i (\tau) d \tau
    |G^\varepsilon_{t_k} \to ~\text{as}~ \varepsilon \to 0 \right]\hfill }
$$\pageoriginale



\section*{Proof of the approximation theorem\hfil\break (Theorem 3.2.2)}

In view of the main limit theorem, all we have to do is to verify
$(A2)_k$ and $(A3)_k$. In this case, we have 
\begin{align*}
  b^\varepsilon (x,t) & = \sum_{k=1}^r F_k (x,t) v^\varepsilon_k (t) + F_o
  (x,t),\\ 
  \bar{b}^\varepsilon (x,t)& = F_o (x,t), \tilde{b}^\varepsilon (x,t) =
  \sum_{k=1}^r F_k (x,t) v^\varepsilon_k (t), \\
  A^\varepsilon_{ij} (x,y,t,r) & =  \sum_{k,\ell} E \left[\int\limits^t_s
    F_\ell^i (x,\tau) v^\varepsilon_\ell (\tau) d \tau |
    G^\varepsilon_r\right] F^j_k (y,r) v^\varepsilon_k (r) 
\end{align*}

Therefore
\begin{multline*}
  E \left[\int\limits_s^t A_{ij}^\varepsilon (x,y,t,r) dr |
    G^\varepsilon_s\right]\\ 
  = \sum_{k, \ell} E \left[\int\limits_s^t d \tau F^i_\ell ( x,\tau ) v
    ^\varepsilon_\ell (\tau) \int\limits_s^t F^j_k (y, \sigma)
    v^\varepsilon_k (\sigma) d \sigma | G^\varepsilon_s\right]\\ 
   \xrightarrow[\varepsilon \to 0]{} \sum_{k, \ell} \int\limits_s^t
  F^i_\ell (x,r) F^j_k \nu_{ij}( r ) dr. 
\end{multline*}
$(A3)_k$ can be proved similarly. Let $\tilde{P}_m{^{(\varepsilon)}}$
denote the law of $(\phi^\varepsilon_t, X^\varepsilon_t,
B_t^\varepsilon)$ defined on $W^2_m \times V_r$. Then
$\{\tilde{P}_m{^{(\varepsilon)}}\}_{\varepsilon > 0}$ converges to
$\tilde{P}_m^{^{(0)}}$ weakly. This can be  proved in the same manner
as we did earlier. The limit 
$$((\phi_t, X_t,B_t), \tilde{P}_m^{(0)})$$
satisfies: 
\begin{enumerate}[(i)]
\item $X_t$ is a C-Brownian motion with local characteristics
  $a_{ij}(x,y,t)= \sum\limits_{k, \ell} F^i_\ell (x,t) F^j_k (y,t)
  \bar{\nu}_{ij}(t)$, where $\bar{\nu}_{ij} = \nu_{ij} + \nu_{ij}, b
  (x,t) = F_o (x,t)$. 

\item $\phi_r$ is generated by $X_t + \int\limits_o^t c (x,r) dr$, where
  $$
  c^j (x,t) = \sum_{\ell,k,i} \frac{\partial}{\partial x_i} F^i_\ell
  (x,t) F^i_k (x,t) \nu_{k \ell} (t). 
  $$

\item $B(t)$\pageoriginale is a Brownian motion with mean zero and variance
  $\int\limits_o^t \bar{\nu}_{ij} (r)dr$ and  
  $$
  < X_t (x), B_k (t) >  = \sum_\ell \int_o^t F_\ell (x,r) \bar{\nu}_{\ell k} (r) dr
  $$
  This can be proved the same way as was done in Lemma~3.5.6. Now set
  $$
  \tilde{X}_t (x) = \sum_{k=1}^r \int\limits_o^t F_k (x,s) d B_k (s) +
  \int\limits_o^t F_o(x,s) ds. 
  $$
\end{enumerate}

Then it can be shown that $ < X_t - \tilde{X}_t> = 0$ which implies $X_t =
\tilde{X}_t$. Therefore, with respect to $P_m^{(0)}$ 
\begin{equation*}
  d \phi_t = \sum_{k=1}^r F_k (\phi_t,t) d B_k (t) 
+ c(\phi_t,t) \, dt \tag{3.8.4}\label{c3:eq3.8.4}
\end{equation*}

We shall now change the Ito form to Stratonovich form.
\begin{multline*}
  \int_o^t F_k (\phi_s ( x ),s) o dB_k (s) = \int_o^t  F_k (\phi_s ( x
  ),s)  dB_k (s)\\ 
  +\frac{1}{2} \sum_{\ell,i} \int_o^t
  \frac{\partial}{\partial x_i} F_x (\phi_s (x),s) \times  F^i_\ell (\phi_s
  (x), s) \bar{\nu}_{k,\ell}(s) ds. \tag{3.8.5}\label{c3:eq3.8.5} 
\end{multline*}

Also
\begin{multline*}
c^j (x,s) -\frac{1}{2} \sum_{k,\ell,i} \frac{\partial}{\partial x_i}
F^i_k(x,s) F^j_\ell (x,s) \bar{\nu}_{k \ell}(s) \\
   = \frac{1}{2} \sum_{1 \le k \le \ell \le r} (\nu_{k,\ell}(s) -
   \nu_{\ell k}(s)) [F_k, F_\ell]^j (x,s). \tag{3.8.6}\label{c3:eq3.8.6} 
\end{multline*}
 
Combining all these results,we conclude the proof.




\section{Ergodic Case}\label{chap3:sec3.9}

Let $z (t), t \ge 0$ be a time-homogeneous Markov process with state
space $S$. Let $P_t (x,A)$ be the transition probability
function. Assume $P_t(x,.)$ has a unique invariant probability measure
$\mu$ and $z(t)$ is a stationary ergodic process such that 
$$
P_t(z(t)\in A)=\mu (A).
$$\pageoriginale

For $z \in S$, $x \in \mathbb{R}^d$, let $F(x,z)$ and $G(x,z)$ be
$\mathbb{R}^d$-valued functions smooth in $x$ and the derivation
bounded and continuous in $(x,z)$. Let $Y(x,z,t, \omega)$ and
$Z(x,z,t,\omega)$ be continuous random fields such that for each fixed
$z,Y(.,z,t)$ and $Z(.,z,t)$ are continuous $C^ \infty$-valued
martingales with local 
characteristics $a^Y(x,y,z)$ and $a^Z(x,y,z)$ respectively. Assume
$z(t)$ and $\{ Y,Z \}$ are independent. Consider the following
stochastic differential equation 
\begin{align*}
  d \phi^\varepsilon_t & = \varepsilon F(\phi^\varepsilon_t, z (t))dt
  + \varepsilon^2 G (\phi^\varepsilon_t, z(t)) \, dt\\ 
 & + \sqrt{\varepsilon} Y(\phi^\varepsilon_t, z (t), dt) + \varepsilon
  Z (\phi^\varepsilon_t, z(t), \, dt).  \tag{3.9.1}\label{c3:eq3.9.1} 
\end{align*}

Let $\phi^\varepsilon_t (x)$ denote the solution of \eqref{c3:eq3.9.1} with the initial
condition $\phi^\varepsilon_o (x)=x$. Then if $\varepsilon \to 0$,
$\phi^\varepsilon_t \to$ 
trivial flow, i.e.,  $\phi^o_t (x)\equiv x$. We will see that if we
change the scale of time then the limit flow becomes nontrivial. Set
$\psi^\varepsilon_t = \phi^\varepsilon_{t/ \varepsilon}$. Then \eqref{c3:eq3.9.1} becomes 
\begin{multline*}
d \psi^\varepsilon_t= F(\psi^\varepsilon_t, z(t/ \varepsilon)) \, dt
+ \varepsilon G (\psi^\varepsilon_t, z(t /\varepsilon))\, dt \\
+ Y^\varepsilon (\psi^\varepsilon_t, z(t/ \varepsilon),dt)+ \sqrt{3}
Z^\varepsilon (\psi^\varepsilon_t, 
z(t/ \varepsilon), dt) \tag{3.9.2}\label{c3:eq3.9.2} 
\end{multline*}
where
\begin{equation*}
  \left.
  \tag{3.9.3}\label{c3:eq3.9.3}
  \begin{aligned}
    Y^ \varepsilon (x,z(t/ \varepsilon), dt) & = \sqrt{\varepsilon}
    Y(x,z(t/ \varepsilon),dt/ \varepsilon) \\  
    Z^\varepsilon(X,z(t/ \varepsilon), dt) & = \sqrt{\varepsilon}
    Z(x,z(t/ \varepsilon), dt / \varepsilon).  
  \end{aligned}
  \right\}
\end{equation*}

Let
$$
G^ \varepsilon_t =\sigma (z(r),Y(x,z,r),Z(x,z,r):r \leq t/
\varepsilon). 
$$
                                        
Set
\begin{multline*}
  X^\varepsilon (x,t)= \int  \limits^t_o
  F\left(x,z\left(\frac{r}{\varepsilon}\right)\right)dr + \varepsilon \int 
  \limits ^t_o G\left(x,z\left(\frac{r}{\varepsilon}\right)\right)dr\\ 
  + \int \limits ^t_o Y^ \varepsilon
  \left(x,z\left(\frac{r}{\varepsilon}\right),dr\right) +\sqrt{\varepsilon} \int 
  ^t_o Z^ \varepsilon \left(x,z\left(\frac{r}{\varepsilon}\right),dr\right). 
\tag{3.9.4}\label{c3:eq3.9.4} 
\end{multline*}

Claim:\pageoriginale $(\psi ^ \varepsilon_t, X^\varepsilon _t)$ converges as stochastic flows.

Indeed, for any $k>0$ $(A 2)_k$ can be verified as follows:
\begin{multline*}
  E\left[ \int ^t_s D^ \alpha _x F\left(x,
    z\left(\frac{r}{\varepsilon}\right)\right)dr |G^ \varepsilon
    _s\right]  = \varepsilon  
  E\left[\int ^{t/ \varepsilon}_{s/ \varepsilon}F\left(x, 
    z\left(\frac{r}{\varepsilon}\right)\right)dr |G^ \varepsilon
    _s\right]\\   
  \to (t-s) \int D^ \alpha _x F(x,z)\mu (dz) ~\text{in}~ L^1 ~\text{-sense}
  \tag{3.9.5}\label{c3:eq3.9.5}
\end{multline*}
by the Ergodic theorem
\begin{equation*}
  E \left[\int ^t_s D^ \beta _y D^\alpha _x
    a^Y\left(x,y,z\left(\frac{r}{\varepsilon}\right)\right)dr |G^ \varepsilon
    _s\right] \to \int D^ \beta _y D^\alpha _x a^Y(x,y,z) \mu
  (dz). \tag{3.9.6}\label{c3:eq3.9.6} 
\end{equation*}
$(A3)_k$ is clear. Therefore $(\psi ^\varepsilon_t,X^\varepsilon_t)$ converge weakly
as stochastic flow. The limit is a pair of Brownian flow and $C^
\infty$-Brownian motion with local characteristics $\int ^t_o a^Y
(x,y,z) \mu (dz), \int F(x,y) \mu (dz)$. 

\medskip
\noindent
{\bf Special Cases} Consider the following stochastic ordinary
differential equation 
\begin{equation*}
  \frac{d \phi^\varepsilon_t}{dt} = \varepsilon F (\phi^\varepsilon_t,
  z(t))+ \varepsilon^2 G 
(\phi^\varepsilon_t,z(t)). \tag{3.9.7}\label{c3:eq3.9.7} 
\end{equation*}

In this case $\psi ^\varepsilon_t \to \psi_t$ such that $\psi_t$ satisfies a
deterministic equation 
\begin{equation*}
  \frac{d \psi _t}{dt} =\bar{F}(\psi_t), \tag{3.9.8}\label{c3:eq3.9.8}
\end{equation*}
where
\begin{equation*}
  \bar{F}= \int F(x,z) \mu (dz). \tag{3.9.9}\label{c3:eq3.9.9}
\end{equation*}

This may be regarded as a low of large number for the flow $\psi ^\varepsilon _t$.

Let $F(x,t)$ be a period function of $t$ with period 1 and $z(t)=t$
on $T^1=[0,1]$ (one dimensional torus), an ergodic process with
invariant measure $dt$. Consider 
\begin{equation*}
  \frac{dx}{dt}= F (x, \frac{t}{\varepsilon}). 
\tag{3.9.10}\label{c3:eq3.9.10}
\end{equation*}\pageoriginale

The solution of \eqref{c3:eq3.9.10} $\phi^\varepsilon_t(x)
\xrightarrow[\varepsilon \to 0] \phi^0_t$, where $\phi^o_t$ satisfies 
\begin{equation*}
  \frac{d \phi^o_t}{dt}= \bar{F}
(\phi ^o_t), \tag{3.9.11}\label{c3:eq3.9.11}
\end{equation*}
where
\begin{equation*}
  \bar{F}(x)= \int ^1_o F(x,t)dt. \tag{3.9.12}\label{c3:eq3.9.12}
\end{equation*}
This is sometimes called up the averaging of the equation 
\eqref{c3:eq3.9.10}.

Let us next consider the case when
\begin{equation*}
  a^Y(x,y,z)=0, \int F(x,z)\mu (dz)=0 \tag{3.9.13}\label{c3:eq3.9.13}
\end{equation*}


In this case the limits of $\psi^\varepsilon_t$ is also a trivial flow. So we
have to change the time scale in a different way. Set
$\tilde{\psi}^\varepsilon_t = \phi_{t/ \varepsilon^2}$. Then
$\tilde{\psi}^\varepsilon_t$ is 
generated by $\tilde{X}^\varepsilon_t$ where 
\begin{multline*}
  X^\varepsilon_t (x) = \frac{1}{\varepsilon} \int^t_o F\left(x,z
  \left(\frac{r}{\varepsilon^2} \right) \right) dr +
  \int^t_o G\left(x,z \left(\frac{r}{\varepsilon^2}\right)\right)dr\\ 
  + \int^t_o \tilde{Z}^\varepsilon  \left(x,z \left(\frac{r}{\varepsilon^2}\right),
  dr\right),\qquad  \tag{3.9.14}\label{c3:eq3.9.14}    
\end{multline*}
with
\begin{equation*}
  \tilde{Z}^ \varepsilon \left(x,z
  \left(\frac{t}{\varepsilon^2}\right), t\right) = 
  \varepsilon Z \left(x, z\left(t/
  \varepsilon^2\right),\frac{t}{\varepsilon^2}\right). 
\tag{3.9.15}\label{c3:eq3.9.15}  
\end{equation*}

We now make an assumption regarding the existence of a recurrent
potential. $(A5)$ There exists a unique recurrent potential, viz. 
\begin{equation*}
  \psi (z,A)= \lim_{t \to \infty} \int^t_o (P_r(z,A)- 
\mu (A))dr. \tag{3.9.16}\label{c3:eq3.9.16}
\end{equation*}
\hfill {[uniform convergence $w.r.t.z$]}

\noindent
and 
$$
\psi (f)(z)=\int \psi (z,dz')f(z')
$$
maps $C^\infty_b$ into $C^\infty_b$. It is clear that if $\int
f(x) \mu (dx)=0$ then 
\begin{equation*}
  \psi (f)(z)= \lim_{t \to \infty} \int^t_o 
T_r f(z)dr. \tag{3.9.17}\label{c3:eq3.9.17}
\end{equation*}\pageoriginale
where $T_r$ is the semigroup corresponding to the transition function\break
$P_t(x,.)$. 


\begin{theorem}\label{c3:thm3.9.1} % 3.9.1
  Assume $(A5)$. Then $(\tilde{\psi}^\varepsilon_t, \tilde{X}^\varepsilon_t)$
  converge to a Brownian flow of $C^\infty$-diffeomorphism and $C^
  \infty$-Brownian flow. The local characteristics are given by 
  \begin{align*}
    \bar{a}_{ij}(x,y)& = \int a^Z_{ij}(x,y,z) \mu (dz)\\ 
    & ~ + \int \left\{ \hat{F}^i
    (x,z)F^j (y,z)+ \hat{F}^j (y,z)F^i (x,z) \right\} \mu (dz) \tag{3.9.18}\label{c3:eq3.9.18}\\ 
    \bar{b}(x) & = \int G(x,z) \mu (dz), \tag{3.9.19}\label{c3:eq3.9.19}\\
    c^i (x) & = \sum ^d_{k=1} \int \frac{\partial \hat{F}^i}{\partial
      x_k} (x,z)F^k (x,z)\mu (dz) \tag{3.9.20}\label{c3:eq3.9.20}\\ 
    \text{where}~~\hat{F}(x,z) & = \int \psi (z,dz')F(x,z'). 
    \tag{3.9.21}\label{c3:eq3.9.21}
  \end{align*}
\end{theorem}

\begin{proof}
  Let $\tilde{G}^\varepsilon_t= \sigma(Z(x,z,s),z(s):s \le t/\varepsilon^2)$. We
  verify $(A2)_k$ for any $k>0$. 
  \begin{align*}
    E\left[ \int ^t_s G\left(x, z\left(\frac{r}{\varepsilon}\right)\right)dr
      |\tilde {G}^ \varepsilon _s\right]
    & = \varepsilon^2 E\left[\int ^{t/ \varepsilon}_{s/
        \varepsilon^2}G(x,z(r))dr |\tilde{G}^ 
      \varepsilon _s\right]\\ 
    & \to (t-s) \int G(x,z)\mu (dz) \text{in} L^1 ~\text{-sense,}
  \end{align*}
  by the Ergodic theorem
\begin{alignat*}{3}
& E \left[ \int\limits^t_s
    a^Z \left(x,y,z \left(\frac{r}{\varepsilon^2}\right)\right)dr
    |\tilde{G}^\varepsilon_s\right]&&\to  
  (t-s)\int  a^Z (x,y,z) \mu (dz). \\
    & E \left[\frac{1}{\varepsilon} \int\limits^t_s D^\alpha_x
      F\left(x,z\left(\frac{r}{\varepsilon^2}\right)\right)dr 
      |\tilde{G}^\varepsilon_s\right]& &= \varepsilon E
  \left[\int\limits^{t/\varepsilon^2}_{s/ \varepsilon^2} D^\alpha_x
    F(x,z(r))dr |\tilde{G}^\varepsilon_s\right]\\  
    &&&=\varepsilon \int\limits^{(t-s)/ \varepsilon^2}_o T_r(D^\alpha
    F) \left(x,z\left(\frac{r}{\varepsilon^2}\right)\right)dr. \tag{{[by
          Markov property]}}
\end{alignat*}
      [where $T_r F(x,z) = \int T_r(z,dz')F(x,z')$] $\xrightarrow[\varepsilon
        \to 0]{}$ by $(A5)$.\pageoriginale 
\begin{align*}
  &E \left[\frac{1}{\varepsilon}\int\limits^t_s F^i (x,z(\tau
    /\varepsilon^2))d \tau \int\limits^\tau_s F^j (y,z(\sigma
    /\varepsilon^2))d \sigma | 
    \tilde{G}^\varepsilon _s\right]\\  
  & = \varepsilon^2 E \left[\int\limits^{t/ \varepsilon}_{\;s/
      \varepsilon^2} F^j (y,z (\sigma))d 
    \sigma \int\limits^{t/ \varepsilon^2}{\sigma} F^i(x,z(\tau))dt |
    \tilde{G}^\varepsilon_s\right]\\   
  & = \varepsilon^2 \int\limits^{(t-s)/ \varepsilon^2}_o d \sigma
  \left(\int\limits P_\sigma (z,dz')F^j 
  (y,z')\right) \left(\int\limits^{t/ \varepsilon^2 -\sigma}_o T_\tau 
  (F^i)(x,z')d \tau) |_{z=z(s/ \varepsilon^2)}\right)\\ 
  & \xrightarrow[\varepsilon \to 0]{}  (t-s)\int\limits F^j(y,z')
  \hat{F}^i   (x,z') \mu (dz'). 
\end{align*}

\hfill {[using Ergodic theorem and $(A5)$]}
\end{proof}

Similarly
\begin{multline*}
  \sum_k \frac{1}{\varepsilon^2} 
  E\left[\int\limits^t_s \frac{\partial F^i}{\partial
      x_k}(x,z(\tau /\varepsilon^2))d \tau \int\limits^\tau _s F^k (x,z (\sigma
    /\varepsilon^2))d \sigma | \tilde{G}^\varepsilon _s\right]\\ 
\xrightarrow[\varepsilon \to 0]{}
  (t-s)c^i(x). 
\end{multline*}

We next check $(A3)_k$.
\begin{multline*}
  \frac{1}{\varepsilon}E \left[ \int\limits^t_s D^\alpha
    F\left(x,z\left(\frac{r}{\varepsilon^2}\right)\right)dr |\tilde 
    {G}^\varepsilon _s\right] \frac{1}{\varepsilon} D^\beta F(x,z(s/ \varepsilon^2))\\ 
  = \int\limits^{(t-s)/\varepsilon^2}_o T_r(D^\alpha F)(x,z(s/ \varepsilon^2))dr D^\beta
  F(x,z(s/ \varepsilon^2)) 
\end{multline*}
is bounded (independent of $\varepsilon$). This completes the proof

We shall next consider an example concerning the limit theorems studied
by Papanicolaou-Stroock-Varadhan \cite{29}. 

\setcounter{example}{1}
\begin{example}\label{c3:exam3.9.2} % exam 3.9.2
  Let $(x^\varepsilon (t), z^\varepsilon (t))$ be the diffusion process in
  $\mathbb{R}^d \times \mathbb{R}^d$ defined by the following system
  of stochastic differential equations:  
  \begin{align*}
    dx^\varepsilon (t) & = \frac{1}{\varepsilon}F(x^\varepsilon
    (t),z^\varepsilon (t))dt + G(x^\varepsilon 
    (t),z^\varepsilon (t))dt\\ 
    & \hspace{2cm} + \sum ^r_{j=1} \sigma_{.j} (x^\varepsilon (t),z^\varepsilon
    (t))d \beta _j (t) \tag{3.9.22}\label{c3:eq3.9.22}\\
    dz^\varepsilon (t) & =
    \frac{1}{\varepsilon^2}\tilde{F}(x^\varepsilon (t),z^\varepsilon
    (t))dt +   \frac{1}{\varepsilon}\sum_j \tilde {\sigma}_{.j}
    (x^\varepsilon (t),z^\varepsilon (t))d   \tilde {\beta}_j (t)
    \tag{3.9.23}\label{c3:eq3.9.23}  
  \end{align*}\pageoriginale
  where $(\beta _1 (t), \ldots, \beta_r(t))$ and $(\tilde{\beta}_1
  (t), \ldots, \tilde{\beta}_r(t))$ are $r$-dimensional Brownian
  motions independent of each other. Here $z^\varepsilon (t)$ is called the
  driving process and $x^\varepsilon(t)$ the driven process. As $\varepsilon \to 0$,
  the right hand side of \eqref{c3:eq3.9.23} diverges and hence the system of
  solutions $z^\varepsilon (t)$, $\varepsilon >0$ does not converge. On the other hand,
  the first component $x^ \varepsilon (t)$ varies slowly compared with $z^\varepsilon
  (t)$. Papanicolaou-Stroock-Varadhan have shown that under some
  conditions on the coefficients, $x ^\varepsilon (t)$ converges weakly to a
  diffusion process. Let $(\phi ^\varepsilon _t(x,z), \psi ^\varepsilon _t (x,z))$
  denote the solution of \eqref{c3:eq3.9.22}, \eqref{c3:eq3.9.23} starting from $(x,z)$ at
  time $t=0$. The pair defines a stochastic flow of diffeomorphisms,
  but the first component $\phi ^\varepsilon _t (x,z)\equiv \phi^\varepsilon
  _t(.,z),z$ being fixed, does not in general. However, if $\tilde
  {F}(x,z) = \tilde{F}(z), \tilde{\sigma}(x,z) = \tilde{\sigma}(z)$
  then \eqref{c3:eq3.9.23} defines a 
  closed system, $\psi^\varepsilon_t (x,z)$ does not depend on $x$. In this
  case the mapping $\phi^\varepsilon_t (.,z): \mathbb{R}^d \to \mathbb{R}^d$
  becomes a stochastic flow for each $z$, generated by 
  \begin{equation*}
    X^\varepsilon (x,dt)=\frac{1}{\varepsilon}F(x,z^\varepsilon (t))dt + \sum _j \sigma
    _{.j}(x,z^\varepsilon (t))d \beta_j (t). 
\tag{3.9.24}\label{c3:eq3.9.24} 
  \end{equation*}  
\end{example}

Now the solution $z^\varepsilon (t)$ has the same law as 
$z^1(t/ \varepsilon^2)$. Put
\begin{multline*}
  \tilde{X}^\varepsilon (x,t) = \int\limits^t_o
  \left[\frac{1}{\varepsilon}F(x,z^1(r/ \varepsilon^2))+ 
    G(x,z^1 (r/ \varepsilon^2))\right]dr \\
  + \sum _j \int ^t_o \sigma_{.j} (x,z^1(r/ \varepsilon^2))d \beta _j
  (r). \tag{3.9.25}\label{c3:eq3.9.25} 
\end{multline*}

Let $\tilde{\phi}^\varepsilon _t (x,z)$ be the flow generated by
$\tilde{X}^\varepsilon _t$ where $z$ is the initial value. Then the law of
$(\tilde{\phi}_t(x,z),\tilde{X}^\varepsilon _t)\equiv$ the law of 
$(\phi^\varepsilon_t (x,z), X^\varepsilon_t)$. Therefore $(\tilde{\phi}_t^\varepsilon
(x,z), \tilde{X}^\varepsilon_t)$ converges.  

\setcounter{remark}{2}
\begin{remark}\label{c3:rem3.9.3} % 3.9.3
  These convergence problems do not include the homogenization
  problem. In fact here our conditions are more stringent than those in
  homogenization. If we write $a^\varepsilon (x,y)= a(\dfrac{x}{\varepsilon},
  \dfrac{y}{\varepsilon}), b^\varepsilon (x)=b (\frac{x}{\varepsilon})$, then for the
  convergence of stochastic\pageoriginale flows associated with $a^\varepsilon, b^\varepsilon$ we need
  boundedness conditions on the derivatives of $a^\varepsilon$ and
  $b^\varepsilon$. Such condition are not satisfied for the homogenization. 
\end{remark}


\section{Mixing Case}\label{chap3:sec3.10}

Suppose we are given a filtration $\{ G_{s,t} \},0 \le s\le t< \infty$
such that $G_{s,t}\subset G_{s't'}$ if $s' \le s \le t \le t'$. For
each $t>0$ we define the strong mixing rate $\beta (t)$ as follows 
\begin{equation*}
  \beta (t)= \sup_s \sup_{A \varepsilon G_{o,s'}, \;  B \varepsilon G_{s+t,
      \infty}} |P(A  \cap B) - P(A)P(B)|. \tag{3.10.1}\label{c3:eq3.10.1} 
\end{equation*}

Not that if $G_{o,s}$ and $G_{s+t, \infty}$ are independent for any
$s$, then $\beta (t)=0$. Hence $\beta(t) \sim 0$ means $G_{o,s}$ and
$G_{s+t, \infty}$ are close to being independent. In
particular, if $u$ is a $G_{t,\infty}$-measurable random variable such
that $E[u]=0$  then $E[u|G_{o,s}|]\sim 0$ if $\beta (t-s) \sim
0$. more precisely, we have the following lemma. 


\begin{Lemma}\label{c3:lem3.10.1} % 3.10.1
  Let $u$ be a $G_{t, \infty}$-measurable random variable such that
  $E[u]=0$. Then for any $p,q>1$ with $p^{-1}+q^{-1}<1$, we have 
  \begin{equation*}
    E\left[|E[u|G_{o,s}]|^r\right]^{1/r} \le C \beta
    (t-s)^{1/p}E[|u|^q]^{1/q} \tag{3.10.2}\label{c3:eq3.10.2} 
  \end{equation*}
  where $C=C(p,q)$ and $\dfrac{1}{r}=\dfrac{1}{p}+\dfrac{1}{q}$.
\end{Lemma}

\begin{proof}
  (Ibragimov-linnik \cite{11}) let $v$ be a bounded $G_{o,s}$-measurable
  random variable. For any $p',q' >1$ with
  $\dfrac{1}{p'}+\dfrac{1}{q'}=1$, we have 
  \begin{align*}
    E[|E[u|G_{o,s}]||v|] & \le E[|E[u|G_{o,s}]|]^{1/p'}
    E[|E[u|G_{o,s}]| \; |v|^{q'}]^{1/q'}\\ 
    & \le E[uv_1]^{1/ p'} E [|u| \; |v|^{q'}]^{1/q'}, \tag{3.10.3}\label{c3:eq3.10.3}
  \end{align*}
  where\pageoriginale $v_1 = sign E[u|G_{0,s}]$. Similarly we have for $p'',q'' >1$ with
  $\dfrac{1}{p''}+ \dfrac{1}{q''} = 1$  
  \begin{align*}
    E[u v_1] & = E[u(v_1 - E(v_1))]\\
    &\le E[u_1(v_1 - E (v_1))]^{1/p''} E[|v_1 - E(v_1)| \; |u|^{q''}]^{1/q''},
  \end{align*}
  where $u_1 = ~\text{sign}~ (E[v_1|G_{t,\infty}] - E[v_1])$.  Set $A = \{ \omega:
  u_1 = 1\}$, $B = \{ \omega:v_1=1 \}$. Then  
  \begin{align*}
  |E[u_1v_1]& -E[u_1]E[v_1]|  \le |P (A\cap B^c)-P(A^c \cap B)+P(A^c
  \cap B^c)|\\ 
    &+ P(A)P(B^c)+P(A^c)P(B)-P(A)P(B)-P(A^c)P(B^c)|\\
    \le \quad & 4\beta (t-s).
  \end{align*}
  Since $|v_1 - E[v_1] | \le 2$, the above inequality implies
  $$
  E [uv_1] \leq 2^{\frac{1}{q''} + \frac{2}{p''}}
  \beta(t-s)^{\frac{1}{p''}} E [|u|^{q''}]^{1/q''}. 
  $$
  Substituting the above in \eqref{c3:eq3.10.3}, we get
  $$
  E[E[u|G_{o,s}]| \; |v|] \le (2^{\frac{1}{q''}+\frac{2}{p''}})^{1/p'} \beta
  (t-s)^{\frac{1}{p'p''}} E[|u|^{q''}]^{1/ q''}
  E[|v|^{q'p''}]^{\frac{1}{p'p''}}. 
  $$
  Set $q=q''$ and $p=p' p''$, then $q'p''=r'$, where
  $\dfrac{1}{r'} + \dfrac{1}{r}=1$ and complete the proof. 
\end{proof}
   
We establish a similar estimate for $C^1$-valued random variable.

\begin{Lemma}\label{c3:lem3.10.2} % 3.10.2
  Let $u(x,\omega)$ be a $C^1$-valued random variable, $G_{t,
    \infty}$-\break measurable and $E[u(x)]=0$ for any $x$. Then for any
  $p,q>1$ with $p^{-1}+q^{-1}<1$ and for any $K>0$ there exists a
  constant $C=C(p,q,d,K)$ such that 
  \begin{multline*}
    E \left[\sup _{|x|\le K}|E\left[u(x)|G_{o,s}\right]^r\right]^{1/r} \le C\beta
    (t-s)^{\frac{1}{p(d+1)}} 
    E \left[ \sum_{|\alpha|\le 1} \sup _{|x|\le
        K}|D^\alpha u(x)|^q\right]^{1/q} \tag{3.10.4} \label{c3:eq3.10.4}
  \end{multline*}
  where $\dfrac{1}{r}=\dfrac{1}{p}+\dfrac{1}{q}$.
\end{Lemma}

\begin{proof}
  Let\pageoriginale $N$ be any positive number. There exists $x_1,  \ldots,  x_n (n
  \le (2N+1)^d)$ such that $\{ B(x_i; \dfrac{K}{N}),i=1, \ldots, n
  \}$, where $B(x_i; \dfrac{K}{N})$ is the ball with centre $x_i$ and
  of radius $\dfrac{K}{N}$, covers the cube $[-K,K]^d$. Then we have  
  \begin{align*}
  \sup_{|x|\leq K}|E[u(x)|G_{o,s}]| & \leq \max_i \sup_{x \in
    B(x_i,\frac{K}{N})} |E[u(x)-u(x_i)|G_{o,s}]| \\
  &\qquad  + \max |E[u(x_i)|G_{o,s}]| \\
  & = I_1+I_2, \text{say}. \tag{3.10.5}\label{c3:eq3.10.5}
  \end{align*}
\end{proof}

Using mean value theorem, we have
\begin{align*}
  I_1 & \leq \frac{K}{N} \max_i \sum_{|\alpha|=1}\sup_{x \varepsilon
    B(x_i,\frac{K}{N})}|E[D^\alpha u (x)|G_{o,s}]| \\
  & \leq \frac{K}{N} \sum_{|\alpha |=1}\sup_{|x|\leq K}|E[D^\alpha u (x)|G_{o,s}]|.
\end{align*}
Therefore
\begin{multline*}
  E\left[\sup_{|x|\leq K}|E[u (x)|G_{o,s}]|^r\right]^{1/r} \leq
  \frac{K}{N}E\left[\sum_{|\alpha|=1} \sup_{|x|\leq K}|E[D^\alpha u
   (x)|G_{o,s}]|^r\right]^{1/r}\\
  +C(2N+1)^d \beta(t-s)^{1/P}\max E[|u(x_i)|^q]^{\frac{1}{q}}. 
\end{multline*}
Set $N=\beta(t-s)^{-\dfrac{1}{p(d+1)}}$. Then $N^d
\beta(t-s)^{1/p} = \beta(t-s)^{\dfrac{1^i}{p(d+1)}}$. Since $r< q$ the
above is bounded by  
$$
(K+C3^d) \beta (t-s)^{\dfrac{1}{p(d+1)}} E\left[ \sum_{|\alpha| \leq
    1} \sup\limits_{|x| \leq K} |D^\alpha u(x)|^q\right]^{1/q}. 
$$
This proves the lemma.


\begin{Lemma}\label{c3:lem3.10.3}%lem 3.10.3
  Let $u(x)$ (resp. $v(x)$) be a $C^1$-valued random variable which is
  $G_{u,u}$ (resp. $G_{t,t}$)-measurable where $t < u$. Suppose
  $E[u(x)]=0$ for all $x$ and set $w(x)=E[u(x)v(x)]$. Then for any
  $p,q$ with $p^{-1}+2q^{-1}<1$ and for any $K > 0$, there is a
  positive constant $C=C(p,q,K)$ such that for $s<t$ 
  \begin{align*}
    E & \left[ \sup_{|x| \leq K} |E [u(x)v(x) - 
    w (x)|G_{o,s}]|^{\delta}\right]^{1/
      \delta} 
    \leq C \left\{ \beta (u-t) \beta(t-s)\right\}^{\frac{1}{2p(d+1)}} \\ 
    E & \left[\sum_{|\alpha| \leq 1} \sup_{|x| \leq K} |D^{\alpha}u
      (x)|^{2q}\right]^{1/ 2q} \times
    E\left[\sum_{|\alpha| \leq 1} \sup_{|x| \leq K} |D^{\alpha}v
    (x)|^{2q}\right]^{\frac{1}{2q}} \tag{3.10.6}\label{c3:eq3.10.6} 
  \end{align*}\pageoriginale
  where $p^{-1} + 2q^{-1} = \delta^{-1}$.
\end{Lemma}

\begin{proof}
  Lemma~\ref{c3:lem3.10.2} with uv-w substituted for $u$.  Then 
  \begin{align*}
  E&\left[ \sup_{|x| \leq K}  | E [ u (x) v(x) -w(x)|G_{o,s}]|^{r}\right]^{1 /r}\\
    \leq C \beta.(t-s)^{\frac{1}{p(d+1)}} & E\left[\sum_{|\alpha| \leq 1}
      \sup_{|x| \leq K} |D^{\alpha}(uv- w)|^{q}\right]^{1/ q}\\ 
    \leq C' \beta.(t-s)^{\frac{1}{p(d+1)}} & E\left[\sum_{|\alpha| \leq 1}
      \sup_{|x| \leq K} |D^{\alpha}u(x) |^{2q}\right]^{1/ 2q} \times \\ 
     & E\left[\sum_{|\alpha| \leq 1} \sup_{|x| \leq K} |D^{\alpha}v(x)
      |^{2q}\right]^{1/ 2q}, \tag{3.10.7}\label{c3:eq3.10.7} 
  \end{align*}
  where $p^{-1}+ q^{-1} = r^{-1}$. Since $\delta < r, r$ can be
  replaced by $\delta$ in the left hand side of \eqref{c3:eq3.10.7}. Next note
  that $\delta^{-1} =  r^{-1}+q^{-1}$. Then by Holder inequality  
  \begin{align*}  
    E& \left[ \sup_{|x| \leq K}
      |E[u(x)v(x)|G_{o,s}]|^{\delta}\right]^{1 / \delta}\\ 
    & = E\left[ \sup_{|x| \leq K}
      |E[E[u(x)|G_{o,s}]v(x)|G_{o,s}]|^{\delta}\right]^{1 / \delta}\\ 
    & \leq  E\left[ \sup_{|x| \leq K}
      |E[u(x)|G_{o,t}]|^rG_{o.s}\right]^{\frac{\delta}{r}} 
        E\left[ \sup_{|x| \leq K} |E[v(x)|^q|
              G_{o,s}]^{\frac{\delta}{q}}\right]^{ 1/ \delta}\\ 
    & \leq E\left[ \sup_{|x| \leq K} |E[u(x)|G_{o,t}]|^r\right]^{1 /r}. E\left[
              \sup_{|x| \leq K} |v(x)|^q\right]^{\frac{1}{q}}\\ 
    & \leq C \beta.(u-t)^{\frac{1}{p(d+1)}} E\left[\sum_{|\alpha| \leq 1}
              \sup_{|x| \leq K} |D^{\alpha}u(x)|^{q}\right]^{1/ q} E\left[
              \sup_{|x| \leq K} |v(x)|^q\right]^{1/q}. 
  \end{align*}
\end{proof}

Finally\pageoriginale we have by Lemma~\ref{c3:lem3.10.1}
\begin{align*}
  |w(x)|& \leq C' \beta(u-t)^{1 /p} E[|u(x)|^q]^{1 /q} E[|v(x)|^r]^{1 /r}\\
  &\leq C' \beta(u-t)^{1 /p} E[|u(x)|^q]^{1 /q} E[|v(x)|^q]^{1 /q},
\end{align*}
where $p^{-1} + q^{-1} + r^{-1} =1$ and $r<q$. Therefore the left hand
of \eqref{c3:eq3.10.6} is bounded by  
\begin{equation*}
  C \beta (u-t)^{\frac{1}{p(d+1)}} E[\sum_{|\alpha| \leq 1} \sup_{|x|
      \leq K} |D^{\alpha}u(x)|^{q}]^{1/ q} E[\sup_{|x|\leq
      K}|v(x)|^q]^{1 /q}. \tag{3.10.8}\label{c3:eq3.10.8} 
\end{equation*}

Hence the square of the left hand side of \eqref{c3:eq3.10.6} is bounded by the
the product of the right hand side \ref{c3:eq3.10.7} and \eqref{c3:eq3.10.8}. This
proves the lemma. 

Consider a stochastic defferential equation with a parameter $\varepsilon > 0$
\begin{align*}
  dx(t) &= \varepsilon F(x(t),t) dt + \varepsilon^2 G(x(t),t)dt +
  \varepsilon Y(x(t),dt) \tag{3.10.9}\label{c3:eq3.10.9}\\
x~ & ~\varepsilon~ \mathbb{R}^d,  0 \leq t < \infty,
\end{align*}
where $F(x,t, \omega),G(x,t, \omega)$ are $\mathbb{R}^d$-valued random
fields, $F_o$-measurable, continuous in $(x,t)$ and k-times
continuously differentiable in $x$, the first derivatives are
bounded. 

Further $E[F(x,t)] =0$ for any $x$, $t$. $Y(x,t),0 \leq t < \infty$, is a
continuous $C^{k-1}$-martingale adapted to $F_t$ with local
characteristics $a (x, y,t, \omega)$ with properties similar to the
above. The equation is similar to the one introduced in the previous
section. In fact $F(x,z(t)), G(x,z(t))$ etc. of the previous section
correspond to $F(x,t, \omega),G(x,t, \omega)$ etc. of this
section. Let $\phi^{\varepsilon}_t(x)$ be the stochastic flow
determined by the above stochastic differential equation. Then both
$\phi^{\varepsilon}_t$ and $\phi^{\varepsilon}_{t/ \varepsilon}$. We
shall study the weak convergence of $\psi^{\varepsilon}_t$ as
$\varepsilon \to 0$ as stochastic flows under a mixing condition on 
the stochastic defferential equation. Such a limit theorem has
extensively been studied in the case $Y \equiv 0$, i.e.,  in the case
of stochastic ordinary differential equation\pageoriginale see Khasminskii \cite{17},
Kohler-Papanicolaou \cite{28}, Kesten-Papanicolaou \cite{15}. In those works the
weak convergence on diffusion processes has been studied. Here we
shall study the weak convergence as stochastic flows including the
case $Y \not \equiv 0$. Let 
$$
G_{s,t} = \sigma(F(.,u), G(.,u) -   Y(.,v), s \leq u, v \leq t)
$$
and let $\beta(t)$ be the strong mixing rate associated with
$G_{s,t}$. We shall introduce the assumptions: 

\noindent
$(A6)_k$ There are continuous functions $a =(a_{ij}(x,y,t), A =
(A_{ij}(x,y,t),\break b = (b^i(x,t)), c= (c^i(x,t))$ which are $k$-times
continuously differentiable with respect to $x,y$ or $x$, as the case
may be, and the first derivatives are bounded such that the following
are satisfied  
\begin{align*}
  &|a_{ij}(x,y,t) - \lim_{\varepsilon \to 0} \frac{1}{\varepsilon}
  \int \limits^{t+ \varepsilon}_t
  E\left[a_{ij} \left(x,y,\frac{r}{\varepsilon^2}\right)\right] dr | = 0,\\ 
  & |A_{ij}(x,y,t) - \lim_{\varepsilon \to 0} \frac{1}{\varepsilon^3}
  \int \limits^{t+ \varepsilon}_t \int \limits^{\tau}_t E\left[F^i (x,
    \frac{\sigma}{\varepsilon^2}) F^j \left(y,
    \frac{\tau}{\varepsilon^2}\right)\right] 
  \sigma d \tau |=0,\\ 
  &| b^i (x,t) - \lim_{\varepsilon \to 0} \frac{1}{\varepsilon}\int
  \limits^{t+ \varepsilon}_t
  E\left[G^i\left(x,\frac{r}{\varepsilon^2}\right)\right] dr | =0,\\ 
  &|(x,t) -\lim_{\varepsilon \to 0} \frac{1}{\varepsilon^3}\int
  \limits^{t+ \varepsilon}_t d \tau \int \limits^{\tau}_t d \sigma
  \sum_j E \left[ F^j(x,\frac{\sigma}{\varepsilon^2})
    \frac{\partial}{\partial x_j} F^j\left(x,
    \frac{\tau}{\varepsilon^2}\right)\right]|  = 0, 
\end{align*}
uniformly on compact sets.

Let $p,q>1$ be such that $p^{-1} + q^{-1}<1$.

\noindent
\begin{tabbing}
$(A7)_{k,p,q}$ \=   (a)  \=  $\int \limits^{\infty}_o \beta
  (r)^{\frac{1}{2p(d+1)}} dr < \infty$,  \\
\> (b) \> For any $K>0$ the 2qth moments of $\sup\limits_{|x| \leq K, |y|\leq
  K}|D^{\alpha}_x D^{\beta}_y a(x,y,t)|$,\\
\> ~~ \>  $\sup\limits_{|x|\leq K} |D^{\alpha}
  G (x,t)|$ are all bounded by a positive constant\\
\> ~~ \>  independent of
  $t$, $|\alpha| \leq k$, $|\beta| \leq K$. 
\end{tabbing}\pageoriginale

\setcounter{remark}{3}
\begin{remark}\label{c3:rem3.10.4} % 3.10.4
  Note that here we are assuming the existence of infinitesimal limits
  whereas in the previous cases we have assumed the existence of
  global limits. 
\end{remark}

\setcounter{theorem}{4}
\begin{theorem}\label{c3:thm3.10.5} % 3.10.5
  Assume $(A6)_k$ and $(A7)_{k,p,q}$ for some $p,q$ such that $p^{-1}+
  q^{-1} < 1$. Then the conclusion of the main limit theorem is valid. 

  We shall give the proof of the theorem in a more general setting.

  Let $X^{\varepsilon}_t = X^{\varepsilon}(x,t), \varepsilon >0$ be a
  family of continuous $C^{k-1}$- semimartingales adapted to
  $F^{\varepsilon}_t, \varepsilon >0$ with local characteristics
  $a^{\varepsilon}(x,y,t,\omega),\break b^{\varepsilon}(x,t,\omega)$. We
  assume as before $a^{\varepsilon}$ and $b^{\varepsilon}$ are
  continuous in $t$ and k-times continuously differentiable in $x,y$
  and $x$ respectively. Set $\bar{b}^{\varepsilon} = E[
    b^{\varepsilon} (x,t)]$ and
  $\bar{b}^{\varepsilon} = b^{\varepsilon} -
  \bar{b}^{\varepsilon}$. In   place of $(A6)_k$ we assume:   
  \begin{description}
  \item [$(A8)_k$] There are continuous functions $a=(a_{ij}(x,y,t)),
    \bar{b} = (\bar{b}^i(x,t)), c=(c^i(x,t)), A= (A_{ij}(x,y,t))$
    which are k-times continuously differentiable with respect to
    $x,y$ or $x$ and the first derivatives are bounded and  
    \begin{align*}
      & |a(x,y,t) - \frac{1}{\varepsilon} \int \limits^{t+
        \varepsilon}_t E[a^{\varepsilon}(x,y,r)]dr| \to 0 \text{ as }
      \varepsilon \to 0,\\ 
      & | \bar{b}(x,t) - \frac{1}{\varepsilon} \int \limits^{t+
        \varepsilon}_t \bar{b}^{\varepsilon}(x,r) dr| \to 0 \text{ as }
      \varepsilon \to 0,\\ 
      &|A_{ij}(x,y,t) - \frac{1}{\varepsilon} \int \limits^{t+
        \varepsilon}_t d \tau \int \limits^{\tau}_t d \sigma
      E[\tilde{b}^{\varepsilon}_i (x, \tau)b^{\varepsilon}_j (y,
        \sigma)]|\to 0 \text{ as } \varepsilon \to 0,\\ 
      &| c^i(x,t) - \frac{1}{\varepsilon} \int \limits^{t+
        \varepsilon}_t d_{\tau} \int \limits^{\tau}_t d \sigma \sum_j
      E[b^{\varepsilon}_j (x, \sigma) \frac{\partial}{\partial x_j}
        \tilde{b}^{\varepsilon}_i(x, \tau)]|\to 0 \text{ as }
      \varepsilon \to 0, 
    \end{align*}
    uniformly\pageoriginale on compact sets.
    \noindent
    Set 
    $$
    G^{\varepsilon}_{s,t} =
    \sigma(Y^{\varepsilon}(.,u)-Y^{\varepsilon}(.,v),b^{\varepsilon}(.,u), 
    s \leq u, v \leq t). 
    $$

    Let $\beta^{\varepsilon}(t)$ be the strong mixing rate associated
    with $G^{\varepsilon}_{s,t}$. We introduce an assumption so that
    as $\varepsilon \to 0,\beta^{\varepsilon}(t) \to 0,
    E[|\tilde{b}^{\varepsilon}|r|] \to \infty $, but the rates of the
      convergence and divergence are balanced. Let $p,q>1$ be such
      that $p^{-1} + 2q^{-1} < 1$. We restrict the time interval to
      $[0,T]$.
 
    \item [$\gamma:  (A9)_{k,p,q}$] $a^{\varepsilon} =
      (a^{\varepsilon}(x,y,t))$ and $\bar{b}^{\varepsilon} =
      (\bar{b}^{\varepsilon}(x,t))$ satisfy the same moments
      conditions as in $(A7)_{k,p,q}$ independently of
      $\varepsilon$. Furthermore the mixing rate
      $\beta^{\varepsilon}(t)$ satisfies: with $\gamma$ such that
      $p^{-1}+q^{-1} = \gamma^{-1}$: 
      \begin{enumerate}[(a)] 
      \item $\lim \limits_{\varepsilon \to 0} \int \limits^T_0
        \beta^{\varepsilon}(\tau)^{\frac{\gamma}{p(d+1)}}d \tau =0,
        \int\limits^T_{\varepsilon} \beta^{\varepsilon}
        (\tau)^{\frac{\gamma}{p(d+1)}}d \tau = 0\left( 
        \int \limits^T_0
        \beta^{\varepsilon}(\tau)^{\frac{\gamma}{p(d+1)}}d \tau \right)$ 
 
     \item For each $K>0$ there is a positive constant $L =L_K$ such
        that $ \int \limits^T_0
        \beta^{\varepsilon}(\tau)^{\frac{\gamma}{p(d+1)}}d \tau.  E\left[
          \sup \limits_{|x| \leq K}| D^{\beta}
          \tilde{b}^{\varepsilon}(x,t)|^{2q}\right]^{1 /q} \leq L$ for all
        $\varepsilon > 0$. 
      \end{enumerate}
  \end{description}
\end{theorem}

\setcounter{theorem}{5}
\begin{theorem}\label{c3:thm3.10.6} %3.10.6
  Assume $(A8)_k$ and $(A9)_{k,p,q}$ for $p,q$ such that $p^{-1}+
  2q^{-1} <1$. Then the conclusion of the main limit theorem is
  valid. 

  Before describing the proof of the above Theorem let us point out that
  Theorem~\ref{c3:thm3.10.5} can be deduced form Theorem~\ref{c3:thm3.10.6}. 
  Indeed, set $b^{\varepsilon}(x,t) = \frac{1}{\varepsilon} F(x,t/ \varepsilon^2) +
  G(x, t / \varepsilon^2), \gamma^{\varepsilon}(x.t) =
  \frac{1}{\sqrt{\varepsilon}}Y(x,t/ \varepsilon^2)$. Then $(A8)_k$
  immediately follows from $(A6)_k$. Set $G^{\varepsilon}_{s,t} = 
  G_{\frac{s}{\varepsilon^2}, \frac{t}{\varepsilon^2}}$.  
\end{theorem}

Then we have $\beta^{\varepsilon}(r) = \beta(r/
\varepsilon^2)$.  Therefore 
\begin{multline*}
  \int\limits^T_0
  \beta^{\varepsilon}(\tau)^{\frac{\gamma}{2p(d+1)}}d \tau <
  \varepsilon^2   \int \limits^T_0
  \beta^{\varepsilon}(\tau)^{\frac{\gamma}{2p(d+1)}}d, \\
  E\left[\sup_{|x|\leq K}|D^{\beta} \tilde{b}^{\varepsilon}(x,t)|^{2q}\right]^{1/q}
  = \frac{1}{\varepsilon^2}E\left[\sup_{|x|\leq K}|D^{\beta}
    F(x,t)|^{2q}\right]^{1/q}. 
\end{multline*}
Hence\pageoriginale $(A9)_{k,p,q}$ is satisfied.

\setcounter{proofoftheorem}{5}
\begin{proofoftheorem}
  Set $D^{\alpha}  \tilde{b}^{\varepsilon}(x,\tau) =
  u^{\varepsilon}(x, \tau)$. Then by\break Lemma~\ref{c3:lem3.10.2}, we have for
  $\gamma$ such that $\gamma^{-1}= p^{-1}+ q^{-1}$ 
  \begin{multline*}
    E\left[\sup_{|x|\leq K} | \int \limits^s_t E[u^{\varepsilon}(x, \tau)
        |G^{\varepsilon}_{o,s}]|^{\gamma}\right]^{1 / \gamma}\\ 
    \leq C\left(\int \limits^s_t
    \beta^{\varepsilon}(\tau-s)^{\frac{\gamma}{p(d+1)}}d
    \tau\right)^{\frac{1}{\gamma}} \sup_{\tau} E \left[ \sum_{|\beta| \leq 1}
      \sup_{|x| \leq K} |D^{\beta}u^{\varepsilon}(x, \tau)|^q\right]^{1/q}. 
  \end{multline*}
\end{proofoftheorem}

Now pick $\delta$ such that $p^{-1} + 2 q^{-1} = \delta^{-1}$ and
$\tilde{p} = r \delta^{-1}$. Then $\tilde{\delta q} =q$, where
$\tilde{q}$ is the conjugate of $\tilde{p}$. Then by Holder inequality 
\begin{align*}
  E& \left[\sup_{|x|\leq K} | \int \limits^s_t E[u^{\varepsilon}(x,
      \tau) |G^{\varepsilon}_{o,s}]d \tau|
    u^{\varepsilon}(x,s)|^{\delta}\right]^{1 / \delta}\\ 
  &\leq C\left(\int \limits^s_t
  \beta^{\varepsilon}(\tau-s)^{\frac{\gamma}{p(d+1)}}d
  \tau\right)^{\frac{1}{\gamma}} \sup_{\tau} E \left[ \sum_{|\beta| \leq 1}
    \sup_{|x| \leq K} |D^{\beta}u^{\varepsilon}(x,
    \tau)|^{q}\right]^{1/q}\times\\
  & E \left[\sup_{|x| \leq k} | u ^\varepsilon (x, s)|^q \right]^{1/q}\\
   & \leq L
\end{align*}

This proves $(A3)_k$. We next prove $(A2)_k$. Set
$$\bar{a}^{\varepsilon}(x,y,r) = E[a^{\varepsilon}(x,y,r)].$$ 
Then  
$$
\int \limits^t_s \bar{a}^{\varepsilon}(x,y,r) dr \to \int \limits^t_s a(x,y,r)dr
$$
follows immediately.
$$
\sup_{|x| \leq K} | \int \limits^t_s 
E[\bar{a}^{\varepsilon}(x,y,r)|G^{\varepsilon}_{o,s}]dr| \to 0
\text{ in }L^1- \text{ sense}, 
$$
where $\tilde{a}^{\varepsilon}=
a^{\varepsilon}-\bar{a}^{\varepsilon}$. Similar estimate implies  
\begin{align*}
  & E\left[\sup_{|x|\leq K} | \int \limits^s_t E[u^{\varepsilon}(x, \tau)
      |G^{\varepsilon}_{o,s}]d \tau|^r\right]^{\frac{1}{\gamma}}\\ 
  &\leq C \left(\int \limits^s_t
  \beta^{\varepsilon}(\tau-s)^{\frac{\gamma}{p(d+1)}}d
  \tau\right)^{\frac{1}{\gamma}}   E\left[ \sum_{|\beta| \leq 1} \sup_{|x| \leq
      K} |D^{\beta}u^{\varepsilon}(x, \tau)|^q\right]^{1/q}\\ 
  & \to 0 ~\text{as}~ \varepsilon \to 0.
\end{align*}
Set\pageoriginale $K^\varepsilon (\tau, \sigma, x, y) = \tilde{b}^\varepsilon (x,
\tau) b^\varepsilon (y, \sigma)$ and $\bar{K}^\varepsilon =
E[K^\varepsilon ], \tilde{K}^\varepsilon = K^\varepsilon -
\bar{K}^\varepsilon $ 

Then by $(A. 8)_k$
\begin{equation*}
  \int^{t+\varepsilon}_t d \tau \int^\tau_t d^\sigma
  \bar{K}^\varepsilon_{ij} (\tau, \sigma, x, y) = \varepsilon A_{ij}
  (x, y, t) + o(t, \varepsilon) \tag{3.10.10}\label{c3:eq3.10.10} 
\end{equation*}
where $\dfrac{o(t, \varepsilon)}{\varepsilon} \to 0$ uniformly in $t$
as $\varepsilon \to 0$. By Lemma~\ref{c3:lem3.10.1}, we have 
\begin{gather*}
  | \int_t^{t + \varepsilon} d \tau \int^t_s  d \sigma
  \bar{K}_{ij}^\varepsilon (\tau, \sigma, x, y) |   
\le \int_t^{t+ \varepsilon} d \tau \int^t_s d \sigma
\beta^\varepsilon (\tau - \sigma)^{\dfrac{1}{p(d+1)}}\\ 
\sup_\sigma E [|b^\varepsilon _ j (y, \sigma) |^{2q}]^{\frac{1}{2q}}
\times \sup_\tau E[|\tilde{b}_i^\varepsilon (x, \tau)
  + |^{2q}]^{1/2q}. \tag{3.10.11}\label{c3:eq3.10.11}  
\end{gather*}

Note that
$$
\int^{t+\varepsilon}_t d \tau \int_s^t d\sigma \beta^\varepsilon (\tau
- \sigma)^{\frac{1}{p(d+1)}} = o \left( \int^T_o \beta^\varepsilon
(\tau)^{\dfrac{1}{p(d+1)}} d \tau\right). 
$$

Therefore~\eqref{c3:eq3.10.11} converges to zero. Hence
$$
\int^{t+\varepsilon}_t d \tau \int^\tau_s d \sigma
\bar{K}_{ij}^{\varepsilon} (\tau, \sigma, x, y) = \varepsilon A_{ij}
(x, y, t) + o(t, \varepsilon). 
$$

This proves
$$
\lim_{\varepsilon \to 0} \int^t_s d \tau \int^\tau_s d \sigma
\bar{K}^\varepsilon_{ij} (\tau, \sigma, x, y) = \int^t_s A_{ij} (x, y,
\tau) d \tau. 
$$

On the other hand, by Lemma~\ref{c3:lem3.10.3},
\begin{align*}
  E & \left[| E \left[ \int^t_s d \tau \int^\tau_s d \sigma
      \tilde{K}_{ij}^\varepsilon (\tau, \sigma, x, y) | G^\varepsilon
      _{o,s}\right] |^\delta \right]^{1/\delta}\\ 
  & \le C \sup_\tau E \left[|\tilde{b}^\varepsilon (x, \tau) |^{2q}\right]^{1/2q}
  E [|b^\varepsilon (y, \tau) |^{2q}]^{1/2q} \\
  & \hspace{2cm} \times \left(\int^t_s d \tau
  \int^\tau_s d \sigma \beta^\varepsilon (\tau -
  \sigma)^{\frac{\gamma}{2p(d+1)}} \beta^\varepsilon (\sigma -
  s)^{\frac{\gamma}{2p(d+1)}}\right)^{\frac{1}{\gamma}}\\ 
  & \le C \sup_\tau E \left[ | \tilde{b}^\varepsilon (x, \tau) |^{2q}
  \right]^{1/2_q} E [|b^\varepsilon (y, \tau)|^{2q}]^{1/2q} \\
  & \hspace{2cm} \left(\int_o ^T
  \beta^\varepsilon (\tau)^{\frac{\gamma}{2p(d+1)}} d \tau \right)^{2/
    \gamma} \to 0 \text{ as } \varepsilon \to 0.
\end{align*}\pageoriginale

These two computations yield
$$
\lim_{\varepsilon \to 0} E\left[ \int^t_s d \tau \int^\tau_s d \sigma
  K^\varepsilon_{ij} (\tau, \sigma, x, y) | G^\varepsilon_{o, s}\right] =
\int^t_s A_{ij} (x, y, \tau) d^\tau. 
$$
This completes the proof.

\section{Tightness and Weak Convergence 
of Inverse Flows}\label{chap3:sec3.11}%sec 3.11

Let $X^\varepsilon (x, t), \varepsilon \to 0$ be a family of
continuous $C$-semimartingales adapted to $F^\varepsilon_t$ with local
characteristics $(a^\varepsilon, b^\varepsilon)$ satisfying Lipschitz
continuity and linear growth properties. Assume that $X^\varepsilon
(x, t)$ is a backward semimartingale, i.e.,  it satisfies $(A5)$ of
Chapter~\ref{chap2}. Set 
$$
\displaylines{\hfill 
  \hat{X}^\varepsilon (x, t) = - X^\varepsilon (x, t) + \int^t_o
  d^\varepsilon (x, r) dr, \hfill \cr
  \text{where}\hfill
  d^\varepsilon_i (x,t) = \sum_j \frac{\partial}{\partial x_j} a_{ij}
  (x, y, t) |_{y=x}.\hfill } 
$$


It is a backward semimartingale with local characteristics
$a^\varepsilon, -b^\varepsilon + d^\varepsilon$. We make the following
assumption:  

$(A2\hat{)}_k$ The tightness condition $(A2)_k$ is satisfied for the
backward semimartingale. Set $\Psi^\varepsilon_{s,t} =
(\phi^\varepsilon_{s, t})^{-1}$ and $\Psi^\varepsilon_t =
\Psi^\varepsilon_{o,t}$, where $\phi^\varepsilon_{s,t}$ is the flow
generated by $X^\varepsilon (x, t)$. Then for each $t, \{
\Psi^\varepsilon_{s,t}\}$, $s \in [0, T]$ is tight. We claim
that for each $s, \{\Psi^\varepsilon_{s,t}\}$, $t \in[s, T]$ is
tight. 

\begin{theorem}\label{c3:thm3.11.1}\pageoriginale %the 3.11.1
Let $k \ge 4$. The family of laws of $\Psi^\varepsilon_t$ 
on $W_m, m \le k-4$, is tight. 
\end{theorem}

\begin{proof} %proof
  Let $\phi^{\varepsilon, K}_{s,t}$ be the flow generated by the
  truncated process $X^{\varepsilon, K} (x, t)$ and let
  $\Psi^{\varepsilon, K}_{s, t} = (\phi^{\varepsilon, K}_{s,
    t})^{-1}$. We shall prove the tightness of $\Psi^{\varepsilon,
    K}_t$ in $W_{o, p} $. We suppress $K$. Let $s < t$. Then 
  \begin{align*}
    \Psi^\varepsilon_t (x) - \Psi^\varepsilon_s (x) & =
    \Psi^\varepsilon_s o \Psi^\varepsilon_{s, t} (x) -
    \Psi^\varepsilon_s (x)\\ 
    & = \partial \Psi^\varepsilon_s (\xi^\varepsilon_s (x))
  (\Psi^\varepsilon_{s, t}(x) - x), \tag {3.11.1}\label{c3:eq3.11.1} 
  \end{align*}
  where $| \xi^\varepsilon_s (x) | \le K$. For any $p \ge 3$, there is
  a positive constatnt $C'$ such that  
  $$
  E | \Psi^\varepsilon_{s,t} (x) - x |^p \le C' |t-s|^{2-\dfrac{3}{p}}.
  $$
\end{proof}

In fact $\Psi ^\varepsilon_{s,t}(x)$ satisfies the following backward
stochastic differential equation 
$$
\Psi^\varepsilon_{s, t}( x) = x - \int^t_s X^\varepsilon
(\Psi^\varepsilon_{r, t}(x), \hat{d}r) + \int^t_s d^\varepsilon
(\Psi^\varepsilon_{r, t}(x), r ) \hat{d}r. 
$$

Therefore arguing as in Lemma~\ref{c3:lem3.4.2} we get the above estimate. On
the other hand, the inverse of the Jacobian matrix $-\partial
\phi^\varepsilon_t$ satisfies as in the case of a usual stochastic
differential equation (see Ikeda-Watanabe~\cite{13}) 
$$
(\partial \phi^\varepsilon_t)^{-1} = I - \int^t_o (\partial
\phi^\varepsilon_r)^{-1} X^\varepsilon (\phi^\varepsilon_r, dr) -
\int^t_o (\partial \phi_r)^{-1} G (\phi^\varepsilon_r (x), r) dr 
$$
where $G_{ij}(x, t) = \sum\limits_k \dfrac{\partial}{\partial y_j}
\dfrac{\partial}{\partial x_k} a^\varepsilon_{ik} (x, y, t) 
|_{y = x}$. Then we can show that\break $(\partial \phi^\varepsilon_t )^{-1}$
converges weakly in the same way as we did earlier. Therefore the
associated laws are tight. Consequently, for any $\delta > 0$, there
is a $C = C(\delta)$ such that 
$$
P \left(\sup_{\substack{| x | \le K \\ t \in [\bar{0}, T]}} | 
(\partial \phi^\varepsilon_t)^{-1} (x) | \le C\right) > 1 -  \delta. 
$$

Now,\pageoriginale since $\partial \Psi^\varepsilon_t (x) = (\partial
\phi^\varepsilon_t)^{-1} (\Psi^\varepsilon_t (x))$ and $|
\Psi^\varepsilon_t (x) | \le K$ if $|x| \le K$, we get 
$$
P \left(\sup_{\substack{|x| \le K \\ t \in [0, T]}} | \partial
\Psi^\varepsilon_t (x) | \le C \right) > 1 - \delta. 
$$

Define
$$
A_{\varepsilon, \delta}  = \left\{\omega:  \sup_{|x| \le K, t \in
  [0, T]} | \partial \Psi^\varepsilon_t, (x) | \le C \right\}. 
$$

Then $P (A_{\varepsilon, \delta} > 1 - \delta$ for 
any $\varepsilon$. Therefore
\begin{align*}
  E \left[|\Psi^\varepsilon_t (x) - \Psi^\varepsilon_s (x) |^P:
    A_{\varepsilon, \delta}\right] & \le C^P E [| ~ \Psi^\varepsilon_{s,t}
    (x) - x |^P:  A_{\varepsilon, \delta}]\\ 
  & \le C^P.  C' |t - s|^{2-\frac{3}{P}}.
\end{align*}

Therefore the measure $P^{(\varepsilon, \delta)}(.) = P(. |
A_{\varepsilon, \delta})$ satisfies 
$$
E^{(\varepsilon, \delta)} [ | \Psi^\varepsilon_t (x) -
  \Psi^\varepsilon_s (x) |^P ] \le C^P.C'(1-\delta)^{-1} |t-s|^{2-
  \frac{3}{P}}. 
$$

Hence the family of laws $\{ P^{(\in, \delta)} \}$ is tight for
any $\delta > 0$. Since $\delta$ is arbitrary, we see that the family
of laws of $\Psi^\varepsilon_t, \varepsilon > 0$ is tight. Now
consider the nontruncated case. We see, as before, that the laws of
$\Psi^\varepsilon_t$ converge weakly with respect to the weak topology
of $W_{o, p}$. Hence they are tight in the weak topology of $W_{o,
p}$. We can prove the tightness of $\Psi^\varepsilon_t$ with respect
to the weak topology of $W_m, m \le k - 4$ in the same manner. 

\setcounter{remark}{1}
\begin{remark}\label{c3:rem3.11.2}%Remark 3.11.2
  In the mixing case the tightness assumption is symmetric with
  respect to the forward and backward cases. Hence the tightness and
  weak convergence of inverse flows are always valid. 
\end{remark}


\begin{remark}\label{c3:rem3.11.3}%rem 3.11.3
  The limit of $\Psi^\varepsilon_t$ is unique and it coincides with
  the inverse of the limit of $\phi^\varepsilon_t$. 
\end{remark}

\newpage

\begin{center}
\textbf{NOTES ON REFERENCES}\pageoriginale
\end{center}

\noindent
\textbf{Chapter 1}
\begin{itemize} 
\item[] [\ref{chap1:sec1.1}]The study of N-point processes was initiated by T.E.Harris
  \cite{9} and P. Baxendale \cite{1}. 

\item[] [\ref{chap1:sec1.2}] The local characteristics of Brownian flow was introduced
  by Le Jan \cite{23}. 

\item[] [\ref{chap1:sec1.3}] and [\ref{chap1:sec1.4}]. Most materials are taken from Kunita \cite{18} and
  are ada\-pted to this context. 
\end{itemize}

\noindent
\textbf{Chapter 2}
\begin{itemize}
\item[] [\ref{chap2:sec2.3}] Stochastic integral based on C-semimartingale is due to
  Le Jan \cite{23} and Le Jan-Watanabe \cite{24}. Also see Borkar \cite{4} for
  related problems. 

\item[] [\ref{chap2:sec2.5}] The infinitesimal generators of stochastic flows were
  obtained by \cite{23}, \cite{24} and Fujiwara-Kunita \cite{8}. 

\item[] [\ref{chap2:sec2.6}] This section is adapted from \cite{18}.

\item[] [\ref{chap2:app2.7}] Appendix:  Generalized Ito formula presented here is an
  improvement of the same titled formula in \cite{18}. Conditions imposed
  here are much simpler. 
\end{itemize}

\noindent
\textbf{Chapter 3}
\begin{itemize}
\item[] [\ref{chap3:sec3.1}] Ideas of using Sobolev spaces for the tightness of
  measures originated from Kushner \cite{22}. See also Ikeda-Watanabe
  \cite{13}. 

\item[][\ref{chap3:sec3.4}] Moment inequalities in this section are some modification
  of Kunita \cite{20}. The method of introducing truncated process is due
  to Kestern-Papanicolaou \cite{15}. 

\item[][\ref{chap3:sec3.5}] The weak convergence of $(N + M)$-point processes is
  suggested by~\cite{15}. Lemma~\ref{c3:lem3.5.2} is originated from 
Khasminskii \cite{16}. 

\item[][\ref{chap3:sec3.6}--\ref{chap3:sec3.9}] The arguments are adapted from \cite{20}.

\item[][\ref{chap3:sec3.10}] Mixing lemmas\pageoriginale given here are analogues of those in \cite{15},
  although in \cite{15} the forms are apparently different. 

\item[][\ref{chap3:sec3.11}] The tightness of inverse flow is adapted from \cite{20}.
\end{itemize}

\begin{thebibliography}{99}\pageoriginale
\bibitem {1} {P. Baxendale}:  Brownian motions in the diffeomorphisms
  Group I. Composito Mathematica \textbf{53} (1984), 19--50. 

\bibitem {2}{P. Billingsley}:  Convergence of probability measures,
  John Wiley and Sons, New York, 1968. 

\bibitem {3}{J. Bismut}:  Mechanique Aleatoire, Lecture Notes in
  Math. 866, Springer-Verlag, Berlin, Heidelberg, New York, 1981. 

\bibitem {4}{V.S. Borkar}:  Evolutions of interacting particles in a
  Brownian medium, Stochastics, 1984, Vol 14, pp.~33--79. 

\bibitem {5}{A.V.Borodin}:  A limit theorem for solutions of
  differential equations with random right hand side, Theory
  Probab. Appl. \textbf{22} (1977) 482--497. 

\bibitem {6}{R.M.Dowell}:  Differentiable approximations to Brownian
  motion on manifolds, Ph.D. Thesis, University of Warwick,
  Mathematics Institute, Coventy, 1980. 

\bibitem {7}{D. Elworthy}:  Stochastic dynamical systems and their
  flows, Stochastic analysis ed. by A. Friedman and M. Pinsky,
  79--95, Academic Press, New York, 1978. 

\bibitem {8}{T. Fujiwara-H. Kunita}:  Stochastic differential
  equations of jump type and $L\acute{e}$ vy processes in
  diffeomorphisms group, Kyoto Math. J. \textbf{25} (1985), 71--106. 

\bibitem {9}{T.E. Harris}:  Brownian motions on the diffeomorphisms of
  the plane, Ann. Prob. \textbf{9} (1981), 232--254. 

\bibitem {10}{T.E.Harris}:  Coalescing and non-coalescing stochastic
  flows in $\mathbb{R}^1$, Stochastic Proc. Appl. \textbf{17} (1984),
  187--210. 

\bibitem {11}{J.A.Ibragimov - Yu. V. Linnik}:  Independent and
  stationary sequences of random variables, Groningen,
  Wolter-Noordhoff, 1971. 

\bibitem{12}{N. Ikeda\pageoriginale - S. Nakao - Y. Yamato}:  A class of
  approximations of Brownian motion, Publ. RIMS, Kyoto Univ. 
  \textbf{13} (1977), 285--300. 

\bibitem {13}{N. Ikeda - S. Watanabe}:  Stochastic differetial
  equations and diffusion processes, North-Holland/Kodansha, 1981. 

\bibitem{14}{N. Ikeda - S. Watanabe}, Stochastic flow of
  diffeomorphisms, Stochastic analysis and appl. edited by
  M.A. Pinsky, 179--198, (1984). 

\bibitem{15}{H. Kesten - G.C. Papanicolaou}:  A limit theorem for
  turbulent diffusion, Commun. Math. Phys. \textbf{65} (1979), 97--128. 

\bibitem {16} {R.Z.Khasminkii}:  Principle of averaging for parabolic
  and elliptic differential equations and for Markov processes with
  small diffusion, Theory Probab. Appl. \textbf{8} (1963), 1--21. 

\bibitem {17}{R.Z. Khasminskii}:  A limit theorem for solutions of
  differential equations with random right hand sides, Theory
  Probab. Appl. \textbf{11} (1966), 390--406. 

\bibitem {18} {H. Kunita}:  Stochastic differential equations and
  stochastic flow of diffeomorphisms, Lecture Notes in Math. 1099
  (1984), 144--303. 

\bibitem {19} {H. Kunita}:  On the convergence of solutions of
  stochastic ordinary differential equations as stochastic flows of
  diffeomorphisms, Osaka J. Math. \textbf{21} (1984), 883--991. 

\bibitem {20}{H. Kunita}:  Convergence of stochastic flows connected
  with stochastic ordinary differential equations, stochastics, to
  appear. 

\bibitem{21}{H. Kunita}:  Limit theorems for stochastic differential
  equations and stochastic flows of diffeomorphisms, Lecture Notes in
  Control and Information sciences, \textbf{78} (1985), Springer - Verlag. 

\bibitem{22}{H. Kushner}: An application of Sobolev imbedding
  theorems to criteria for the continuity of the processes with a
  vector parameter, The Annals of Math. Statist. \textbf{40}, 517--528
  (1964). 

\bibitem{23}{Y. Le Jan}:\pageoriginale  Flots de diffusions dans $IR^d$,
  C.R. Acad. Sci Paris 294 (1982), Serie \textbf{1}, 697--699. 

\bibitem{24}{Y. Le Jan - S. Watanabe}:  Stochastic flows of
  diffeomorphisms, Taniguchi Symp. S.A. Katata, 1982, 307--332. 

\bibitem {25}{P. Malliavin}:  Stochastic calculus of variations and
  hypoelliptic operators, Proc. of Inter. Symp. SDE Kyoto, 1976,
  Kinokuniya, Tokyo, 1978. 

\bibitem {26} {H. Matsumoto}:  Convergence of driven flows of
  diffeomorphisms, Stochastics. 

\bibitem{27}{E.J.McShane}:  Stochastic calculus and stochastic models,
  AP, New York, 1974. 

\bibitem {28}{G.C. Papanicolaou - W. Kohler}: Asymptotic theory of
  mixing stochastic ordinary differential equations,
  Comm. Pure. Appl. Math. \textbf{27} (1974), 641--668.  

\bibitem{29} {C.G.Papanicolaou - D.W. Stroock - S.R.S. Varadhan}:
  Martingale approach to some limit theorems, 1976 Duke Turbulence
  Conf., Duke Univ. Math Series III, 1977. 

\bibitem {30}{S.L.Sobolev}:  Application of functional analysis in
  mathematical physics, Amer. Math. Society (1963). 

\bibitem {31}{D.W. Stroock}:  Topics in stochastic differential
  equations, Tata Institute of Fundamental Research, Bombay 1982.  

\bibitem {32}{D.W. Stroock - S.R.S. Varadhan}:  Multidimensional
  diffusion processes, Springer - Verlag, Berlin,  1979. 

\bibitem {33}{S. Watanabe}:  Stochastic flow of diffeomorphisms,
  Proc. fourth Japan U.S.S.R. Symp. Probab. Theory, Lecture Notes in
  Math. \textbf{1021} (1983), 690--698. 

\bibitem{34}{E. Wong - M. Zakai}:  On the relation between ordinary
  and stochastic differntial equations, Inter. J. Engng. Sci \textbf{31}
  (1965), 213--229.
\end{thebibliography}      






