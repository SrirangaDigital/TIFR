\chapter{Analytic theory of Indefinite quadratic forms}\label{chap4}

\section{The theta series}\label{chap4:sec1}\pageoriginale

Let
\begin{equation*}
f(x_{1},\ldots,x_{n})=a_{11}x^{2}_{1}+\cdots+a_{nn}
x_{n}^{2}+b_{1}x_{1}+\cdots+b_{n}x_{n}+c=0\tag{1}\label{c4:eq1}
\end{equation*}
be a Diophantine equation with integral coefficients. Let $S$ denote
the matrix of the homogeneous part. We consider integral linear
homogeneous transformations
$$
x_{i}\to \sum^{n}_{j=1}q_{ij}x_{j},\quad i=1,\ldots,n
$$
where the matrix $Q=(q_{ij})$ is unimodular. Let the resulting
function be $Q(x_{1},\ldots,x_{n})$. Then $f=0$ has an integral
solution if and only if $Q=0$ has an integral solution.

Suppose the matrix $S$ has rank $r$, $0<r<n$ so that $|S|=0$. Let
$\ub{p}$ be a primitive vector such that $S\ub{p}=\ub{0}$. Let $U$ be
the unimodular matrix with $\ub{p}$ as its first column. Then 
$$
S[U]=
\begin{pmatrix}
0 & \ub{0}'\\
\ub{0} & S_{1}               
\end{pmatrix}.
$$
We may repeat the process with $S_{1}$ instead of $S$. Finally
therefore we arrive at a unimodular matrix $V$ so that
$$
S[V]=
\begin{pmatrix}
0 & 0\\
0 & S_{r}
\end{pmatrix}
$$
$|S_{r}|\neq 0$. Put now
$$
(b_{1},b_{2},\ldots,b_{n})V=(c_{1},\ldots,c_{n}).
$$
If\pageoriginale $c_{1},\ldots,c_{n-r}$ are zero it means that by a
unimodular transformation we can bring $f$ into a quadratic form in
$r$-variables. Suppose now that $c_{1},\ldots,c_{n-r}$ are not all
zero. Since they are integers, there exists a unimodular matrix
$V_{1}$ of $n-r$ rows such that
$$
(c_{1},\ldots,c_{n-r})V_{1}=(0,0,\ldots,d).
$$

Put now
$$
V_{2}=
\begin{pmatrix}
V_{1} & 0\\
0 & E_{r}
\end{pmatrix}.
$$
Then $S[VV_{2}]=S[V]$ and $f(x_{1},\ldots,x_{n})$ becomes transformed
into 
\begin{align*}
\varphi(x_{n-r},\ldots,x_{n}) &= d_{11}x^{2}_{n-r+1}+\cdots
d_{rr}x^{2}_{n}\\
 &+dx_{n-r}+d_{1}x_{n-r+1}+\cdots+d_{r}x_{n}+d'.
\end{align*}
This is the form into which $f$ can, in general, be transformed by
unimodular transformation.

We shall hereafter assume $|S|\neq 0$ and that the quadratic form is
integral valued, that is, that for $x_{1},\ldots,x_{n}$ integral,
$f(x_{1},\ldots,x_{n})$ is an integer. This means
\begin{equation*}
\alpha)~S\text{ is semi-integral}\tag{2}\label{c4:eq2}
\end{equation*}
that is that its diagonal elements are integers and twice the
non-diagonal elements are integers. \eqref{c4:eq1} can now be written in
the form
$$
S[\ub{x}]+\ub{b}'\ub{x}+c=S[\ub{x}+\frac{1}{2}S^{-1}\ub{b}]+c-\frac{1}{4}S^{-1}[\ub{b}]. 
$$
If we put $t=c-\frac{1}{4}S^{-1}[\ub{b}]$ and $2S$ $\ub{a}=\ub{b}$,
then \eqref{c4:eq1} takes the simple form
\begin{equation*}
S[\ub{x}+\ub{a}]-t=0.\tag{3}\label{c4:eq3}
\end{equation*}\pageoriginale
Obviously
\begin{equation*}
\beta)~2S\ub{a} \text{ is integral.}\tag{4}\label{c4:eq4}
\end{equation*}
We shall therefore consider the diophantine problem in which the left
side is $S[\ub{x}+\ub{a}]$. Clearly and rational number $t$ which can
be represented by $S[\ub{x}+\ub{a}]$ satisfies
\begin{equation*}
t\equiv S[\ub{a}] (\rm{mod} \; 1).\tag{5}\label{c4:eq5}
\end{equation*}

Consider the diophantine equation $S[\ub{x}+\ub{a}]=t$ under the
conditions \eqref{c4:eq2} and \eqref{c4:eq4}. If $S>0$, the number of integral
solutions, denoted $A(S,\ub{a},t)$, is finite. We now form the {\em
  generating series}
\begin{equation*}
\sum_{t}A(S,\ub{a},t)e^{2\pi itz}\tag{6}\label{c4:eq6}
\end{equation*}
where $z=\xi+i\eta$, $\eta>0$. It follows that
\begin{equation*}
\sum_{t}A(S,\ub{a},t)e^{2\pi itz}=\sum^{\infty}_{\ub{x}\equiv
  -\infty} e^{2\pi iS[x+a] z} \tag{7}\label{c4:eq7}
\end{equation*}
and since $S>0$, the series on the right of \eqref{c4:eq7} converges. The
right side of \eqref{c4:eq7} is a so-called {\em theta series} studied
extensively by {\em Jacobi}. In particular if $S=E_{4}$, the unit
matrix of order $4$ and $\ub{a}=\ub{0}$, we have
$$
\sum^{\infty}_{t=0}A_{4}(t)e^{2\pi
  itz}=\left(\sum^{\infty}_{x=-\infty}e^{2\pi ix^{2}z}\right)^{4}
$$
where $A_{4}(t)$ is the number of integral solutions of
\begin{equation*}
t=x^{2}_{1}+x^{2}_{2}+x^{2}_{3}+x^{2}_{4}.\tag{8}\label{c4:eq8}
\end{equation*}

It\pageoriginale was conjectured by {\em Fermat} and proved by {\em
  Lagrange} that for every $t\geq 1$, \eqref{c4:eq8} has a solution. Jacobi
proved, by using the theory of Elliptic theta series, that
$$
A_{4}(t)=8\sum_{\substack{d/t\\ 4\nmid d}}d.
$$
Since for every $t$, unity divider $t$, we find that $A_{4}(t)\geq 1$,
for all $t\geq 1$. This, besides proving Lagrange's theorem, is a
quantitative improvement of it.

If $S$ is indefinite, $A(S,\ub{a},t)$, if different from zero, is
infinite. This means that the right side of \eqref{c4:eq7} diverges. It is
therefore desirable to define an analogue of $A(S,\ub{a},t)$ for
indefinite $S$. To this end we shall introduce the theta series.

Let $z=\xi+i\eta$, $\eta \succ 0$ be a complex parameter. Let $S$ be
$n$-rowed symmetric and $H$ any matrix in the representation space
$\mathfrak{H}$ of the orthogonal group of $S$. Then $H>0$ and
$HS^{-1}H=S$. Put
\begin{equation*}
R=\xi S+i\eta H.\tag{9}\label{c4:eq9}
\end{equation*}
Then $R=zK+\ob{z}L$ where $K=\frac{1}{2}(S+H)$, $L=\frac{1}{2}(S
-H)$. $R$ is now a complex symmetric matrix whose imaginary part $\eta 
H$ is positive definite. Let $\ub{a}$ be a rational column satisfying
\eqref{c4:eq4}. Put $\ub{y}=\ub{x}+\ub{a}$, $\ub{x}$ being an integral
column. Define
\begin{equation*}
f_{\ub{a}}(z,H)=\sum_{\ub{y}\equiv a (\rm{mod} \; 1)}e^{2\pi iR[\ub{y}]Z}\tag{10}\label{c4:eq10}
\end{equation*}
where the summation runs over all rational columns $\ub{y}\equiv
\ub{a}(\rm{mod} \; 1)$. Since $H>0$, \eqref{c4:eq10} is absolutely convergent for
every $H$ in $\mathfrak{H}$. 

For\pageoriginale our purposes, it seems practical to consider a more
general function $f_{\ub{a}}(z,H,\ub{w})$ defined by
\begin{equation*}
f_{\ub{a}}(z,H,\ub{w})=\sum_{\ub{y}\equiv\ub{a}(\rm{mod} \; 1)}e^{2\pi
  iR[\ub{y}]+2\pi i\ub{y}'\ob{\omega}}\tag{11}\label{c4:eq11}
\end{equation*}
where $\ub{w}$ is a complex column of $n$ rows of elements
$w_{1},\ldots,w_{n}$. It is clear that $f_{\ub{a}}(z,H,\ub{w})$ are
still convergent. \eqref{c4:eq11} is the general theta series.

It is to be noticed that if $S>0$, \eqref{c4:eq10} coincides with
\eqref{c4:eq7}.

Consider now all the rational vectors $\ub{a}$ which satisfy
\eqref{c4:eq4}, namely that $2S\ub{a}$ is integral. It is clear that there
are only finitely many such $\ub{a}$ incongruent $(\rm{mod} \; 1)$. The number
$l$ of such residue classes is clearly at most equal to
\begin{equation*}
d=abs 2 |S|.\tag{12}\label{c4:eq12}
\end{equation*}
Let $\ub{a}_{1},\ub{a}_{2},\ldots,\ub{a}_{l}$ be the complete set of
these $l$ residue classes incongruent $(\rm{mod} \; 1)$. For each class
$\ub{a}_{i}$ form the function $f_{\ub{a}_{i}}(z,H,\ub{w})$. We denote
by $f(x,H,\ub{w})$ the functional vector
\begin{equation*}
f(z,H,\ub{w})=
\begin{pmatrix}
f_{\ub{a}_{1}}(z,H,\ub{w})\\
\vdots\\
f_{\ub{a}_{l}}(z,H, \ub{w}
\end{pmatrix}.\tag{13}\label{c4:eq13}
\end{equation*}

\section{Proof of a lemma}\label{chap4:sec2}

Let $P>0$ be an $n$-rowed real matrix and
$\ub{u}=\left(\begin{smallmatrix} u_{1} \\ \vdots\\ u_{n}
\end{smallmatrix}\right)$ a column of $n$ real numbers. The function
$$
f(u_{1},\ldots,u_{n})=\sum_{\ub{x}}e^{-\pi P[\ub{x}+\ub{u}]}
$$
where\pageoriginale $\ub{x}$ runs through all integral $n$-rowed
columns, is a continuous function of the $n$-variables
$u_{1},\ldots,u_{n}$ and has in each variable the period $1$. It has
the fourier series
$$
\sum_{\ub{l}}c_{\ub{l}}e^{2\pi i\ub{l}'\ub{u}}
$$
$\ub{l}$ running through all integral $n$-rowed vectors and
\begin{equation*}
c_{\ub{l}}=\int\limits_{\sigma}f(u_{1},\ldots,u_{n})e^{-2\pi
  i\ub{l}'\ub{u}}du_{1}\ldots du_{n}\tag{14}\label{c4:eq14}
\end{equation*}
where $\sigma$ is the unit cube in the $n$ dimensional space $R_{n}$
of $u_{1},\ldots,u_{n}$.

Since $P>0$, we can write $P=M'M$ for a non-singular $M$. Put
\begin{equation*}
M\ub{u}=\ub{v},\quad {M'}^{-1}\ub{l}=\ub{k}.\tag{15}\label{c4:eq15}
\end{equation*}
Then 
$$
c_{\ub{l}}:|P|^{-\frac{1}{2}}\int\limits_{R_{n}}e^{-\pi
  \ub{y}'\ub{v}-2\pi i\ub{k}'\ub{v}}d\ub{v} 
$$
where we take the positive value of the square root. If
$\ub{v}=\left(\begin{smallmatrix} v_{1}\\ \vdots\\ v_{n}
\end{smallmatrix}\right)$ and
$\ub{k}=\left(\begin{smallmatrix}k_{1}\\ \vdots\\ k_{n}
\end{smallmatrix}\right)$, then
$\ub{v}'\ub{v}=v_{1}^{2}+\cdots+v_{n}^{2}$ and
$\ub{k}'\ub{v}=k_{1}v_{1}+\cdots+k_{n}v_{n}$ so that
\begin{equation*}
c_{\ub{l}}=|P|^{-\frac{1}{2}}e^{-\pi
  P^{-1}[\ub{l}]}\left(\int\limits^{\infty}_{-\infty}e^{-\pi
  t^{2}}dt\right)^{n}\tag{16}\label{c4:eq16} 
\end{equation*}
That the value of the integral on the right of \eqref{c4:eq16} is unity is
seen as follows: In the first place from the uniform convergence of
the series $f(u_{1},\ldots,u_{n})$, it follows that
\begin{equation*}
\sum_{\ub{x}}e^{-\pi
  P[\ub{x}+\ub{u}]}=|P|^{-\frac{1}{2}}\lambda^{n}\sum_{\ub{l}}e^{-\pi
  P^{-1}[\ub{l}]+2\pi i\ub{l}'\ub{u}}\tag{17}\label{c4:eq17} 
\end{equation*}
where\pageoriginale
$$
\lambda=\int\limits^{\infty}_{-\infty}e^{-\pi t^{2}}dt.
$$

Secondly, $\lambda$ is independent of $P$ and so putting $n=1$, $P=1$
and $u=0$, we see from \eqref{c4:eq17} that $\lambda=1$.

Suppose now that in \eqref{c4:eq17}, $u_{1},\ldots,u_{n}$ are complex
variables. Since $f(u_{1},\ldots,u_{n})$ is absolutely convergent
which moreover is uniformly convergent in every compact subset of the
$n$-complex space of $u_{1},\ldots,u_{n}$, it follows that
$f(u_{1},\ldots,u_{n})$ is an analytic function of the $n$-complex
variables $u_{1},\ldots,u_{n}$. The same is true of the right side of
\eqref{c4:eq17} also. Since \eqref{c4:eq17} holds for all real
$u_{1},\ldots,u_{n}$, it holds, by analytic continuation, for complex
$u_{1},\ldots,u_{n}$ also.

Suppose now that $P$ is a complex symmetric matrix $P=X+iY$ whose real
part $X$ is positive definite. Then $P^{-1}$ also has this
property. For, since $X$ and $Y$ are real symmetric and $X>0$, there
exists a non-singular real $C$ such that
$$
X=C'C,\quad Y=C'DC
$$
where $D=[d_{1},\ldots,d_{n}]$ is a real diagonal matrix with diagonal
elements $d_{1},\ldots,d_{n}$. Now $P=X+iY=C'(E+iD)C$ so that
\begin{equation*}
P^{-1}[C']=\left[\frac{1-id_{1}}{1+d_{1}^{2}},\ldots,\frac{1-id_{n}}{1+d^{2}_{n}}\right]\tag{18}\label{c4:eq18} 
\end{equation*}
which shows, since $C$ is real, that the real part of $P^{-1}$ is
positive definite symmetric. Incidentally we have shown that $P$ is
non-singular. 

If\pageoriginale we now take $u_{1},\ldots,u_{n}$ to be fixed complex
numbers, then $f(P)=\sum\limits_{\ub{x}}e^{-\pi[\ub{x}+\ub{u}]}$ is an
analytic function of the $\dfrac{n(n+1)}{2}$ complex variables
constituting the matrix $P$. Since \eqref{c4:eq17} is true for $P$ real, by
analytic continuation, it is true also if $P$ is complex symmetric
with positive real part. For $|P|^{-\frac{1}{2}}$ one takes that
branch of the algebraic function which is positive for real $P$. We
thus have the

\setcounter{lem}{0}
\begin{lem}\label{chap4:lem1}
Let $P$ be a complex $n$-rowed symmetric matrix with real part
positive. Let $\ub{u}$ be any complex column. Then
$$
\sum_{\ub{x}}e^{-\pi
  P[\ub{x}+\ub{u}]}=|P|^{-\frac{1}{2}}\sum_{\ub{x}}e^{-\pi
  P^{-1}[\ub{x}]+2\pi i\ub{x}'\ub{u}} 
$$
where $\ub{x}$ runs through all integral columns and
$|P|^{-\frac{1}{2}}$ is that branch of the algebraic function which is
positive real $P$.
\end{lem}

\section{Transformation formulae}\label{chap4:sec3}

We now wish to study the transformation theory of the theta series
defined in \eqref{c4:eq11}, under the modular substitutions
\begin{equation*}
z\to z_{M}=\frac{\alpha z+\beta}{\gamma z+\delta},\quad M=
\begin{pmatrix}
\alpha & \beta\\
\gamma & \delta
\end{pmatrix},\quad |M|=1\tag{19}\label{c4:eq19}
\end{equation*}
where $M$ is an integral matrix. Since $H$ will be fixed throughout
this section we shall write $f_{\ub{a}}(z,\ub{w})$ instead of
$f_{\ub{a}}(z,H,\ub{w})$. Following {\em Hermite}, we first consider
the case $\gamma\neq 0$, and write
\begin{equation*}
\frac{\alpha z+\beta}{\gamma
  z+\delta}=\frac{\alpha}{\gamma}+\gamma^{-2}z_{1},\quad
-z^{-1}_{1}=z_{2},\quad
z_{2}=z+\frac{\delta}{\gamma}\tag{20}\label{c4:eq20} 
\end{equation*}

Clearly\pageoriginale
\begin{equation*}
f_{\ub{a}}(z_{M},\ub{w})=f_{\ub{a}}\left(\frac{\alpha}{\gamma}
+\gamma^{-2}z_{1},\ub{w}\right).\tag{21}\label{c4:eq21} 
\end{equation*}
Denote by $R_{M}$ the matrix $R$ in \eqref{c4:eq9} with $z$ replaced by
$z_{M}$, then 
$$
R_{M}=\frac{\alpha}{\gamma}S+\gamma^{-2}R_{1}
$$
where
\begin{equation*}
R_{1}=z_{1}\frac{S+H}{2}+\ob{z}_{1}\frac{S-H}{2}\tag{22}\label{c4:eq22}
\end{equation*}

By definition of $y$, $\ub{y}-\ub{a}$ is integral. We may
therefore write $\ub{v}-\ub{a}=\ub{x}\gamma +\ub{g}$ where $y$
belongs to the finite set of residue classes of integral vectors
$(\rm{mod} \;\gamma)$. When $\ub{x}$ runs through all integral vectors and
$\ub{g}$ through a complete system of $(\rm{mod} \; \gamma)$ incongruent
integral vectors, then $\ub{y}$ runs through all rational vectors
$\equiv \ub{a}(\rm{mod} \; 1)$. We have therefore
$$
f_{\ub{a}}(z_{M},\ub{w})=\sum_{\ub{g}(\rm(mod) \; \gamma)} e^{2\pi
  i\frac{\alpha}{\gamma}S[\ub{g}+\ub{a}]}\sum_{\ub{x}}e^{2\pi
  i(\gamma^{-2}R_{1}[\ub{x}\gamma+\ub{g}+\ub{a}]+\ub{w}'(\ub{w}\gamma+\ub{g}+\ub{a}))} 
$$

$R_{1}$ being non-singular, we can complete squares in the exponent in
the inner sum and obtain,
\begin{equation*}
\left.
\begin{aligned}
& f_{\ub{a}}(z_{M},\ub{w})=e^{-\frac{\pi
      i}{2}R^{-1}_{1}[\ub{w}\gamma]}\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
    i\frac{\alpha}{\gamma}S[\ub{g}+\ub{a}]}\\ 
& \sum_{\ub{x}}e^{2\pi iR_{1}[\ub{x}+(\ub{g}+\ub{a})\gamma^{-1}+R^{-1}_{1}\ub{w}\gamma/2]}
\end{aligned}
\right\}
\tag{23}\label{c4:eq23}
\end{equation*}

In order to be able to apply lemma \ref{chap4:lem1} to the inner sum in
\eqref{c4:eq23} we first compute $R^{-1}_{1}$. Since $S$ and $H$ are
symmetric and $H>0$, there exists a real non-singular $C$ such that 
$$
S=C'
\begin{pmatrix}
E_{p} & 0\\
0 & -E_{q}
\end{pmatrix}C, H=C'C.
$$\pageoriginale
Then $R_{1}$ is given by
\begin{equation*}
R_{1}=C'
\begin{pmatrix}
z_{1}E_{p} & 0\\
0 & -\ob{z}_{1}E_{q}
\end{pmatrix}
C\tag{24}\label{c4:eq24}
\end{equation*}
where $z_{1}$ is given by \eqref{c4:eq20}. It readily follows that
\begin{equation*}
-R^{-1}_{1}=-z^{-1}_{1}\frac{S^{-1}+H^{-1}}{2}-\ob{z}_{1}^{-1}
\frac{S^{-1}-H^{-1}}{2}.\tag{25}\label{c4:eq25}
\end{equation*}

Using \eqref{c4:eq20}, we finally have
\begin{equation*}
-R^{-1}_{1}=\left(\frac{\delta}{\gamma}S+R\right)[S^{-1}]\tag{26}\label{c4:eq26}
\end{equation*}

Applying lemma \ref{chap4:lem1} to the inner sum we now find
\begin{equation*}
\left.
\begin{aligned}
& f_{\ub{a}}(z_{M},\ub{w})=|-2iR_{1}|^{-\frac{1}{2}}e^{-\frac{\pi i}{2}R^{-1}_{1}[w\gamma]}\sum_{\ub{g}(\rm{mod} \; \gamma)}e^{2\pi i\frac{\alpha}{\gamma}S[\ub{g}+\ub{a}]}\\
& \sum_{l}e^{\frac{\pi i}{2}\left(\frac{\delta}{\gamma}S+R\right)[S^{-1}\ub{l}]+2\pi i\ub{l}'((\ub{g}+\ub{a})\gamma^{-1}+R^{-1}_{1}\ub{w}\frac{\gamma}{2})}
\end{aligned}
\right\}\tag{27}\label{c4:eq27}
\end{equation*}
where the square root has to be taken according to the prescription in
lemma \ref{chap4:lem1}. It follows then, using \eqref{c4:eq24}, that
\begin{equation*}
|-2iR_{1}|^{-\frac{1}{2}}=\epsilon d^{-\frac{1}{2}}\left(z+\frac{\delta}{\gamma}\right)^{\frac{p}{2}}\left(\ob{z}+\frac{\delta}{\gamma}\right)^{\frac{q}{2}}\tag{28}\label{c4:eq28} 
\end{equation*}
where
\begin{equation*}
\epsilon=e^{\frac{\pi i}{4}(q-p)}\tag{29}\label{c4:eq29}
\end{equation*}
is an eighth root of unity and $d$ is given by \eqref{c4:eq12}.

In order to simplify the inner sum in \eqref{c4:eq27}, we prove the following 

\begin{lem}\label{chap4:lem2}
If\pageoriginale $\ub{a}$ and $\ub{b}$ are two rational columns such that $2S\ub{a}$ and $2S\ub{b}$ are integral, $\alpha$, $\gamma$, $\delta$ integers such that $\alpha\delta\equiv 1(\rm(mod) \; \gamma)$ and $x$ is any integral column, then
$$
\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{\frac{2\pi i}{\gamma}\left\{\alpha S[\ub{g}+\ub{a}]-2(\ub{x}+\ub{b})'S(\ub{g}+\ub{a})+\delta S[\ub{x}+\ub{b}]\right\}} 
$$
is independent of $\ub{x}$.
\end{lem}

\begin{proof}
We have only to consider the exponent in each term $(\rm{mod} \;\gamma)$. In
the first place we have 
\begin{gather*}
\alpha S[\ub{g}+\ub{a}]=\alpha S[\ub{g}+\ub{a}-\delta \ub{x}]+2\alpha
\delta \ub{x}'S(\ub{g}+\ub{a})-\alpha \delta^{2}S[\ub{x}]\\
\delta S[\ub{x}+\ub{b}]=\delta S[\ub{b}]+\delta
S[\ub{x}]-2\ub{b}'S(\ub{g}+\ub{a}-\delta\ub{x})+2\ub{b}'S(\ub{g}+\ub{a})\\
2(\ub{x}+\ub{b})'S(\ub{g}+\ub{a})=2\ub{b}'S(\ub{g}+\ub{a})+2\ub{x}'S(\ub{g}+\ub{a}). 
\end{gather*}

Using the fact that $\alpha\delta\equiv 1(\rm(mod) \; \gamma)$ we see that
the exponent in each term is congruent $(\rm{mod} \; \gamma)$ to 
$$
\alpha
S[\ub{g}+\ub{a}-\delta\ub{x}]-2\ub{b}'S(\ub{g}+\ub{a}-\delta\ub{x})+\delta
S[\ub{b}]. 
$$
Since now $\delta$ and $\ub{x}$ are fixed and $\ub{g}$ runs over a
complete system of residue classes $(\rm{mod} \; \gamma)$, it follows that
$\ub{g}-\delta \ub{x}$ also runs through a complete system
$(\rm{mod} \;\gamma)$. This proves the lemma.

In the inner sum in \eqref{c4:eq27}, $\ub{l}$ runs through all integral
vectors, so that we may write
$$
\frac{1}{2}S^{-1}\ub{l}=-(\ub{x}+\ub{b})
$$
where $\ub{x}$ is an integral column and $\ub{b}$ is one of the
finitely many representatives $\ub{a}_{1},\ldots,\ub{a}_{n}$ of the
residue classes $(\rm{mod} \; 1)$ given in \eqref{c4:eq13}. Also when $\ub{x}$ runs
through all integral vectors and $\ub{b}$ through these residue class
representatives, $-(\ub{x}+\ub{b})$ runs through all rational columns
of the type $\frac{1}{2}S^{-1}\ub{l}$. We have thus 
\begin{equation*}
\left.
\begin{aligned}
& f_{\ub{a}}(z_{M},\ub{w})=|-2iR_{1}|^{-\frac{1}{2}}e^{-\frac{\pi
      i}{2}R^{-1}_{1}[\ub{W}\gamma]}\\
& \sum_{\ub{b}}\sum_{\ub{x}}\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{\frac{2\pi
      i}{\gamma}\left\{\alpha
    S[\ub{g}+\ub{a}]-2(\ub{x}+\ub{b})'S(\ub{g}+\ub{a})+\delta
    S[\ub{x}+\ub{b}]\right\}}\\
& e^{2\pi iR[\ub{x}+\ub{b}]+2\pi i(\ub{x}+b)'SR^{-1}_{1}\ub{w}\gamma}
\end{aligned}
\right\}
\end{equation*}\pageoriginale


Let us use the abbreviation
\begin{equation*}
\lambda_{\ub{a}\ub{b}}(M)=\sum_{g(\rm(mod) \; \gamma)}e^{\frac{2\pi
    i}{\gamma}\left\{\alpha
  S[\ub{g}+\ub{a}]-2\ub{b}'S(\ub{g}+a)+\delta
  S[\ub{b}]\right\}}\tag{30}\label{c4:eq30} 
\end{equation*}
Then we have
\begin{gather*}
f_{\ub{a}}(z_{M},\ub{w})=|-2iR_{1}|^{-\frac{1}{2}}e^{-\frac{\pi
      i}{2}R^{-1}_{1}[\ub{w}\gamma]}\\
\sum_{b}\sum_{\ub{x}}\lambda_{\ub{a},\ub{b}}(M)e^{2\pi
  iR[\ub{x}+b]+2\pi
  i(\ub{x}+\ub{b})'SR^{-1}_{1}\ub{w}\gamma}\tag{31}\label{c4:eq31} 
\end{gather*}

Let us now define the vector $\ub{w}_{M,z}$ by
\begin{equation*}
-SR^{-1}_{1}\ub{w}_{M,z}=\ub{w}\gamma^{-1}.\tag{32}\label{c4:eq32}
\end{equation*}
Using \eqref{c4:eq22} for $R_{1}$ we get
\begin{equation*}
\ub{w}_{M,z} = \left(\frac{1}{\gamma z+\delta} K +
\frac{1}{\gamma \ob{z}+\delta} L\right)S^{-1}\ub{w}.\tag{33}\label{c4:eq33} 
\end{equation*}

With this definition of $\ub{w}_{M,z}$ we see that
$$
R^{-1}_{1}[\ub{w}_{M,z}\gamma]=R_{1}[S^{-1}\ub{w}].
$$
Use now the abbreviation
\begin{equation*}
\rho(M,z,\ub{w})=e^{-\frac{\pi i}{2}R_{1}[S^{-1}\ub{w}]};\tag{34}\label{c4:eq34}
\end{equation*}
then substituting $\ub{w}_{M,z}$ for $\ub{w}$ in \eqref{c4:eq27}, we get
the formula
\begin{equation*}
\left.
\begin{aligned}
f_{\ub{a}}(z_{M},\ub{w}_{M,z}) &= \epsilon
d^{-\frac{1}{2}}\left(z+\frac{\delta}{\gamma}\right)^{p/2}\left(\ob{z}+\frac{\delta}{\gamma}\right)^{q/2}\\ 
&\quad
\sum_{\ub{b}}\lambda_{\ub{a},\ub{b}}(M)f_{\ub{b}}(z,\ub{w})\rho(M,z,\ub{w}) 
\end{aligned}
\right\}\tag{35}\label{c4:eq35}
\end{equation*}
\end{proof}

Till\pageoriginale now we considered the case $\gamma\neq 0$. Let now
$\gamma=0$. Then
$$
M=
\begin{pmatrix}
\alpha & \beta\\
0 & \delta
\end{pmatrix}
$$
and $\alpha\delta=1$. Also $z_{M}=\dfrac{\alpha
  z+\beta}{\delta}=z+\alpha\beta$. The definition of $\ub{w}_{M,z}$
given in \eqref{c4:eq33} is valid even if $\gamma=0$. Thus
$$
e^{-2\pi i\alpha\beta
  S[\ub{a}]}f_{\ub{a}}(z_{M},\ub{w}_{M,z})=\sum_{\ub{x}}e^{2\pi
  iR[\alpha\ub{x}+\ub{a}\alpha]+2\pi i(\ub{x}+\ub{a})'\ub{w}_{M,z}}.
$$

Since $\alpha=\pm 1$, $\alpha \ub{x}$ also runs through all integral
vectors and so
\begin{equation*}
f_{\ub{a}}(z_{M},\ub{w}_{M,z})=e^{2\pi i\alpha\beta
  S[\ub{a}]}f_{\ub{a}\alpha}(z,\ub{w})\tag{36}\label{c4:eq36} 
\end{equation*}
$\ub{a}\alpha$ being again some one of the
$\ub{a}_{1},\ldots,\ub{a}_{l}$ determined by $\ub{a}$ and $\alpha$.

For any two rational columns $\ub{a}$, $\ub{b}$ with $2S\ub{a}$ and
$2S\ub{b}$ integral, let us define
$$
e_{\ub{a}\ub{b}}=
\begin{cases}
1 & \text{if } \ub{a}\equiv \ub{b}(\rm{mod} \; 1)\\
0 & \text{otherwise.}
\end{cases}
$$

Define now the $l$-rowed matrix $G(M,z)$ by
\begin{equation*}
G(M,z)=
\begin{cases}
\epsilon
d^{-\frac{1}{2}}\left(z+\frac{\delta}{\gamma}\right)^{\frac{p}{2}}\left(\ob{z}+\frac{\delta}{\gamma}\right)^{\frac{q}{2}}(\lambda_{\ub{a}\ub{b}}(M)),
\text{ if } \gamma\neq 0\\
\left(e_{a \; a_\alpha}e^{2\pi i\alpha \beta
  S[\ub{a}]}\right),\text{ if } \gamma=0
\end{cases}\tag{37}\label{c4:eq37}
\end{equation*}
Also put
\begin{equation*}
\rho(M,z,\ub{w})=
\begin{cases}
e^{-\frac{\pi i}{2}R_{1}[S^{-1}\ub{w}]} & \text{ if } \gamma\neq 0\\
1 & \text{ if } \gamma=0
\end{cases}\tag{38}\label{c4:eq38}
\end{equation*}
We\pageoriginale then have the following fundamental formula for the
vector $\ub{f}(z,M,\break\ub{w})$ defined in \eqref{c4:eq13}:

\setcounter{thm}{0}
\begin{thm}\label{chap4:thm1}
If $M=\left(\begin{smallmatrix} \alpha & \beta\\ \gamma & \delta
\end{smallmatrix}\right)$ is any modular matrix and
$z_{M}=\dfrac{\alpha z+\beta}{\gamma z+\delta}$, then
$$
\boxed{\ub{f}(z_{M},\ub{w}_{M,z})=G(M,z)\ub{f}(z,\ub{w})\rho(M,z,\ub{w})}
$$
where $\ub{w}_{M,z}$ is defined by \eqref{c4:eq33} and $G(M,z)$ and
$\rho(M,z,\ub{w})$ by \eqref{c4:eq37} and \eqref{c4:eq38} respectively.
\end{thm}

We shall now obtain a composition formula for the $l$-rowed matrices
$G(M,z)$.

Let $M$ and $M_{1}$ be two modular matrices
$$
M=
\begin{pmatrix}
\alpha & \beta\\
\gamma & \delta
\end{pmatrix},\quad M_{1}=
\begin{pmatrix}
\alpha_{1} & \beta_{1}\\
\gamma_{1} & \delta_{1}
\end{pmatrix},\quad 
MM_{1}=
\begin{pmatrix}
\alpha_{2} & \beta_{2}\\
\gamma_{2} & \delta_{2}
\end{pmatrix}
$$
By the definition of $z_{M}$, it follows that
\begin{equation*}
(z_{M_{1}})_{M}=z_{MM_{1}}\tag{39}\label{c4:eq39}
\end{equation*}

From definition \eqref{c4:eq33}, it follows that
\begin{equation*}
\left.
\begin{aligned}
(\ub{w}_{M_{1},z})_{M,z_{M_{1}}} &= \left(\frac{1}{\gamma
    z_{M_{1}}+\delta}K+\frac{1}{\gamma
    \ob{z}_{M_{1}}+\delta}L\right)S^{-1}.\\[5pt]
&\quad \left(\frac{1}{\gamma_1 z+\delta_{1}}K+\frac{1}{\gamma_{1}\ob{z}+\delta_{1}}L\right)S^{-1}\ob{w}. 
\end{aligned}
\right\}
\end{equation*}
Using the properties of the matrices $K$ and $L$ we get
$$
(\ub{w}_{M_{1},z})_{M,z_{M_{1}}} = \left(\frac{1}{\gamma_{2}z +
  \delta_{2}} K + \frac{1}{\gamma_{2} \ob{z} + \delta_{2}}L\right) 
S^{-1}\ub{w}  
$$
which gives the formula
\begin{equation*}
\ub{w}_{MM_{1},z}=(\ub{w}_{M_{1},z})_{M,z_{M_{1}}}.\tag{40}\label{c4:eq40}
\end{equation*}

Using\pageoriginale the definition of $R_{1}$ and of $\ub{w}_{M,z}$ we
get
\begin{equation*}
R_{1}[S^{-1}\ub{w}]=-\ub{w}'S^{-1}\ub{w}_{M,z}\gamma\tag{41}\label{c4:eq41}
\end{equation*}

Let us now assume, for a moment, that $\gamma$, $\gamma_{1}$,
$\gamma_{2}$ are all different from zero. Using definition \eqref{c4:eq38}
let us write
$$
\rho(M_{1},z,\ub{w})\cdot
\rho(M,z_{M_{1}},\ub{w}_{M_{1},z})=e^{-\frac{\pi i}{2}\varphi}. 
$$
Then using \eqref{c4:eq41}, it follows that 
\begin{align*}
-\rho&=\ub{w}'S^{-1}\left\{\gamma_{1}+\left(\frac{1}{\gamma_{1} z
+\delta_{1}}  K + \frac{1}{\gamma_{1}\ob{z}+\delta_{1}}L\right)\gamma 
S^{-1}\right.\\
&\qquad\left.\left(\frac{1}{\gamma
  z_{M_{1}}+ \delta}K+\frac{1}{\gamma \ob{z}_{M_1} +
  \delta}L\right)S^{-1}\right\}\ub{w}_{M_{1}},z  
\end{align*}

Using again the properties of $K$ and $L$ we obtain
$$
-\varphi=\ub{w}'S^{-1}\gamma_{2}\left(\frac{\gamma_{1}z+\delta_{1}}{\gamma_{2}z+\delta_{2}}K+\frac{\gamma_{1}\ob{z}+\delta_{1}}{\gamma_{2}\ob{z}+\delta_{2}}L\right)S^{-1}\ub{w}_{M_{1},z} 
$$
which is seen to be equal to
$$
\ub{w}'S^{-1}\gamma_{2}\ub{w}_{MM_{1},z}.
$$

By \eqref{c4:eq41} therefore we get the formula
\begin{equation*}
\rho(M_{1},z,\ub{w})\cdot
\rho(M,z_{M_{1}},\ub{w}_{M_{1},z})=\rho(MM_{1},z,\ub{w})\tag{42}\label{c4:eq42} 
\end{equation*}

We can now release the condition on $\gamma$, $\gamma_{1}$,
$\gamma_{2}$. If some or all of them are zero, then using definition
\eqref{c4:eq38}, we can uphold \eqref{c4:eq42}. Thus \eqref{c4:eq42} is true for any
two modular matrices $M$, $M_{1}$.

If we now use theorem \ref{chap4:thm1} we have
\begin{equation*}
\ub{f}(z_{MM_{1}},\ub{w}_{MM_{1},z})=G(MM_{1},z)\ub{f}(z,\ub{w})\rho(MM_{1},z,\ub{w}).\tag{43}\label{c4:eq43}
\end{equation*}
Using \eqref{c4:eq39} and \eqref{c4:eq40} we have
$$
\ub{f}(z_{MM_{1}},\ub{w}_{MM_{1},z})=G(M,z_{M_{1}})\ub{f}(z_{M_{1}},\ub{w}_{M_{1},z})\rho(M,z_{M_{1}},\ub{w}_{M_{1},z}) 
$$
which\pageoriginale again gives the formula
\begin{gather*}
\ub{f}(z_{MM_{1}},\ub{w}_{MM_{1},z})=\\
=G(M,z_{M_{1}})G(M_{1},z)\ub{f}(z,\ub{w})\rho(M,z_{M_{1}},\ub{w}_{M_{1},z})\rho(M_{1}z,\ub{w})\tag{44}\label{c4:eq44}
\end{gather*}
Using \eqref{c4:eq42}, \eqref{c4:eq43} and \eqref{c4:eq44} and observing that
$\rho(MM_{1},z,\ub{w})\neq 0$, we get the matrix equation
\begin{equation*}
(G(MM_{1},z)-G(M,z_{M_{1}})G(M_{1},z))\ub{f}(z,\ub{w})=\ub{0}.\tag{45}\label{c4:eq45}
\end{equation*}

We remark that the $1$-rowed matrix on the left hand side of equation
\eqref{c4:eq45} is independent of $\ub{w}$. Let us now prove

\begin{lem}\label{chap4:lem3}
The $l$ functions
$f_{\ub{a}_{1}}(z,\ub{w}),\ldots,f_{\ub{a}_{1}}(z,\ub{w})$ are
linearly independent over the field of complex numbers.
\end{lem}

\begin{proof}
By definition $f_{\ub{a}}(z,\ub{w})=\sum\limits_{\ub{x}}e^{2\pi
  iR[\ub{x}+\ub{a}]+2\pi i\ub{w}'(\ub{x}+\ub{a})}$ so that it is a\break
fourier series in the $n$ variables $w_{1},\ldots,w_{n}$. We may write
$$
f_{\ub{a}}(z,\ub{w})=\sum_{\ub{r}}c_{r}e^{2\pi i\ub{w}'\ub{r}}
$$
where $\ub{r}$ runs through all rational vectors $\equiv
\ub{a}(\rm{mod} \; 1)$. If $\alpha_{1},\ldots,\alpha_{r}$ be complex numbers
such that
$$
\sum^{l}_{i=1}\alpha_{i}f_{\ub{a}_{i}}(z,\ub{w})=0
$$
then by uniqueness theorem of fourier series, every fourier
coefficient must vanish. But since $\ub{a}_{1},\ldots,\ub{a}_{l}$ are
all distinct $(\rm{mod} \; 1)$, it follows that the exponents in the $l$ series
$f_{\ub{a}}(z,\ub{w})$ are all distinct. Hence
$\alpha_{i}=0,i=1,\ldots,n$, and our lemma is proved.
\end{proof}

Using\pageoriginale \eqref{c4:eq45} and lemma \ref{chap4:lem3}, it follows that
the $l$-rowed matrix on the left of \eqref{c4:eq45} is identically
zero. Hence the 

\begin{thm}\label{chap4:thm2}
For any two modular matrices $M$, $M_{1}$ we have the composition
formula
$$
\boxed{G(MM_{1},z)=G(M,z_{M_{1}})G(M_{1},z)}
$$
\end{thm}

Let $M=\left(\begin{smallmatrix} \alpha & \beta\\ \gamma & \delta
\end{smallmatrix}\right)$ be a modular matrix so that
$$
M^{-1}=
\begin{pmatrix}
\delta & -\beta\\
-\gamma & \alpha
\end{pmatrix}
$$
Let us assume that $\gamma\neq 0$. Let as before $\ub{a}$, $\ub{b}$ be
two rational columns chosen from the set
$\ub{a}_{1},\ldots,\ub{a}_{l}$. We shall prove 
\begin{equation*}
\overline{\lambda_{\ub{a} \; \ub{b}}(M)} =
\lambda_{\ub{b} \; \ub{a}}(M^{-1})\tag{46}\label{c4:eq46}   
\end{equation*}
where $\lambda_{\ub{a} \; \ub{b}}(M)$ is the sum defined in \eqref{c4:eq30}.

In order to prove \eqref{c4:eq46}, put $t=\lambda_{\ub{a} \; \ub{b}}(M)$ and
$t'=\lambda_{\ub{b} \; \ub{a}}(M^{-1})$. Because of lemma \ref{chap4:lem2}, we
have
$$
t=\sum_{\ub{y}(\rm(mod) \; \gamma)}e^{\frac{2\pi i}{\gamma}\left\{\alpha
  S[\ub{y}+\ub{a}]-2(\ub{y}+\ub{b})'S(\ub{x}+\ub{a})+\delta
  S[\ub{x}+\ub{b}]\right\}} 
$$
Taking the sum over all integral $\ub{x}(\rm{mod} \; \gamma)$ we have
$$
\text{t.abs}\gamma^{n}=\sum_{\ub{x}(\rm(mod) \; \gamma)}\sum_{\ub{y}(\rm(mod) \; \gamma)}e^{\frac{2\pi
    i}{\gamma}\{\alpha S[\ub{y}+\ub{a}]-2(\ub{y}+\ub{b})'S(\ub{x}+\ub{a})+\delta
  S[\ub{x}+\ub{b}]\}} 
$$

Interchanging the two summations we have
$$
t\ abs
\gamma^{n}=\sum_{\ub{y}(\rm(mod) \; \gamma)}\sum_{\ub{x}(\rm(mod) \; \gamma)}e^{-\frac{2\pi
    i}{-\gamma}\{\delta
  S[\ub{x}+\ub{b}]-2(\ub{x}+\ub{a})'S(\ub{y}+\ub{b})+\alpha
  S[\ub{y}+\ub{a}]\}} 
$$
But\pageoriginale by lemma \ref{chap4:lem2} again we see that the inner sum
is independent of $\ub{y}$ and equal to $\ob{t'}$, the complex
conjugate of $t'$. Thus
$$
t\ abs\gamma^{n}=\ob{t'}abs\gamma^{n}
$$
and since $\gamma\neq 0$, it follows that $\ob{t}=t'$ and \eqref{c4:eq46}
is proved.

In the composition formula of theorem \ref{chap4:thm2}, let us put
$M_{1}=M^{-1}$. Then $G(E,z)=E$ is the unit matrix of order $1$. From
the definition of $G(M,z)$ we have
\begin{align*}
G(M^{-1},z) &= \epsilon
d^{-\frac{1}{2}}\left(z-\frac{\alpha}{\gamma}\right)^{\frac{p}{2}}\left(\ob{z}-\frac{\alpha}{\gamma}\right)^{\frac{q}{2}}(\lambda_{\ub{a}
  \; \ub{b}}(M^{-1}))\\
G(M,z_{M^{-1}}) &= \epsilon d^{-\frac{1}{2}}(\gamma^{-1}(-\gamma
z+\alpha))^{\frac{p}{2}}(\gamma^{-1}(-\gamma\ob{z}+\alpha))^{\frac{q}{2}}(\lambda_{\ub{a}\ub{b}}(M)) 
\end{align*}

Let us put
\begin{equation*}
\Lambda (M)=\epsilon d^{-\frac{1}{2}}abs
\gamma^{-\frac{n}{2}}(\lambda_{\ub{a}\ub{b}}(M)).\tag{47}\label{c4:eq47} 
\end{equation*}
Then we get from the previous equations
$$
\Lambda(M)\cdot \overline{\Lambda(M)}'=E
$$
which shows that $\Lambda(M)$ is unitary.

In case $\gamma=0$, from \eqref{c4:eq37}, $G(M,z)$ is clearly unitary. We
therefore put
\begin{equation*}
\Lambda(M)=G(M,z),\quad \text{if } \gamma=0.\tag{48}\label{c4:eq48}
\end{equation*}

Let us now put $\ub{w}=\ub{0}$ in theorem \ref{chap4:thm1}. Then
$\rho(M,z,\ub{w})=1$ so that if we write as in \S\ \ref{chap4:sec1},
$\ub{f}(z,H)$ instead of $\ub{f}(z,H,\ub{w})$, when $\ub{w}=\ub{0}$,
we get
\begin{equation*}
\ub{f}(z_{M},H)=G(M,z)\ub{f}(z,H).\tag{49}\label{c4:eq49}
\end{equation*}
Using the definitions \eqref{c4:eq47} and \eqref{c4:eq48}, we get

\begin{thm}\label{chap4:thm3}
If $M=\left(\begin{smallmatrix}\alpha & \beta\\ \gamma & \delta
\end{smallmatrix}\right)$ is a modular matrix, then 
$$
(\gamma_{z}+\delta)^{-\frac{p}{2}}(\gamma\ob{z}+\delta)^{-\frac{q}{2}}\ub{f}(z_{M},H)=\Lambda(M)\ub{f}(z,H)
$$\pageoriginale
where $\Lambda(M)$ is a certain unitary matrix and the radical scalar
factors on the left side are taken with their principal parts.
\end{thm}

We remark that we introduced the vector $\ub{w}$ only to prove the
composition formula in theorem \ref{chap4:thm2}. Hereafter we will have only
$\ub{f}(z,H)$, the column consisting of $f_{\ub{a}_{i}}(z,H)$ defined
in \eqref{c4:eq10}.

From the composition formula we get
\begin{equation*}
\Lambda(MM_{1})=\Lambda(M)\cdot \Lambda(M_{1})\tag{50}\label{c4:eq50}
\end{equation*}
which shows that the mapping $M\to \Lambda(M)$ is a unitary
representation of the modular group.

\section{Convergence of an integral}\label{chap4:sec4}

Let $S$ be the matrix of a quadratic form and let $S$ be non-singular
and semi-integral. Let $S$ have signature $p$, $q$ so that
$p+q=n$. Let us assume that $pq>0$. With $S$ we had associated the
$\mathfrak{H}$ space of matrices $H$ with $H>0$, $HS^{-1}H=S$. Let
$\Gamma$ be the group of units of $S$. Let $\ub{a}$ denote a rational
column vector with $2S\ub{a}$ integral. Denote by $\Gamma_{\ub{a}}$
the group of units of $S$ satisfying
\begin{equation*}
U\ub{a}\equiv \ub{a}(\rm{mod} \; 1)\tag{51}\label{c4:eq51}
\end{equation*}
$\Gamma_{\ub{a}}$ is obviously a subgroup of $\Gamma$ of finite
index. Let $U_{1},\ldots,U_{s}$ denote a complete system of
representatives of left cosets of $\Gamma \rm{mod} \; \Gamma_{\ub{a}}$ so that
$$
\Gamma=\sum^{s}_{i=1}U_{i}\Gamma_{\ub{a}},\quad
s=(\Gamma:\Gamma_{\ub{a}}).
$$
Denote\pageoriginale by $F$ a fundamental region of $\Gamma$ in
$\mathfrak{H}$ and by $F_{k}$ the image by $U_{k}$ of $F$ in
$\mathfrak{H}$. Put
$$
F_{\ub{a}}=\bigcup_{k}F_{k};
$$
then it is easy to verify that $F_{\ub{a}}$ is a fundamental domain
for $\Gamma_{\ub{a}}$ in $\mathfrak{H}$.

For every $H$ in $\mathfrak{H}$ we had defined the theta series
$$
f_{\ub{a}}(z,H)=\sum_{\ub{y}\equiv \ub{a}(\rm{mod} \; 1)}e^{2\pi iR[\ub{y}]}
$$
so that regarded as a function of $H$, $f_{\ub{a}}(z,H)$ is a function
on the manifold $\mathfrak{H}$. If $U\in\Gamma_{\ub{a}}$ then
$$
f_{\ub{a}}(z,H[U])=\sum_{\ub{y}\equiv \ub{a}(\rm{mod} \; 1)}e^{2\pi i(\xi
  S+i\eta H[U])[\ub{y}]}.
$$
Writing $S[U]$ instead of $S$ and observing that $U\ub{y}\equiv
U\ub{a}\equiv \ub{a}(\rm{mod} \; 1)$ and that $U\ub{y}$ runs through all
rational columns $\equiv\ub{a}(\rm{mod} \; 1)$ if $\ub{y}$ does, we have
\begin{equation*}
f_{\ub{a}}(z,H[U])=f_{\ub{a}}(z,H)\tag{52}\label{c4:eq52}
\end{equation*}
so that we may regard $f_{\ub{a}}(z,H)$ as a function on
$F_{\ub{a}}$. Let $dv$ be the invariant volume measure in the
$\mathfrak{H}$ space. We shall now prove that
$$
\int\limits_{F_{\ub{a}}}f_{\ub{a}}(z,H)dv
$$
converges, in particular, if $n>4$ and that
\begin{equation*}
\int\limits_{F_{\ub{a}}}f_{\ub{a}}(z,H)dv=\sum_{\ub{y}\equiv
  \ub{a}(\rm{mod} \; 1)}\int\limits_{F_{\ub{a}}}e^{2\pi iR[\ub{y}]}dv\tag{53}\label{c4:eq53}
\end{equation*}
For proving this it is enough to show that the series of absolute
values of the terms of $f_{\ub{a}}(z,H)$ converges uniformly in every
compact subset of $F_{\ub{a}}$ and that the integral over $F_{\ub{a}}$
of this series of\pageoriginale absolute values converges.

Because of the property \eqref{c4:eq52} and the invariance of the volume
measure it is enough to consider the integral over $F$ instead of
$F_{\ub{a}}$. By our method of construction
$$
F=\bigcup_{k}(\mathfrak{H}\cap \mathscr{R}_{k})
$$
where $\mathscr{R}_{k}$ is obtained from the Minkowski fundamental
domain $\mathscr{R}$. It is therefore enough to consider the integral
$$
\int\limits_{\mathfrak{H}\cap \mathscr{R}}f_{\ub{a}}(z,H)dv
$$
and prove \eqref{c4:eq53} for $\mathfrak{H}\cap \mathscr{R}$ instead of
$F_{\ub{a}}$.

The general term of the integrand is $e^{2\pi iR[\ub{y}]}$ and its
  absolute value is
$$
e^{-2\pi \eta H[\ub{x}+\ub{a}]}
$$
where $\eta>0$, $H>0$ is reduced in the sense of Minkowski, $\ub{x}$
an integral column and $\ub{a}$ a rational column with $2S\ub{a}$
integral. If $H=(h_{kl})$ then $H$ being reduced, there exists a
constant $c_{1}$, such that
$$
H[\ub{y}]>c_{1}(h_{1}y^{2}_{1}+\cdots+h_{n}y^{2}_{n})
$$
$\ub{y}$ being a real column with $n$ elements
$y_{1},\ldots,y_{n}$. Therefore 
$$
\prod^{n}_{i=1}\sum_{y_{i}}e^{-2\pi c_{1}\eta h_{i}y_{i}^{2}}
$$
is a majorant for the sum of the absolute values of the terms of
$f_{\ub{a}}(z,H)$. Since, for a constant $c_{2}>0$,
$$
\sum^{\infty}_{t=-\infty}e^{-c_{2}ht^{2}}<c_{3}(1+h^{-\frac{1}{2}})
$$
where\pageoriginale $h>0$ is a positive real number and $c_{3}$ is a
constant depending on $c_{2}$, it follows that it is enough to prove,
for our purpose, the convergence of
\begin{equation*}
\int\limits_{\mathfrak{H}\cap\mathscr{R}}\prod^{n}_{k=1}(1+h^{-\frac{1}{2}}_{k})dv\tag{54}\label{c4:eq54} 
\end{equation*}

If $D$ is any compact subset of $\mathfrak{H}\cap\mathscr{R}$, then
because $H$ is reduced,
$\prod\limits^{n}_{k=1}(1+h^{-\frac{1}{2}}_{k})$ is uniformly bounded
in $D$. This will prove \eqref{c4:eq53} as soon as \eqref{c4:eq54} is proved.

The proof depends on an application of Minkowski's reduction theory.

Consider any element $H=(h_{k l})$ in
$\mathfrak{H}\cap \mathscr{R}$ and consider the products
$h_{k}h_{n-k}$, $k=1,2,\ldots,n-1$. There exists an integer $r$
\begin{equation*}
0\leq r\leq \frac{n}{2}\tag{55}\label{c4:eq55}
\end{equation*}
such that
\begin{align*}
& h_{k} h_{n-k}\geq \frac{1}{4}\qquad r<k<n-r\tag{56}\label{c4:eq56}\\
& h_{r}h_{n-r}<\frac{1}{4}\tag{57}\label{c4:eq57}
\end{align*}
If $r=0$, \eqref{c4:eq57} is empty and if $r=\frac{n}{2}$ (which implies
that $n$ is even) \eqref{c4:eq56} is empty. Let us denote by $M_{r}$ the
subset of $\mathfrak{H}\cap \mathscr{R}$ consisting of those $H$ which
have the same integer $r$ associated with them. Clearly
$\mathfrak{H}\cap \mathscr{R}=\bigcup\limits_{r}M_{r}$. It is enough
therefore to prove that for every $r$,
$$
\int\limits_{M_{r}}\prod^{n}_{k=1}(1+h^{-\frac{1}{2}}_{k})dv
$$
converges.

We\pageoriginale first obtain a parametrical representation for the
matrices $H$ in $M_{r}$.

Let $K=\dfrac{H+S}{2}=(u_{kl})$ and $-L=\dfrac{H-S}{2}=(v_{kl})$; then
$K$ and $-L$ are non-negative matrices so that
$$
\pm u_{kl}\leq \sqrt{u_{k}u_{1}},\quad \pm v_{kl}\leq
\sqrt{v_{k}v_{1}}
$$
where for a real number $g$, $\pm g$ denotes its absolute value. Since
$K+L=S$ we get
$$
\pm s_{kl}\leq \sqrt{u_{k}u_{l}}+\sqrt{v_{k}v_{l}}.
$$

But since $u_{k}+v_{k}=h_{k}$ we obtain, by using Schwarz's
inequality,
$$
\pm s_{kl}\leq \sqrt{h_{k}h_{1}}.
$$

$H$ being reduced we get for $k\leq r$, $l\leq n-r$, using \eqref{c4:eq57},
\begin{equation*}
\pm s_{kl}\leq \sqrt{h_{k}h_{1}}\leq
\sqrt{h_{r}h_{n-r}}<\frac{1}{2}\tag{58}\label{c4:eq58} 
\end{equation*}
Since $S$ is a semi-integral matrix, it follows that
$$
s_{kl}=0,\quad k\leq r,\quad l\leq n-r.
$$
We have therefore a decomposition of $S$ into the form
\begin{equation*}
S=
\begin{pmatrix}
0 & 0 & P\\
0 & F & Q\\
P' & Q' & G
\end{pmatrix}\tag{59}\label{c4:eq59}
\end{equation*}
where $P$ is an $r$-rowed non-singular matrix. It has to be noted that
if $r=0$, then
\begin{equation*}
S=F\tag{60}\label{c4:eq60}
\end{equation*}
and if $r=\dfrac{n}{2}$ ($n$ is then even),
\begin{equation*}
S=
\begin{pmatrix}
0 & P\\
P' & G
\end{pmatrix}\tag{61}\label{c4:eq61}
\end{equation*}\pageoriginale

We now put $S^{\ast}=S[C]$ where
\begin{equation*}
S^{\ast}=
\begin{pmatrix}
0 & 0 & P\\
0 & F & 0\\
P' & 0 & 0
\end{pmatrix},\quad
C=
\begin{pmatrix}
E & -{P'}^{-1}Q' & -\frac{1}{2}{P'}^{-1}G\\
0 & E & 0\\
0 & 0 & E
\end{pmatrix}\tag{62}\label{c4:eq62}
\end{equation*}

We split up $H$ also in the same fashion, by the Jacobi
transformation, $H=H_{0}[C_{0}]$ where
\begin{equation*}
H_{0}=
\begin{pmatrix}
H_{1} & 0 & 0\\
0 & H_{2} & 0\\
0 & 0 & H_{3}
\end{pmatrix},\quad
C_{0}=
\begin{pmatrix}
E & L_{1} & L_{2}\\
0 & E  & L_{3}\\
0 & 0 & E
\end{pmatrix}\tag{63}\label{c4:eq63}
\end{equation*}
where $H_{1}$ and $H_{3}$ are $r$-rowed symmetric matrices. Put
\begin{equation*}
C_{0}C=L=
\begin{pmatrix}
E & Q_{1} & Q_{2}\\
0 & E & Q_{3}\\
0 & 0 & E
\end{pmatrix}\tag{64}\label{c4:eq64}
\end{equation*}

If we put $H^{\ast}=H[C]$, then since $S^{-1}[H]=S$, it follows that
$S^{\ast-1}[H^{\ast}]=S^{\ast}$. Using the matrix $L$ we have
$$
(LS^{\ast-1}L')[H_{0}]=S^{\ast}[L^{-1}].
$$
Substituting for the various matrices above, we get
\begin{align*}
F^{-1}[H_{2}] &= F\tag{65}\label{c4:eq65}\\
H_{3}P^{-1}H_{1} &= P'\tag{66}\label{c4:eq66}\\
Q_{3} &= -F^{-1}Q'_{1}P\tag{67}\label{c4:eq67}\\
Q_{2} &= (A-\frac{1}{2}F^{-1}[Q'_{1}])P\tag{68}\label{c4:eq68}
\end{align*}
where\pageoriginale $A$ is a skew symmetric matrix of $r$-rows. It is
obvious that if $H_{1}$, $H_{2}$, $H_{3}$, $Q_{1}$, $Q_{2}$ and
$Q_{3}$ satisfy the above conditions, then the corresponding $H$ is in
$\mathfrak{H}$. We therefore choose the parameters for the $M_{r}$
space in the following manner: We have $H_{1}$ is arbitrary, $r$-rowed
and positive. From this $H_{3}$ is uniquely fixed. $Q_{1}$ is an
arbitrary matrix of $r$-rows and $n-2r$ columns. $Q_{3}$ is then
determined uniquely. Choose $A$ to be arbitrary skew symmetric. Then
\eqref{c4:eq68} determines $Q_{2}$. $H_{2}$ is now a positive matrix
satisfying \eqref{c4:eq65}. Thus the parameters are $H_{1}$, $Q_{1}$, $A$
and the parameters required to parametrize the space of positive
$H_{2}$ satisfying \eqref{c4:eq65}. A simple calculation shows that the
number of parameters is
\begin{equation*}
\frac{r(r+1)}{2}+r(n-2r)+\frac{r(r-1)}{2}+(p-r)(q-r).\tag{69}\label{c4:eq69}
\end{equation*}

We now compute the volume element in terms of these parameters. The
metric in the $\mathscr{H}$ space is
$$
ds^{2}=\frac{1}{8}\sigma(H^{-1}dHH^{-1}dH).
$$
We substitute for $H$ in terms of these new parameters. We denote
differentiation by $(\bigdot)$ dot. Since $C$ is a constant matrix we
get 
\begin{align*}
ds^{2} &=
\frac{1}{8}\sigma(H^{\ast-1}dH^{\ast}H^{\ast-1}dH^{\ast})\tag{70}\label{c4:eq70}\\ 
\text{As } H^{\ast} &= H_{0}[L]\text{ \ we get}\\
H^{\ast-1}\overdot{H}^{\ast} &=H^{-1}_{0}[{L'}^{-1}](\overdot{H}_{0}[L]+\overdot{L}'H_{0}L+LH_{0}\overdot{L}). 
\end{align*}
This gives the expression
\begin{equation*}
\left.
\begin{aligned}
\sigma (H^{\ast-1}\overdot{H}^{\ast})^{2} &=
  \left\{\sigma(H^{-1}_{0}\overdot{H}_{0}H^{-1}_{0}\overdot{H}_{0})+4\sigma(H^{-1}_{0}\overdot{H}_{0}\overdot{L}L^{-1}\right\}\\
&+2\sigma(\overdot{L}L^{-1}\overdot{L}L^{-1})+2\sigma(H_{0}[\overdot{L}L^{-1}]H^{-1}_{0})
\end{aligned}
\right\}\tag{71}\label{c4:eq71}
\end{equation*}

We\pageoriginale shall now simplify the expression on the right of
\eqref{c4:eq71}. Since $L=C_{0}C$ and $C$ is a constant matrix, we get
$$
\overdot{L}L^{-1}=
\begin{pmatrix}
0 & \overdot{Q}_{1} & \overdot{Q}_{2} & -\overdot{Q}_{1}Q_{3}\\
 0 & 0 & & \overdot{Q}_{3}\\
0 & 0 & & 0
\end{pmatrix}
$$
which shows that
\begin{equation*}
\sigma(\overdot{L}L^{-1}\overdot{L}L^{-1})=0,\quad
\sigma(H^{-1}_{0}\overdot{H}_{0}\overdot{L}L^{-1})=0.\tag{72}\label{c4:eq72}
\end{equation*}

Using the expression for $H_{0}$ in \eqref{c4:eq63} we get
$$
\sigma(H^{-1}_{0}\overdot{H}_{0}H^{-1}_{0}\overdot{H}_{0})=\sum^{3}_{i=1}\sigma(H^{-1}_{i}\overdot{H}_{i}H^{-1}_{i}\overdot{H}_{i}). 
$$

Differentiating \eqref{c4:eq66} with regard to the variables $H_{1}$ and
$H_{3}$ we get
$$
\overdot{H}_{3}P^{-1}H_{1}+H_{3}P^{-1}\overdot{H}_{1}=0 
$$
which shows that
$H^{-1}_{1}\overdot{H}_{1}=-{P'}^{-1}\overdot{H}_{3}H^{-1}_{3}P'$ and
therefore
\begin{equation*}
\sigma(H^{-1}_{1}\overdot{H}_{1}H^{-1}_{1}\overdot{H}_{1})=\sigma(H^{-1}_{3}\overdot{H}_{3}H^{-1}_{3}\overdot{H}_{3})\tag{73}\label{c4:eq73}
\end{equation*}
Using the expressions for $\overdot{L}L^{-1}$ and $H_{0}$, we obtain
\begin{align*}
\sigma(H_{0}[\overdot{L}L^{-1}]H^{-1}_{0}) &=
\sigma(H_{1}[\overdot{Q}_{1}]H^{-1}_{2})+\sigma(H_{1}[\overdot{Q}_{2}-\overdot{Q}_{1}Q_{3}]H^{-1}_{3})\\
&+\sigma(H_{2}[\overdot{Q}_{3}]H^{-1}_{3}).
\end{align*}

Differentiating \eqref{c4:eq67} and \eqref{c4:eq68} with regard to the variables
$Q_{1}$, $Q_{2}$, $Q_{3}$, we get
\begin{align*}
\overdot{Q}_{3} &= -F^{-1}\overdot{Q}'_{1}P\\
\overdot{Q}_{2} &=
(\overdot{A}-\frac{1}{2}\overdot{Q}_{1}F^{-1}Q'_{1}-\frac{1}{2}Q_{1}F^{-1}\overdot{Q}'_{1})P\tag{74}\label{c4:eq74} 
\end{align*} 

We now introduce the matrix $B$ defined by 
$$
B=\frac{1}{2}(\overdot{Q}_{1}F^{-1}Q'_{1}-Q_{1}F^{-1}\overdot{Q}'_{1}). 
$$\pageoriginale
We can then write
$$
\overdot{Q}_{2}-\overdot{Q}_{1}Q_{3}=(\overdot{A}+B)P.
$$

We have finally, except for a positive constant, the metric in the
space $M_{r}$
\begin{align*}
ds^{2}=\sigma\left\{(H^{-1}_{2}\overdot{H}_{2})^{2}\right\} &+
2\sigma\left\{(H^{-1}_{1}\overdot{H}_{1})^{2}\right\}+4\sigma
(H_{1}[\overdot{Q}_{1}]H^{-1}_{2})\\
&+2\sigma(H_{1}[\overdot{A}+B]H_{1}).\tag{75}\label{c4:eq75}
\end{align*}
The determinant of the quadratic differential form
$$
2\sigma\left\{(H^{-1}_{1}\overdot{H}_{1})^{2}\right\}+4\sigma(H_{1}[\overdot{Q}_{1}]H^{-1}_{2})+2\sigma(H_{1}[\overdot{A}+B]H_{1}) 
$$
is given by
\begin{equation*}
2^{r(n-r-1)}|H_{1}|^{n-2r-2}|H_{2}|^{-r}\tag{76}\label{c4:eq76}
\end{equation*}
If $dv_{2}$ denotes the invariant volume measure in the space of
$H_{2}$, satisfying \eqref{c4:eq65}, then we have to prove
\begin{equation*}
\int\limits_{M_{r}}\prod^{n}_{k=1}(1+h^{-\frac{1}{2}}_{k})|H_{1}|^{\frac{n-2r-2}{2}}|H_{2}|^{-\frac{r}{2}}\{dH_{1}\}\{dQ_{1}\}\{dA\}dv_{2}\tag{77}\label{c4:eq77} 
\end{equation*}
is convergent.

The constants $c_{3},\ldots$ appearing in the sequel all depend only
on $n$ and $S$. Moreover `bounded' shall mean bounded in absolute
value by such constants.

Since $|S|\neq 0$, at least one term in the expansion of $|S|$ does
not vanish. This means there is a permutation
$$
\begin{pmatrix}
1,2,\ldots,n\\
l_{1},l_{2},\ldots,l_{n}
\end{pmatrix}
$$
such\pageoriginale that $s_{kl_{k}}\neq 0$, $k=1,\ldots,n$.

Since $S$ is semi-integral, $\pm s_{kl_{k}}\geq \frac{1}{2}$ which
shows that
\begin{equation*}
h_{k}h_{l_{k}}\geq s^{2}_{kl_{k}}\geq \frac{1}{4}\tag{78}\label{c4:eq78}
\end{equation*}

Consider now the integers $1,2,\ldots,a$ and the corresponding
integers $l_{1},\ldots,l_{a}$, $a\leq n$. At least one of the latter,
say $l_{t}\leq n-a+1$. Therefore 
$$
t\leq a,\quad l_{t}\leq n-a+1.
$$
Since $H$ is reduced, $h_{t}\leq h_{a}$, $h_{l_{t}}\leq
h_{n-a+1}$. Using \eqref{c4:eq78} we get
\begin{equation*}
h_{a}h_{n-a+1}\geq \frac{1}{4},\quad a=1,\ldots,n.\tag{79}\label{c4:eq79}
\end{equation*}

Let us consider the identity
$$
\prod^{n}_{k=1}(h_{k}h_{n-k+1})=\prod^{r}_{k=1}(h_{k}h_{n-k+1})^{2}\cdot \frac{\prod\limits_{k=r+1}^{n}(h_{k}h_{n-k+1})}{\prod\limits^{r}_{k=1}(h_{k}h_{n-k+1})}
$$
Since $r\leq n-r$, it follows using \eqref{c4:eq56}
$$
\frac{\prod\limits^{n}_{k=r+1}(h_{k}h_{n-k+1})}{\prod\limits_{k=1}^{r}(h_{k}h_{n-k+1})}\geq c_{3}h^{2}_{n-r}
$$

Therefore we obtain
$$
\prod^{n}_{k=1}(h_{k}h_{n-k+1})\geq
c_{3}h^{2}_{n-r}\prod^{r}_{k=1}(h_{k}h_{n-k+1})^{2} 
$$
Using \eqref{c4:eq79} and the fact that $H$ is reduced, we get the
inequality
\begin{equation*}
h_{n-r}\leq c_{4}.\tag{80}\label{c4:eq80}
\end{equation*}

Since\pageoriginale $H$ is reduced, $H_{0}\in\mathscr{R}^{\ast}_{c}$
for a $c>0$ depending only on $n$ and $S$. It follows from \eqref{c4:eq80},
therefore, that the elements of $H_{1}$ and $H_{2}$ are bounded. Also
from \eqref{c4:eq79} and \eqref{c4:eq80}, it follows that
\begin{equation*}
h_{k}\geq c_{5}\quad r<k\leq n\tag{81}\label{c4:eq81}
\end{equation*}
which shows that the elements of $H^{-1}_{2}$ are bounded.

From equations \eqref{c4:eq63} and \eqref{c4:eq64} we get
$$
Q_{1}=L_{1}-{P'}^{-1}Q',\quad Q_{2}=L_{2}-\frac{1}{2}{P'}^{-1}G.
$$
Since $P$, $Q$ and $G$ are constant matrices and $L_{1}$, $L_{2}$ have
bounded elements (since $H$ is reduced), it follows that the elements
of $Q_{1}$ and $Q_{2}$ are bounded.

From the definition of $A$ in \eqref{c4:eq68}, it follows that its elements
are also bounded. From \eqref{c4:eq80} and the fact that $H$ is reduced we
get
\begin{equation*}
h_{1}\leq h_{2}\leq h_{3}\ldots\leq h_{r}\leq c_{4}\tag{82}\label{c4:eq82}
\end{equation*}
and therefore
\begin{equation*}
\prod^{r}_{k=1}(1+h^{-\frac{1}{2}}_{k})\leq c_{6}(h_{1}\ldots
h_{r})^{-\frac{1}{2}}\tag{83}\label{c4:eq83} 
\end{equation*}

We therefore finally see that it is enough to prove
$$
\int |H_{1}|^{\frac{n-2r-2}{2}}(h_{1}\ldots
h_{r})^{-\frac{1}{2}}\{dH_{1}\}
$$
converges, $H_{1}$ being reduced and satisfying
\eqref{c4:eq82}. $dH_{1}=\prod\limits_{1\leq i\leq j\leq r}dh_{ij}$. Since
$-h_{i}\leq 2h_{ij}\leq h_{i}$, $i<j$, it follows that the variation
of $h_{ij}$ is $h_{i}$. Therefore it is enough to prove that the
integral 
$$ 
\int (h_{1}\ldots h_{r})^{-\frac{1}{2}}(h_{1}\ldots
h_{r})^{\frac{n}{2}-r-1}h^{r-1}_{1}\ldots h_{r-1}dh_{1}\ldots dh_{r},
$$
extended\pageoriginale over the set $0<h_{1}\leq h_{2}\leq
h_{3}\ldots\leq h_{r}\leq c_{4}$ converges. We make a change of
variables
\begin{equation*}
\left.
\begin{aligned}
h_{1} &= s_{1}\ldots s_{r}\\
h_{2} &= s_{2}\ldots s_{r}\\
h_{r} &= s_{r}
\end{aligned}
\right\}\tag{84}\label{c4:eq84}
\end{equation*}

The integral then becomes transformed into 
\begin{equation*}
\int s^{\lambda_{1}}_{1}\ldots s^{\lambda_{\gamma}}_{r}\cdot
\frac{ds_{1}\ldots ds_{r}}{s_{1}\ldots s_{r}}\tag{85}\label{c4:eq85}
\end{equation*}
where since $s_{k}=\dfrac{h_{k}}{h_{k+1}}$, $0<s_{k}<\Min (1,c_{4})$
and $\lambda_{k}=k\left(\dfrac{n-k}{2}-1\right)$, $k=1,\ldots,r$.

Now $\lambda_{k}\geq \dfrac{n-r}{2}-1$, $k=1,2,\ldots,r$ so that if
$n-r-2>0$, the integral obviously converges. If $n>4$, since $r\leq
\dfrac{n}{2}$, this condition is satisfied and the integral converges.

Now the maximum value of $r$ is $\leq \Min (p,q)$. Let $n=4$ and
$S[\ub{x}]$ be not a quaternionic form, \iec it is not the norm of a
general element of a quaternion algebra over the field of rational
numbers. In that case the maximum value of $r$ is $0$ or $1$ so that
$n-r-2>0$ and the integral converges. If $n=3$ and $S[\ub{x}]$ is not
a zero form, then $r=0$ and $n-r-2>0$.

If $r=0$, then all elements of $H$ in $M_{0}$ are bounded and the
integral over $M_{0}$ converges. This shows that if $n=2$ and
$S[\ub{x}]$ is not a zero form, the integral again converges. 

In particular, we have 

\begin{thm}\label{chap4:thm4}
If\pageoriginale $n>4$ the integral
$$
\int\limits_{F_{\ub{a}}}f_{\ub{a}}(z,H)dv
$$
converges and
$$
\int\limits_{F_{\ub{a}}}f_{\ub{a}}(z,H)dv=\sum_{\ub{y}}\int\limits_{F_{\ub{a}}}e^{2\pi
  iR[\ub{y}]}dv. 
$$
\end{thm}

Let us now consider the integral $\int\limits_{F_{\ub{a}}}dv$. In
order to prove it is finite, it is enough to prove
$\int\limits_{M_{r}}dv$ is finite for every $r$. Thus we have to prove
\begin{align*}
&\int h_{1}^{\frac{n-4}{2}}\ldots h_{r}^{\frac{n-2r-2}{2}}dh_{1}\ldots
  dh_{r}\\
& 0<h_{i}\leq h_{2}\leq \ldots \leq h_{r}\leq c_{4}
\end{align*}
is finite. By the same change of variables we see that instead of
$\lambda_{k}$, one has $\mu_{k}=k\left(\dfrac{n-k-1}{2}\right)$ so
that since $\mu_{k}\geq \dfrac{n-k-1}{2}$, the integral converges if
$n-r-1>0$. Since $r\leq \dfrac{n}{2}$, the integral converges if
$n>2$. If $n=2$ and $r=0$, then again the integral converges. If $n=2$
and $r=1$, $S[\ub{x}]$ is a binary zero form and we had see in the
previous chapter that $\int\limits_{F}dv$ diverges. We have thus
    proved

\begin{thm}\label{chap4:thm5}
If $S[\ub{x}]$ is not a binary zero form
$$
\int\limits_{F_{\ub{a}}}dv 
$$
converges.
\end{thm}

\section{A theorem in integral calculus}\label{chap4:sec5}\pageoriginale

For out later purposes we shall prove a theorem on multiple integrals.

Let $R_{m}$ denote the Euclidean space of $m$ dimensions with
$x_{1},\ldots,x_{m}$ forming a coordinate system. Let
$$
y_{k}=f_{k}(x_{1},\ldots,x_{m}),k=1,\ldots,n,
$$
be $n$ differentiable functions with $n\leq m$. Let
$a_{1},\ldots,a_{n}$ be $n$ real numbers and let $F$ be the `surface'
determined by the $n$ equations
$$
y_{k}=a_{k}\quad k=1,\ldots,n.
$$
Let us moreover assume that the functional matrix
$$
\left(\frac{\partial f_{i}}{\partial x_{j}}\right)\qquad 
\begin{cases}
i=1,\ldots, n\\
j=1,\ldots,m
\end{cases}
$$
has the maximum rank $n$ at every point of $F$. Introduce $m-n$
differentiable functions $y_{n+1},\ldots,y_{m}$ of
$x_{1},\ldots,x_{m}$ so that the Jacobian
$$
J=\left|\left(\frac{\partial y_{i}}{\partial x_{j}}\right)\right|\quad
i,j=1,\ldots,m
$$
is different from zero at every point of $F$. The
$y_{n+1},\ldots,y_{m}$ are the local coordinates of the `surface'
$F$. Let $\Delta$ denote the absolute value of $J$ and put
\begin{equation*}
d\omega=\Delta^{-1}dy_{n+1}\ldots dy_{m}.\tag{86}\label{c4:eq86}
\end{equation*}
The properties of Jacobians show that $d\omega$ is independent of the
choice of $y_{n+1},\ldots,y_{m}$. We shall denote $d\omega$
symbolically by 
\begin{equation*}
d\omega=\frac{\{dx\}}{\{dy\}}\tag{87}\label{c4:eq87}
\end{equation*}\pageoriginale
and take $d\omega$ as the measure of volume on `surface' $F$.

In case $m=n$, because of the conditions on the Jacobian, the point
set $F$ is zero dimensional and we define $d\omega$ to be the measure
which assigns to each point the measure $\dfrac{1}{\Delta}$.

As an example put $m=2$, and consider in $R_{2}$ the point set $F$
defined by
$$
y_{1}=\sqrt{x^{2}_{1}+x^{2}_{2}},\quad y_{1}=1.
$$
Then $\dfrac{\partial y_{1}}{\partial x_{1}}$, $\dfrac{\partial
  y_{1}}{\partial x_{2}}$ cannot both vanish at any point of
$F$. Choose now $y_{2}$ as
$$
y_{2}=\tan^{-1}\dfrac{x_{2}}{x_{1}}
$$
The Jacobian is 
$$
=\left|\frac{\partial (y_{1},y_{2})}{\partial
  (x_{1},x_{2})}\right|=\frac{1}{y_{1}}. 
$$
and the volume element on the circle $F:x^{2}_{1}+x^{2}_{2}=1$ is
$$
d\omega=y_{1}dy_{2}.
$$

Let $X=X^{(r,s)}$ be a real matrix of $r$ rows and $s$ columns with
elements $x_{kl}$ constituting a coordinate system in $R_{rs}$. We
denote by
$$
\{dX\}=\prod^{r}_{k=1}\prod^{s}_{l=1}dx_{kl}
$$
the Euclidean volume element in $R_{rs}$. If however $X=X'$ is
$r$-rowed symmetric, then
$$
\{dX\}=\prod_{1\leq k\leq l\leq r}dx_{kl}
$$

Let\pageoriginale $V$ be a $k$-rowed real non-singular symmetric
matrix with signature $\alpha$, $k-\alpha$. Let $F$ be a rectangular
matrix with $k$-rows and $\beta$ columns so that the matrix $T$
defined by
$$
V[F]=T
$$
is non-singular and has signature $\alpha$, $\beta-\alpha$. Obviously
$\alpha\leq \beta\leq k$. Let $W$ be a fixed matrix of $\beta+\lambda$
rows and of signature $\alpha$, $\beta+\lambda-\alpha$. Then
$\beta+\lambda\leq k$. Let $D$ be the `surface consisting of real
matrices $X$ of $k$ rows and $\lambda$ columns satisfying
$$
V[F,X]=W.
$$
If we write
$$
W=
\begin{pmatrix}
T & Q\\
Q' & R
\end{pmatrix}
$$
then $D$ is the surface defined by the equations
\begin{equation*}
\left.
\begin{aligned}
F'VX=Q\\
X'VX=R
\end{aligned}
\right\}\tag{88}\label{c4:eq88}
\end{equation*}

In conformity with our previous notation, let the volume element on
the `surface' $D$ be denoted by
$$
\frac{\{dX\}}{\{dQ\}\{dR\}}.
$$
We have then the following

\begin{thm}\label{chap4:thm6}
$\int\limits_{D} \dfrac{\{dX\}}{\{dQ\}\{dR\}} =
  \dfrac{\rho_{k-\beta}}{\rho_{k-\beta}-\lambda}
||V||^{\frac{-\lambda}{2}} ||T||^{\frac{\beta-k+1}{2}}
||W||^{\frac{k-\beta-\lambda-1}{2}}$   

\medskip
\noindent
  where
  $\rho_{h}=\prod\limits^{h}_{i=1}\dfrac{\pi^{i/2}}{\Gamma(i/2)}$
  and\pageoriginale $\rho_{0}=1$. Also if $\beta=0$, $||T||$ has to be
  taken equal to $1$. 
\end{thm}

\begin{proof}
First let $\beta>0$. Denote by $I$ the integral
$$
I=||V||^{\frac{\lambda}{2}}||T||^{-\frac{\beta-k+1}{2}}||W||^{-\frac{k-\beta-\lambda-1}{2}}\int\limits_{D}\frac{\{dX\}}{\{dQ\}\{dR\}} 
$$
Let $C$ be a $k$-rowed non-singular matrix. Consider the
transformation,
$$
X\to CX,\quad F\to CF,\quad V\to V[C^{-1}].
$$
This leaves $W$ unaltered. Also
$$
\{d(CX)\}=||C||^{\lambda}\{dX\}
$$
which shows that I is unaltered. We shall choose $C$ in such a manner
that the resulting integral can be easily evaluated.
\end{proof}

Since $F$ has rank $\beta$, there exists a matrix $C_{0}$ such that
$$
C_{0}F=\binom{E_{\beta}}{0}
$$
$E_{\beta}$ being the unit matrix of order $\beta$. Since $V[F]=T$ is
non-singular, we have
$$
V[C^{-1}_{0}]=
\begin{pmatrix}
T & L\\
L' & N
\end{pmatrix}
=
\begin{pmatrix}
T & 0\\
0 & M
\end{pmatrix}
\begin{bmatrix}
E & T^{-1}L\\
0 & E
\end{bmatrix}
$$
As $V$ has signature $\alpha$, $k-\alpha$, it follows that $-M>0$. Put
$-M=P'P$, where $P$ is non-singular. Then 
$$
V[C^{-1}_{0}]=
\begin{pmatrix}
T & 0\\
0 & -E_{k-\beta}
\end{pmatrix}
\begin{bmatrix}
E & T^{-1}L\\
0 & P
\end{bmatrix}
$$

We now choose $C$ so that 
$$
C=
\begin{pmatrix}
E_{\beta} & T^{-1}L\\
0 & P
\end{pmatrix}
C_{0}
$$\pageoriginale
A simple computation of determinants now shows that I reduces to
$$
||T||^{-\frac{\beta-k-\lambda+1}{2}}||W||^{-\frac{k-\beta-\lambda-1}{2}}\int\limits_{D}
\frac{\{dX\}}{\{dQ\}\{dR\}} 
$$
where $D$ is now the domain defined by $X=\binom{X_{1}}{X_{2}}$,
$X_{1}=X_{1}^{(\beta,\lambda)}$ satisfying
\begin{equation*}
\left.
\begin{aligned}
& \begin{pmatrix}
    T & 0\\
    0 & -E_{k-\beta}
  \end{pmatrix}
  \begin{bmatrix}
    E & X_{1}\\
    0 & X_{2}
  \end{bmatrix}=
  \begin{pmatrix}
    T & Q\\
    Q' & R
  \end{pmatrix}
=W\\
& Q=TX_{1},\quad R=X'_{1}TX-X'_{2}X_{2}.
\end{aligned}
\right\}\tag{89}\label{c4:eq89}
\end{equation*}

Completing squares, we get
\begin{equation*}
\begin{pmatrix}
T & 0\\
0 & -E
\end{pmatrix}
\begin{bmatrix}
E & X_{1}-T^{-1}Q\\
0 & X_{2}
\end{bmatrix}
=
\begin{pmatrix}
T & 0\\
0 & R_{1}
\end{pmatrix}\tag{90}\label{c4:eq90}
\end{equation*}
where
\begin{equation*}
R_{1}=-X'_{2}X_{2}\tag{91}\label{c4:eq91}
\end{equation*}
and $-R_{1}>0$.
\begin{gather*}
\text{Now \ } \{dX\}=\{dX_{1}\}\{dX_{2}\}\text{ \ and from
  \eqref{c4:eq89}}\\
\{dX_{1}\}=||T||^{-\lambda}\{dQ\}
\end{gather*}
Also \eqref{c4:eq90} shows that
$$
||W||=||T||\;||R_{1}||.
$$
Therefore I reduces to
$$
||R_{1}||^{-\frac{(k-\beta-\gamma-1)}{2}}\int\limits_{D}\frac{\{dX_{2}\}}{\{dR_{1}\}} 
$$
where $D$ is the domain defined by $X_{2}$ satisfying \eqref{c4:eq91}.

Let\pageoriginale $G$ be a non-singular matrix such that
\begin{equation*}
\left.
\begin{aligned}
X_{2}G &= Y\\
R_{1}[G] &= -S
\end{aligned}
\right\}\tag{92}\label{c4:eq92}
\end{equation*}
Then $\{dY\}=||G||^{k-\beta}\{dX_{2}\}$ and
$\{dS\}=||G||^{\lambda+1}\{dR_{1}\}$ so that if we choose $G$ such
that $R_{1}[G]=-E_{\lambda}$, then in order to prove the theorem it is
enough to prove
\begin{equation*}
\int\limits_{D}\frac{\{dY\}}{\{dS\}}=\frac{\rho_{k}}{\rho_{k-\lambda}}\tag{93}\label{c4:eq93} 
\end{equation*}
where we have written $k$ instead of $k-\beta$ and $D$ is the domain
of $Y$ with
\begin{equation*}
Y'Y=S,\quad S=E_{\lambda}.\tag{94}\label{c4:eq94}
\end{equation*}

Note that \eqref{c4:eq93} is a special case of the theorem we want to
prove, namely with $V=E_{k}$, $W=E_{\lambda}$, $\beta=0$.

In order to prove \eqref{c4:eq93} we shall use induction on
$\lambda$. Assume theorem \ref{chap4:thm6} to have been proved for
$\lambda-1\geq 1$. Let be an integer $0<\beta<\lambda$. Put 
$$
Y=\left(Y^{(k,\beta)}_{1}, Y^{(k,\lambda-\beta)}_{2}\right)
$$
and 
$$
S=
\begin{pmatrix}
T & Q\\
Q' & R
\end{pmatrix}
$$
where $T=Y'_{1}Y_{1}$, $Q=Y'_{1}Y_{2}$, $R=Y'_{2}Y_{2}$. Then
$\{dY_{1}\}\{dY_{2}\}=\{dY\}$ and $\{dS\}=\{dT\}\{dQ\}\{dR\}$. Assume
now $Y_{1}$ fixed. Varying $Y_{2}$ we get
$$
\int\limits_{D} \frac{\{dY\}}{\{dS\}}=\int
\frac{\{dY_{1}\}}{\{dT\}}\frac{\{dY_{2}\}}{\{dQ\}\{dR\}} 
$$
Induction\pageoriginale hypothesis works and hence
$$
\int\limits_{D}\frac{\{dY\}}{\{dS\}}=\frac{\rho_{k-\beta}}{\rho_{k-\lambda}}\int\frac{\{dY_{1}\}}{\{dT\}} 
$$
with $T=Y'_{1}Y_{1}$. Again induction hypothesis works since
$0<\beta<\lambda$ and we have
$$
\int\frac{\{dY_{1}\}}{\{dT\}}=\frac{\rho_{k}}{\rho_{k-\beta}}
$$
\eqref{c4:eq93} is thus proved. In order to uphold induction, we have to
prove \eqref{c4:eq93} in case $\lambda=1$ that is
\begin{equation*}
\int\limits_{D}\frac{\{dX\}}{\{dt\}}=\frac{\rho_{k}}{\rho_{k-1}}\tag{95}\label{c4:eq95} 
\end{equation*}
where $D$ is the space
$$
x^{2}_{1}+\cdots+x^{2}_{k}=t,\quad t=1.
$$

We now use induction on $k$. For $k=1$, the proposition is trivial; so
let $k>1$ and \eqref{c4:eq95} proved for $k-1$ instead of $k$. Introducing
$x_{1},\ldots,x_{k-1}$ as a coordinate system on $D$ we get
$$
\int\frac{\{d\ub{x}\}}{\{dt\}}=\int\frac{\{(dx_{1}\ldots
  dx_{k-1})\}}{x_{k}} 
$$
since $2x_{k} \ dx_{k}=dt$ and we consider only positive values of
$x_{k}$. Now
$$
x_{k}=(1-u)^{\frac{1}{2}}
$$
where $u=x^{2}_{1}+\cdots+x^{2}_{k-1}$. We therefore have
$$
\int\frac{\{dx_{1}\ldots
  dx_{k}\}}{dt}=\int(1-u)^{-\frac{1}{2}}\frac{dx_{1}\ldots
  dx_{k-1}}{du} 
$$
By\pageoriginale induction hypothesis,
$$
\frac{dx_{1}\ldots
  dx_{k-1}}{du}=\frac{\pi^{\frac{k-1}{2}}}{\Gamma(\frac{k-1}{2})}u^{\frac{k-1}{2}-1} 
$$

Therefore we get
$$
\int\frac{\{dx\}}{dt}=\int\limits^{1}_{0}(1-u)^{-\frac{1}{2}}u^{\frac{k-1}{2}-1}du
\frac{\pi^{\frac{k-1}{2}}}{\Gamma(\frac{k-1}{2})} 
$$
Evaluating the beta integral, we get the result.

The case $\beta=0$ is also contained in the above discussion

Theorem \ref{chap4:thm6} is now completely demonstrated.

\section{Measure of unit group and measure of
  representation}\label{chap4:sec6}

Let $S$ be the matrix of a non-degenerate real quadratic form with
signature $p$, $q$, $(p+q=n)$. Let $\Omega$ denote the orthogonal
group of $S$, hence the group of real matrices $Y$ with
$$
S[Y]=S,
$$
$\Omega$ is then a locally compact group and there exists on $\Omega$
a left invariant Haar measure determined uniquely upto a positive
multiplicative constant. Instead of $\Omega$ we shall consider the
surface $\Omega(W)$ consisting of all solutions $Y$ of the matrix
equation 
$$
S[Y]=W,
$$
where $W$ is a fixed matrix, non-singular and of signature $p$,
$q$. Clearly if $Y_{1}$ and $Y_{2}$ lie in $\Omega(W)$,
$Y_{1}Y^{-1}_{2}\in \Omega$ so that $\Omega(W)$ consists of all $CY$
where $Y$ is a fixed solution of $S[Y]=W$ and $C$ runs through all
elements in $\Omega$. According to the previous section,\pageoriginale
we can introduce on $\Omega(W)$, a volume measure
\begin{equation*}
\frac{\{dY\}}{\{dW\}}\tag{96}\label{c4:eq96}
\end{equation*}

The surface $\Omega(W)$ has the property that the orthogonal group of
$S$ acts as a transitive group of left translations
\begin{equation*}
Y\to CY\tag{97}\label{c4:eq97}
\end{equation*}
$C\in \Omega$, on it. Also the measure \eqref{c4:eq96} defined above is
invariant under these left translations. Since $\Omega$ and
$\Omega(W)$ are homeomorphic and $\Omega(W)$ is locally compact,
\eqref{c4:eq96} is the Haar measure in $\Omega(W)$ invariant under
\eqref{c4:eq97}.

It is practical to consider on $\Omega(W)$ the measure
\begin{equation*}
||S||^{-\frac{1}{2}}||W||^{\frac{1}{2}}\frac{\{dY\}}{\{dW\}}\tag{98}\label{c4:eq98}
\end{equation*}
instead of \eqref{c4:eq96}, for the following reason. \eqref{c4:eq96} already
has the invariance property under the transformations
\eqref{c4:eq97}. Consider now the mapping
\begin{equation*}
Y\to YP,\quad W\to W[P]\tag{99}\label{c4:eq99}
\end{equation*}
where $P$ is an $n$-rowed non-singular matrix. Since
$\{d(YP)\}=||P||^{n}\{dY\}$ and $\{dW[P]\}=||P||^{n+1}\{dW\}$, it
follows that \eqref{c4:eq98} remains unaltered by the transformations
\eqref{c4:eq99}. Thus \eqref{c4:eq98} is independent of $W$ which means, we can
choose for $W$ a matrix suitable to us. In particular, if $W=S$,
\eqref{c4:eq98} gives the Haar measure on $\Omega$ required for our
purposes.

Let now $S$ be a rational matrix and $\mathfrak{H}$ the representation
space of the unit group of $S$. The unit group $\Gamma(S)$ of $S$ is a
discrete\pageoriginale subgroup of $\Omega$ and is represented in
$\mathfrak{H}$ by the mapping $H\to H[U]$, $H\in\mathfrak{H}$,
$U\in\Gamma(S)$. We constructed in $\mathfrak{H}$ for $\Gamma(S)$ a
fundamental domain $F$. By theorem \ref{chap4:thm5} it follows that, if
$S[x]$ is not a binary zero form,
\begin{equation*}
V=\int\limits_{F}dv<\infty\tag{100}\label{c4:eq100}
\end{equation*}
where $dv$ is the invariant volume element in $\mathfrak{H}$.

$\Gamma(S)$ being a discrete subgroup of $\Omega$, there exists a
fundamental set $F_{0}$ for $\Gamma(S)$ in $\Omega$. By means of the
translation $Y\to CY$, $C\in\Omega$, we construct a fundamental set
$\widetilde{F}$ for $\Gamma(S)$ in $\Omega(W)$. Let $\mu(S)$ denote
\begin{equation*}
\mu(S)=||S||^{-\frac{1}{2}}||W||^{\frac{1}{2}}\int\limits_{\widetilde{F}}\frac{\{dY\}}{\{dW\}}\tag{101}\label{c4:eq101} 
\end{equation*}
It is to be noted that the value of $\mu(S)$ is independent of the way
$\widetilde{F}$ is constructed. Since \eqref{c4:eq98} is independent of
$W$, $\mu(S)$ is actually the Haar measure of the fundamental set
$F_{0}$ for $\Gamma(S)$ in $\Omega$. We call $\mu(S)$, {\em the
  measure of the unit group} $\Gamma(S)$.

It is to be noticed that the mappings $H\to H[{}^{\pm}U]$ are
identical in $\mathfrak{H}$, whereas for $U\in\Gamma(S)$, the
representations $Y\to UY$ and $Y\to -UY$ are distinct in
$\Omega(W)$. We now prove the important 

\begin{thm}\label{chap4:thm7}
If $\rho_{h}=\prod\limits^{h}_{k=1}\dfrac{\pi^{k.2}}{\Gamma(k/2)}$,
then $\mu(S)$ and $V$ are connected by the relation
$$
\boxed{2\mu(S)=\rho_{p}\rho_{q}||S||^{-(\frac{n+1}{2})} V} 
$$
provided\pageoriginale $S$ is not the matrix of a binary zero form.
\end{thm}

\begin{proof}
In order to prove this we consider the homogeneous as well as the
inhomogeneous parametrical representation of the $\mathfrak{H}$
space. In the homogeneous parametrization $H$ in $\mathfrak{H}$ is
given by 
\begin{equation*}
H=2K-S,\quad K=T^{-1}[Z'S],\quad T=S[Z]>0\tag{102}\label{c4:eq102}
\end{equation*}
where $Z=Z^{(n,p)}$ is a real matrix. $Z$ determines $H$ uniquely, but
$H$ determines $Z$ only upto a non-singular $p$-rowed matrix factor on
the right. Let us put as before
$$
S=S_{0}[C^{-1}]
$$
where $S_{0} = \left(\begin{smallmatrix} E_{p} & 0\\ 0 & -E_{q}
\end{smallmatrix}\right)$. Let
$$
Z=C\binom{E_p}{X}L
$$
with $X=X^{(q,p)}$, and $|L|\neq 0$. The inhomogeneous parametrical
representation is given by
\begin{equation*}
T_{0}=E-X'X>0\tag{103}\label{c4:eq103}
\end{equation*}
with $X$ real and
\begin{equation}
T=T_{0}[L]\tag{104}\label{c4:eq104}
\end{equation}

Let $W$ be a symmetric $n$-rowed matrix of signature $(p,q)$ and
having the form
\begin{equation*}
W=\begin{pmatrix}
T & Q\\
Q' & R
\end{pmatrix},\quad
T=T^{(p)}>0\tag{105}\label{c4:eq105}
\end{equation*}
$\Omega (W)$ will now be the space of solutions $Y$,
$$
Y=(Y_{1}Y_{2}),\quad Y_{1}=Y_{1}^{(n,p)},\quad Y_{2}=Y_{2}^{(n,q)}
$$
satisfying $W=S[Y]$ so that
\begin{equation*}
T=S[Y_{1}]>0,\quad Q=Y'_{1}SY_{2},\quad R=S[Y_{2}]\tag{106}\label{c4:eq106}
\end{equation*}\pageoriginale 
Every $Y_{1}$ satisfying \eqref{c4:eq106} determines a $H$ in
$\mathfrak{H}$ uniquely by \eqref{c4:eq102}. Since with $Y_{1}$, $UY_{1}$
also is a solution where $U\in\Gamma(S)$, we construct a fundamental
set $\widetilde{F}$ in $\Omega(W)$ to be the set consisting of those
$Y_{1}$ for which the corresponding $H$ determined by \eqref{c4:eq102} lie
in $F$, the fundamental domain for $\Gamma(S)$ in $\mathfrak{H}$. It
is easy to verify that $\widetilde{F}$ is actually a fundamental set.
\end{proof}

Now
$$
\{dY\}=\{dY_{1}\}\{dY_{2}\};\quad \{dW\}=\{dT\}\{dQ\}\{dR\}.
$$

Let $Y_{1}$ be fixed so that the corresponding $H$ which it determines
in in $\mathfrak{H}$ is in $F$. Let now $Y_{2}$ satisfy
\eqref{c4:eq106}. We then have
$$
Z\mu
(S)=||S||^{-\frac{1}{2}} ||W||^{\frac{1}{2}} \int\limits_{\tilde{F}(Y_{1})}
\frac{\{dY_{1}\}}{\{dT\}} \int\limits_{D}
\frac{\{dY_{2}\}}{\{dQ\}\{dR\}}   
$$
where $D$ is the domain determined by $Y_{2}$ satisfying \eqref{c4:eq106}
with $Y_{1}$ fixed and $\widetilde{F}(Y_{1})$ is the set of $Y_{1}$
which determine $H$ in $F$. For the inner integral we apply theorem
\ref{chap4:thm6} and so
$$
2\mu(S)=\rho_{q}||S||^{-\frac{q+1}{2}}||T||^{\frac{1-q}{2}}\int\limits_{\widetilde{F}(Y_{1})}\frac{\{dY_{1}\}}{\{dT\}} 
$$

Now $X$ and $L$ determine $Y_{1}$ uniquely, $X$ satisfying \eqref{c4:eq103}
and $L$ satisfying \eqref{c4:eq104}. Thus 
$$
\{dY_{1}\}=||C||^{p}||L||^{q}\{dX\}\{dL\}.
$$
Expressing $L$ in terms of $T_{0}$ \eqref{c4:eq104}, we get
$$
2\mu
(S)=\rho_{q}||S||^{-\frac{n+1}{2}}||T||^{\frac{1}{2}}\int\limits_{F(Y_{1})}|T_{0}|^{-\frac{q}{2}}\frac{\{dX\}\{dL\}}{\{dT\}} 
$$
But\pageoriginale since $T=T_{0}[L]$ we get
$$
\int\frac{\{dL\}}{\{dT\}}=\rho_{p}||T_{0}||^{-p/2}||T||^{-\frac{1}{2}}
$$
We therefore finally have the formula
$$
2\mu (S)=\rho_{p}\rho_{q}||S||^{-\frac{n+1}{2}}\int
|T_{0}|^{-\frac{n}{2}}\{dX\}
$$
From the form of $T_{0}$ we see that the integral has the value $V$
and our theorem is proved.

Let now $S$ be the matrix of a non-degenerate, rational quadratic form
of signature $p$, $q$ so that $p+q=n$. Let $t$ be a rational number
represented by $S$ so that
\begin{equation*}
S[\ub{y}]=t\tag{107}\label{c4:eq107}
\end{equation*}
for an integral column $\ub{y}$. It is obvious that with $\ub{y}$,
$U\ub{y}$ is also a solution of \eqref{c4:eq107} where $U$ is a unit of
$S$. We shall associate with a given solution $\ub{y}$ of \eqref{c4:eq107}
a real number $\mu(\ub{y},S)$ called the {\em measure of the
  representation} $\ub{y}$, which will allow us to generalize, later,
to indefinite forms the notion of ``number'' of representations.

Let $W$ be the real symmetric matrix of signature $p$, $q$ given by
$$
W=
\begin{pmatrix}
t & \ub{q}'\\
\ub{q} & R
\end{pmatrix}
$$
We consider all the real solutions $Y_{0}=Y_{0}^{(n,n-1)}$ satisfying 
\begin{equation*}
S[Y]=W\tag{108}\label{c4:eq108}
\end{equation*}
where $Y=(\ub{y} \; Y_{0})$. Let $\Omega(\ub{q},R)$ be the surface
determined by $Y_{0}$. Thus $Y_{0}$ satisfies 
\begin{equation*}
\ub{q}'=\ub{y}'SY_{0},\quad R=S[Y_{0}]\tag{109}\label{c4:eq109}
\end{equation*}\pageoriginale
$W$ being a fixed matrix. Clearly $\Omega(\ub{q},R)$ is a locally
compact topological space.

Let $\Omega(\ub{y})$ be the subgroup of the orthogonal group of $S$
consisting of those matrices $V$ in $\Omega$ with 
\begin{equation*}
V\ub{y}=\ub{y}.\tag{110}\label{c4:eq110}
\end{equation*}
Then $\Omega(\ub{y})$ is a locally compact topological group. Since
with $Y$, $VY$ for $V\in\Omega(\ub{y})$ is also a solution of
\eqref{c4:eq108}, it follows that the mapping
\begin{equation*}
Y_{0}\to VY_{0}\tag{111}\label{c4:eq111}
\end{equation*}
gives a representation of $\Omega(\ub{y})$ in
$\Omega(\ub{q},R)$. Clearly this representation is faithful. Also
since $W$ is fixed, the representation \eqref{c4:eq111} of $\Omega(\ub{y})$
on $\Omega(\ub{q},R)$ is transitive on $\Omega(\ub{q},R)$. We
introduce the volume element
\begin{equation*}
||S||^{-\frac{1}{2}}||W||^{\frac{1}{2}}\frac{\{dY_{0}\}}{\{d\ub{q}\}\{dR\}}\tag{112}\label{c4:eq112} 
\end{equation*}
which is clearly invariant under the mappings \eqref{c4:eq111}. Thus
\eqref{c4:eq112} gives the left invariant Haar measure in the locally
compact space $\Omega(\ub{q},R)$.

The volume element \eqref{c4:eq112} introduced above has another
property. Let $P$ be a real matrix of the form
$$
P=
\begin{pmatrix}
1 & \ub{p}'\\
\ub{0} & P_{0}
\end{pmatrix}
$$
where $|P_{0}|\neq 0$ so that $P$ is non-singular. Consider the
transformation 
\begin{equation*}
\left.
\begin{aligned}
& Y\to \ub{y}\ub{p}'+Y_{0}P_{0}\\
& W\to W[P]
\end{aligned}
\right\}\tag{113}\label{c4:eq113}
\end{equation*}\pageoriginale
Then 
\begin{align*}
\{d(\ub{y}\ub{p}'+Y_{0}P_{0})\} &= ||P_{0}||^{n}\{dY_{0}\}\\
\{dW[P_{0}]\} &= ||P_{0}||^{n+1}\{dW\}
\end{align*}
which shows that the transformations \eqref{c4:eq113} leave \eqref{c4:eq112}
unaltered. Thus \eqref{c4:eq112} is independent of $W$ and we may therefore
choose $W$ a particular way suitable to us.

Put $Y=(\ub{y}Y_{1}Y_{2})$ where $Y_{1}=Y_{2}^{(n,p)}$,
$Y_{2}=Y_{2}^{(n,q-1)}$ and write
$$
W=
\begin{pmatrix}
W_{1} & Q\\
Q' & R_{1}
\end{pmatrix},\quad W_{1}=W_{1}^{(p+1)}
$$
where
\begin{equation*}
W_{1}=S[\ub{y}Y_{1}]=
\begin{pmatrix}
t & \ub{v}'\\
\ub{v} & T
\end{pmatrix}\tag{114}\label{c4:eq114}
\end{equation*}

We now choose $W$ so that 
\begin{equation*}
|W_{1}|\neq 0,\quad T>0.\tag{115}\label{c4:eq115}
\end{equation*}
Since $T$ has $p$ rows and columns and $S$ has signature $p$, $q$, it
follows that $W_{1}$ has signature $p$, $1$.

The subgroup $\Gamma(\ub{y})$ of units $U$ of $S$ with
$U\ub{y}=\ub{y}$ is a discrete subgroup of $\Omega(\ub{y})$ and so the
representation \eqref{c4:eq111} with $V\in\Gamma(\ub{y})$ is discontinuous
in $ \Omega (\ub{q},R)$. Let $F(\ub{y})$ be a fundamental region in
$\Omega(q,R)$, for this discrete subgroup $\Gamma(y)$. We define the
measure $\mu(\ub{y},S)$ of the representation $\ub{y}$ by 
\begin{equation*}
\mu(\ub{y},S)=||S||^{-\frac{1}{2}}||W||^{\frac{1}{2}}\int\limits_{\widetilde{F}(\ub{y})}\frac{\{dY_{0}\}}{\{d\ub{q}\}\{dR\}}\tag{116}\label{c4:eq116} 
\end{equation*}\pageoriginale

We shall first show how to construct the fundamental region
$\widetilde{F}(\ub{y})$. Let $Y$ be a solution of the equations
\eqref{c4:eq115}, \eqref{c4:eq114}. According to \eqref{c4:eq102}, this determines
uniquely a $H$ in the $\mathfrak{H}$ space. If $U\in \Gamma(\ub{y})$,
then $UY_{1}$ determines the point $H[U^{-1}]$ in $\mathfrak{H}$. Let
$F(\ub{y})$ be the fundamental region in $\mathfrak{H}$ for the
discrete subgroup $\Gamma(\ub{y})$ of $\Gamma(S)$, the unit group of
$S$. This $F(\ub{y})$ can be constructed as follows: Let $\Gamma(S)$
be written as a union of left cosets modulo $\Gamma(\ub{y})$,
$$
\Gamma(S)=\sum_{i}U_{i}\Gamma(\ub{y}).
$$
Let $F$ be the fundamental region for $\Gamma(S)$ in
$\mathfrak{H}$. Let $F(\ub{y})=\bigcup_{i} F(U_{i})$. Then
$F(\ub{y})$ is the required region. Since $Y_{0}=(Y_{1},Y_{2})$ we
define $\widetilde{F}(\ub{y})$ to be the set of $Y_{0}$ for which the
$Y_{1}$ determines a point in $F(\ub{y})$. It can be easily verified
that $\widetilde{F}(\ub{y})$ determined in this manner is a
fundamental region for $\Gamma(\ub{y})$ in $\Omega(\ub{q},R)$.

Because of \eqref{c4:eq109} and \eqref{c4:eq114} we may write,
\begin{equation*}
\ub{q}=\binom{\ub{v}}{\ub{v}_{1}},\quad
R=
\begin{pmatrix}
T & T_{1}\\
T'_{1} & R_{1}
\end{pmatrix}\tag{117}\label{c4:eq117}
\end{equation*}
Then 
\begin{align*}
\{d\ub{q}\} &= \{d\ub{v}\} \{d\ub{v}_{1}\}\\
\{dR\} &= \{dT\}\{dT_{1}\}\{dR_{1}\}
\end{align*}

Since\pageoriginale $\{dY_{0}\}=\{dY_{1}\}\{dY_{2}\}$ we fix $Y_{1}$
so that the $H$ that it determines in $\mathfrak{H}$ is in $F(\ub{y})$
and integrate over the space of $Y_{2}$ which clearly is determined by
$$
S[\ub{y}Y_{1},Y_{2}]=
\begin{pmatrix}
W_{1} & Q\\
Q' & R_{1}
\end{pmatrix}
$$
Since 
$$
\frac{\{dY_{2}\}}{\{dQ\}\{dR_{1}\}}=
\frac{\{dY_{2}\}}{\{d\ub{v}_{1}\}\{dT_{1}\}\{dR_{1}\}}
$$
We have, on using theorem \ref{chap4:thm6},
\begin{equation*}
\mu(\ub{v},S)=\rho_{q-1}||S||^{-q/2}||W_{1}||^{1-\frac{q}{2}}\int
\frac{\{dY_{1}\}}{\{d\ub{v}\}\{dT\}}\tag{118}\label{c4:eq118} 
\end{equation*}
where the domain of integration is over those $Y_{1}$ which determine
points $H$ in $F(\ub{y})$. Since $T>0$, \eqref{c4:eq114} now gives
$$
W=
\begin{pmatrix}
t-w & \ub{0}'\\
\ub{0} & T
\end{pmatrix}
\begin{bmatrix}
1 & \ub{0}'\\
T^{-1}\ub{v} & E
\end{bmatrix}
$$
where $w=T^{-1}[\ub{v}]$. Since $T \succ 0$ and $W_{1}$ has signature
$(p,1)$, it follows that
\begin{equation*}
\left.
\begin{aligned}
w-t &> 0\\
 w &\geq 0
\end{aligned}
\right\}\tag{119}\label{c4:eq119}
\end{equation*}
Substituting $|W_{1}|=(t-w)|T|$, we get from \eqref{c4:eq118}
\begin{equation*}
\mu(\ub{y},S)=\rho_{q-1}||S||^{-q/2}||T||^{1-\frac{q}{2}}(w-t)^{1-\frac{q}{2}}\int
\frac{\{dY_{1}\}}{\{d\ub{v}\}\{dT\}}\tag{120}\label{c4:eq120} 
\end{equation*}

We now remark that $w$ depends only on the $H$ which $Y_{1}$
determines in $\mathfrak{H}$. For, 
$$
w=T^{-1}[\ub{v}]=T^{-1}[Y'_{1}S\ub{y}]=T^{-1}[Y'_{1}S][\ub{y}]
$$\pageoriginale
But from \eqref{c4:eq102}, $T^{-1}[Y'_{1}S]=\dfrac{H+S}{2}$ so that
$$
2W=(H+S)[\ub{y}]=H[\ub{y}]+t
$$
or that
\begin{equation*}
w=\frac{H[\ub{y}]+t}{2}\tag{121}\label{c4:eq121}
\end{equation*}

Let now $g(w)$ be an integrable function of $w$ to be chosen
later. Multiply both sides of \eqref{c4:eq120} by $g(w)$
$(w-t)^{\frac{q}{2}-1}$ and integrate over the $\ub{v}$ space
satisfying
$$
T^{-1}[\ub{v}]=w>t.
$$
We then get, by applying theorem \ref{chap4:thm6} and using
$$
\{d\ub{v}\}=\frac{\{d\ub{v}\}}{dw}\cdot dw
$$
the result
\begin{gather*}
\rho_{1}\mu(\ub{y},S)\int\limits_{w>\Max(0,t)}g(w)
w^{\frac{p}{2}-1}(\omega-t)^{\frac{q}{2}-1}dw\\
=\rho_{p-1}\rho_{q-1}||S||^{-q/2}||T||^{- \frac{1}{2} - \frac{q}{2}} 
\int g(w)\frac{\{dY_{1}\}}{\{dT\}}\tag{122}\label{c4:eq122} 
\end{gather*}
The function $g(w)$ has to be so chosen that the integrals are
convergent. We will see later that this can be done. The domain of
integration for the integral on the right of \eqref{c4:eq122} is over that
set of $Y_{1}$ which determine $H$ in $F(\ub{y})$. Since every $H$ in
$F(\ub{y})$ determines a $Y_{1}$, we see that we have to apply the
analysis in the proof of theorem \ref{chap4:thm7} to obtain 

\begin{thm}\label{chap4:thm8}
Let $\mu(\ub{y},S)$ be the measure of the representation $\ub{y}$ of
$S[\ub{y}]=t$. Then
$$
\mu(\ub{y},S)\int\limits_{\substack{w>0\\ w>t}}g(w)w^{\frac{p}{2}-1}(w-t)^{\frac{q}{2}-1}dw=\rho_{p-1}\rho_{q-1}||S||^{-
  \frac{n}{2}}\int\limits_{F(\ub{y})}g\left(\frac{H[\ub{y}]+t}{2}\right)dv  
$$\pageoriginale
where $g(w)$ is an integrable function making the integrals converge
and $dv$ is the invariant volume element in the $\mathfrak{H}$ space.
\end{thm}

\section{Integration of the theta series}\label{chap4:sec7}

We shall hereafter assume that $n>4$.

Let us denote by $V_{\ub{a}}$ the volume of the fundamental region
$V_{\ub{a}}$ for $\Gamma_{\ub{a}}$ in the $\mathfrak{H}$ space so that
$$
V_{\ub{a}}=\int\limits_{F_{\ub{a}}}dv.
$$
$F_{\ub{a}}$ is finite by theorem \ref{chap4:thm5}. We put
\begin{equation*}
\varphi_{\ub{a}}(z)=V^{-1}_{\ub{a}}\int\limits_{F_{\ub{a}}}
f_{\ub{a}}(z,H)dv\tag{123}\label{c4:eq123} 
\end{equation*}
Then by theorem \ref{chap4:thm4},
$$
\varphi_{\ub{a}}(z)=V^{-1}_{\ub{a}}\sum_{\ub{y}\equiv \ub{a}(\rm{mod} \;
  1)}\int\limits_{F_{\ub{a}}} e^{2\pi iR[\ub{y}]}dv
$$

If $\ub{a}\equiv \ub{0}(\rm{mod} \; 1)$, then $\ub{y}\equiv\ub{0}$ is a
possible value of $\ub{y}$ and then we have the term
\begin{equation*}
V^{-1}_{\ub{a}}\int\limits_{F_{\ub{a}}}dv\tag{124}\label{c4:eq124}
\end{equation*}
By definition of $V_{\ub{a}}$, the value of \eqref{c4:eq124} is unity. Let
us therefore put
$$
\gamma_{\ub{a}}=
\begin{cases}
1 & \text{if } \ub{a}\equiv \ub{0}(\rm{mod} \; 1)\\
0 & \text{otherwise.}
\end{cases}
$$
Then\pageoriginale we have
$$
\varphi_{\ub{a}}(z)=\gamma_{\ub{a}}+V^{-1}_{\ub{a}}\sum_{\substack{\ub{y}\equiv
\ub{a}(\rm{mod} \; 1)\\ \ub{y}\not\equiv
\ub{0}}}\int\limits_{F_{\ub{a}}}e^{2\pi iR[\ub{y}]}dv.
$$

Let us call two rational vectors $\ub{y}_{1}$ and $\ub{y}_{2}$ {\em
  associated}, if there exists a matrix $U$ in $\Gamma_{\ub{a}}$ such
that $\ub{y}_{1}=U\ub{y}_{2}$. Otherwise they are said to be
non-associated. For any $\ub{y}$ consider the subgroup
$\Gamma_{\ub{a}}(\ub{y})$ of $\Gamma_{\ub{a}}$ with
$U\ub{y}=\ub{y}$. We can write
\begin{equation*}
\Gamma_{\ub{a}}=\sum_{k}U_{k}\Gamma_{\ub{a}}(\ub{y})\tag{125}\label{c4:eq125}
\end{equation*}
as a union of left cosets. $U_{k}\ub{y}$ then run through all vectors
associated with $\ub{y}$. Because of uniform convergence, we can write
$$
\varphi_{a}(z)=\gamma_{a}+ {\mathop{\sum}}'_{\ub{y}}\sum_{k}V^{-1}_{\ub{a}}\int\limits_{F_{\ub{a}}}e^{2\pi
  iR[U_{k}\ub{y}]}dv 
$$
where the accent indicates that we should sum over all non-associate
vectors $\ub{y}$ with $\ub{y}\neq \ub{0}$ and
$y\equiv\ub{a}(\rm{mod} \; 1)$. Since the volume element $dv$ has the
invariance property we may write
$$
\varphi_{\ub{a}}(z)=\gamma_{a} +
       {\mathop{\sum}}'_{\ub{y}}\sum_{k}V^{-1}_{\ub{a}}\int\limits_{F_{\ub{a}}[U_{k}]}e^{2\pi 
  iR[\ub{y}]}dv
$$
where $F_{\ub{a}}[U_{k}]$ is the image of the fundamental region
$F_{\ub{a}}$ by the transformation $H\to H[U_{k}]$. Because of
\eqref{c4:eq125} a fundamental region $F(\ub{y})$ for
$\Gamma_{\ub{a}}(\ub{y})$ in $\mathfrak{H}$ is given by 
$$
F(\ub{y})=\sum_{k}F_{\ub{a}}[U_{k}].
$$

Consider the group $\Gamma_{\ub{a}}(y)$. Now $-E$ is not an element of
$\Gamma_{\ub{a}}(\ub{y})$ since that means $-\ub{y}=\ub{y}$ or
$\ub{y}=\ub{0}$. But $-E$ may be in\pageoriginale
$\Gamma_{\ub{a}}$. This means that $-\ub{a}\equiv \ub{a}(\rm{mod} \; 1)$ or
$2\ub{a}\equiv\ub{0}(\rm{mod} \; 1)$. In this the $U_{k}$'s in \eqref{c4:eq125}
may be so chosen that with $U_{k}$, $-U_{k}$ is also a representative
of a coset. Since $H\to H[U]$ and $H\to H[-U]$ define the same mapping
in the $\mathfrak{H}$ space, it shows that if
$2\ub{a}\equiv\ub{0}(\rm{mod} \; 1)$, the $F_{\ub{a}}[U_{k}]$ give a double
covering of the fundamental region $F(\ub{y})$. So let us define
$$
j_{\ub{a}}=
\begin{cases}
2 & \text{if } 2\ub{a}\equiv\ub{0}(\rm{mod} \; 1)\\
1 & \text{if } 2\ub{a}\not\equiv \ub{0}(\rm{mod} \; 1).
\end{cases}
$$
Then we can write
$$
\varphi_{\ub{a}}(z)=\gamma_{\ub{a}}+j_{\ub{a}}\sum_{\ub{y}}V^{-1}_{\ub{a}}\int\limits_{F(\ub{y})}e^{2\pi
  iR[\ub{y}]}dv
$$

Let us now put in theorem \ref{chap4:thm8}
$$
g(w)=e^{2\pi it\ob{z}-4\pi \eta w}
$$
and use the abbreviation
\begin{equation*}
h_{t}(z)=e^{2\pi it\ob{z}}\int\limits_{w>\max
  (0,t)}w^{\frac{p}{2}-1}(w-t)^{\frac{q}{2}-1}e^{-4\pi \eta
  w}dw,\tag{126}\label{c4:eq126} 
\end{equation*}
then, since \eqref{c4:eq126} converges for $p>0$, $q>0$, $\eta>0$, we get
\begin{equation*}
\varphi_{\ub{a}}(z)=\gamma_{\ub{a}}+\frac{j_{\ub{a}}}{V_{\ub{a}}}\sum'_{\ub{y}}\frac{\mu(\ub{y},S)}{\rho_{p-1}\rho_{q-1}}||S||^{\frac{n}{2}}h_{t}(z)\tag{127}\label{c4:eq127} 
\end{equation*}

It can be shown that for each rational number $t\neq 0$, the number of
non-associate representations
$$
S[\ub{y}]=t,\quad \ub{y}\equiv \ub{a}(\rm{mod} \; 1)
$$
is finite. If $t=0$, one has to consider only non-associate primitive
representations. If therefore we put
\begin{equation*}
M(S,\ub{a},t)=\sum_{\ub{y}}\mu(\ub{y},S)\tag{128}\label{c4:eq128}
\end{equation*}
where\pageoriginale the summation runs on the right through the
finitely many non-associate representations of $S[\ub{y}]=t$, we can
write \eqref{c4:eq127} in the form
\begin{equation*}
\varphi_{\ub{a}}(z)=\gamma_{\ub{a}}+\frac{j_{\ub{a}}}{V_{\ub{a}}}\frac{||S||^{n/2}}{\rho_{p-1}\rho_{q-1}}\sum_{t\equiv S[\ub{a}](\rm{mod} \; 1)}M(S,\ub{a},t)h_{t}(z)\tag{129}\label{c4:eq129}
\end{equation*}

Just as we defined $\mu(S)$ in theorem \ref{chap4:thm7} for the unit group
$\Gamma$, we can define $\mu_{\ub{a}}(S)$ for the subgroup
$\Gamma_{\ub{a}}$ of $\Gamma$ also. $\Gamma_{\ub{a}}$ is a subgroup of
finite index $(\Gamma:\Gamma_{\ub{a}})$ in $\Gamma$. Let
$$
\Gamma=\sum_{U}U\Gamma_{\ub{a}}
$$
be a decomposition of $\Gamma$ into left cosets $\rm{mod} \;
\Gamma_{\ub{a}}$. If $F$ is a fundamental region for $\Gamma$, then
$$
F_{\ub{a}}=\sum_{U}F[U]
$$
is a fundamental region for $\Gamma_{\ub{a}}$. Since $U$ and $-U$
give rise to the same mapping in $\mathfrak{H}$ space, we have to
consider whether $-E$ belongs to $\Gamma_{\ub{a}}$; \iec
$2\ub{a}\equiv\ub{0}(\rm{mod} \; 1)$, which means that $U$ and $-U$ are in
the same coset and so
$$
V_{\ub{a}} = (\Gamma:\Gamma_{\ub{a}})V.
$$
If however $2\ub{a}\not\equiv \ub{0}(\rm{mod} \; 1)$, then $U$ and $-U$
belong to different cosets and so $\sum\limits_{U}F[U]$ gives a double
covering of $F_{\ub{a}}$. Thus
$$
V_{\ub{a}}=\frac{1}{2}(\Gamma:\Gamma_{\ub{a}})V.
$$

Using the definition of $j_{\ub{a}}$ we get
$$
\frac{\mu(S)}{V}=\frac{\mu_{\ub{a}}(S)}{V_{\ub{a}}}\cdot
\frac{j_{\ub{a}}}{2}
$$
If\pageoriginale we denote $\mu(S,\ub{a},t)$ the quantity
\begin{equation*}
\mu(S,\ub{a},t)=\frac{M(S,\ub{a},t)}{\mu_{\ub{a}}(S)}\tag{130}\label{c4:eq130}
\end{equation*}
We get, on using theorem \ref{chap4:thm7}, the final formula
\begin{equation*}
\varphi_{\ub{a}}(z)=\gamma_{\ub{a}}+\frac{\pi^{n/2}||S||^{-\frac{1}{2}}}{\Gamma(p/2)\Gamma(q/2)}\sum_{t\equiv
  S[\ub{a}](\rm{mod} \; 1)}\mu(S,\ub{a},t)h_{t}(z)\tag{131}\label{c4:eq131} 
\end{equation*}

We call $M(S,\ub{a},t)$ the {\em measure of representation} of $t$ by
$S[\ub{x}+\ub{a}]$. \eqref{c4:eq131} is the analogue, for indefinite forms,
of the generating function \eqref{c4:eq6}.

Let us now consider the functional vector
\begin{equation*}
\varphi(z)=
\begin{pmatrix}
\varphi_{\ub{a}_{1}}(z)\\
\vdots\\
\varphi_{\ub{a}_{l}}(z)
\end{pmatrix}\tag{132}\label{c4:eq132}
\end{equation*}
$\ub{a}_{1},\ldots,\ub{a}_{l}$ having the same meaning as before. Let
$d=abs 2S$ and $\ob{\Gamma}$ the subgroup of units $U$ in $\Gamma$
satisfying 
$$
U\equiv E (\rm{mod} \; d).
$$
Since for every $\ub{a}_{i}$, $2S\ub{a}_{i}$ is an integral vector, it
follows that
$$
\Gamma\ub{a}_{i}\supset \ob{\Gamma},\quad (i=1,2,\ldots,1).
$$
Also $\Gamma/\ob{\Gamma}$ is a finite group. If $F_{0}$ is a
fundamental region for $\ob{\Gamma}$ in $\mathfrak{H}$ and $\ob{V}$
its volume then because of invariance of volume element we have 
$$
\varphi_{\ub{a}}(z)=\ob{V}^{-1}\int\limits_{F_{0}}f_{\ub{a}}(z,H)dv.
$$

Let now $\ub{\mu}(S,t)$ and $\ub{\gamma}$ denote the vectors 
$$
\ub{\mu}(S,t)=
\begin{pmatrix}
\mu(S,\ub{a}_{1},t)\\
\vdots\\
\mu(S,\ub{a}_{1},t)
\end{pmatrix},\quad 
\ub{\gamma}=
\begin{pmatrix}
\gamma_{\ub{a}_{l}}\\
\vdots\\
\gamma_{\ub{a}_{l}}
\end{pmatrix}
$$\pageoriginale
where $\mu(S,\ub{a},t)$ is defined by \eqref{c4:eq130} and
$\gamma_{\ub{a}}=0$ or $1$ according as $\ub{a}\equiv \ub{0}(\rm{mod} \; 1)$
or not. Then from \eqref{c4:eq49} and \eqref{c4:eq131} we have the 

\begin{thm}\label{chap4:thm9}
Let $n>4$ and $M=\left(\begin{smallmatrix} \alpha & \beta\\ \gamma &
  \delta
\end{smallmatrix}\right)$ be a modular matrix. Then
$$
\varphi(z)=\ub{\gamma}+\frac{\pi^{n/2}||S||^{-\frac{1}{2}}}{\Gamma(p/2)\Gamma(q/2)}\sum_{t}\ub{\mu}(S,t)h_{t}(z) 
$$
satisfies
$$
\ub{\varphi}(z_{M})=G(M,z)\ub{\varphi}(z).
$$
\end{thm}

The function $h_{t}(z)$ introduced in \eqref{c4:eq126} can be expressed in
terms of the confluent hypergeometric function $h(\alpha,\beta,\eta)$
defined by
$$
h(\alpha,\beta,\eta)=\int\limits^{\infty}_{0}
w^{\alpha-1}(w+1)^{\beta-1} e^{-w\eta})dw 
$$
where $\alpha$ and $\beta$ are complex numbers with positive real
parts and $\eta$ is a positive real parameter. $h(\alpha,\beta,\eta)$
is a solution of the second order differential equation
$$
\eta
\frac{d^{2}h}{d\eta^{2}}+(\alpha+\beta+\eta)\frac{dh}{d\eta}-\alpha
h =0.
$$

From the definition of $h_{t}(z)$ we have
$$
h_{0}(z)=\int\limits^{\infty}_{0}W^{n/2-2}e^{-4\pi w\eta}dw
$$
which reduces to the $\Gamma$-integral. We have hence 
\begin{equation*}
h_{0}(z)=(4\pi
\eta)^{1-n/2}\Gamma\left(\frac{n}{2}-1\right).\tag{134}\label{c4:eq134} 
\end{equation*}

Let now $t<0$. Changing, in \eqref{c4:eq126}, the variable $w$ to $-tw$ we
get easily 
\begin{equation*}
h_{t}(z)=e^{2\pi it\ob{z}}(-t)^{n/2-1}h(p/2,q/2,-4\pi t\eta)\tag{135}\label{c4:eq135}
\end{equation*}\pageoriginale

In case $t>0$, we make a change of variable $w\to wt+t$. One then
obtains
\begin{equation*}
h_{t}(z)=e^{2\pi it\ob{z}}t^{n/2-1}h(q/2,p/2,4\pi t\eta).\tag{136}\label{c4:eq136}
\end{equation*}

If we put $h_{t}(z)=u(\xi,\eta)=u$ as a function of the two real
variables $\xi$ and $\eta$, then $u$ satisfies the partial
differential equation
\begin{equation*}
\Delta u=0\tag{137}\label{c4:eq137}
\end{equation*}
where
\begin{equation*}
\Delta=\eta \left(\frac{\partial^{2}}{\partial\xi^{2}} +
\frac{\partial^{2}}{\partial\eta^{2}}\right) +
 \frac{n}{2}\frac{\partial}{\partial\eta} + 
i\frac{(q-p)}{2}\frac{\partial}{\partial\xi}\tag{138}\label{c4:eq138} 
\end{equation*}

The interesting fact to be noticed is that the differential operator
$\Delta$ is {\em independent} of $t$. Since $\ub{\varphi}(z)$ in
theorem \ref{chap4:thm9} is a linear function in $h_{t}(z)$ we see that
\begin{equation*}
\boxed{\Delta\ub{\varphi}(z)=\ub{0}}\tag{139}\label{c4:eq139}
\end{equation*}

It is to be noted also that $\ub{f}(z,H)$ is {\em not} a solution of
the differential equation \eqref{c4:eq137}.

\section{Eisenstein series}\label{chap4:sec8}

Let $M=\left(\begin{smallmatrix} \alpha & \beta\\ \gamma & \delta
\end{smallmatrix}\right)$ be a modular matrix. By \eqref{c4:eq30}, for any
two vectors $\ub{a}$, $\ub{b}$ among $\ub{a}_{1},\ldots,\ub{a}_{l}$ we
have
$$
\lambda_{\ub{a}\ub{b}}(M)=\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{\frac{2\pi
    i}{\gamma}(\alpha S[\ub{g}+\ub{a}]-2\ub{b}'S(\ub{g}+\ub{a})+\delta
  S[\ub{b}])} 
$$
Let us consider the case $\ub{b}=\ub{0}$ which is a possible value of
$a_{1},\ldots,a_{l}$. Then 
$$
\lambda_{\ub{a},\ub{0}}(M)=\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{\frac{2\pi
    i}{\gamma}\alpha S[\ub{g}+\ub{a}]} 
$$\pageoriginale
which is an ordinary Gaussian sum. It is to be noted that
$\lambda_{\ub{a},\ub{0}}(M)$ depends only on the first column of the
matrix $M$ and is independent of $\beta$ and $\delta$.

Let $G$ denote the group of proper unimodular matrices and $G_{0}$ the
subgroup consisting of all modular matrices with $\gamma=0$. Let
\begin{equation*}
G=\sum_{M}MG_{0} \tag{141}\label{c4:eq141}
\end{equation*}
be a decomposition of $G$ as a sum of left cosets modulo $G_{0}$. If
$M$ and $M_{1}$ belong to the same left coset, then
$$
M^{-1}_{1}M=
\begin{pmatrix}
\pm 1 & \ast\\
0 & \pm 1
\end{pmatrix}
$$
so that the values of $\lambda_{\ub{a},\ub{0}}(M)$ and
$\lambda_{\ub{a},\ub{0}}(M_{1})$ are equal. Also since $G_{0}$
contains the matrix $\left(\begin{smallmatrix} -1 & 0\\ 0 & -1
\end{smallmatrix}\right)$, we may choose the representatives in
\eqref{c4:eq141} so that $\gamma\geq 0$.

Let $\ub{g}(M,z)$ denote the first column of the matrix $G(M,z)$
defined in \eqref{c4:eq37}. Let $M$ have $\gamma>0$. Then because of
theorem \ref{chap4:thm2} we have
\begin{equation*}
\left.
\begin{aligned}
& \ub{g}(M, z_{M^{-1}})=\epsilon^{-1}d^{-\frac{1}{2}}\gamma^{-n/2}(\gamma
  z-\alpha)^{-p/2}(\gamma\ob{z}-\alpha)^{-q/2}
\begin{pmatrix}
\lambda_{\ub{a}_{1}} & \ub{0}\\
\vdots & \\
\lambda_{\ub{a}_{l}} & \ub{0}
\end{pmatrix}\\
&\hspace{3cm} \ub{g}(E,z)=\gamma
\end{aligned}
\right\}\tag{142}\label{c4:eq142}
\end{equation*}

We now form the series
$$
\ub{\psi}(z)=\sum_{M}\ub{g}(M,z_{M^{-1}})
$$
the sum taken over all representatives in \eqref{c4:eq141}. $\ub{\psi}(z)$
is a vector of\pageoriginale functions
$\psi_{\ub{a}_{1}}(z),\ldots,\psi_{\ub{a}_{l}}(z)$ where
\begin{gather*}
\psi_{\ub{a}}(z)=\gamma_{\ub{a}}+\epsilon^{-1}d^{-\frac{1}{2}}\sum_{\substack{(\alpha,\gamma)=1\\ \gamma>0}}\gamma^{-n}\left(z-\frac{\alpha}{\gamma}\right)^{-p/2}\left(\ob{z}-\frac{\alpha}{\gamma}\right)^{-q/2}\tag{143}\label{c4:eq143}\\
\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
  i\frac{\alpha}{\gamma}S[\ub{g}+\ub{a}]} 
\end{gather*}
In order to prove the absolute convergence of the above series for
$\psi_{\ub{a}}(z)$, observe that by \eqref{c4:eq47} and \eqref{c4:eq48},
$\lambda(M)$ is unitary and so it is enough to prove the convergence
of
$$
\sum_{(\alpha,\gamma)=1}|\gamma_{z}-\alpha|^{-\frac{n}{2}} 
$$
It is well-known that this converges for $n>4$. The convergence is
even uniform in every compact subdomain of the $z$-plane.

From theorem \ref{chap4:thm2} we have
$$
\ub{g}(MM_{1},z_{M_{1}^{-1}})=G(M,z)\ub{g}(M_{1},z_{M_{1}^{-1}})
$$
If $M$ is fixed and $M_{1}$ runs through a complete system of
representatives in \eqref{c4:eq141}, then $MM_{1}$ also runs through the
representatives in \eqref{c4:eq141}. This gives
\begin{equation*}
\ub{\psi}(z_{M})=G(M,z)\ub{\psi}(z).\tag{144}\label{c4:eq144}
\end{equation*}

Thus $\ub{\psi}(z)$ also satisfies the same transformation formula as
$\ub{\varphi}(z)$. We shall now obtain a fourier expansion for the
function $\psi_{\ub{a}}(Z)$. To this end we first prove

\begin{lem}\label{chap4:lem4}
Let $a>1$, $b>1$ {\em be two real numbers and}
$$
E(z)=\sum^{\infty}_{k=-\infty}(z-k)^{-a}(\ob{z}-k)^{-b}
$$
Then 
$$
E(z)=\frac{i^{b-a}(2\pi)^{a+b}}{\Gamma(a)\Gamma(b)}\sum^{\infty}_{-\infty}
e^{2\pi il\ob{z}} \int\limits^{\infty}_{\max(0,1)}u^{a-1}(u-1)^{b-1}e^{-4\pi
  \eta u}du.
$$\pageoriginale
where $z=\xi+i\eta$, $\eta>0$.
\end{lem}

\begin{proof}
Since $a+b>2$, it follows that $E(z)$ is absolutely convergent and is
also uniformly convergent in every bounded domain of the $z$-plane. It
is clearly periodic of period $1$ in $\xi$. Hence
$$
E(z)=\sum^{\infty}_{l=-\infty}e^{2\pi i
  l\xi}\left(\int\limits^{1}_{0}\sum^{\infty}_{-\infty}(z-k)^{-a}(\ob{z}-k)^{-b}e^{-2\pi
  il\xi}d\xi\right). 
$$

This shows that the fourier coefficient equals
$$
\int\limits^{\infty}_{-\infty}z^{-a}\ob{z}^{-b}e^{-2\pi il\xi}d\xi.
$$
By means of the substitution $\xi\to -i\xi$ we get for this fourier
coefficient the integral
$$
i^{b-a-1}\int\limits^{i\infty}_{-i\infty}(\eta-\xi)^{-a}(\eta+\xi)^{-b}e^{-2\pi
  l \xi}d\xi. 
$$
We now write $\xi$ instead of $\xi+\eta$ obtaining thus the integral
$$
i^{b-a-1}e^{2\pi
  i\eta}\int\limits^{\eta+i\infty}_{\eta - i\infty}\xi^{-b}(2\eta-\xi)^{-a}e^{-2\pi
  l\xi}d\xi. 
$$

In order to evaluate the integral above we use the $\Gamma$-integral
and obtain 
\begin{equation*}
\frac{1}{\Gamma(a)}\int\limits^{\eta+i\infty}_{\eta-i\infty}e^{-2\pi
  l\xi}\xi^{-b}\left(\int\limits^{\infty}_{0}u^{a-1}e^{-(2\eta-\xi)u}du\right)d\xi
\tag{145}\label{c4:eq145} 
\end{equation*}

We can change the order of integration and hence the above integral
equals 
$$ 
\frac{1}{\Gamma(a)}\int\limits^{\infty}_{0}u^{a-1}
e^{-2\eta u} \left(\int\limits^{\eta+i\infty}_{\eta-i\infty}\xi^{-b}e^{\xi(u-2\pi
  l)}d\xi\right)du
$$\pageoriginale
We now use the well-known Weierstrass' formula in $\Gamma$-functions,\break
namely
$$
\frac{1}{2\pi i}\int\limits^{c+i\infty}_{c-i\infty}x^{-b}e^{\lambda
  x}dx=
\begin{cases}
\frac{\lambda^{b-1}}{\Gamma(b)} & \text{if } \lambda>0\\
0 & \text{if } \lambda\leq 0
\end{cases}
$$
where $c>0$, $b>0$.
\end{proof}

From this formula, it follows that the integral in \eqref{c4:eq145} equals
$$
\frac{2\pi
  i}{\Gamma(a)\Gamma(b)}\int\limits_{u>\max(0,2\pi,l)} u^{a-1}(u-2\pi
l)^{b-1}e^{-2\eta u}du.
$$
We once again make a change of variable $u$ to $2\pi u$. We then
obtain the fourier coefficient as given in the lemma.

Actually the lemma can be seen to be true for $a>0$, $b>0$ and
$a+b \succ 1$. In particular, if we put $a=p/2$ and $b=q/2$ and use the
definition of $h_{t}(z)$ in \eqref{c4:eq126} and $\epsilon$ in \eqref{c4:eq29},
we obtain the formula 
\begin{equation*}
\sum^{\infty}_{k=-\infty}(z-k)^{-p/2}
(\ob{z}-k)^{-q/2}=\frac{(2\pi)^{n/2} \varepsilon}{\Gamma(p/2)\Gamma(q/2)}
\sum^{\infty}_{1=-\infty}h_{l}(z)\tag{146}\label{c4:eq146}
\end{equation*}
Let us now consider the expression for $\psi_{\ub{a}}(z)$, namely
{\fontsize{10}{12}\selectfont
$$
\psi_{\ub{a}}(z)=\gamma_{\ub{a}}+\epsilon^{-1}d^{-\frac{1}{2}}\sum_{\substack{(\alpha,\gamma)=1\\ \gamma>0}}\gamma^{-m}\left(z-\frac{\alpha}{\gamma}\right)^{-p/2}\left(\ob{z}-\frac{\alpha}{\gamma}\right)^{-q/2}\cdot
\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi i\frac{\alpha}{\gamma}S[\ub{g}+\ub{a}]}
$$}
Put $D=2d$. We shall prove that $\psi_{\ub{a}}(z)$ has the period $D$
in $\xi$. 
\begin{gather*}
\psi_{\ub{a}}(z+D)=\gamma_{\ub{a}}+\epsilon^{-1}d^{-\frac{1}{2}}\sum_{(\alpha,\gamma)=1}\gamma^{-n}\left(z+D-\frac{\alpha}{\gamma}\right)^{-p/2}\left(\ob{z}+D-\frac{\alpha}{\gamma}\right)^{-q/2}\\
\gamma>0\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi i\frac{\alpha}{\gamma}S[\ub{g}+\ub{a}]}
\end{gather*}\pageoriginale
But since $2S\ub{a}$ is integral, it follows that $DS[\ub{a}]$ is an
integer. Hence
$$
DS[\ub{g}+\ub{a}]\equiv 0(\rm{mod} \; 1)
$$
We may therefore write
\begin{gather*}
\psi_{\ub{a}}(z+D)=\gamma_{\ub{a}} + \epsilon^{-1}d^{-\frac{1}{2}}
\sum_{(\alpha,\gamma)=1}\gamma^{-n}
\left(z + D-\frac{\alpha}{\gamma}\right)^{-p/2}
\left(\ob{z}+D-\frac{\alpha}{\gamma}\right)^{-q/2}\\
\gamma>0\cdot
\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
  i}\left(\frac{\alpha}{\gamma}-D\right)S[\ub{g}+\ub{a}] 
\end{gather*}
Since $\dfrac{\alpha}{\gamma}+D$ runs through all rational fractions
when $\dfrac{\alpha}{\gamma}$ does so, we see that
$$
\psi_{\ub{a}}(z+D)=\psi_{\ub{a}}(z).
$$

Because of absolute convergence we can write the series for
$\psi_{\ub{a}}(z)$ in the following way: We put all rational numbers
$\dfrac{\alpha}{\gamma}$ into residue classes modulo $D$. If $0\leq
\dfrac{\alpha}{\gamma}<D$ is a fixed rational number with
$(\alpha,\gamma)=1$, then all rational numbers in the class of
$\dfrac{\alpha}{\gamma}$ are obtained in the form
$\dfrac{\alpha}{\gamma}\pm kD$ where $k$ runs through integers. Thus 
\begin{gather*}
\psi_{\ub{a}}(z)=\gamma_{\ub{a}}+\epsilon^{-1}d^{-\frac{1}{2}}\sum_{0\leq
  \frac{\alpha}{\gamma}<D}\gamma^{-n}D^{-\frac{n}{2}}\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
  i\frac{\alpha}{\gamma}S[\ub{g}+\ub{a}]}\\
\sum^{\infty}_{k=-\infty}(\zeta-k)^{-\frac{p}{2}}(\ob{\zeta}-k)^{-\frac{q}{2}}\tag{147}
\label{c4:eq147} 
\end{gather*}
where $\zeta=\left(z-\dfrac{\alpha}{\gamma}\right)D^{-1}$. Using
\eqref{c4:eq146} we get 
\begin{equation*}
\begin{aligned}
\psi_{\ub{a}}(z) &=
\gamma_{\ub{a}}+\frac{d^{-\frac{1}{2}}(2\pi)^{n/2}}{\Gamma(p/2)\Gamma(q/2)}\sum_{0\leq
  \frac{\alpha}{\gamma}<D}\gamma^{-n}D^{-n/2}\\
&\quad \sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
  i\frac{\alpha}{\gamma}S[\ub{g}+\ub{a}]}
\sum^{\infty}_{l=-\infty}h_{l}\left(D^{-1}
\left(z-\frac{\alpha}{\gamma}\right)\right) 
\end{aligned}\tag{148}\label{c4:eq148}
\end{equation*}\pageoriginale

Using \eqref{c4:eq126} we have
$$
h_{l}\left(D^{-1}\left(z-\frac{\alpha}{\gamma}\right)\right)=D^{n/2-1}e^{-2\pi
  i\frac{\alpha}{\gamma}\frac{l}{D}}h_{l/D}(z). 
$$
We may therefore write \eqref{c4:eq148} in the form
\begin{align*}
\psi_{\ub{a}}(z) &=
\gamma_{\ub{a}}+\frac{\pi^{n/2}||S||^{-\frac{1}{2}}}{\Gamma(p/2)\Gamma(q/2)}
\sum^{\infty}_{l=-\infty}h_{1/D}(z)\sum_{0\leq
  \frac{\alpha}{\gamma}<D}D^{-1}\gamma^{-n}\\
\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
  i\left(S[\ub{g}+\ub{a}]-\frac{l}{D}\right)\frac{\alpha}{\gamma}}
\end{align*}

We now contend that the inner sum is zero if
$$
S[\ub{a}]-\frac{l}{D}\not\equiv 0(\rm{mod} \; 1)
$$
For, from \eqref{c4:eq147}, it is obvious that instead of the summation
over $0\leq \dfrac{\alpha}{\gamma}<D$, we could equally well have the
summation range as $1\leq \dfrac{\alpha}{\gamma}<D+1$. This means that
the expression
\begin{equation*}
\sum_{0\leq
  \frac{\alpha}{\gamma}<D}D^{-1}\gamma^{-n}\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
  i\frac{\alpha}{\gamma}\left(S[\ub{g}+\ub{a}]-\frac{l}{D}\right)}
\tag{149}\label{c4:eq149} 
\end{equation*}
is unaltered by changing $\dfrac{\alpha}{\gamma}$ into
$\dfrac{\alpha}{\gamma}+1$. But this change multiplies \eqref{c4:eq149} by
$$
e^{2\pi i\left(S[\ub{a}]-\frac{l}{D}\right)} 
$$
This proves our contention. We can therefore write
\begin{gather*}
\psi_{\ub{a}}(z) =
\gamma_{\ub{a}}+\dfrac{\pi^{n/2}||S||^{-\frac{1}{2}}}{\Gamma(p/2)\Gamma(q/2)}\sum_{t\equiv
  S[\ub{a}](\rm{mod} \; 1)}h_{t}(z)\\ 
\sum_{0\leq
  \frac{\alpha}{\gamma}<D}D^{-1}\gamma^{-n}\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
  i\frac{\alpha}{\gamma}(S[\ub{g}+\ub{a}]-t)}
\end{gather*}

We\pageoriginale can now write all numbers $0\leq
\dfrac{\alpha}{\gamma}<D$ in the form $\dfrac{\alpha}{\gamma}+r$ where
$0\leq \dfrac{\alpha}{\gamma}<1$ and $r=0,1,2,\ldots,D-1$. Because of
the property of the expression \eqref{c4:eq149} we get finally the

\begin{thm}\label{chap4:thm10}
The function $\psi_{\ub{a}}(z)$ has the expansion
\begin{gather*}
\psi_{\ub{a}}(z)=\gamma_{\ub{a}}+\frac{\pi^{n/2}||S||^{-\frac{1}{2}}}{\Gamma(p/2)\Gamma(q/2)}\sum_{t\equiv
  S[\ub{a}](\rm{mod} \; 1)}h_{t}(z)\\
\sum_{0\leq
  \frac{\alpha}{\gamma}<1}\gamma^{-n}\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
  i\frac{\alpha}{\gamma}(S[\ub{g}+\ub{a}]-t)} 
\end{gather*}
\end{thm}

The expression
\begin{equation*}
\beta_{t}=\sum_{0\leq
  \frac{\alpha}{\gamma}<1}\gamma^{-n}\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi i\frac{\alpha}{\gamma}(S[\ub{g}+\ub{a}]-t)}\tag{150}\label{c4:eq150}
\end{equation*}
is a so-called `singular series'. Series of this type were studied by
Hardy and littlewood in their researches on Waring's problem. We shall
now give some properties of this singular series. 


Let $q>0$ be an integer. Put
$$
f_{q}=\sum_{\gamma|q}\left(\sum_{0\leq
  \frac{\alpha}{\gamma}<1}\gamma^{-n}\sum_{\ub{g}(\rm(mod) \; \gamma)}e^{2\pi
  i\frac{\alpha}{\gamma}(S[\ub{g}+\ub{a}]-t)}\right)
$$
Since $q=\gamma s$ where $s$ is an integer, we may take the inner
summation over a complete residue system modulo $q$. Then each of the
terms will be repeated $s^{n}$ times. This gives
$$
f_{q}=q^{-n}\sum^{q-1}_{\lambda=0} \; \sum_{\ub{g} (\rm{mod} \; q)}e^{\frac{2\pi
    i\lambda}{q}(S[\ub{g}+a]-t)} 
$$
Interchanging the two summations above we have
\begin{equation*}
f_{q}=q^{-n}\sum_{\ub{g} (\rm{mod} \; q)}\left(\sum^{q-1}_{\lambda=0}
e^{\frac{2\pi i\lambda}{q}(S[\ub{g}+\ub{a}]-t)}\right)\tag{151}\label{c4:eq151}
\end{equation*} 

Because\pageoriginale of the well-known formula
$$
\sum^{q-1}_{\mu=0}e^{\frac{2\pi i\lambda_{\mu}}{q}}=
\begin{cases}
0 & \text{if } q \nmid \lambda\\
q & \text{if } q|\lambda
\end{cases}
$$
We see that the inner sum in \eqref{c4:eq151} vanishes if the congruence
\begin{equation*}
S[\ub{x}+\ub{a}]\equiv t (\rm{mod} \; q)\tag{152}\label{c4:eq152}
\end{equation*}
has no solution. If it has a solution $\ub{g}$, then the inner sum has
the value $q$. Thus
\begin{equation*}
f_{q}=\frac{A_{0}(S,\ub{a},t)}{q^{n-1}}\tag{153}\label{c4:eq153}
\end{equation*}
where $A_{q}(S,\ub{a},t)$ is the number of incongruent solutions
$\rm{mod} \;
q$ of the congruence \eqref{c4:eq152}. It will then follow from the
definition of $\beta_{t}$ that if $q\to \infty$ through a sequence of
integers $q_{1}$, $q_{2}$, $q_{3},\ldots$ such that every natural
number divides all but a finite number of these $q$'s,
\begin{equation*}
\beta_{t}=\lim\limits_{q\to   \infty}f_{q}
 =\lim\limits_{q\to\infty}\frac{A_{q}(S,\ub{a},t)}{q^{n-1}}\tag{154}\label{c4:eq154}  
\end{equation*}

From the definition of $A_{q}(S,\ub{a},t)$ and the Chinese-remainder
theorem, it follows that
$$
A_{q}(S,\ub{a},t)\cdot A_{q'}(S,\ub{a},t)=A_{qq'}(S,\ub{a},t)
$$
for two coprime integers $q$, $q'$. This shows that
$f_{q}=\dfrac{A(S,\ub{a},t)}{q^{n-1}}$ is a multiplicative arithmetic
function. In order to compute $f_{q}$ for a given $q$, it is enough to
compute $f_{q}$ for $q=p^{l}$ where $p$ is a prime number and $l>0$ is
an integer.

If $q=p^{l}$, $l>0$ and $p$ a prime number, it can be shown that
$$
\delta_{p}(S,\ub{a},t)=\lim\limits_{l \to
  \infty}\frac{A_{q}(S,\ub{a},t)}{q^{n-1}}
$$\pageoriginale
exists. In fact, if $l$ is sufficiently large the value of
$\dfrac{A_{q}(S,\ub{a},t)}{q^{n-1}}$ is independent of $l$. This shows
that $\delta_{p}(S,\ub{a},t)$ is really a rational number. Furthermore
for all except a finite number of primes (for instance, for $p\nmid
2d$)
$$
\delta_{p}(S,\ub{a},t)=\frac{A_{p}(S,\ub{a},t)}{p^{n-1}}.
$$
this enables us to compute $\delta_{p}(S,\ub{a},t)$ for almost all
primes $p$. From the fact that $\delta_{p}(S,\ub{a},t)$ exists for
every $p$ one can construct the product
$$
\delta(S,\ub{a},t)=\prod_{p}\delta_{p}(S,\ub{a},t).
$$
It is proved in the analytic theory of quadratic forms that the
product above converges and is different from zero only if every
factor is different from zero. Moreover
\begin{equation*}
\beta_{t}=\delta(S,\ub{a},t)\tag{155}\label{c4:eq155}
\end{equation*}
This gives an arithmetical meaning for $\beta_{t}$ namely that
$\beta_{t}>0$ if and only if $A_{q}(S,\ub{a},t)\neq 0$ for {\em every
  integer} $q>1$. For a proof of these statements see \cite{c4:key2}.

It should be noticed that since $\psi_{\ub{a}}(z)$ is a linear
function in $h_{t}(z)$ and as $h_{t}(z)$ is a solution of the equation
\eqref{c4:eq137}, the function $\psi_{\ub{a}}(z)$ and hence the vector
$\ub{\psi}(z)$ defined in \eqref{c4:eq143} is a solution of the
differential equation
\begin{equation*}
\Delta\ub{\psi}(z)=\ub{0}.\tag{156}\label{c4:eq156}
\end{equation*}

The\pageoriginale series $\psi_{\ub{a}}(z)$ are called the Eisenstein
series. The vector $\ub{\psi}(z)$ of Eisenstein series and the vector
$\ub{\varphi}(z)$ satisfy the same differential equation and have the
same transformation formula with regard to modular substitutions.

\section{Main Theorem}\label{chap4:sec9}

We shall now prove the main theorem of the analytic theory of
indefinite quadratic forms, namely,

\begin{thm}\label{chap4:thm11}
If $n>4$ and $S$ is a rational symmetric matrix which is
semi-integral, of signature $p$, $q$, $p+q=n$, $pq>0$ and $\ub{a}$ a
rational vector with $2S\ub{a}$ integral, then for $t\equiv
S[\ub{a}](\rm{mod} \; 1) $
$$
M(S,\ub{a},t)=\mu_{\ub{a}}(S)\prod_{p}\delta_{p}(S,\ub{a},t)
$$
the product running over all primes $p$.
\end{thm}

\begin{proof}
The series
$$
\varphi_{\ub{a}}(z)=\gamma_{\ub{a}}+\frac{\pi^{\frac{n}{2}}||S||^{-1/2}}{\Gamma(p/2)\Gamma(q/2)}\sum_{t}\frac{M(S,\ub{a},t)}{\mu_{\ub{a}}(S)}h_{t}(z) 
$$
and 
$$
\psi_{\ub{a}}(z)=\gamma_{\ub{a}}+\frac{\pi^{n/2}||S||^{-\frac{1}{2}}}{\Gamma(p/2)\Gamma(q/2)}\sum_{t}\beta_{t}h_{t}(z)
$$
are fourier series in the real part $\xi$ of $z$. In order to prove
theorem \ref{chap4:thm11}, it is enough to prove that
\begin{equation*}
\varphi_{\ub{a}}(z)-\psi_{\ub{a}}(z)=0.\tag{157}\label{c4:eq157}
\end{equation*}
Then from \eqref{c4:eq155} and the uniqueness theorem of fourier series, it
would follow that the coefficients of
$\varphi_{\ub{a}}(z)-\psi_{\ub{a}}(z)$ are zero and\pageoriginale the
theorem is proved.
\end{proof}

We shall therefore prove \eqref{c4:eq157}.

Let $\ub{\chi}(z)$ be the vector
$$
\ub{\chi}(z)=
\begin{pmatrix}
\chi_{\ub{a}_{1}} (z)\\
\vdots\\
\chi_{a_{l}}(z)
\end{pmatrix}
$$
where 
$$
\chi_{\ub{a}}(z)=\varphi_{\ub{a}}(z)-\psi_{\ub{a}}(z).
$$
If we put $\alpha_{t}=\dfrac{M(S,\ub{a},t)}{\mu_{\ub{a}}(S)}$, then
\begin{equation*}
\chi_{\ub{a}}(z)=\frac{\pi^{n/2}||S||^{-\frac{1}{2}}}{\Gamma(p/2)\Gamma(q/2)}\sum_{t}\quad
(\alpha_{t}-\beta_{t})h_{t}(z)\tag{158}\label{c4:eq158} 
\end{equation*}
It is to be noticed that $\chi_{\ub{a}}(z)$ lacks the constant
term. From theorem \ref{chap4:thm9} and \eqref{c4:eq144} we have
\begin{equation*}
\ub{\chi}(z_{M})=G(M,z)\ub{\chi}(z).\tag{159}\label{c4:eq159}
\end{equation*}
The Unitary matrix $\Lambda(M)$ is defined in \S\ \ref{chap4:sec3} by 
$$
G(M,z)=
\begin{cases}
(\gamma z+\delta)^{p/2}(\gamma\ob{z}+\delta)^{q/2}\Lambda(M) &\text{if
  } \gamma\neq 0\\
\Lambda(M) & \text{if } \gamma=0.
\end{cases}
$$
If we put $z_{M}=\xi_{M}+i\eta_{M}$, then
\begin{equation*}
\eta_{M}=\frac{\eta}{|\gamma z+\delta|^{2}}\tag{160}\label{c4:eq160}
\end{equation*}

Let us now prove some properties of the function $h_{t}(z)$ introduced
in \eqref{c4:eq126}. In the first place
\begin{equation*}
h_{t}(z)\sim e^{2\pi i t\xi}(4\pi \eta)^{1-n/2}\Gamma(n/2-1)\text{
  \ for \ } \eta\to 0.\tag{161}\label{c4:eq161}
\end{equation*}
This can be proved easily: For, if $t=0$, then
$$
h_{0}(z)=(4\pi\eta)^{1-n/2}\Gamma(n/2-1) 
$$
as\pageoriginale was seen in \eqref{c4:eq134}. Let now $t>0$. Let us make
the substitution $w\to \dfrac{w}{4\pi \eta}$ in the integral for
$h_{t}(z)$. Then
$$
e^{-2\pi it\xi}h_{t}(z)=\int\limits_{w>4\pi\eta
  t}(4\pi\eta)^{1-n/2}w^{p/2-1}(w-4\pi\eta t)^{q/2-1}e^{-w}dw
$$
But when $\eta\to 0$
$$
\int_{w>4\pi\eta t}w^{p/2-1}(w-4\pi \eta
t)^{q/2-1}e^{-w}dw\sim\Gamma(n/2-1). 
$$
This proves \eqref{c4:eq161}. The case $t<0$ is dealt with in a similar
fashion.

In case $\eta\to \infty$ we have
\begin{equation*}
\left.
\begin{array}{l@{\qquad}c@{\quad}l}
h_{t}(z)\to 0 & \text{if} & t\neq 0\\
h_{t}(z)\eta^{n/2-1}\to 0 & \text{if} & t=0
\end{array}
\right\}\tag{162}\label{c4:eq162}
\end{equation*}
This is easily seen from the expression for $h_{t}(z)$ and
\eqref{c4:eq134}. In fact, if $t\neq 0$, $h_{t}(z)\to 0$ exponentially as
$\eta\to \infty$.

Let us now consider the equation \eqref{c4:eq158} for
$\chi_{\ub{a}}(z)$. The function $h_{t}(z)$ is monotonic and
decreasing in $\eta$ for fixed $\xi$. This means that the series for
$\chi_{\ub{a}}(z)$ is uniformly bounded in the whole of the
fundamental region of the modular group in the $z$ plane. Let
$\omega_{\ub{a}}(z)=\eta^{n/4}\chi_{\ub{a}}(z)$. Since $n > 4$ and
\eqref{c4:eq162} holds with $h_{t}(z)\to 0$ exponentially as $\eta\to
\infty$, $t\neq 0$, it follows that $\omega_{\ub{a}}(z)$ is bounded,
uniformly in $\xi$, in the fundamental region of the modular group in
the upper half $z$ plane.

Let $\omega(z)$ be the vector 
$$
\ub{\omega}(z)=
\begin{pmatrix}
\omega_{\ub{a}_{1}}(z)\\
\vdots\\
\omega_{\ub{a}_{l}}(z)
\end{pmatrix}
=\eta^{n/4}\ub{\chi}(z)
$$\pageoriginale
Then because of \eqref{c4:eq160} and the transformation formula \eqref{c4:eq159}
for $\ub{\chi}(z)$ it follows that, if $M$ is a modular matrix,
\begin{equation*}
|\omega_{\ub{a}_{i}}(z_{M})\leq
\sum_{j}|\theta_{ij}||\omega_{\ub{a}_{j}}(z)|\quad
(i=1,\ldots,l)\tag{163}\label{c4:eq163} 
\end{equation*}
where $\Lambda(M)=(\theta_{ij})$. But $\Lambda(M)$ is a unitary matrix
so that $|\theta_{ij}|\leq 1$. This means that
$$
|\omega_{\ub{a}_{i}}(z_{M})|\leq \sum_{j}|\omega_{\ub{a}_{j}}(z)|
$$
From what we have seen above, it follows that $\omega_{\ub{a}}(z)$ is
bounded in the whole of the upper half $z$-plane.

Now $\varphi_{\ub{a}}(z)$ and $\psi_{\ub{a}}(z)$ are fourier series in
the real variable $\xi$ and have the period $2d=D$. The fourier
coefficient of $\chi_{\ub{a}}(z)=\varphi_{\ub{a}}(z)-\psi_{\ub{a}}(z)$
is
\begin{equation*}
\frac{1}{D}\int\limits^{D}_{0}\chi_{\ub{a}}(z)e^{-2\pi
  it\xi}d\xi\tag{164}\label{c4:eq164} 
\end{equation*}
which clearly equals
\begin{equation*}
\frac{\pi^{n/2}||S||^{-\frac{1}{2}}}{\Gamma(p/2)\Gamma(q/2)}(\alpha_{t}-\beta_{t})h_{t}(i\eta)\tag{165}\label{c4:eq165}
\end{equation*}

Since $n>4$ and $\eta^{n/4}\chi_{\ub{a}}(z)$ is bounded in the upper
half $z$ plane, it follows that
\begin{equation*}
\frac{1}{D}\eta^{n/4-1}\int\limits^{D}_{0}\eta^{n/4}\chi_{\ub{a}}(z)e^{-2\pi
  it\xi}d\xi\to 0\tag{166}\label{c4:eq166}
\end{equation*} 
as\pageoriginale $\eta\to 0$. On the other hand the expression on the
left of \eqref{c4:eq166} is, in virtue of \eqref{c4:eq164}, \eqref{c4:eq165}, equal
to
$$
\frac{\pi^{n/2}||S||^{-\frac{1}{2}}}{\Gamma(p/2)\Gamma(q/2)}(\alpha_{t}-\beta_{t})h_{t}(i\eta)\eta^{\frac{n}{2}-1} 
$$
Because of \eqref{c4:eq161}
$$
h_{t}(i\eta)\eta^{n/2-1}\sim (4\pi)^{1-n/2}\Gamma(n/2-1)\neq 0
$$
as $\eta\to 0$. Because of \eqref{c4:eq166} therefore, it follows that 
$$
\alpha_{t}-\beta_{t}=0.
$$
Our theorem is thus completely proved.

Going back to the definitions of $\varphi_{\ub{a}}(z)$ in \eqref{c4:eq123}
and $\psi_{\ub{a}}(z)$ in \eqref{c4:eq143} we have the partial fraction
decomposition 

\begin{thm}\label{chap4:thm12}
If $n>4$, then
\begin{gather*}
V^{-1}_{\ub{a}}\int\limits_{F_{\ub{a}}}f_{\ub{a}}(z,H)dv=\\
=\gamma_{\ub{a}}+\epsilon^{-1}d^{-\frac{1}{2}}\sum_{\substack{(\alpha,\gamma)=1\\ \gamma>0}}\gamma^{-n}\left(\sum_{\ub{g}(\rm(mod)
  \; \gamma)}e^{2\pi
  i\frac{\alpha}{\gamma}S[\ub{g}+\ub{a}]}\right)\left(\ub{z}-\frac{\alpha}{\gamma}\right)^{-\frac{p}{2}}\left(\ob{z}-\frac{\alpha}{\gamma}\right)^{-\frac{q}{2}} 
\end{gather*}
\end{thm}

\section{Remarks}\label{chap4:sec10}

Let us consider the main theorem. The right hand side is a product
extended over all the primes and is zero if and only if at least one
factor is zero. The left hand side is different from zero only if the
equation
\begin{equation*}
S[\ub{x}+\ub{a}]=t\tag{167}\label{c4:eq167}
\end{equation*}
has an integral solution. Thus the main theorem shows that \eqref{c4:eq167}
has an integral solution if and only if 
$$
S[\ub{x}+\ub{a}]\equiv t (\rm{mod} \; m)
$$\pageoriginale
has a solution for every integer $m \succ 1$. Because of the definition of
$\delta_{p}(S,\ub{a},t)$ we may also say that if $S$ is indefinite and
$n>4$, then \eqref{c4:eq167} has an integral solution if and only if
\eqref{c4:eq167} is true in $p$-adic integers for every $p$. In the case
$t=0$, this is the Meyer-Hasse theorem. But our main theorem is a
quantitative improvement of the Meyer-Hasse theorem, in as much as it
gives an expression for the measure of representation of $t$ by
$S[\ub{x}+\ub{a}]$. 

The method of proof consisted in first obtaining a `generating
function' $f(z)$ for the Diophantine problem \eqref{c4:eq167} and then
constructing a function $E(z)$, the Eisenstein series, which behaves
like $f(z)$ for all modular substitutions. In other words, we
construct a function $E(z)$ which behaves like $f(z)$ when $z$
approaches, in the upper half plane, a rational point on the real
axis. This idea was originally used by Hardy and Ramanujan in the
problem of representation of integers by sums of $k$ squares. The
generating function $f(z)$ here was the theta series
$$
f(z)=\left(\sum^{\infty}_{1=-\infty}e^{2\pi il^{2}z}\right)^{k}
$$
The function $E(z)$ is constructed in the same way as here and Hardy
and Ramanujan showed that for $k=5$, $6$, $7$, $8$
$$
f(z)=E(z).
$$
But for $k=9$, $f(z)\neq E(z)$. It is remarkable that in the case of
indefinite forms, one has equality if $k \succ 4$. One does not
have,\pageoriginale in general, for representation of integers by
definite forms, a formula like that in theorem \ref{chap4:thm11}. One can
obtain a modified formula by introducing a genus of forms. If $S>0$ is
an integral matrix, the genus of $S$ consists of all integral matrices
$P \succ 0$ which are such that for each integer $m>1$ there is an integral
matrix $U$ with 
$$
S[U]\equiv P (\rm{mod} \; m),\quad (|U|,m)=1.
$$
It is then known that a genus consists of a finite number of
classes. Let $S_{1},\ldots,S_{a}$ be representatives of the finitely
many classes in the genus of $S$. If $T>0$ is any $k$-rowed integral
matrix, we can define for each $i$, $i=1,\ldots,a$ the number,
$A(S_{i},T)$, of representations
$$
S_{i}[X]=T.
$$
If $E(S_{i})$ denotes the order of the unit group of $S_{i}$ (this
being finite since $S_{i}>0$) we can form
$$
\ob{A}(S,T)=
\frac{\sum\limits^{a}_{i=1}\dfrac{A(S_{i},T)}{E(S_{i})}}{\sum\limits^{a}_{i=1}\dfrac{1}{E(S_{i})}} 
$$
the average measure of representation of $T$ by a genus of $S$. Just
as in \eqref{c4:eq154} we can define for each $p$,
$$
\delta_{p}(S,T)=\lim\limits_{1\to
  \omega}\frac{A_{p^{l}}(S,T)}{p^{l\lambda}}
$$
where $\lambda=nk-\dfrac{k(k+1)}{2}$. This is finite, rational and
$\prod\limits_{p}\delta_{p}(S,T)$ converges if $k\leq n$. The main
theorem would then be 
\begin{equation*}
\ob{A}(S,T)=c\prod_{c}\delta_{p}(S,T)\tag{168}\label{c4:eq168}
\end{equation*}\pageoriginale
$c$ being a constant depending on $n$ and $k$.

A similar formula, with suitable restrictions, exists if $S$ and $T$
are indefinite also.

One might ask if our theorem \ref{chap4:thm12} could be extended to the
cases $n=2$, $3$, $4$. In case $n=4$, and $S[\ub{x}]$ is not a
quaternion zero form, then one can prove that $f(z)=E(z)$. The method
is slightly more complicated. The differential operator $\Delta$, or
slight variants of it, which we had not used in our main theorem,
plays an important role here. In case $n=2$ and $3$ it can be proved
that our main theorem is false.

Generalizations of the main theorem may be made by considering
representations not of numbers, but of rational symmetric
matrices. One can generalize the results by considering instead of the
domain of rational integers, the ring of integers in an algebraic
number field or more generally an order in an involutorial simple
algebra over the rational number field. The bibliography gives the
sources for these generalizations.


\begin{thebibliography}{99}
\bibitem{c4:key1} C.\@ L.\@ Siegel :\pageoriginale  Additive theorie
  der-Zahlk\"orper II {\em 
  Math.\@ Ann}. 88 (1923) P.\@ 184-210.

\bibitem{c4:key2} C.\@ L.\@ Siegel :  \"Uber die analytische theorie der
quadratischen Formen, {\em Annals of Math}. 36 (1935) P.\@ 527-606; 37
(1936) P.\@ 230-263; 38 (1937) P.\@ 212-291.

\bibitem{c4:key3} C.\@ L.\@ Siegel :  \"Uber die zeta funktionen indefiniter
quadratischen Formen, {\em Math.\@ Zeir} 43 (1938) P.\@ 682-708; 44
(1939) P.\@ 398-426.

\bibitem{c4:key4} C.\@ L.\@ Siegel :  Einheiten quadratischer Formen {\em
  Abhandlungen.\@ Math.\@ Sem.\@ Hansischen.\@ Univ}. 13 (1940) P.\@
209-239.

\bibitem{c4:key5} C.\@ L.\@ Siegel : Equivalence of quadratic forms Amer.\@ Jour.\@
of Math.\@ 63 (1941) P.\@ 658-680.

\bibitem{c4:key6} C.\@ L.\@ Siegel : On the theory of indefinite quadratic forms,
{\em Annals of Math}. 45 (1944) P.\@ 577-622.

\bibitem{c4:key7} C.\@ L.\@ Siegel : Indefinite quadratische Formen and
Modulfunktionen, {\em Courant.\@ Anniv.\@ Volume} (1948) P.\@
395-406.

\bibitem{c4:key8} C.\@ L.\@ Siegel : Indefinite quadratische Formen and
Funktionentheorie, {\em Math.\@ Annalen}, 124 (1951) I, P.\@ 17-54; II
ibid P.\@ 364-387.

\bibitem{c4:key9} C.\@ L.\@ Siegel :\pageoriginale A generalization of
  the Epstein zeta-function, 
{\em Jour.\@ Ind.\@ Math.\@ Soc.\@} Vol.\@ 20 (1956) P.\@ 1-10.

\bibitem{c4:key10} C.\@ L.\@ Siegel : Die Funktionalgleichungen einiger
Dirichletscher Reihen, {\em Math.\@ Zeit} 63 (1956) P.\@
363-373.

\bibitem{c4:key11} H.\@ Weyl : Fundamental domains for lattice groups
  in division algebras {\em Festschrift to A.\@ Speiser}, 1945.
\end{thebibliography}
