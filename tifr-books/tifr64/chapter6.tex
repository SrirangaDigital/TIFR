\chapter{Markov Properties of Brownian Motion}\label{chap6}

\begin{notation*}
\begin{enumerate}
\item A\pageoriginale random variable of a stochastic process
  $\{X(t)\}_{t\in I}$ shall be denoted by $X_{t}$ or $X(t)$. $0\leq
  t<\infty$. 

\item $\mathscr{F}_{s}$ will denote the $\sigma$-algebra generated by
  $\{X_{t}:0\leq t\leq s\}$;
  $\mathscr{F}_{s+}=\{\mathscr{F}_{a}:a>s\}$; $\mathscr{F}_{s-}$ will
  be the $\sigma$-algebra generated by
  $\cup\{\mathscr{F}_{a}:a<s\}s>0$. It is clear that
  $\{\mathscr{F}_{t}\}$ is an increasing family.

\item For the Brownian motion, $\mathscr{B}=$ the $\sigma$-algebra
  generated by $\cup\{\mathscr{F}_{t}:t<\infty\}$ will be denoted by
  $\mathscr{F}$. 
\end{enumerate}
\end{notation*}

\begin{theorem*}
Let $\{X_{t}:0\leq t<\infty\}$ be a Brownian motion. Then
$X_{t}-X_{s}$ is independent of $\mathscr{F}_{s}$.
\end{theorem*}

\begin{proof}
Let
$$
0\leq t_{1}<t_{2}<t_{3}<\ldots<t_{k}\leq s.
$$

Then the $\sigma$-algebra generated by $X_{t_{1}},\ldots,X_{t_{k}}$ is
the same as the $\sigma$-algebra generated by
$$
X_{t_{1}},X_{t_{2}}-X_{t_{1}},\ldots,X_{t_{k}}-X_{t_{k-1}}.
$$

Since $X_{t}-X_{s}$ is independent of these increments, it is
independent of $\sigma\{X_{t_{1}},\ldots,X_{t_{k}}\}$. This is true
for every finite set $t_{1},\ldots,t_{k}$ and therefore $X_{t}-X_{s}$
is independent of $\mathscr{F}_{s}$.

Let us carry out the following calculation very {\em formally}. 
\begin{align*}
P[X_{t}\in A\mid\mathscr{F}_{s}](w) &= P[X_{t}-X_{s}\in B\mid
  \mathscr{F}_{s}](w),\ B=A-X_{s}(w),\\
&= P[X_{t}-X_{s}\in B],\q \text{by independence,}
\end{align*}
i.e.\pageoriginale
$$
P[X_{t}\in A\mid\mathscr{F}_{s}](w)=\int\limits_{A}\frac{1}{(2\pi
  t)^{d/2}}\exp -\frac{|y-X_{s}(w)|^{2}}{2(t-s)}dy.
$$
\end{proof}

This formal calculation leads us to

\begin{theorem*}
$$
P[X_{t}\in A\mid \mathscr{F}_{s}](w)=\int\limits_{A}\frac{1}{(2\pi
  t)^{d/2}}\exp -\frac{|y-X_{s}(w)|^{2}}{2(t-s)}dy.
$$
where $A$ is Borel in $\mathbb{R}^{d}$, $t>s$.
\end{theorem*}

\begin{remark*}
It may be useful to note that $p(s,X_{s}(w),t,y)$ can be thought of as
a conditional probability density.
\end{remark*}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item We show that
$$
f_{A}(w)=\int\limits_{A}\frac{1}{(2\pi
  t)^{d/2}}\exp-\frac{|y-X_{s}(w)|^{2}}{2(t-s)}dy
$$
is $\mathscr{F}_{S}$-measurable. Assume first that $A$ is bounded and
Borel in $\mathbb{R}^{d}$. If $\omega_{n}\to \omega$, then
$f_{A}(\omega_{n})\to f_{A}(\omega)$, i.e.\@ $f_{A}$ is continuous and
hence $\mathscr{F}_{s}$-measurable. The general case follows if we
note that any Borel set can be got as an increasing union of a
countable number of bounded Borel sets.

\item For any $C\in \mathscr{F}_{s}$ we show that
\begin{equation*}
\int^{X}_{C}X^{-1}_{t}(A)dP(\omega)=\int\limits_{C}\int\limits_{A}\frac{\exp-|y-X_{s}(\omega)|^{2}}{(2\pi
  (t-s))^{d/2}}dy\ dP(\omega).\tag{*}
\end{equation*}

It is enough to verify $(*)$ for $C$ of the form
{\fontsize{10pt}{12pt}\selectfont
$$
C=\big\{\omega:(X_{t_{1}}(\omega),\ldots,X_{t_{k}}(\omega))\in
A_{1}\times\cdots\times A_{k};\ 0\leq t_{1}<\ldots<t_{k}\leq s\big\},
$$}\relax
where $A_{i}$ is Borel in $\mathbb{R}^{d}$ for $i=1,2\ldots k$. The
left side of $(*)$ is then
{\fontsize{10pt}{12pt}\selectfont
$$
\int\limits_{A_{i}\times\cdots\times A_{k}\times
  A}p(0,0,t_{1},x_{t_{1}})p(t_{1},x_{t_{1}},t_{2},x_{t_{2}})\ldots
p(t_{k},x_{t_{k}},t,x_{t})dx_{t_{1}}\ldots dx_{t}.
$$}\pageoriginale

To compute the right side define
$$
f:\mathbb{R}^{(k+1)d}\to \mathbb{B}
$$
by
$$
f(u_{1},\ldots,u_{k},u)=X_{A_{1}}(u_{1})\ldots
X_{A_{k}}(u_{k})p(s,u,t,y).
$$

Clearly $f$ is Borel measurable. An application of Fubini's theorem to
the right side of $(*)$ yields
\begin{align*}
&\int\limits_{A}dy\int\limits_{\Omega}X_{A_{1}}(X_{t_{1}}(\omega))\ldots
  X_{A_{k}}(X_{t_{k}}(\omega))p(s,X_{s}(\omega),t,y)dP(\omega)\\
&\q =
  \int\limits_{A}dy\int\limits_{\substack{\mathbb{R}^{d}\times\cdots\times
  \mathbb{R}^{d}\\ (k+1)\q\text{times}}}f(x_{1}\ldots
  x_{k},x_{s})dF_{t_{1}\ldots t_{k}},s\\
&\q =\int\limits_{A}dy\int\limits_{A_{1}\times\cdots\times A_{k}\times
    \mathbb{R}^{d}}p(0,0,t_{1},x_{1})\ldots p(t_{k-1},t_{k},x_{k})\\
&\hspace{3.5cm} p(t_{k},x_{k},s,x_{s})p(s,x_{s},t,y)dx_{1}\ldots
  dx_{k}dx_{s}\\
&\q =\int\limits_{A_{1}\times\cdots \times A_{k}\times
    A}p(0,0,t_{1},x_{1})\ldots
  p(t_{k-1},x_{k-1},t_{k},x_{k})p(t_{k},x_{k},t,y)\\
&\hspace{7cm} dx_{1}\ldots,dx_{k}dy\\
&\hspace{5.5cm} \text{(by the convolution rule)}\\
&\q = \text{left side}.
\end{align*}
\end{enumerate}
\end{proof}

\noindent
{\large\bf Examples of Stopping Times.}
\begin{enumerate}
\item Let $(X_{t})$ be a one-dimensional Brownian motion. Define
  $\tau$ by
$$
\{\tau\leq
s\}=\bigcap\limits^{\infty}_{n=1}{\displaystyle{\mathop{\varlimsup}_{\substack{\theta_{1},\theta_{2}\\ \theta_{1},\theta_{2}\text{~
  rational in~ } [0,s]}}}}\{w:w(\theta_{1})\in D_{n},w(\theta_{2})\in
C_{n}\}, 
$$\pageoriginale
where 
$$
D_{n}=\left\{x\in\mathbb{R}^{d}:d(x,D)\leq \frac{1}{n}\right\},
C_{n}=\left\{x\in \mathbb{R}^{d}:d(x,C)\leq \frac{1}{n}\right\}
$$
\end{enumerate}

\setcounter{exercise}{0}
\begin{exercise}\label{chap6-exer1}
Let $\tau$ be as in Example 1.
\begin{itemize}
\item[(a)] If $A=\{w:X_{1}(w)\leq \tau\}$ show that $A\notin
  \mathscr{F}_{\tau}$. 

(Hint: $A\cap \{\tau\leq 0\}\not\in \mathscr{F}_{0}$). This shows that
  $\mathscr{F}_{\tau}\subsetneqq \mathscr{F}_{0}$.

\item[(b)] $P_{0}\{w:\tau(w)=\infty\}=0$.

(Hint: $P_{0}\{w:|w(t)|<1\}\leq \int\limits_{|y|\leq
  t^{-1/2}}e^{-1/2|y|^{2}}dy\ \forall t$).
\end{itemize}
\end{exercise}

\begin{theorem*}
(Strong Markov Property of Brownian Motion). Let $\tau$ be any finite
  stopping time, i.e.\@ $\tau<\infty$ a.e. Let
  $Y_{t}=X_{\tau+t}-X_{\tau}$. Then
\begin{enumerate}
\item $P[(Y_{t_{1}}\in A_{1},\ldots,Y_{t_{k}}\in A_{k})\cap
  A]=P(X_{t_{1}}\in A_{1},\ldots X_{t_{k}}\in A_{k})\cdot P(A)$,
  $\forall\ A\in \mathscr{F}_{\tau}$ and for every $A_{i}$ Borel in
  $\mathbb{R}^{d}$. Consequently,

\item $(Y_{t})$ is a Brownian motion.

\item $(Y_{t})$ is independent of $\mathscr{F}_{\tau}$.
\end{enumerate}
\end{theorem*}

The assertion is that a Brownian motion starts afresh at every
stopping time.

\begin{proof}
\setcounter{step}{0}
\begin{step}
Let $\tau$ take only countably many values, say $s_{1}$, $s_{2}$,
$s_{3}\ldots$. Put $E_{j}=\tau^{-1}\{s_{j}\}$. Then each $E_{j}$ is
$\mathscr{F}_{\tau}$-measure and 
$$
\Omega=\bigcup\limits^{\infty}_{j=1}E_{j},\ E_{j}\cap
E_{i}=\emptyset\ j\neq i.
$$\pageoriginale

Fix $A\in \mathscr{F}_{\tau}$.
\begin{align*}
&P[(Y_{t_{1}}\in A_{1},\ldots,Y_{t_{k}}\in A_{k})\cap A]\\
&\q =\sum\limits^{\infty}_{j=1}P[(Y_{t_{1}}\in
    A_{1},\ldots,Y_{t_{k}}\in A_{k})\cap A\cap E_{j}]\\
&\q =\sum\limits^{\infty}_{j=1}P[(X_{t_{1}+s_{j}}-X_{s_{j}})\in
    A_{1},\ldots,X_{t_{k}+s_{j}}-X_{s_{j}}\in A_{k})\cap A\cap
    E_{j}]\\
&\q =\sum\limits^{\infty}_{j=1}P[(X_{t_{1}}\in
    A_{1}),\ldots,(X_{t_{k}}\in A_{k})]P(A\cap E_{j})\\
&\hspace{2cm}\text{~ (by the
    Markov property)}\\
&\q =P(X_{t_{1}}\in A_{1},\ldots,X_{t_{k}}\in A_{k})\cdot P(A)
\end{align*}
\end{step}

\begin{step}%2
Let $\tau$ be any stopping time; put
$\tau_{n}=\dfrac{[n\tau]+1}{n}$. A simple calculation shows that
$\tau_{n}$ is a stopping time taking only countably many values. As
$\tau_{n}\shortdownarrow \tau$, $\mathscr{F}_{\tau}\subset
\mathscr{F}_{\tau_{n}}\forall_{n}$. Let
$Y^{(n)}_{t}=X_{\tau_{n}+t}-X_{\tau_{n}}$. 
\end{step}

By Step 1,
\begin{align*}
& P[(Y^{(n)}_{t_{1}}<x_{1},\ldots,Y^{(n)}_{t_{k}}<x_{k})\cap A]\\
& =P(X_{t_{1}}<x_{1},\ldots,X_{t_{k}}<x_{k})\cdot P(A)
\end{align*}
(where $x<y$ means $x_{i}<y_{i} \ i=1,2,\ldots,d$) for every $A\in
\mathscr{F}_{\tau}$. As all the Brownian paths are continuous,
$Y^{(n)}_{t}\to Y_{t}$ a.e. Thus, if $x_{1},\ldots,x_{k}$ is a point
of continuity of the joint distribution of
$X_{t_{1}},\ldots,X_{t_{k}}$, we have
$$
P[(Y_{t_{1}}<x_{1},\ldots,Y_{t_{k}}<x_{k})\cap A]=P(X_{t_{1}}<x_{1},\ldots,X_{t_{k}}<x_{k})P(A)
$$\pageoriginale
$\forall\ A\in \mathscr{F}_{\tau}$. Now assertion (1) follows easily.

For (2), put $A=\Omega$ in (1), and (3) is a consequence of (1) and (2).
\end{proof}
