\begin{center}
{\huge\bf Appendix}
\bigskip

{\Large\bf Language of Probability}
\end{center}
\bigskip

\addcontentsline{toc}{chapter}{Appendix}
\addcontentsline{toc}{chapter}{\quad Language of Probability}

\begin{defi*}
A\pageoriginale probability space is a measure space
$(\Omega,\mathscr{B},P)$ with $P(\Omega)\break =1$. $P$ is called a {\em
  probability measure} or simply a {\em probability}. Elements of
$\mathscr{B}$ are called {\em events}. A measurable function
$X:(\Omega,\mathscr{B})\to R^{d}$ is called $d$-dimensional {\em
  random variable}. Given the random variable $X$, define $F:\mathbb{R}^{d}\to
\mathbb{R}$ by
$$
F((a_{1},\ldots a_{n}))=P\{w:X_{i}(w)<a_{i},\text{~ for~ } i=1,2,\ldots,d\}
$$
where $X=(X_{1},X_{2},\ldots,X_{d})$. Then $F$ is called the {\em
  distribution function} of the random variable $X$. For any random
variable $X$, $\int X\ dP=(\int X_{1}dP,\ldots,\int X_{d}dP)$, if it
exists, is called {\em mean} of $X$ or {\em expectation} of $X$ and is
denoted by $E(X)$. Thus $E(X)=\int XdP=\mu$. $E(X^{n})$, where
$X^{n}=(X^{n}_{1},X^{n}_{2},\ldots,X^{n}_{d})$ is called the
$n^{\text{th}}$ {\em moment} about zero. $E((X-\mu)^{n})$ is called
the $n^{\text{th}}$ {\em central moment}. The 2nd central moment is
called {\em variance} and is denoted by $\sigma^{2}$ we have the
following.
\end{defi*}

\medskip
\noindent
{\bf Tchebyshev's Inequality.}
\smallskip

Let $X$ be a one-dimensional random variable with mean and variance
$\mu$. Then for every $\epsilon>0$, $P\{w:|X(w)-\mu|\geq\epsilon\}\leq
\sigma^{2}/\epsilon^{2}$. 

\medskip
\noindent
{\bf Generalised Tchebyshev's Inequality.}~ Let $f:\mathbb{R}\to
\mathbb{R}$ be measurable such that $f(u)=f(-u)$, $f$ is strictly
positive and increasing on $(0,\infty)$.\pageoriginale Then for any
random variable $X:\Omega\to R$,
$$
P(w:|X(w)|>\epsilon)\leq \frac{E(f(X))}{f(\epsilon)}
$$
for every $\epsilon>0$.

For any random variable $X:\Omega\to R^{d}$,
$\phi(t)=E(e^{itX}):R^{d}\to C$ is called the {\em characteristic
  function} of $X$. Here $t=(t_{1},\ldots,t_{d})$ and
$tX=t_{1}X_{1}+t_{2}X_{2}+\cdots+t_{d}X_{d}$. 

\medskip
\noindent
{\bf Independence.}~ Events $E_{1},\ldots,E_{n}$ are called {\em
  independent} if for every $\{i_{1},\ldots,i_{k}\}\subset
\{1,2,\ldots,n\}$ we have
$$
P(E_{i_{1}}\cap \ldots \cap E_{i_{k}})=P(E_{i_{1}})P(E_{i_{2}})\ldots
P(E_{i_{k}}).
$$

An arbitrary collection of events $\{E_{\alpha}:\alpha \in I\}$ is
called independent if every finite sub-collection is independent. Let
$\{\mathscr{F}_{\alpha}:\alpha\in I\}$ be a collection of
sub-$\sigma$-algebras of $\mathscr{B}$. This collection is said to be
independent if for every collection $\{E_{\alpha}:\alpha \in I\}$,
where $E_{\alpha}\in \mathscr{F}_{\alpha}$, of events is
independent. A collection of random variables $\{X_{\alpha}:\alpha\in
I\}$ is said to be independent if $\{\sigma(X_{\alpha}):\alpha\in I\}$
is independent where $\sigma(X_{\alpha})$ is the $\sigma$-algebra
generated by $X_{\alpha}$.

\begin{theorem*}
Let $X_{1}$, $X_{2},\ldots,X_{n}$ be random variables with
$F_{X_{1}},\ldots,F_{X_{n}}$ as their distribution functions and let
$F$ be distribution function of $X=(X_{1},\ldots,X_{n})$,
$\phi_{X_{1}},\ldots,\phi_{X_{n}}$ the characteristic functions of
$X_{1},\ldots,X_{n}$ and $\phi$ that of
$X=(X_{1},\ldots,X_{n})$. $X_{1},\ldots,X_{n}$ are independent if and
only if $F((a_{1},\ldots,a_{n}))=F_{X_{1}}(a_{1})\ldots
F_{X_{n}}(a_{n})$ for all $a_{1},\ldots,a_{n}$, iff
$\phi((t_{1},\ldots,t_{n}))\break =\phi_{X_{1}}(t_{1})\ldots\phi_{X_{n}}(t_{n})$\pageoriginale
for all $t_{1},\ldots,t_{n}$. 
\end{theorem*}


\noindent
{\bf Conditioning.}

\begin{theorem*}
Let $X:(\Omega,\mathscr{B},P)\to \mathbb{R}^{d}$ be a random variable, with
$E(X)$ finite, i.e. if $X=(X_{1},\ldots,X_{d})$, $E(X_{i})$ is finite
for each $i$. Let $\mathscr{C}$ be a sub-$\sigma$-algebra of
$\mathscr{B}$. Then there exists a random variable
$Y:(\Omega,\mathscr{C})\to \mathbb{R}^{d}$ such that
$\int\limits_{C}YdP=\int\limits_{C}XdP$ for every $C$ in
$\mathscr{C}$.

If $Z$ is any random variable with the same properties then $Y=Z$
almost everywhere $(P)$.
\end{theorem*}

\begin{defi*}
Any such $Y$ is called the {\em conditional expectation of $X$ with
  respect to $\mathscr{C}$} and is denoted by $E(X|\mathscr{C})$.

If $X=\chi_{A}$, the characteristic function of $A$ in $\mathscr{B}$,
then $E(\chi_{A}|\mathscr{C})$ is also denoted by $P(A|\mathscr{C})$.
\end{defi*}

\noindent
{\bf Properties of conditional expectation.}
\begin{enumerate}
\item $E(1|\mathscr{C})=1$.

\item $E(aX+bY|\mathscr{C})=aE(X|\mathscr{C})+bE(Y|\mathscr{C})$ for
  all real numbers $a$, $b$ and random variables $X$, $Y$.

\item If $X$ is a one-dimensional random variable and $X\geq 0$, then\break
  $E(X|\mathscr{C})\geq 0$.

\item If $Y$ is a bounded $\mathscr{C}$-measurable real valued random
  variable and $X$ is a one-dimensional random variable, then 
$$
E(YX|\mathscr{C})=YE(X|\mathscr{C}).
$$

\item If $\mathscr{D}\subset \mathscr{C}\subset \mathscr{B}$ are
  $\sigma$-algebras, then
$$
E(E(X|\mathscr{C})|\mathscr{D})=E(X|\mathscr{D}).
$$

\item $\int\limits_{\Omega}|E(X|\mathscr{D})|d(P|\mathscr{D})\leq
  \int\limits_{\Omega}E(X|\mathscr{C})d(P|\mathscr{C})$.\pageoriginale 
\end{enumerate}

\setcounter{exercise}{0}
\begin{exercise}\label{app-exer1}
Let $(\Omega,\mathscr{B},P)$ be a probability space, $\mathscr{C}$ a
sub-$\sigma$-algebra of $\mathscr{B}$. Let
$X(t,\cdot)Y(t,\cdot):\Omega\to R$ be measurable with respect to
$\mathscr{B}$ and $\mathscr{C}$ respectively where $t$ ranges over the
real line. Further let $E(X(t,\cdot)|\mathscr{C})=Y(t,\cdot)$ for each
$t$. If $f$ is a simple $\mathscr{C}$-measurable function then show
that
$$
\int\limits_{C}X(f(w),w)d(P|\mathscr{C})=\int\limits_{C}Y(f(w)w)dP
$$
for every $C$ in $\mathscr{C}$.
\end{exercise}

\noindent
[{\bf Hint.} Let $A_{1},\ldots,A_{n}$ be a $\mathscr{C}$-measurable
  partition such that $f$ is constant on each $A_{i}$. Verify the
  equality when $C$ is replaced by $C\cap A_{i}$.]

\begin{exercise}\label{app-exer2}
Give conditions on $X$, $Y$ such that exercise \ref{app-exer1} is
valid for all bounded $\mathscr{C}$-measurable functions and prove
your claim.

The next lemma exhibits conditioning as a projection on a Hilbert space.
\end{exercise}

\begin{lemma*}
Let $(\Omega,\mathscr{B},P)$ be any probability space $\mathscr{C}$ a
sub-$\sigma$-algebra of $\mathscr{B}$. Then
\begin{itemize}
\item[\rm(a)] $L^{2}(\Omega,\mathscr{C},P)$ is a closed subspace of
  $L^{2}(\Omega,\mathscr{B},P)$.

\item[\rm(b)] If $\pi:L^{2}(\Omega,\mathscr{B},P)\to
  L^{2}(\Omega,\mathscr{C},P)$ is the projection,then
  $\pi(f)=E(f|\mathscr{C})$. 
\end{itemize}
\end{lemma*}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item is clear, because for any $f\in L^{1}(\Omega,\mathscr{C},P)$
$$
\int\limits_{\Omega}fd(P|\mathscr{C})=\int\limits_{\Omega}fdP
$$\pageoriginale
(use simple function $0\leq s_{1}\leq \ldots \leq f$, if $f\geq 0$)
and $L^{2}(\Omega,\mathscr{C},P)$ is complete.

\item To prove this it is enough to verify it for characteristic
  functions because both $\pi$ and $f\to E(f|\mathscr{C})$ are linear
  and continuous.

Let $A\in \mathscr{B}$, $C\in \mathscr{C}$ then
$\pi(\chi_{C})=\chi_{C}$. As $\pi$ is a projection
$$
\int \pi(\chi_{A})\overline{\chi}_{C}d(P|\mathscr{B})=\int
\chi_{A}\overline{\pi(\chi_{C})}d(P|\mathscr{B}), 
$$
i.e.
$$
\int\limits_{C}\pi(\chi_{A})d(P|\mathscr{B})=\int\limits_{C}X_{A}d(P|\mathscr{B}). 
$$

Since $\pi(\chi_{A})$ is $\mathscr{C}$-measurable,
$$
\int\limits_{C}\pi(\chi_{A})d(P|\mathscr{B})=\int\limits_{C}\pi(\chi_{A})d(P|\mathscr{C}) 
$$

Therefore
$$
\int\limits_{C}\pi(\chi_{A})d(P|\mathscr{C})=\int\limits_{C}\chi_{A}d(P|\mathscr{B}),\ \forall
C \text{~ in~ }\mathscr{C}.
$$

Hence
$$
\pi(\chi_{A})=E(\chi_{A}|\mathscr{C}).
$$
\end{enumerate}
\end{proof}

\noindent
{\bf Kolmogorov's Theorem.}
\smallskip

\addcontentsline{toc}{chapter}{\quad Kolmogorovs Theorem}
\noindent
{\bf Statement.}~ Let $A$ be any nonempty set and for each finite
ordered subset $(t_{1},t_{2},\ldots,t_{n})$ of $A$
[i.e. $(t_{1},\ldots,t_{n})$ an ordered $n$-tuple with $t_{i}$ in
  $A$], let $P_{(t_{1},\ldots,t_{n})}$ be a probability on the Borel
sets\pageoriginale in $R^{dn}=R^{d}\times R^{d}\times\cdots
R^{d}$. Assume that the family $P_{(t_{1},\ldots,t_{n})}$ satisfies
the following two conditions

\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item Let $\tau:\{1,2,\ldots,n\}\to \{1,2,\ldots,n\}$ be any
  permutation and $f_{\tau}:R^{dn}\to R^{dn}$ be given by
$$
f_{\tau}((x_{1},\ldots,x_{n}))=(x_{\tau(1)},\ldots,x_{\tau(n)}).
$$

We have
$$
P{\displaystyle{\mathop{(E)}_{(t_{\tau(1)},\ldots,t_{\tau(n)})}}}=P_{(t_{1},\ldots,t_{n})}(f^{-1}_{\tau}(E)) 
$$
for every Borel set $E$ of $R^{dn}$. In short, we write this condition
as $P_{\tau t}=P_{t}\tau^{-1}$.

\item
  $P{\displaystyle{\mathop{(E)}_{(t_{1},\ldots,t_{n})}}}=P_{(t_{1},t_{2},\ldots,t_{n},t_{n+1},\ldots
  t_{n+m})}(E\times R^{dm})$ for all Borel sets $E$ of $R^{dn}$ and
  this is true for all $t_{1},\ldots,t_{n}$, $t_{n+1},\ldots,t_{n+m}$
  of $A$.
\end{enumerate}

Then, there exists a probability space $(\Omega,\mathscr{B},P)$ and a
collection of random variable $\{X_{t}:t\in
A\}:(\Omega,\mathscr{B})\to R^{d}$ such that
$$
P{\displaystyle{\mathop{(E)}_{(t_{1},\ldots,t_{n})}}}=P\{w:(X_{t_{1}}(w),\ldots,X_{t_{n}}(w))\in E\}
$$
for all Borel sets $E$ of $R^{dn}$.

\begin{proof}
Let $\Omega=\pi\{R^{d}_{t}:t\in A\}$ where $R^{d}_{t}=R^{d}$ for each
$t$. Define $X_{t}:\Omega\to R^{d}$ to be the projection given by
$X_{t}(w)=w(t)$. Let $\mathscr{B}_{0}$ be the algebra generated by
$\{X_{t}:t\in A\}$ and $\mathscr{B}$ the $\sigma$-algebra generated by
$\{X_{t}:t\in A\}$. Having got $\Omega$ and $\mathscr{B}$ we have to
construct a probability $P$ on $(\Omega,\mathscr{B})$ satisfying the
conditions of the theorem.

Given\pageoriginale $t_{1},\ldots,t_{n}$ define
$$
\pi_{(t_{1},\ldots,t_{n})}:\Omega\to R^{d}\times
R^{d}\times\cdots\times R^{d}(n~\text{~ times})
$$
by
$$
\pi_{(t_{1},\ldots,t_{n})}(w)=(w(t_{1}),\ldots,w(t_{n})).
$$

It is easy to see that every element of $\mathscr{B}_{0}$ is
$\pi^{-1}_{(t_{1},\ldots,t_{n})}(E)$ for suitable $t_{1},\ldots,t_{n}$
  in $A$ and a suitable Borel set $E$ of $R^{dn}$. Define $P$ on
  $\mathscr{B}_{0}$ by
  $P(\pi^{-1}_{(t_{1},\ldots,t_{n})}(E))=P_{(t_{1},\ldots,t_{n})}(E)$. Conditions
  (1) and (2) ensure that $P$ is a well-defined function on
  $\mathscr{B}_{0}$ and that, as $P_{(t_{1},\ldots,t_{n})}$ are
  measures, $P$ is finitely additive on $\mathscr{B}_{0}$.
\end{proof}

\begin{claim*}
Let $C_{1}\supset C_{2}\supset \ldots \supset C_{n}\supset \ldots$ be
a decreasing sequence in $\mathscr{B}_{0}$ with $\limit\limits_{n\to
  \infty}P(C_{n})\geq \delta>0$. Then $\cap C_{n}$ is non-empty. Once
the claim is proved, by Kolmogorov's theorem on extension of measures,
the finitely additive set function $P$ can be extended to a measure
$P$ on $\mathscr{B}$. One easily sees that $P$ is a required
probability measure.
\end{claim*}


\noindent
{\bf Proof of the Claim.}~ As $C_{n}\in \mathscr{B}_{0}$, we have 
$$
C_{n}=\pi^{-1}_{(t^{(n)}_{1},\ldots,t^{(n)}_{k(n)})}(E_{n})\text{~ for
  suitable~ } t^{(n)}_{i}\text{~ in~ } A
$$
and Borel set $E_{n}$ in $R^{dk(n)}$. Let
$$
T_{n}=(t^{(n)}_{1},\ldots,t^{(n)}_{k(n)})\q\text{and}\q
A_{n}=\{t^{(n)},\ldots,t^{(n)}_{k(n)}\}. 
$$
We can very well assume
that $A_{n}$ is increasing with\pageoriginale 
$n$. Choose a compact subset $E'_{n}$ of
$E_{n}$ such that 
$$
P_{T_{n}}(E_{n}-E'_{n})\leq \delta/2^{n+1}.
$$

If $C'_{n}=\pi^{-1}_{T_{n}}(E'_{n})$, then $P(C_{n}-C'_{n})\leq
\delta/2^{n+1}$. If $C''_{n}=C'_{1}\cap C'_{2}\cap \ldots\cap C'_{n}$
then $C''_{n}\subset C'_{n}\subset C_{n}$, $C''_{n}$ is decreasing and 
$$
P(C''_{n})\geq P(C_{n})-\sum\limits^{n}_{i=1}P(C_{i}-C'_{i})\geq \delta/2.
$$

We prove $\cap C''_{n}$ is not empty, which proves the claim.

Choose $w_{n}$ in $C''_{n}$. As $\pi_{T_{1}}(w_{n})$ is in the compact
set $E_{1}$ for all $n$, choose a subsequence
$$
n^{(1)}_{1}, n^{(1)}_{2},\ldots\text{~ of~ } 1,2,\ldots \text{~ such
  that~ } \pi_{T_{1}}(w_{n_{k}}(1))
$$
converges as $k\to \infty$. But for finitely many $n^{(1)}_{k}$'s,
$\pi_{T_{2}}(\omega_{n_{m}}(1))$ is in the compact set $E'_{2}$. As before
choose a subsequence $n^{(2)}_{k}$ of $n^{(1)}_{k}$ such that
$\pi_{T_{1}}(\omega_{n_{k}}(2))$ converges as $k\to \infty$. By the
diagonal process obtain a subsequence, $w^{*}_{n}$ of $w_{n}$ such
that $\pi_{T_{m}}(w^{*}_{n})$ converges as $n\to \infty$ for all
$m$. Thus, if $t$ is in
$$
\bigcup\limits^{\infty}_{m=1}A_{m},\q\text{then}\q \limit\limits_{n\to \infty}w^{*}_{n}(t)=x_{t}
$$
exists. Define $w$ by $w(t)=0$ if $t\in
A\bigcup\limits^{\infty}_{m=1}A_{m}$, $w(t)=x_{t}$ if
$t\in\bigcup\limits^{\infty}_{m=1}A_{m}$. One easily sees that $w\in
\bigcap\limits^{\infty}_{n=1}C''_{n}$, completing the proof of the theorem.

\medskip
\noindent
{\bf Martingales.}\pageoriginale
\smallskip


\addcontentsline{toc}{chapter}{\quad Martingales}
\begin{defi*}
Let $(\Omega,\mathscr{F},P)$ be a probability space, $(T,\leq)$ a
totally ordered set. Let $(\mathscr{F}_{t})_{t\in T}$ be an increasing
family of sub-$\sigma$-algebras of $\mathscr{F}$. A collection
$(X_{t})_{t\in T}$ of random variables on $\Omega$ is called a {\em
  martingale} with respect to the family $(\mathscr{F}_{t})_{t\in T}$
if
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item $E(|X_{t}|)<\infty$, $\forall t\in T$;

\item $X_{t}$ is $\mathscr{F}_{t}$-measurable for each $t\in T$;

\item $E(X_{t}|\mathscr{F}_{s})=X_{s}$ a.s.\@ for each $s$, $t$ in $T$
  with $t\geq s$. (Markov property).

If instead of (iii) one has

\setcounter{enumi}{2}
\renewcommand{\labelenumi}{(\theenumi)$'$}
\item $E(X_{t}|\mathscr{F}_{s})\geq (\leq)X_{s}$ a.s.,

then $(X_{t})_{t\in T}$ is called a {\em submartingale} (respectively
{\em supermartingale}). 

From the definition it is clear that $(X_{t})_{t\in T}$ is a
submartingale if and only if $(-X_{t})_{t\in T}$ is a supermartingale,
hence it is sufficient to study the properties of only one of
these. $T$ is usually any one of the following sets
$$
[0,\infty), N,Z,\{1,2,\ldots,n\}, [0,\infty]\q\text{or}\q N\cup \{\infty\}.
$$
\end{enumerate}
\end{defi*}

\begin{examples*}
\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item Let $(X_{n})_{n=1,2\ldots}$ be a sequence of independent random
  variables with
$$
E(X_{n})=0.
$$

Then\pageoriginale $Y_{n}=X_{1}+\cdots+X_{n}$ is a martingale with
respect to $(\mathscr{F}_{n})_{n=1,2,\ldots}$ where
$$
\mathscr{F}_{n}=\sigma\{Y_{1},\ldots,Y_{n}\}=\sigma\{X_{1},\ldots,X_{n}\}.
$$
\end{enumerate}
\end{examples*}

\begin{proof}
By definition, each $Y_{n}$ is $\mathscr{F}_{n}$-measurable.
\begin{align*}
&\qq\q E(Y_{n})=0.\\
&
  E((X_{1}+\cdots+X_{n}+X_{n+1}+\cdots+X_{n+m})|\sigma\{X_{1},\ldots,X_{n}\})\\ 
&
  =X_{1}+\cdots+X_{n}+E((X_{n+1}+\cdots+X_{n+m})|\sigma\{X_{1},\ldots,X_{n}\})\\
&= Y_{n}+E(X_{n+1}+\cdots+X_{n+m})=Y_{n}.
\end{align*}
\end{proof}

\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\setcounter{enumi}{1}
\item Let $(\Omega,\mathscr{F},P)$ be a probability space, $Y$ a
  random variable with $E(|Y|)<\infty$. Let $\mathscr{F}_{t}\subset
  \mathscr{F}$ be a $\sigma$-algebra such that $\forall t\in
          [0,\infty)$
$$
\mathscr{F}_{t}\subset \mathscr{F}_{s}\q\text{if}\q t\leq s.
$$

If $X_{t}=E(Y|\mathscr{F}_{t})$, $X_{t}$ is a martingale with respect
to $(\mathscr{F}_{t})$.
\end{enumerate}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item By definition, $X_{t}$ is $\mathscr{F}_{t}$-measurable.

\item $E(X_{t})=E(Y)$ (by definition) $<\infty$.

\item if $t\geq s$,
$$
E(X_{t}|\mathscr{F}_{s})=E(E(Y|\mathscr{F}_{t})|\mathscr{F}_{s})=E(Y|\mathscr{F}_{s})=X_{s} 
$$
\end{enumerate}
\end{proof}

\setcounter{exercise}{0}
\begin{exercise}
$\Omega=[0,1]$, $\mathscr{F}=\sigma$-algebra of all Borel sub sets of
  $\Omega$, $P=$ Lebesgue measure.

Let $\mathscr{F}_{n}=$-algebra generated by the sets
$$
\Big[0,\dfrac{1}{2^{n}}\Big)\Big[\dfrac{1}{2^{n}},\dfrac{2}{2^{n}}\Big);\ldots,\Big[\dfrac{2^{n}-1}{2^{n}},1\Big].
$$\pageoriginale
Let $f\in L'[0,1]$ and define
\begin{align*}
X_{n}(w) &=
2^{n}\left(\sum\limits^{2^{n}-1}_{j=1}\chi_{[\frac{j-1}{2^{n}},\frac{j}{2^{n}})}\int\limits^{j/2^{n}}_{j-1/2^{n}}f\ dy
  +\chi_{[\frac{2^{n}-1}{2^{n}},1]}\int\limits^{1}_{2^{n}-1/2^{n}}f\ dy\right) 
\end{align*}

Show that $(X_{n})$ is a martingale relative to $(\mathscr{F}_{n})$.
\end{exercise}

\begin{exer*}
Show that a submartingale or a supermartingale $\{X_{s}\}$ is a
martingale iff $E(X_{s})=$ constant.
\end{exer*}

\begin{theorem*}
If $(X_{t})_{t\in T}$, $(Y_{t})_{t\in T}$ are supermartingales then 
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item $(aX_{t}+bY_{t})_{t\in T}$ is a supermartingale, $\forall a$,
  $b\in \mathbb{R}^{+}=[0,\infty)$.

\item $(X_{t}\wedge Y_{t})_{t\in T}$ is a supermartingale.
\end{enumerate}
\end{theorem*}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item Clearly $Z_{t}=aX_{t}+bY_{t}$ is $\mathscr{F}_{t}$-measurable
  and $E(|Z_{t}|)\leq aE(|X_{t}|)+bE(|Y_{t}|)<\infty$.
\begin{align*}
E(aX_{t}+bY_{t}|\mathscr{F}_{s}) &=
aE(X_{t}|\mathscr{F}_{s})+bE(Y_{t}|\mathscr{F}_{s})\\
&\leq aX_{s}+bY_{s}=Z_{s},\q\text{if}\q t\geq s.
\end{align*}

\item Again $X_{t}\wedge Y_{t}$ is $\mathscr{F}_{t}$-measurable and
  $E(|X_{t}\wedge Y_{t}|)<\infty$,
$$
E(X_{t}\wedge Y_{t}|\mathscr{F}_{s})\leq E(X_{t}|\mathscr{F}_{s})\leq
X_{s}. 
$$

Similarly\pageoriginale
$$
E(X_{t}\wedge Y_{t}|\mathscr{F}_{s})\leq E(Y_{t}|\mathscr{F}_{s})\leq
Y_{s},\q\text{if}\q t\geq s.
$$

Therefore
$$
E(X_{t}\wedge Y_{t}|\mathscr{F}_{s})\leq X_{s}\wedge Y_{s}.
$$
\end{enumerate}
\end{proof}

\noindent
{\bf Jensen's Inequality.}~ Let $X$ be a random variable in
$(\Omega,\mathscr{B},P)$ with\break $E(|X|)<\infty$ and let $\phi(x)$ be a
convex function defined on the real line such that
$E(|\phi_{0}X|)<\infty$. Then
$$
\phi(E(X|\mathscr{C}))\leq E(\phi_{0}X|\mathscr{C})\q\text{a.e.}
$$
where $\mathscr{C}$ is any sub-$\sigma$-algebra of $\mathscr{B}$.

\begin{proof}
The function $\phi$ being convex, there exist sequences $a_{1}$,
$a_{2},\ldots a_{n},\break \ldots,b_{1},b_{2},\ldots$ of real numbers such
that $\phi(x)=\sup\limits_{n}(a_{n}x+b_{n})$ for each $x$. Let
$L_{n}(x)=a_{n}x+b_{n}$. Then 
$$
L_{n}(E(X|\mathscr{C}))=E(L_{n}(X)|\mathscr{C})\leq E(\phi(X)|\mathscr{C})
$$
for all $n$ so that
$$
\phi (E(X|\mathscr{C}))\leq E(\phi(X)|\mathscr{C}).
$$
\end{proof}

\begin{exer*}
\begin{enumerate}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item If $\{X_{t}:t\in T\}$ is a martingale with respect to
  $\{\mathscr{F}_{t}:t\in T\}$ and $\phi$ is a convex function on the
  real line such that $E(|\phi(X_{t})|)<\infty$ for every $t$, then
  $\{\phi(X_{t})\}$ is a sub martingale.

\item If $(X_{t})_{t\in T}$ is a submartingale and $\phi(x)$ is a
  convex function and nondecreasing and if
  $E(|\phi_{0}X_{t}|)<\infty$, $\forall t$ then $\{\phi(X_{t})\}$ is a
  submartingale. (Hint: Use Jensen's inequality).
\end{enumerate}
\end{exer*}

\begin{defi*}
Let\pageoriginale $(\Omega,\mathscr{B},P)$ be a probability space and
$(\mathscr{F}_{t})_{t\in [0,\infty)}$ an increasing family of
  sub-$\sigma$-algebras of $\mathscr{F}$. Let $(X_{t})_{t\in
    [0,\infty)}$ be a family of random variables on $\Omega$ such that
    $X_{t}$ is $\mathscr{F}_{t}$-measurable for each $t\geq
    0$. $(X_{t})$ is said to be {\em progressively measurable} if
$$
X:[0,t]\times \Omega\to \mathbb{R}\q \text{defined by}\q
X(s,w)=X_{s}(w)
$$
is measurable with respect to the $\sigma$-algebra
$\mathscr{B}[0,t]\times \mathscr{F}_{t}$ for every $t$.
\end{defi*}

\noindent
{\bf Stopping times.}~ Let us suppose we are playing a game of chance,
say, tossing a coin. The two possible outcomes of a toss are $H$
(Heads) and $T$ (Tails). We assume that the coin is unbiased so that
the probability of getting a head is the same as the probability of
getting a tail. Further suppose that we gain $+1$ for every head and
lose 1 for every tail. A game of chance of this sort has the following
features. 
\begin{enumerate}
\item A person starts playing with an initial amount $N$ and finishes
  with a certain amount $M$.

\item Certain rules are specified which allow one to decide when to
  stop playing the game. For example, a person may not have sufficient
  money to play all the games, in which case he may decide to play
  only a certain number of games.
\end{enumerate}

It is obvious that such a game of chance is fair in that it is neither
advantageous nor disadvantageous to play such a game and
on\pageoriginale the average $M$ will equal $N$, the initial
amount. Furthermore, the stopping rules that are permissible have to
be reasonable. The following type of stopping rule is obviously
unreasonable. 

\medskip
\noindent
{\bf Rule.}~ If the first toss is a tail the person quits at time $0$
and if the first toss is a head the person quits at time $t=1$.


This rule is unreasonable because the decision to quit is made on the
basis of a future event, whereas if the game is fair this decision
should depend only on the events that have already occured. Suppose,
for example, 10 games are played, then the quitting times can be $0$,
$1,2,\ldots,10$. If $\xi_{1},\ldots,\xi_{10}$ are the outcomes
$(\xi_{i}=+1$ for $H$, $\xi_{i}=-1$ for $T)$ then the quitting time at
the 5th stage (say) should depend only on $\xi_{1},\ldots,\xi_{4}$ and
not any of $\xi_{5},\ldots,\xi_{10}$. If we denote
$\xi=(\xi_{1},\ldots,\xi_{10})$ and the quitting time $\tau$ as a
function of $\xi$ then we can say that $\{\xi:\tau=5$ depends only
$\xi_{1},\ldots,\xi_{4}\}$. This leads us to the notion of stopping
times.

\begin{defi*}
Let $(\Omega,\mathscr{F},P)$ be a probability space,
$(\mathscr{F}_{t})_{t\in [0,\infty)}$ an increasing family of
  sub-$\sigma$-algebras of $\mathscr{F}$. $\tau:\Omega\to [0,\infty]$
  is called a {\em stopping time} or {\em Markov time} (or a random
  variable independent of the future) if
$$
\{w:\tau(w)\leq t\}\in \mathscr{F}_{t}\q\text{for each}\q t\geq 0.
$$

Observe that a stopping time is a measurable function with respect to
$\sigma(\cup \mathscr{F}_{t})\subset \mathscr{F}$.
\end{defi*}

\begin{examples*}
\begin{enumerate}
\item $\tau=$\pageoriginale constant is a stopping time.

\item For a Brownian motion $(X_{t})$, the hitting time of a closed
  set is stopping time.
\end{enumerate}
\end{examples*}


\begin{exercise}
Let $\mathscr{F}_{t+}\equiv \bigcap\limits_{\Def
  s>t}\mathscr{F}_{s}\equiv \mathscr{F}_{t}$.
\end{exercise}

\noindent
[If this is satisfied for every $t\geq 0$, $\mathscr{F}_{t}$ is said
  to be {\em right continuous}]. If $\{\tau<t\}\in \mathscr{F}_{t}$
for each $t\geq 0$, then $\tau$ is a stopping time. (Hint: $\{\tau\leq
t=\bigcap\limits^{\infty}_{n=k}\{\tau<t+1/n\}$ for every $k$).

We shall denote by $\mathscr{F}_{\infty}$ the $\sigma$-algebra
generated by $\bigcup\limits_{t\in T}\mathscr{F}_{t}$. If $\tau$ is a
stopping time, we define
$$
\mathscr{F}_{\tau}=\{A\in \mathscr{F}_{\infty}:A\cap \{\tau\leq t\}\in
\mathscr{F}_{t},\forall t\geq 0\}
$$

\begin{exercise}%4
\begin{enumerate}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item Show that $\mathscr{F}_{\tau}$ is a $\sigma$-algebra. (If $A\in
  \mathscr{F}_{\tau}$, 
$$
A^{c}\cap \{\tau\leq t\}=\{t\leq t\}-A\cap \{\tau \leq t\}).
$$

\item If $\tau=t$ (constant) show that
  $\mathscr{F}_{\tau}=\mathscr{F}_{t}$. 
\end{enumerate}
\end{exercise}

\begin{theorem*}
Let $\tau$ and $\sigma$ be stopping times. Then
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\rm(\theenumi)}
\item $\tau+\sigma$, $\tau v\sigma$, $\tau\wedge \sigma$ are all
  stopping times.

\item If $\sigma\leq\tau$, then $\mathscr{F}_{\sigma}\subset
  \mathscr{F}_{\tau}$. 

\item $\tau$ is $\mathscr{F}_{\tau}$-measurable.

\item If $A\in \mathscr{F}_{\sigma}$, then $A\cap \{\sigma=\tau\}$ and
  $A\cap \{\sigma\leq \tau\}$ are in $\mathscr{F}_{\sigma\wedge
  \tau}\subset \mathscr{F}_{\sigma}\cap \mathscr{F}_{\tau}$. In
  particular, $\{\tau<\sigma\}$,\pageoriginale 
  $\{\tau=\sigma\}$, $\{\tau>\sigma\}$
  are all in $\mathscr{F}_{\tau}\cap \mathscr{F}_{\sigma}$.

\item If $\tau'$ is $\mathscr{F}_{\tau}$-measurable and $\tau'\geq
  \tau$, then $\tau'$ is a stopping time.

\item If $\{\tau_{n}\}$ is a sequence of stopping times, then
  $\varliminf \tau_{n}$. $\varlimsup \tau_{n}$ are also stopping times
  provided that $\mathscr{F}_{t+}=\mathscr{F}_{t}$, $\forall t\geq 0$.

\item If $\tau_{n}\downarrow \tau$, then
  $\mathscr{F}_{\tau}=\bigcap\limits^{\infty}_{n=1}\mathscr{F}_{\tau_{n}}$
  provided that $\mathscr{F}_{t+}=\mathscr{F}_{t}$, $\forall t\geq 0$.
\end{enumerate}
\end{theorem*}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item 
\begin{gather*}
\{\sigma+\tau\}>t\}=\{\sigma+\tau>t,\tau\leq t,\sigma\leq t\}\cup 
\{\tau>t\}\cup \{\sigma>t\};\\
\{\sigma+\tau>t,\sigma\leq t\}=\tau\leq
\mathscr{A}\\ 
=\bigcup\limits_{\substack{r\in \mathscr{Q}\\ 0\leq r\leq
    t}}\{\sigma>r>t-\tau,\tau\leq t,\sigma\leq t\}\\
(\mathscr{Q}=\text{~set of rationals})\\
\{\sigma>r>t-\tau,\tau\leq t,\sigma\leq t\}=\{t\geq \sigma>r\}\cap
\{t\geq \tau>t-r\}\\
=\{\sigma\leq t\}\cap \{\sigma\leq r\}^{c}\cap \{\tau\leq
t\}\cap\{\tau\leq t-r\}^{c}. 
\end{gather*}

The right side is in $\mathscr{F}_{t}$. Therefore $\sigma+\tau$ is a
scopping time.
\begin{align*}
& \{\tau V\sigma\leq t\}=\{\tau\leq t\}\cap \{\sigma\leq t\}\\
& \{\tau \wedge \sigma>t\}=\{\tau>t\}\cap \{\sigma>t\}
\end{align*}

\item Follows from (iv).

\item $\{\tau\leq t\}\{\tau\leq s\}=\{\tau\leq t\wedge s\}\in
  \mathscr{F}_{t\wedge s}\subset \mathscr{F}_{s}$, $\forall s\geq 0$.

\item \begin{tabbing}
\= $A\cap \{\sigma<\tau\}\cap \{\sigma\wedge \tau\leq t\}=[A\cap
  \{\sigma\leq t<\tau\}]$\\[5pt]
\> $U[A\cap \mathop{U}\limits_{\substack{r\in \mathscr{Q}\\ 0\leq r\leq
      t}}\{\sigma \leq <\tau\}\cap \{\tau\leq t\}]\in
\mathscr{F}_{t}$.\\[5pt] 
\> $A\cap \{\sigma\leq \tau\}\cap \{\sigma\wedge \tau\leq t\}=A\cap
\{\sigma \leq \tau\}\cap \{\sigma\leq t\}$.
\end{tabbing}\pageoriginale

It is now enough to show that $(\sigma\leq \tau)\in
\mathscr{F}_{\sigma}$; but this is obvious because
$(\tau<\sigma)=(\sigma\leq \tau)^{c}$ is in $\mathscr{F}_{\sigma\wedge
  \tau}\subset \mathscr{F}_{\sigma}$. Therefore $A\cap \{\sigma\leq
\tau\}\in \mathscr{F}_{\sigma \wedge \tau}$ and (iv) is proved.

\item $\{\tau'\leq t\}=\{\tau'\leq t\}\cap \{\tau\leq t\}\in
  \mathscr{F}_{t}$ as $(\tau'\leq t)\in \mathscr{F}_{\tau}$. Therefore
  $\tau'$ is a stopping time.

\item \begin{tabbing}
$\varliminf \tau_{n}$ \=$\equiv\sup\limits_{n}\inf\limits_{k\geq
    n}\tau_{k}$\\[5pt] 
\>$=\sup\limits_{n}\inf\limits_{\ell}\inf
\{\tau_{n},\tau_{n+1},\ldots,\tau_{n+\ell}\}$. 
\end{tabbing}


By (i), $\inf \{\tau_{n},\tau_{n+1},\ldots,\tau_{n+\ell}\}$ is a
stopping time. Thus we have only to prove that if $\tau_{n}\uparrow
\tau$ or $\tau_{n}\downarrow \tau$ where $\tau_{n}$ are stopping
times, then $\tau$ is a stopping time. Let $\tau_{n}\uparrow
\tau$. Then $\{\tau\leq
t\}=\bigcap\limits^{\infty}_{n=1}\{\tau_{n}\leq t\}$ so that $\tau$ is
a stopping time. Let $\tau_{n}\downarrow \tau$. Then
$$
\{\tau\geq t\}=\bigcap\limits^{\infty}_{n=1}\{\tau_{n}\geq t\}.
$$

By Exercise 3, $\tau$ is a stopping time. That $\varlimsup \tau_{n}$
is a stopping time is proved similarly.

\item Since $\tau\leq \tau_{n}$, $\forall n$,
  $\mathscr{F}_{\tau}\subset
  \bigcap\limits^{\infty}_{n=1}\mathscr{F}_{\tau_{n}}$. Let $A\in
  \bigcap\limits^{\infty}_{n=1}\mathscr{F}_{\tau_{n}}$. Therefore
  $A\cap (\tau_{n}<t)\in \mathscr{F}_{t}$, $\forall n$. $A\cap
  (\tau<t)=\bigcap\limits^{\infty}_{m=1}(A\cap (\tau_{m}<t))\in
  \mathscr{F}_{t}$. Therefore $A\in \mathscr{F}_{\tau}$. 
\end{enumerate}
\end{proof}

\noindent
{\bf Optional Sampling Theorem.} (Discrete case). {\em Let
$\{X_{1},\ldots,X_{k}\}$ be a martingale relative to
  $\{\mathscr{F}_{1},\ldots,\mathscr{F}_{k}\}$. Let
  $\{\tau_{1},\ldots,\tau_{p}\}$ be a collection of stopping times
  relative to $\{\mathscr{F}_{1},\ldots,\mathscr{F}_{k}\}$ such that
  $\tau_{1}\leq \tau_{2}\leq \ldots \leq \tau_{p}$\pageoriginale
  a.s.\@ and each $\tau_{i}$ takes values in $\{1,2,\ldots,k\}$. Then
  $\{X_{\tau_{1}},\ldots,X_{\tau_{p}}\}$ is a martingale relative to
  $\{\mathscr{F}_{\tau_{1}},\ldots,\mathscr{F}_{\tau_{p}}\}$ where for
  any stopping time $\tau$, $X_{\tau}(\omega)=X_{\tau(w)}(\omega)$.}

\begin{proof}
It is easy to see that each $X_{\tau_{i}}$ is a random variable. In
fact
$X_{\tau_{m}}=\sum\limits^{k}_{i=1}X_{i}\chi_{\{\tau_{m}=i\}}$. Let
$\tau\in \{1,2,\ldots,k\}$. Then
$$
E(|X_{\tau}|)\leq \sum\limits^{k}_{j=1}\int |X_{j}|dP<\infty.
$$

Consider
$$
(X_{\tau_{j}}\leq t)\cap (\tau_{j}\leq s)=\bigcap\limits_{\ell \leq
  s}(X_{\ell}\leq t)\in \mathscr{F}_{s}.
$$

Then $(X_{\tau_{j}}\leq t)$ is in $\mathscr{F}_{\tau_{j}}$, i.e.\@
$X_{\tau_{j}}$ is $\mathscr{F}_{\tau_{j}}$-measurable. Next we show
that
\begin{equation*}
E(X_{\tau_{j}}|\mathscr{F}_{\tau_{k}})\leq X_{\tau_{k}},\q\text{if}\q
j\geq k.\tag{*}
\end{equation*}
(*) is true if and only if
$$
\int\limits_{A}X_{\tau_{j}}dP\leq
\int\limits_{A}X_{\tau_{k}}dP\q\text{for every}\q A\in
\mathscr{F}_{\tau_{k}}. 
$$

The theorem is therefore a consequence of the following
\end{proof}

\begin{lemma*}
Let $\{X_{1},\ldots,X_{k}\}$ be a supermartingale relative to
$$
\{\mathscr{F}_{1},\ldots,\mathscr{F}_{k}\}.
$$ 
If $\tau$ and $\sigma$
are stopping times relative to
$\{\mathscr{F}_{1},\ldots,\mathscr{F}_{k}\}$ taking values in
$\{1,2,\ldots,k\}$ such that $\tau\leq \sigma$ then 
$$
\int\limits_{A}X_{\tau}dP\geq \int\limits_{A}X_{\sigma}dP\q\text{for
  every}\q A\in \mathscr{F}_{\tau}. 
$$
\end{lemma*}

\begin{proof}
Assume\pageoriginale 
first that $\sigma-\tau\leq 1$. Then
\begin{align*}
\int\limits_{A}(X_{\tau}-X_{\sigma})dP &=
\sum\limits^{k}_{j=1}\int\limits_{[A\cap (\tau=j)\cap
    (\tau<\sigma)]}(X_{\tau}-X_{\sigma})dP\\
&= \sum\limits^{k}_{j=1}\int\limits_{[A\cap (\tau=j)]}(X_{j}-X_{j+1})dP
\end{align*}
$A\in \mathscr{F}_{i}$. Therefore $A\cap (\tau=j)\in
\mathscr{F}_{j}$. By supermartingale property
$$
\int\limits_{[A\cap (\tau=j)]}(X_{j}-X_{j+1})dP\geq 0.
$$

Therefore
$$
\int\limits_{A}(X_{\tau}-X_{\sigma})dP\geq 0.
$$

Consider now the general case $\tau\leq \sigma$. Define
$\tau_{n}=\sigma\wedge (\tau +n)$. Therefore $\tau_{n}\geq
\tau$. $\tau_{n}$ is a stopping time taking values in
$\{1,2,\ldots,k\}$, 
$$
\tau_{n+1}\geq\tau_{n},\q \tau_{n+1}-\tau_{n}\leq 1,\q
\tau_{k}=\sigma.
$$

Therefore $\int\limits_{A}X_{\tau_{n}}dP\geq
\int\limits_{A}X_{\tau_{n+1}}dP$, $\forall A\in
\mathscr{F}_{\tau_{n}}$. If $A\in \mathscr{F}_{\tau}$ then $A\in
\mathscr{F}_{\tau_{n}}$, $\forall n$. Therefore
$$
\int\limits_{A}X_{\tau_{1}}dP\geq \int\limits_{A}X_{\tau_{2}}dP\geq
\ldots \geq \int\limits_{A}X_{\tau_{k}}dP,\q \forall A\in
\mathscr{F}_{\tau}. 
$$

Now $\tau_{1}-\tau\leq 1$. $\tau\leq \tau_{1}$. Therefore
$$
\int\limits_{A}X_{\tau}dP\geq \int\limits_{A}X_{\tau_{1}}dP\geq
\int\limits_{A}X_{\sigma}dP. 
$$

This completes the proof.
\end{proof}

\noindent
{\bf N.B.}~The\pageoriginale equality in (*) follows by applying the
argument to 
$$
\{-X_{1},\ldots,-X_{k}\}.
$$

\begin{corollary}
Let $\{X_{1},X_{2},\ldots,X_{k}\}$ be a super-martingale relative to
$$
\{\mathscr{F}_{1},\ldots,\mathscr{F}_{k}\}.
$$ 
If $\tau$ is any
stopping time, then
$$
E(X_{k})\leq E(X_{\tau})\leq E(X_{1}).
$$
\end{corollary}

\begin{proof}
Follows from the fact that $\{X_{1},X_{\tau},X_{k}\}$ is a
supermartingale relative to
$\{\mathscr{F}_{1},\mathscr{F}_{\tau},\mathscr{F}_{k}\}$. 
\end{proof}

\begin{corollary}
If $\{X_{1},X_{2},\ldots,X_{k}\}$ is a super-martingale relative to
$$
\{\mathscr{F}_{1},\ldots,\mathscr{F}_{k}\}
$$ 
and $\tau$ is any
stopping time, then
$$
E(X_{\tau})\leq E(|X_{1}|)+2E(X^{-}_{k})\leq 3\sup\limits_{1\leq n\leq
  k}E(|X_{n}|) 
$$
where for any real $x$, $x^{-}=\dfrac{|x|-x}{2}$. 
\end{corollary}

\begin{proof}
$X^{-}_{k}=\dfrac{|X_{k}|-X_{k}}{2}$, so
  $2E(X^{-}_{k})=E(|X_{k}|)-E(X_{k})$. 

By theorem $\{X_{\tau}\wedge 0,X_{k}\wedge 0\}$ is a super-martingale
relative to $\{\mathscr{F}_{\tau},\mathscr{F}_{k}\}$. Therefore
$E(X_{k}\wedge 0|\mathscr{F}_{\tau})\leq E(X_{\tau}\wedge 0)$. Hence
$$
E(X^{-}_{k})\geq E(X^{-}_{\tau})=\frac{E(|X_{\tau}|)-E(X_{\tau})}{2}. 
$$

Therefore
\begin{align*}
E(|X_{\tau}|) &\leq 2E(X^{-}_{k})+E(X_{\tau})\\
&\leq 2E(X^{-}_{k})+E(X_{1})\leq 3\sup\limits_{1\leq n\leq k}E(|X_{n}|).
\end{align*}
\end{proof}

\begin{theorem*}
Let $(\Omega,\mathscr{F},P)$ be a probability space and
$(\mathscr{F}_{t})_{t\geq 0}$ on increasing family of
sub-$\sigma$-algebras of $\mathscr{F}$. Let $\tau$ be a finite
stopping\pageoriginale time, and $(X_{t})_{t\geq 0}$ a progressively
measurable family (i.e. $X:[0,\infty)\times \Omega\to \mathbb{R}$
  defined by $X(t,w)=X_{t}(w)$ is progressively measurable). If
  $X_{\tau}(w)=X_{\tau(w)}(w)$, then $X_{\tau}$ is
  $\mathscr{F}_{\tau}$-measurable. 
\end{theorem*}

\begin{proof}
We show that $\{w:X(\tau(w),w)\leq t, \tau(w)\leq s\}\in
\mathscr{F}_{t}$ for every $t$. Let $\Omega_{s}=\{w:\tau(w)\leq s\}$;
$\Omega_{s}\in \mathscr{F}_{s}$ and hence the $\sigma$-algebra induced
by $\mathscr{F}_{s}$ on $\Omega_{s}$ is precisely
$$
\{A\cap \Omega_{s}:A\in \mathscr{F}_{s}\}=\{A\in
\mathscr{F}_{s}:A\subset \Omega_{s}\}.
$$

Since $\tau(w)$ is measurable,
$$
w\to (\tau(w),w)\q\text{of}\q \Omega_{s}\to [0,s]\times \Omega_{s}
$$
is $(\mathscr{F}_{s},\mathscr{B}[0,s]\times
\mathscr{F}_{s})$-measurable. Since $X$ is progressively measurable,
$$
[0,s]\times \Omega_{s}\xrightarrow{X}\mathbb{R}\q \text{is
  measurable.}
$$

Therefore $\{w:X(\tau(w),w)\leq t,\tau(w)\leq s\}\in \sigma$-algebra
on $\Omega_{s}$. Therefore $X_{\tau}$ is $\mathscr{F}_{\tau}$
measurable.

The next theorem gives a condition under which $(X_{t})_{t\geq 0}$ is
progressively measurable.
\end{proof}

\begin{theorem*}
If $X_{t}$ is right continuous in $t$, $\forall w$ and $X_{t}$ is
$\mathscr{F}_{t}$-measurable, $\forall t\geq 0$ then $(X_{t})_{t\geq
  0}$ is progressively measurable.
\end{theorem*}

\begin{proof}
Define
$$
X_{n}(t,w)=X\left(\frac{[nt]+1}{n},w\right)\cdot
\frac{[nt]+1}{n}\downarrow t.
$$

Then
$$
\Lt\limits_{n\to \infty}X_{n}(t,w)=X(t,w)\q \text{(by right continuity)}
$$\pageoriginale

\setcounter{step}{0}
\begin{step}%1
Suppose $T$ is rational, $T=m/n$ where $m\geq 0$ is an integer. Then
\begin{gather*}
\{(t,w):0\leq t<T,\ X_{n}(t,w)\leq \alpha\}\\
=\bigcup\limits_{0\leq i\leq m-1}\left\{\Big[\frac{i}{n},\frac{i+1}{n}\Big)X\frac{X^{-1}_{i+1}}{n}(-\infty,\alpha]\right\}
\end{gather*}

Thus if $T=m/n$, $X_{n}|_{[0,T)\times \Omega}$ is
  $\mathscr{B}[0,T]\times \mathscr{F}_{T}$-measurable. Now
  $T=\dfrac{km}{kn}$. Letting $k\to \infty$, by right continuity of
  $X(t)$ one gets $X|_{[0,)\times \Omega}$ is $[0,T]\times
    \mathscr{F}_{T}$-measurable. As $X(T)$ is
    $\mathscr{F}_{T}$-measurable, one gets $X|_{[0,T]\times\Omega}$ is
    $[0,T]\times \mathscr{F}_{T}$-measurable. 
\end{step}

\begin{step}%2
Let $T$ be irrational. Choose a sequence of rationals $S_{n}$
increasing to $T$.
\begin{align*}
& \{(t,w):0\leq t\leq T,\ X(t,w)\leq \alpha\}\\
& =\bigcup\limits^{\infty}_{n=1}\{(t,w):0\leq t\leq S_{n}, X(t,w)\leq
  \alpha\}\cup \{T\}\times X^{-1}_{T}(-\infty,\alpha]
\end{align*}

The countable union is in $\mathscr{B}[0,T]\times \mathscr{F}_{T}$ by
Step 1. The second member is also in $\mathscr{B}[0,T]\times
\mathscr{F}_{T}$ as $X(T)$ is $\mathscr{F}_{T}$-measurable. Thus
$X|_{[0,T]\times \Omega}$ is $\mathscr{B}_{[0,T]}\times
\mathscr{F}_{T}$-measurable when $T$ is irrational also.
\end{step}
\end{proof}

\begin{remark*}
The technique used above is similar to the one used for proving that a
right continuous function $f:\mathbb{R}\to \mathbb{R}$ is Borel measurable.
\end{remark*}

\begin{theorem*}
Let\pageoriginale $\{X_{1},\ldots,X_{k}\}$ be a supermartingale and
$\lambda\geq 0$. Then
\begin{itemize}
\item[\rm(1)] \begin{tabbing}
$\lambda P(\sup\limits_{1\leq n\leq k}X_{n}\geq \lambda)$ \=$\leq
  E(X_{1})-\int\limits_{\left\{\substack{\sup X_{n}<\lambda\\ 1\leq
      n\leq k}\right\}}X_{k}dP$\\[7pt]
\>$\leq E(X_{1})+E(X^{-}_{k})$. 
\end{tabbing}

\item[\rm(2)] \begin{tabbing}
$\lambda P(\inf\limits_{1\leq n\leq k}X_{n}\leq -\lambda)$ \=$\leq
  -\int\limits_{\{\inf X_{n}\leq -\lambda\}}X_{k}dP$\\[5pt]
\>$\leq E(X^{-}_{k})$.
\end{tabbing}
\end{itemize}
\end{theorem*}

\begin{proof}
Define 
\begin{align*}
\tau(w) &=\inf\{n:X_{n}\geq \lambda\}\q\text{if}\q\sup X_{n}\geq
\lambda,\\
& =k,\q\text{if}\q \sup\limits_{n}X_{n}<\lambda.
\end{align*}

Clearly $\tau\geq 0$ and $\tau$ is a stopping time. If $\tau<k$, then
$X_{\tau}(w)\geq\lambda$ for each $w$.
\begin{align*}
E(X_{\tau}) &= \int\limits_{(\sup X_{n}\geq
  \lambda)}X_{\tau}dP+\int\limits_{(\sup X_{n}<\lambda)}X_{\tau}dP\\
&\geq \lambda P(\sup X_{n}\geq \lambda)+\int\limits_{(\sup
  X_{n}<\lambda)}X_{k}dP.  
\end{align*}

Therefore
\begin{align*}
& E(X_{1})\geq \lambda P(\sup X_{n}\geq \lambda)+\int\limits_{(\sup
    X_{n}<\lambda)}X_{k}dP,\\ 
&\lambda P(\sup X_{n}\geq \lambda)\leq E(X_{1})-\int\limits_{(\sup
    X_{n}<\lambda)}X_{k}dP\leq E(X_{1})+E(X^{-}_{k})
\end{align*}

The proof of (2) is similar if we define
$$
\tau(w)=
\begin{cases}
\inf \{n:X_{n}\leq -\lambda\}, &\text{if~ } \inf X_{n}\leq -\lambda,\\
k, & \text{if}\q \inf X_{n}>-\lambda.
\end{cases}
$$
\end{proof}

\noindent
{\bf Kolmogorov's Inequality} (Discrete Case). {\em Let\pageoriginale
$\{X_{1},\ldots,X_{k}\}$ be a finite sequence of independent random
  variables with mean $0$. Then}
$$
P\left(\sup\limits_{1\leq n\leq k}(|X_{1}+\cdots+X_{n}|\geq
\lambda)\leq \frac{1}{\lambda^{2}}E((X_{1}+X_{2}+\cdots+X_{k})^{2})\right)
$$

\begin{proof}
If $S_{n}=X_{1}+\cdots+X_{n}$, $n=1,2,\ldots,k$, then
$\{S_{1},\ldots,S_{k}\}$ is a martingale with respect to
$\{\mathscr{F}_{1},\ldots,\mathscr{F}_{k}\}$ where
$\mathscr{F}_{n}=\sigma\{X_{1},\ldots,X_{n}\}$. Therefore
$S^{2}_{1},\ldots,S^{2}_{k}$ is a submartingale (since $x\to x^{2}$ is
convex). By the previous theorem,
$$
\lambda^{2}P\{\inf -S^{2}_{n}\leq -\lambda^{2}\}\leq
E((-S^{2}_{K})^{-})
$$

Therefore
\begin{align*}
& P\{\sup |S_{n}|\geq \lambda\} \leq
  \frac{E((-S^{2}_{k})^{-})}{\lambda^{2}}=\frac{E(S^{2}_{k})}{\lambda^{2}}\\
&\qq =\frac{1}{\lambda^{2}}E((X_{1}+X_{2}+\cdots+X_{k})^{2}).
\end{align*}
\end{proof}

\noindent
{\bf Kolmogorov's Inequality} (Continuous case). {\em Let $\{X(t):t\geq
0\}$ be a continuous martingale with $E(X(0))=0$. If $0<T<\infty$,
then for any $\epsilon>0$}
$$
P\Big\{w:\sup\limits_{0\leq s\leq T}|X(s,w)|\geq \epsilon\Big)\leq
\frac{1}{\epsilon^{1}}E((X(T))^{2}). 
$$

\begin{proof}
For any positive integer $k$ define $Y_{0}=X(0)$,
\begin{align*}
Y_{1}
&=X\left(\frac{T}{2^{k}}\right)-X(0),\ Y_{2}=X\left(\frac{2T}{2^{k}}\right)-X\left(\frac{T}{2^{k}}\right),\ldots,Y_{2}k\\
&=X\left(\frac{2^{k}T}{2^{k}}\right)-X\left(\frac{(2^{k}-1)}{2^{k}}T\right). 
\end{align*}

By Kolmogorov inequality for the discrete case, for any $\delta>0$.
$$
P\left(\sup\limits_{0\leq n\leq
  2^{k}}|X\left(\frac{nT}{2^{k}}\right)|>\delta\right)\leq
\frac{1}{\delta^{2}}E((X(T))^{2}). 
$$

By\pageoriginale continuity of $X(t)$, $A_{k}=\{w:\sup\limits_{0\leq
  n\leq 2^{k}}|X\left(\dfrac{nT}{2^{k}}\right)|>\delta\}$ increases to
$\{\sup\limits_{0\leq s\leq T}|X(s)|>\delta\}$ so that one gets
\begin{equation*}
P\left(\sup\limits_{0\leq s\leq T}|X(s)|>\delta\right)\leq
\frac{1}{\delta^{2}}E((X(T))^{2}).\tag{1} 
\end{equation*}

Now 
\begin{align*}
P\left(\sup\limits_{0\leq s\leq T}|X(s)|\geq \epsilon\right) &\leq
\limit\limits_{m\to \infty}P\left(\sup\limits_{0\leq s\leq
  T}|X(s)|>\epsilon-\frac{1}{m}\right)\\[5pt] 
&\leq \limit\limits_{m\to
  \infty}\frac{1}{(\epsilon-1/m)^{2}}E((X(T))^{2}),\q \text{by
  (1).}\\[5pt]
&= 1/\epsilon^{2}E((X(T))^{2}).
\end{align*}

This completes the proof.
\end{proof}

\noindent
{\bf Optional Sampling Theorem} (Countable case). {\em Let $\{X_{n}:n\geq
1\}$ be a supermartingale relative to $\{\mathscr{F}_{n}:n\geq
1\}$. Assume that for some $X_{\infty}\in L^{1}$, $X_{n}\geq
E(X_{\infty}|\mathscr{F}_{n})$. Let $\sigma$, $\tau$ be stopping times
taking values in $N\cup \{\infty\}$, with $\sigma\leq \tau$. Define
$X_{\tau}=X_{\infty}$ on $\{\sigma=\infty\}$ and $X_{\tau}=X_{\infty}$
on $\{\sigma=\infty\}$. Then $E(X_{\tau}|\mathscr{F}_{\sigma})\leq
X_{\sigma}$.}

\begin{proof}
We prove the theorem in three steps.

\setcounter{step}{0}
\begin{step}%1
Let $X_{\infty}=0$ so that $X_{n}\geq 0$. Let $\tau_{k}=\tau\wedge k$,
$\sigma_{k}=\tau\wedge k$. By optional sampling theorem for discrete
case $E(X_{\tau_{k}})\leq E(X_{k})\leq E(X_{1})$. By Fatou's lemma,
$E(X_{\tau})<\infty$. Again by optional sampling theorem for the
discrete case,
$$
E(X_{\tau_{k}}|\mathscr{F}_{\sigma_{k}})\leq X_{\sigma_{k}}\ldots,(0).
$$

Let\pageoriginale $A\in \mathscr{F}_{\sigma}$. Then $A\cap
\{\sigma\leq k\}\in \mathscr{F}_{\sigma_{k}}$, and by (0)
$$
 \int\limits_{A\cap\{\tau\leq k\}}X_{\tau}dP\leq \int\limits_{A\cap
  (\tau\leq k)}X_{\tau_{k}}dP\leq \int\limits_{A\cap (\sigma\leq
  k)}X_{\sigma_{k}}dP\leq \int\limits_{A\cap (\sigma\leq
  k)}X_{\sigma}dP. 
$$

Letting $k\to \infty$,
\begin{equation*}
\int\limits_{A\cap (\tau \neq \infty)}X_{\tau}dP\leq
\int\limits_{A\cap (\sigma\neq \infty)}X_{\sigma}dP.\tag{1}
\end{equation*}

Clearly
\begin{equation*}
\int\limits_{A\cap
  (\tau=\infty)}X_{\tau}dP=\int\limits_{A}X_{\infty}dP=\int\limits_{A\cap
  (\sigma=\infty)}X_{\sigma}dP\tag{2} 
\end{equation*}

By (1) and (2), $\inf\limits_{A}X\ dP\leq\int\limits_{A}X_{\sigma}dP$,
proving that
$$
E(X_{\tau}|\mathscr{F}_{\sigma})\leq X_{\sigma}.
$$
\end{step}

\begin{step}%2
Suppose $X_{n}=E(X_{\infty}|\mathscr{F}_{n})$. In this case we show
that $X_{\tau}=E(X_{\infty}|\mathscr{F}_{\tau})$ for every stopping
time so that $E(X_{\tau}|\mathscr{F}_{\sigma})=X_{\sigma}$. If $A\in
\mathscr{F}_{\tau}$, then
$$
\int\limits_{(\tau\leq k)}X_{\tau}dP=\int\limits_{A\cap (\tau \leq
  K)}X_{\infty}dP\q\text{for every}\q k.
$$

Letting $k\to \infty$,
\begin{align*}
& \int\limits_{A\cap (\tau \neq \infty)}X_{\tau}dP=\int\limits_{A\cap
    (\tau\neq \infty)}X_{\infty}dP,\tag{1}\\
& \int\limits_{A\cap
    (\tau=\infty)}X_{\tau}dP=\int\limits_{A}X_{\infty}dP=\int\limits_{A\cap
    (\tau=\infty)}X_{\infty}dP\tag{2} 
\end{align*}

The assertion follows from (1) and (2).
\end{step}

\begin{step}%3
Let\pageoriginale $X_{n}$ be general. Then
$$
X_{n}=X_{n}-E(X_{\infty}|\mathscr{F}_{n})+E(X_{\infty}|\mathscr{F}_{n}).
$$

Apply Step (1) to $Y_{n}=X_{n}-E(X_{\infty}|\mathscr{F}_{n})$ and Step
(2) to 
$$
Z_{n}=E(X_{\infty}|\mathscr{F}_{n})
$$ 
to complete the proof.
\end{step}
\end{proof}

\noindent
{\bf Uniform Integrability.}

\addcontentsline{toc}{chapter}{\quad Uniform Integrability}
\begin{defi*}
Let $(\Omega,\mathscr{B},P)$ be any probability space,
$L^{1}=L^{1}(\Omega,\mathscr{B},P)$. A family $H\subset L^{1}$ is
called {\em uniformly integrable} if for every $\epsilon>0$ there
exists a $\delta>0$ such that
$\int\limits_{(|X|\geq\delta)}|X|dP<\epsilon$ for all $X$ in $H$.
\end{defi*}

\begin{note*}
Every uniformly integrable family is a bounded family.
\end{note*}

\begin{prop*}
Let $X_{n}$ be a sequence in $L^{1}$ and let $X_{n}\to X$ a.e. Then
$X_{n}\to X$ in $L^{1}$ iff $\{X_{n}:n\geq 1\}$ is uniformly integrable.
\end{prop*}

\begin{proof}
is left as an exercise.
\end{proof}

As $\{X_{n}:n\geq 1\}$ is a bounded family, by Fatou's lemma $X\in
L^{1}$. Let $\epsilon>0$ be given. By Egoroff's theorem there exists a
set $F$ such that $P(F)<\epsilon$ and $X_{n}\to X$ uniformly on $F$.
\begin{align*}
& \int |X_{n}-X|dP \leq
  ||X_{n}-X||_{\infty,\Omega-F^{+}}\int\limits_{F}|X_{n}-X|dP\\ 
&\qq\q \leq
  ||X_{n}-X||_{\infty,\Omega-F}+\int\limits_{F}|X_{n}|dP+\int\limits_{F}|X|dP\\ 
&\leq ||X_{n}-X||_{\infty,\Omega-F}+\int\limits_{F\cap (|X_{n}|\geq
    \delta)}|X_{n}|dP+\int\limits_{F\cap (|X|\geq \delta)}|X|dP+\\
&\qq\qq +\int\limits_{F\cap \{|X_{n}|\leq
    \delta\}}|X_{n}|dP+\int\limits_{F\cap (|X|\leq \delta)}Xd P\\
&\leq ||X_{n}-X||_{\infty,\Omega-F}+\int\limits_{(|X_{n}|\geq
    \delta)}|X_{n}|dP+\int\limits_{(|X|\geq
    \delta)}|X|dP+2\delta\epsilon 
\end{align*}\pageoriginale

The result follows by uniform integrability of $\{X,X_{n}:n\geq 1\}$. 

\begin{coro*}
Let $\mathscr{C}$ be any sub-$\sigma$-algebra of $\mathscr{B}$. If
$X_{n}\to X$ a.e.\@ and $X_{n}$ is uniformly integrable, then
$E(X_{n}|\mathscr{C})\to E(X|\mathscr{C})$ in
$L^{1}(\Omega,\mathscr{C},P)$. 
\end{coro*}

\begin{prop*}
Let $H\subset L^{1}$. Suppose there exists an increasing convex
function $G:[0,\infty)\to [0,\infty)$ such that 
$$
\limit\limits_{t\to
      \infty}\dfrac{G(t)}{t}=\infty\q\text{and}\q\sup\limits_{X\in
      H}E(G(|X|))<\infty.
$$ 
Then the family $H$ is uniformly integrable.
\end{prop*}

\begin{example*}
$G(t)=t^{2}$ is a function satisfying the conditions of the theorem.
\end{example*}

\begin{proof}
(of the proposition). Let
$$
M=\sup\limits_{X\in H}E(G(|X|)).
$$

Let $\epsilon>0$ be given. Choose $\delta>0$ such that
$$
\frac{G(t)}{t}\geq \frac{M}{\epsilon}\q\text{for}\q t\geq \delta.
$$

Then for $X$ in $H$
$$
\int\limits_{(|X|\geq \delta)}|X|dP\leq
\frac{\epsilon}{M}\int\limits_{(|X|\geq \delta)}G(|X|)dP\leq
\frac{\epsilon}{M}\int\limits_{G}G(|X|)dP\leq \epsilon
$$
\end{proof}

\begin{remark*}
The converse of the theorem is also true.
\end{remark*}

\begin{exer*}
Let $H$ be a bounded set in $L^{\infty}$, i.e.\@ there exists a
constant $M$ such that $||X||_{\infty}\leq M$ for all $X$ in $H$. Then
$H$ is uniformly integrable.
\end{exer*}

\noindent
{\bf Up Crossings and Down Crossings.}\pageoriginale

\addcontentsline{toc}{chapter}{\quad Up Crossings and Down Crossings}

\begin{defi*}
Let $a<b$ be real numbers; let $s_{1}$, $s_{2},\ldots,s_{k}$ be also
given reals. Define $i_{1},i_{2},\ldots,i_{k}$ as follows.
\begin{align*}
i_{1} &= 
\begin{cases}
\inf \{n:s_{n}<a\},\\
k, \text{~ if no~ } s_{i}<a;
\end{cases}\\[5pt]
i_{2} &= 
\begin{cases}
\inf \{n>i_{1}:s_{n}>b\},\\
k,\text{~ if~ } s_{n}\leq b\text{~ for each~ } n>i_{1};
\end{cases}\\[5pt]
i_{3} &= 
\begin{cases}
\inf \{n>i_{2}:s_{n}<a\},\\
k,\text{~ if~ } s_{n}\geq a\text{~ for each~ } n>i_{2};
\end{cases}
\end{align*}
and so on

Let $t_{1}=s_{i_{1}}$, $t_{2}=s_{i_{2}},\ldots$. If $(t_{1},t_{2})$,
$(t_{3},t_{4}),\ldots$, $(t_{2p-1},t_{2p})$ are the only non-empty
intervals and $(t_{2p+1},t_{2p+2}),\ldots$ are all empty, then $p$ is
called the ******** of the sequence $s_{1},\ldots,s_{k}$ for the
interval $[a,b]$ and is denoted by $U(s_{1},\ldots,s_{k};[a,b])$.
\end{defi*}

\begin{note*}
$U$ (the up crossing) always takes values in $\{0,1,2,3,\ldots\}$.
\end{note*}

\begin{defi*}
For any subset $S$ of reals define
$$
U(S;[a,b])=\sup \{U(F;[a,b]):F\text{~ is a finite subset of~ } S\}
$$

The number of down crossings is defined by
$$
D(S;[a,b])=U(-S;[-b,-a]).
$$

For any real valued function $f$ on any set $S$ we define
$$
U(f,S,[a,b])=U(f(S),[a,b]).
$$

If the domain of $S$ is known, we usually suppress it.
\end{defi*}


\begin{prop*}
Let\pageoriginale $a_{1},a_{2},\ldots$ be any sequence of real numbers
and $S=\{a_{1},a_{2},\ldots\}$. If $U(S,[a,b])<\infty$ for all $a<b$,
then these sequence $\{a_{n}\}$ is a convergent sequence.
\end{prop*}

\begin{proof}
It is clear that if $T\subset S$ then $U(T,[a,b])\leq U(S,[a,b])$. If
the sequence were not convergent, then we can find $a$ and $b$ such
that $\lim\inf a_{n}<a<b<\lim\sup a_{n}$. Choose
$n_{1}<n_{2}<n_{3}\ldots$; $m_{1}<m_{2}<\ldots$ such that
$a_{n_{i}}<a$ and $a_{m_{i}}>b$ for all $i$. If
$T=\{a_{n_{1}},a_{m_{1}},a_{n_{2}},a_{m_{2}},\ldots\}$, then
$U(S;[a,b])\geq U(T;[a,b])=\infty$; a contradiction.
\end{proof}

\begin{remark*}
The converse of the proposition is also true.
\end{remark*}

\begin{theorem*}
(Doob's inequalities for up crossings and down crossings). Let
  $\{X_{1},\ldots,X_{k}\}$ be a submartingale relative to
  $\{\mathscr{F}_{1},\ldots,\mathscr{F}_{k}\}\,a<b$. Define
  $U(w,[a,b])=U(X_{1}(w),\ldots,X_{k}(w);[a, b])$ and similarly
  define $D(w,[a,b])$. Then
\begin{itemize}
\item[\rm(i)] $U$, $D$ are measurable functions;

\item[\rm(ii)] $E(U(\cdot,[a,b]))\leq
  \dfrac{E((X_{k}-a)+1)-E((X_{1}-a)^{+})}{b-a}$; 

\item[\rm(iii)] $E(D(\cdot,[a\cdot b]))\leq E((X_{k}-b)^{+})/(b-a)$.
\end{itemize}
\end{theorem*}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item is left as an exercise.

\item Define $Y_{n}=(X_{n}-a)^{+}$; there are submartingales. Then
  clearly $Y_{n}\leq 0$ if and only if $X_{n}\leq a$ and $Y_{n}\geq
  b-a$ iff $X_{n}\geq b$, so that
$$
UY_{1}(w),\ldots,Y_{k}(w);[0,b-a])=U(X_{1}(w),\ldots,X_{k}(w);[a,b])
$$\pageoriginale

Define
\begin{align*}
\tau_{1} &= 1\\[3pt]
\tau_{2} &= 
\begin{cases}
\inf \{n:Y_{n}=0\}\\
k,\text{~ if each~ } Y_{n}=0
\end{cases}\\[5pt]
\tau_{3} &=
\begin{cases}
\inf \{n>\tau_{2}:Y_{n}>b-a,\\
k,\text{~ if~ } Y_{n}<b-a\text{~ for each~ } n>\tau_{2};
\end{cases}\\[5pt]
\tau_{k+1} &= k.
\end{align*}
As $\{Y_{1},\ldots,Y_{k}\}$ is a submartingale, by optional sampling
theorem $Y_{\tau_{1}},\ldots,Y_{\tau_{k+1}}$ is also a
submartingale. Thus
\begin{equation*}
E(Y_{\tau_{2}}-Y_{\tau_{1}})+E(Y_{\tau_{4}}-Y_{\tau_{3}})+\cdots\geq 0.\tag{1}
\end{equation*}
Clearly
\begin{gather*}
[(Y_{\tau_{3}}-Y_{\tau_{2}})+(Y_{\tau_{5}}-Y_{\tau_{4}})+\cdots](w)\geq
(b-a)\cup (Y_{1}(w),\ldots Y_{k}(w);\\
[0,b-a])=(b-a)\cup (w,[a,b]).
\end{gather*}
Therefore
\begin{equation*}
E(Y_{\tau_{3}}-Y_{\tau_{2}})+E(Y_{\tau_{5}}-Y_{\tau_{4}})+\cdots\geq
(b-a)E(U(\cdot,[a,b])).\tag{2} 
\end{equation*}


By (1) and (2),
$$
E(Y_{k}-Y_{1})\geq (b-a)E(U(\cdot,[a,b]))
$$
giving the result.

\item Let $Y_{n}=(X_{n}-a)^{+}$ so that
$$
D(Y_{1}(w),\ldots Y_{k}(w);[0,b-a])=D(X_{1}(w),\ldots,X_{k}(w);[a,b])
$$

Define\pageoriginale
\begin{align*}
\tau_{1} &= 1;\\[3pt]
\tau_{2} &=
\begin{cases}
\inf \{n:Y_{n}\geq b-a\},\\
k,\text{~ if each~ } Y_{n}<b-a;
\end{cases}\\[5pt]
\tau_{3} &=
\begin{cases}
\inf \{n>\tau_{2}:Y_{n}=0\},\\
k,\text{~ if each~ }Y_{n}>0\text{~ for each~ }n>\tau_{2};
\end{cases}\\[5pt]
\tau_{k+1} &= k.
\end{align*}
By optional sampling theorem we get
$$
0\geq E(Y_{\tau_{2}}-Y_{\tau_{3}})+E(Y_{\tau_{4}}-Y_{\tau_{5}})+\cdots.
$$
Therefore
$$
0\geq (b-a)E(D(Y_{1},\ldots,Y_{k};[0,b-a]))+E((b-a)-Y_{k}).
$$
Hence
\begin{align*}
E(D(\cdot,[a,b])) &\leq E((X_{k}-a)^{+}-(b-a))/(b-a)\\[5pt]
&\leq \frac{E((X_{k}-b)^{+})}{(b-a)},\text{~ for~ }
(c-a)^{+}-(b-a)\leq (c-b)^{+}\\
&\qquad\text{for all~ } c.
\end{align*}
\end{enumerate}
\end{proof}

\begin{coro*}
Let $\{X_{1},\ldots,X_{k}\}$ be a supermartingale. $U$, $D$ as in
theorem. Then
\begin{itemize}
\item[\rm(i)] $E(D(\cdot,[a,b]))\leq \dfrac{E(X_{1}\wedge
  b)-E(X_{k}\wedge b)}{b-a}$.

\item[\rm(ii)] $E(U(\cdot,[a,b]))\leq \dfrac{E((X_{k}-b)^{-})}{b-a}$.
\end{itemize}
\end{coro*}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item $E(D(\cdot,[a, b]))=E(U(-X_{1}(w),\ldots,-X_{k}(w),[-b,-a])$
\begin{align*}
&\leq \frac{E((-X_{k}+b)^{+}-(-X_{1}+b)^{+})}{b-a},\q\text{by above
    theorem,}\\[5pt] 
&\leq \frac{E((b\wedge X_{k})-(b\wedge X_{1}))}{b-a},\q \text{since for}
\end{align*}
since for all $a$, $b$, $c$, $(b-c)^{+}-(b-a)^{+}\leq (b\wedge
a)-(b\wedge c)$.

\item \begin{tabbing}
\= $E(U(\cdot,[a,b]))=$\\[5pt]
\> $=E(D(-X_{1}(w),\ldots,-X_{k}(w);[-b,-a]))$\\[5pt]
\> $\leq \dfrac{E((-X_{k}+a)^{+})}{b-a}$,\q by theorem,\\[5pt]
\> $\leq \dfrac{E((X_{k}-b)^{-})}{b-a}$,
\end{tabbing}\pageoriginale

(since $(-X_{k}+a)^{+}\leq (X_{k}-b)^{-}$,
\end{enumerate}
\end{proof}

\begin{theorem*}
Let $\{X_{n}:n=1,2,\ldots\}$ be a supermartingale relative to
$\{\mathscr{F}_{n}:n=1,2,\ldots\}$. Let $(\Omega,\mathscr{F},P)$ be
complete.
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\rm(\theenumi)}
\item If $\sup\limits_{n}E(X^{-}_{n})<\infty$, then $X_{n}$ converges
  a.e.\@ to a random variable denoted by $X_{\infty}$.

\item if $\{X_{n}:n\geq 1\}$ is uniformly integrable, then also
  $X_{\infty}$ exists. Further, $\{X_{n}:n=1,2,\ldots,n=\infty\}$ is a
  supermartingale with the natural order.

\item if $\{X_{n}:n\geq 1\}$ is a martingale, then $\{X_{n}:n\geq
  1,n=\infty\}$ is a martingale.
\end{enumerate}
\end{theorem*}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item Let $U(w[a,b])=U(X_{1}(w),X_{2}(w),\ldots,[a,b])$. By the
  coro\-llary to Doob's inequalities theorem,
$$
E(U(\cdot,[a,b])\leq \sup\limits_{n}E((X_{n}-b)^{-})<\infty
$$\pageoriginale
for all $a<b$. Allowing $a$, $b$ to vary over the rationals alone we
find that the sequence $X_{n}$ is convergent a.e.

\item $\Sup\limits_{n}E(X^{-}_{n})\leq
  \sup\limits_{n}E(|X_{n}|)<\infty$ so that $X_{\infty}$ exists. As
  $X_{n}\to X_{\infty}$ in $L^{1}$ we get that $\{X_{n}:n\geq
  1,n=\infty\}$ is a supermartingale.

\item follows from (ii).
\end{enumerate}
\end{proof}

\begin{prop*}
Let $\{X_{t}:t\geq 0\}$ be a supermartingale relative to
$\{\mathscr{F}_{t}:t\geq 0\}$. $I=[r,s]$, $a<b$ and $S$ any countable
dense subset. Let $U(w,S\cap I,[a,b])=U(\cdot,\{X_{t}(w):t\in S\cap
I\},[a,b])$. Then
$$
E(U(\cdot,S\cap I,[a,b))\leq \frac{E((X_{s}-b)^{-})}{b-a}.
$$
\end{prop*}

\begin{proof}
Let $S\cap I$ be an increasing union of finite sets $F_{n}$: then
$$
E(U(\cdot,F_{n},[a,b]))\leq 
\frac{E((X_{\max F_{n}}-b)^{-})}{b-a}\leq
\frac{E((X_{s}-b)^{-})}{b-a}. 
$$

The result follows by Fatou's lemma.
\end{proof}

\begin{exer*}
If further $X_{t}$ is continuous i.e.\@ $t\to X_{t}(w)$ is continuous
for each $w$, then prove that
$$
E(U(\cdot,I,[a,b]))\leq \frac{E((X_{s}-b)^{-})}{b-a}
$$
\end{exer*}

\begin{theorem*}
Let $(\Omega,\mathscr{F},P)$ be complete and $\{X_{t}:t\geq 0\}$ a
continuous supermartingale.
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\rm(\theenumi)}
\item If\pageoriginale $\sup\limits_{t\geq 0}E(X^{-}_{t})<\infty$, then $X_{t}$
  converges a.e.\@ to a random variable $X_{\infty}$.

\item If $\{X_{t}:t\geq 0\}$ is uniformly integrable then also
  $X_{\infty}$ exists and $\{X_{t}:t\geq 0,t=\infty\}$ is a supermartingale.
\end{enumerate}
\end{theorem*}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item $E(U(\cdot,[0,n],[a,b]))\leq E((X_{n}-b)^{-})/(b-a)$ so that
$$
\limit\limits_{n\to \infty}E(U(\cdot,[0,n],[a,b]))\leq
\sup\limits_{0\leq s}\frac{E((X_{s}-b)^{-})}{b-a}
$$
for all $a<b$. Thus $\{X_{t}(w):t>0\}$ converges a.e.\@ whose limit in
denoted by $X_{\infty}$ which is measurable.

\item As $E(X^{-}_{t})\leq E(|X_{t}|)$ by (i) $X_{\infty}$ exists, the
  other assertion is a consequence of uniform integrability.
\end{enumerate}
\end{proof}

\begin{coro*}
Let $\{X_{t}:t\geq 0\}$ be a continuous uniformly integrable
martingale. Then $\{X_{t}:0\leq t\leq \infty\}$ is also a martingale.
\end{coro*}

\begin{exer*}
Let $\{X_{t}:t\geq 0\}$ be a continuous martingale such that for some
$Y$ with $0\leq Y\leq 1$ $E(Y|\mathscr{F}_{t})=X_{t}$ show that
$X_{t}\to Y$ a.e.
\end{exer*}

\begin{lemma*}
Let $(\Omega,\mathscr{F},P)$ be a probability space,
$\mathscr{F}_{1}\supset \mathscr{F}_{2}\supset \mathscr{F}_{3}\ldots$
be sub-$\sigma$-algebras. Let $X_{1},X_{2},\ldots$ be a real valued
functions measurable with respect to
$\mathscr{F}_{1},\ldots,\mathscr{F}_{n},\ldots$ respectively. Let
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\rm(\theenumi)}
\item $E(X_{n-1}|\mathscr{F}_{n})\leq X_{n}$

\item $\sup\limits_{n}E(X_{n})<\infty$.
\end{enumerate}

Then\pageoriginale $\{X_{n}:n\geq 1\}$ is uniformly integrable.
\end{lemma*}

\begin{proof}
By (i) $E(X_{n})$ is increasing. By (ii) given $\epsilon>0$, we can
find $n_{0}$ such that if $n\geq n_{0}$ then $E(X_{n})\leq
E(X_{n_{0}})+\epsilon$. For and $\delta>0$,
\begin{gather*}
n\geq n_{0}\int\limits_{(|X_{n}|\geq \delta)}|X_{n}|dP\\
=E(X_{n})+\int\limits_{(X_{n}\leq
  -\delta)}-X_{n}dP-\int\limits_{(X_{n}<\delta)}X_{n}dP\\
\leq \int\limits_{(X_{n}\leq
  -\delta)}-X_{n_{0}}dP-\int\limits_{(X_{n}<\delta)}X_{n_{0}}dP+E(X_{n})\q\text{by
  (i)}\\ 
\leq \epsilon +\int\limits_{(X_{n}\geq
  \delta)}X_{n_{0}}dP-\int\limits_{(X_{n}\leq
  -\delta)}X_{n_{0}}dP\q(\text{because~ } E(X_{n})\leq
E(X_{n_{0}})+\epsilon)\\
\leq \epsilon+\int\limits_{(|X_{n}|\geq \delta)}|X_{n_{0}}|dP
\end{gather*}

Thus to show uniform integrability we have only to show $P(|X_{n}|\geq
\delta)\to 0$ uniformly in $n$ as $\delta\to \infty$. Now
\begin{align*}
E(|X_{n}|) &= E(X_{n}+2X^{-}_{n})\\
&\leq E(X_{n})+2E(|X_{1}|)\q\text{by~ (i)}\\
&\leq M<\infty\text{~ for all~ } n\text{~ by (ii)}
\end{align*}

The result follows as $P(|X_{n}|\geq \delta)\leq M/\delta$.
\end{proof}

\noindent
{\bf Optional Sampling Theorem.}~ (Continuous case).
\smallskip

{\em Let $\{X_{t}:t\geq 0\}$ be a right continuous supermartingale
  relative to $\{\mathscr{F}_{t}:t\geq 0\}$. Assume there exists an
  $X_{\infty}\in L'(\Omega,\mathscr{F},P)$ such that $X_{t}\geq
  E(X_{\infty}|\mathscr{F}_{t})$ for $t\geq 0$. For any stopping time
  $\tau$ taking\pageoriginale values in $[0,\infty]$, let
  $X_{\tau}=X_{\infty}$ on $\{\tau=\infty\}$. Then}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item {\em $X_{\tau}$ is integrable.}


\item {\em If $\sigma\leq \tau$ are stopping times, then}
$$
E(X_{\tau}|\mathscr{F}_{\sigma})\leq X_{\sigma}.
$$
\end{enumerate}

\begin{proof}
Define
$$
\sigma_{n}=\frac{[2^{n}\sigma]+1}{2^{n}},\q
\tau_{n}=\frac{[2^{n}\sigma]+1}{2^{n}}. 
$$

Then $\sigma_{n}$, $\tau_{n}$ are stopping times, $\sigma_{n}\leq
\tau_{n}$, $\sigma\leq \sigma_{n}$, $\tau\leq \tau_{n}$. $\sigma_{n}$,
$\tau_{n}$ take values in
$D_{n}=\{\infty,0,1/2^{n},2/2^{n},\ldots,1/2^{n},\ldots\}$ so that we
have $E(X_{\tau_{n}}|\mathscr{F}_{\sigma_{n}})\leq
X_{\sigma_{n}}$. Thus if, $A\in \mathscr{F}_{\sigma}\subset
\mathscr{F}_{\sigma_{n}}$, then 
\begin{equation*}
\int\limits_{A}X_{\tau_{n}}dP\leq \int\limits_{A}X_{\sigma_{n}}dP\tag{*}
\end{equation*}

As $\sigma_{1}\geq \sigma_{2}\geq \ldots$, by optional sampling
theorem for the countable case, we have
$$
E(X_{\sigma_{n-1}}|\mathscr{F}_{\sigma_{n}})\leq X_{\sigma_{n}}.
$$

Further
$$
\mathscr{F}_{\sigma_{1}}\supset \mathscr{F}_{\sigma_{2}}\supset
  \ldots;\q E(X_{\sigma_{n}}|\mathscr{F}_{0})\leq X_{0}.
$$

By the lemma $\{X_{\sigma_{n}}\}$, $\{X_{\tau_{n}}\}$ are uniformly
integrable families. By right continuity $X_{\sigma_{n}}\to
X_{\sigma}$ pointwise and $X_{\tau_{n}}\to X_{\tau}$
pointwise. Letting $n\to \infty$ in (*) we get the required result.
\end{proof}

\begin{lemma*}[(Integration by Parts)] 
Let\pageoriginale $M(t,\cdot)$ be a continuous progressively
  measurable martingale and $A(t,w):[0,\infty)\times \Omega\to
    \mathbb{R}$ be of bounded variation for each $w$. Further, assume
    that $A(t,w)$ is $\mathscr{F}_{t}$-measu\-rable for each $t$. Then
$$
Y(t,\cdot)=M(t,\cdot)A(t,\cdot)=\int\limits^{t}_{0}M(s,\cdot)dA(s,\cdot)
$$
is a martingale if
$$
E(\sup\limits_{0\leq s\leq t}|M(s,\cdot)|~||A(\cdot)||_{t})<\infty
$$
for each $t$, where $||A(w)||_{t}$ is the total variation of $A(s,w)$
in $[0,t]$.
\end{lemma*}

\begin{proof}
By hypothesis,
$$
\sum\limits^{n}_{i=0}M(s,\cdot)(A(s_{i+1},\cdot)-A(s_{i},\cdot))
$$
converges to
$$
\int\limits^{t}_{s}M(u,\cdot)dA(u,\cdot)\text{~ in~ }L^{1}\text{~ as~
} n\to \infty
$$
and as the norm of the partition $s=s_{0}<s_{1}<\ldots<s_{n+1}=t$
converges to zero. Hence it is enough to show that
\begin{gather*}
E([M(t,\cdot)A(t,\cdot)-\sum\limits^{n}_{i=0}M(s_{i+1},\cdot)(A(s_{i+1},\cdot)-A(s_{i},\cdot))]|\mathscr{F}_{s})\\ 
=M(s,\cdot)A(s,\cdot).
\end{gather*}

But the left side $=E(M(s_{n+1},\cdot)A(s_{n+1},\cdot)-$
\begin{align*}
&
  -\sum\limits^{n}_{i=0}M(s_{i+1},\cdot)(A(s_{i+1},\cdot)-A(s_{i},\cdot))|\mathscr{F}_{s})\\ 
& = M(s,\cdot)A(s,\cdot).
\end{align*}

Taking\pageoriginale limits as $n\to \infty$ and observing that
$$
\sup\limits_{0\leq i\leq n}|(s_{i+1}-s_{i})|\to 0,
$$
we get
\begin{gather*}
E(M(t,\cdot)A(t,\cdot)-\int\limits^{t}_{0}M(u,\cdot)dA(u,\cdot)|\mathscr{F}_{s})\\
=M(s,\cdot)A(s,\cdot)-\int\limits^{s}_{0}M(u,\cdot)dA(u,\cdot).
\end{gather*}
\end{proof}

\newpage

\addcontentsline{toc}{chapter}{Bibliography}

\begin{thebibliography}{0}
\bibitem{key1} BILLINGSLEY, P.\pageoriginale {\em Convergence of probability
  measures,} John Wiley, New York, (1968).

\bibitem{key2} DOOB, J.L. {\em Stochastic Processes,} John Wiley, New
  York (1952).

\bibitem{key3} DYNKIN, E.B. Markov Processes and Problems in Analysis,
  {\em Proc. Int. Cong of Math. Stockholm,} (1962) 36 - 58.

\bibitem{key4} DYNKIN, E.B. {\em Markov Processes,} Vols. 1, 2,
  Springer-Verlag, Berlin (1965).

\bibitem{key5} FRIEDMAN, AVNER. {\em Stochastic Differential Equations
  and Applications,} vols. 1, 2, Academic Press, New York (1976).

\bibitem{key6} FRIEDMAN, AVNER. {\em Partial Differential Equations of
  Parabolic type}, Prentice Hall, Englewood Cliffs (1964).

\bibitem{key7} GIHMAN, I.I. and A.V. SKOROHOD. {\em Stochastic
  Differential Equations}, Springer-Verlag, New York (1973).

\bibitem{key8} IT\^O, K. On Stochastic Differential Equations, {\em
  Memoirs Amer. Math. Soc. 4} (1951).

\bibitem{key9} IT\^O, K. and H.P. McKEAN, Jr. {\em Diffusion Processes
  and their sample paths,} Springer-Verlag, Berlin (1965).

\bibitem{key10} KUNITA, H. and S. WATANABE. On Square Integrable
  Martingales, {\em Nagoya Math. Journal.} 30 (1967) 209 - 245.

\bibitem{key11} L\'EVY, P. {\em Processus Stochastic et Mouvement
  Brownien,} Gaulhier-Villars, Paris (1948).

\bibitem{key12} MEYER, P.A.\pageoriginale {\em Probability and
  Potentials,} Blaisdell, Wallham (1966).

\bibitem{key13} NEVEU, J. {\em Discrete Parameter Martingales,} North
  Holland, Amsterdam (1975).

\bibitem{key14} PARTHASARATHY, K.R. {\em Probability Measures on
  Metric Spaces,} Academic Press, New York (1967).

\bibitem{key15} PROTTER, M.H. and H.F. WEINBERGET. {\em Maximum
  Principles in Differential Equations,} Printice Hall, Englewood
  Cliffs (1967).

\bibitem{key16} STROOCK, D.W. and S.R.S. VARADHAN. {\em Diffusion
  Process in Several Dimension,} Springer-Verlag, New York (to appear)

\bibitem{key17} WILLIAMS, D. Brownian motions and diffusions as Markov
  Processes. {\em Bull. Lond. Math. Soc.} 6 (1974) 257 - 303.

\end{thebibliography}
