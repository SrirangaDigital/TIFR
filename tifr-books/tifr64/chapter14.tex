\chapter{Brownian Motion as a Gaussian Process}\label{chap14}

SO\pageoriginale FAR WE have been considering Brownian motion as a
Markov process. We shall now show that Brownian motion can be
considered as a Gaussian process.

\begin{defi*}
Let $X\equiv (X_{1},\ldots,X_{N})$ be an $N$-dimensional random
variable. It is called an $N$-variate normal (or Gaussian)
distribution with mean $\mu\equiv (\mu_{1},\ldots,\mu_{N})$ and
covariance $A$ if the density function is
$$
\frac{1}{(2\pi)^{N/2}}\frac{1}{(\det A)^{1/2}}\exp \left(-\frac{1}{2}[(X-\mu)A^{-1}(X-\mu)^{*}]\right)
$$
where $A$ is an $N\times N$ positive definite symmetric matrix.
\end{defi*}

\begin{note*}
\begin{enumerate}
\item $E(X_{i})=\mu_{i}$.

\item $\Cov (X_{i},X_{j})=(A)_{ij}$.
\end{enumerate}
\end{note*}

\begin{theorem*}
$X\equiv (X_{1},\ldots,X_{N})$ is a multivariate normal distribution
  if and only if for every $\theta\in \mathbb{R}^{N}$, $\langle
  \theta,X\rangle$ is a one-dimensional Gaussian random variable.
\end{theorem*}

We omit the proof.

\begin{defi*}
A stochastic process $\{X_{t}:t\in I\}$ is called a Gaussian process
if $\forall t_{1},t_{2},\ldots,t_{N}\in I$,
$(X_{t_{1}},\ldots,X_{t_{N}})$ is an $N$-variate normal distribution.
\end{defi*}

\setcounter{exercise}{0}
\begin{exercise}\label{chap14-exer1}
Let $\{X_{t}:t\geq 0\}$ be a one dimensional Brownian motion. Then
show that 
\begin{itemize}
\item[(a)] $X_{t}$\pageoriginale is a Gaussian process.

(Hint: Use the previous theorem and the fact that increments are
  independent)

\item[(b)] $E(X_{t})=0$, $\forall t$, $E(X(t)X(s))=s \wedge t$.

Let~ $\rho:[0,1]=[0,1]\to \mathbb{R}$ be defined by
$$
\rho(s,t)=s\wedge t.
$$

Define $K:L^{2}_{\mathbb{R}}[0,1]\to L^{2}_{\mathbb{R}}[0,1]$ by
$$
Kf(s)=\int\limits^{1}_{0}\rho(s,t)f(t)dt.
$$
\end{itemize}
\end{exercise}

\begin{theorem*}
$K$ is a symmetric, compact operator. It has only a countable number
  of eigenvalues and has a complete set of eigenvectors.
\end{theorem*}

We omit the proof.

\begin{exercise}\label{chap14-exer2}
Let $\lambda$ be any eigenvalue of $K$ and $f$ an eigenvector
belonging to $\lambda$. Show that
\begin{itemize}
\item[(a)] $\lambda f''+f=0$ with $\lambda f(0)=0=\lambda f'(1)$.

\item[(b)] Using (a) deduce that the eigenvalues are given by
  $\lambda_{n}=4/(2n+1)^{2}\pi^{2}$ and the corresponding eigenvectors
  are given by 
$$
f_{n}=\surd 2 \Sin 1/2[(2n+1)\pi t]n=0,1,2,\ldots.
$$
\end{itemize}

Let $Z_{0}$, $Z_{1},\ldots,Z_{n}\ldots$ be identically distributed,
independent, normal random variables with mean $0$ and variance
$1$. Then we have
\end{exercise}

\begin{prop*}
$Y(t,w)=\sum\limits^{\infty}_{n=0}Z_{n}(w)f_{n}(t)\surd \lambda_{n}$

\noindent
converges in mean for every real $t$.
\end{prop*}

\begin{proof}
Let\pageoriginale
$Y_{m}(t,w)=\sum\limits^{m}_{i=0}Z_{i}(w)f_{i}(t)\surd\lambda_{i}$. Therefore
\begin{align*}
&
  E\{(Y_{n+m}(t,\cdot)-Y_{n}(t,\cdot))^{2}\}=\sum\limits^{n+m}_{n+1}f^{2}_{i}(t)\lambda_{i},\\
& E(||Y_{n+m}(\cdot)-Y_{n}(\cdot)||^{2}\leq
  \sum\limits^{n+m}_{n+1}\lambda_{i}\to 0.
\end{align*}
\end{proof}

\begin{remark*}
As each $Y_{n}(t,\cdot)$ is a normal random variable with mean $0$ and
variance $\sum\limits^{n}_{i=0}\lambda_{i}f^{2}_{i}(t)$, $Y(t,\cdot)$
is also a normal random variable with mean zero and variance
$\sum\limits^{\infty}_{i=0}\lambda_{i}f^{2}_{i}$. To see this one need
only observe that the limit of a sequence of normal random variables
is a normal random variable.
\end{remark*}

\begin{theorem*}[(Mercer)]
$$
\rho(s,t)=\sum\limits^{\infty}_{i=0}\lambda_{i}f_{i}(t)f_{i}(s),\ (s,t)\in
    [0,1]\times [0,1].
$$

The convergence is uniform.
\end{theorem*}

We omit the proof.

\begin{exercise}\label{chap14-exer3}
Using Mercer's theorem show that $\{X_t:0\leq t\leq 1\}$ is a
  Brownian motion, where
$$
X(t,w)=\sum\limits^{\infty}_{n=0}Z_{n}(w)f_{n}(t)\surd\lambda_{n}.
$$
\end{exercise}

This exercise now implies that
\begin{align*}
\int\limits^{1}_{0}X^{2}(s,w)ds &= (L^{2}-\text{norm of~ }X)^{2}\\
&= \sum\lambda_{n}Z^{2}_{n}(w),
\end{align*}
since\pageoriginale $f_{n}(t)$ are orthonormal. Therefore
\begin{align*}
E(e^{-\lambda\int\limits^{1}_{0}X^{2}(s,\cdot)ds}) &=
E(e^{-\lambda\sum\limits^{\infty}_{n=0}\lambda_{n}Z^{2}_{n}(w)})=\prod\limits^{\infty}_{n=0}E(e^{-\lambda\lambda_{n}Z^{2}_{n}})\\
&\hspace{4cm}\text{(by independence of $Z_{n}$)}\\
&= \prod\limits^{\infty}_{n=0}E(e^{-\lambda\lambda_{n}Z^{2}_{0}})
\end{align*}
as $Z_{0}$, $Z_{n}\ldots$ are identically distributed. Therefore
\begin{align*}
E(e^{-\lambda\int\limits^{1}_{0}X^{2}(s,\cdot)ds}) &=
\prod^{\infty}_{n=0}1/\surd (1+2\lambda\lambda_{n})\\
&=
\prod^{\infty}_{n=0}1/\surd\left(1+\frac{8\ 8\lambda}{(2n+1)^{2}\Pi^{2}}\right)\\
&= 1/\surd (\cosh)\surd(2\lambda). 
\end{align*}

\noindent
{\bf APPLICATION.}~ If $F(a)=P(\int\limits^{1}_{0}X^{2}(s)ds<a)$, then
\begin{gather*}
\int\limits^{\infty}_{0}e^{-\lambda
  a}dF(a)=\int\limits^{\infty}_{-\infty}e^{-\lambda a}dF(a)\\
=E(e^{-\lambda\int^{1}_{0}X^{2}(s)ds})=1/\surd(\cosh)\surd(2\lambda).
\end{gather*}


