\thispagestyle{empty}
\begin{center} 
{\huge\textbf{Siegel's Modular Functions}}
\end{center}

The\pageoriginale theory of modular functions of degree $n$ is not
quite new. The 
conception of this theory is due to Siegel who gave a first
introduction of this in 1939 (Einfuhrung in die Theorie der
Modulfunktionen $n$-ten Grades, Math. Ann., Vol 116(1939)). Since
then, numerous contributions have been made by various authors. My
desire to give a course on this special branch of mathematics arose
out of more than one consideration. Siegel's modular functions
constitute one of the most important classes of analytic functions of
several variables, and we are able to look forward to far reaching
results out of this class. The modular functions of degree $n$ are
connected with the manifolds of the closed Riemann surfaces of genus
$n$ in just the same way as the elliptic modular functions are, with
the manifolds of Riemann surfaces of genus 1. Moreover we realise
the excellent use of modular forms of degree $n$ in the analytic
theory of quadratic forms, first positive quadratic forms and then,
indefinite forms too. But in trying to extend this theory to the case
of indefinite quadratic forms, one meets with new types of functions
defined by partial differential equations and one is led to a variety
of unsolved problems in this direction. Finally, the researches of
Siegel have becomes representative of those on a general class of
automorphic functions of several variables. 


\chapter{The Modular Group of Degree \texorpdfstring{$n$}{n}}

Let $\mathfrak{f}$ be a closed Riemann surface of genus $n$; $d
\omega_i,i=1,2,\ldots n$, a basis of the Abelian differentials of the
first kind on $\mathfrak{f}$, and $\psi_{\nu}$, $\psi'_{\nu}$,
$\nu = 1, 2 \ldots n$, a canonical system of curves which dissect
$\mathfrak{f}$ into a simply connected surface bounded by one closed
curve. We ask for the transformation properties of the periods 
$$ 
q_{\mu \nu} = \int\limits_{\psi'_{\mu}}  d\omega_{\nu} , p_{\mu 
  \nu} = - \int\limits_{\psi_{\mu}}  d\omega_{\nu}  (\mu,\nu =
1,2 \ldots n) 
$$
concerning\pageoriginale a replacement of $d \omega_{\nu} (\nu = 1, 2
\ldots n)$ by 
another basis $d \omega_{\nu}^*\break (\nu = 1,2 \ldots n)$, and
$\psi_{\nu}$, $\psi'_{\nu}$ respectively by another canonical
system of curves $\psi_{\nu}^*$, $\psi^*_{\nu}  (\nu = 1,2
\ldots n)$. We introduce the period matrices 
$$
P = (p_{\mu \nu}),  \; Q = (q_{\mu \nu}), \; (\mu,\nu = 1,2 \ldots n),
$$
and let $P^*$, $Q^*$ denote the corresponding matrices in the changed
system. Let $\mathscr{L}_{\mu}$, $\mathscr{L}_{\nu}'$ be arbitrary oriented
closed curves on $\mathfrak{f}$ and $C_{\mu}$, $C'_{\nu}$ be arbitrary
complex numbers. $A$ homology  
$$
\sum^r_{\mu = 1} C_{\mu}  \mathscr{L}_{\mu} \sim \sum\limits_{\gamma=1}^s
C'_{\nu}  \mathscr{L}'_nu 
$$
means that for every integrable function $F$, the equation
$$
\sum^r_{\mu=1}  C_{\mu} F(\mathscr{L}_{\mu}) = \sum^r_{\nu = 1}
C'_{\nu} F(\mathscr{L}'_{\nu}) \qquad \text{ holds. } 
$$

For every closed curve $\mathscr{L}$, there exists, as is well known,
a representation 
$$
\mathscr{L} \sim \sum^n_{\mu = 1} C_{\mu} \psi_{\mu} +
\sum_{\nu = 1}^n  C_{\nu}'  \psi_{\nu}' 
$$
with uniquely determined integers $C_{\mu}, C'_{\nu} (\mu, \nu = 1, 2
\ldots, n)$. That is to say that the curves of a canonical system
represent a basis of the homology classes of all closed curves. Thus
we have in particular 
\begin{equation*}
\left.
\begin{aligned}
& \psi^*_i   \sim \sum_{\mu} (a_{i \mu} \psi_{\mu} - b_{i
    \mu} \psi_{\mu}')\\ 
& \psi^{*'}_i  \sim \sum_{\mu} (-C_{i \mu} \psi_\mu +
  d_{i \mu} \psi'_\mu ) 
\end{aligned}
\right \} \tag{1}\label{eq1}
\end{equation*}
with integers $a_{\mu \nu}, b_{\mu \nu}, C_{\mu \nu}, d_{\mu \nu} 
(\mu, \nu = 1, 2 \ldots n)$. Since the change from the homology basis
$\psi_{\nu}$, $\psi_{\nu}'$, to the basis
$\psi^{*}_{\nu}$, ${\psi^{*}}_{\nu'}$ will always be effected
by a matrix which is unimodular, the determinant of the matrix 
$M = \begin{pmatrix}
 A & B \\ 
C & D
\end{pmatrix}$ with $A = (a_{\mu \nu})$, $B = (b_{\mu \nu})$, $C = (C_{\mu
  \nu})$,\pageoriginale $D = (d_{\mu \nu})$, denoted by $|M|$, is
equal to $\pm   1$, i.e. $|M| = \pm 1$.    

Furthermore,
$$
d\omega^*_{\mu} = \sum^n_{\nu = 1} d\omega_\nu t_{\nu \mu} (\mu =
1,2 \ldots n) 
$$
with a non-singular matrix $T = (t_{\mu \nu})$

Now we express the periods
$$
q^*_{i\mathscr{R}} = \int\limits_{\psi^{*'}_i},
d\omega^*_{\mathscr{R}},   P^*_{i\mathscr{R}} = -
\int\limits_{\psi^*_i} d\omega^*_{\mathscr{R}}
$$
in terms of $P_{\mu \nu}$, $q_{\mu \nu}$ as follows.
\begin{align*}
p^*_{i\mathscr{R}} & =  - \sum_{\mu} (a_{i\mu}
\int\limits_{\psi_\mu}  d\omega^*_{\mathscr{R}} - b_{i\mu}
\int\limits_{\psi'_\mu} d\omega^*_{\mathscr{R}})\\ 
& = - \sum_{\mu\nu}   (a_{i\mu}   t_{\mu\mathscr{R}}
\int\limits_{\psi_\mu} d\omega_\nu - b_{i\mu}   t_{\nu
  \mathscr{R}} \int\limits_{\psi'_\mu} d\omega_\nu )\\ 
& = \sum_{\mu \nu}   (a_{i\mu}   \psi_{\mu \nu}   t_{\nu
  \mathscr{R}} + b_{i \mu}   q_{\mu \nu}   t_{\nu\mathscr{R}},\\ 
q^*_{i\mathscr{R}} & = \sum_\mu   (-C_{i\mu}
\int\limits_{\psi_\mu}   d\omega^*_{\mu} + d_{i\mu}  
\int\limits_{\psi'_\mu}   d\omega^*_{\mathscr{R}})\\ 
& = \sum_{\mu\nu}   (-C_{i\mu}   t_{\nu \mathscr{R}}  
\int\limits_{\psi_{\mu}}   d\omega_\nu + d_{i\mu}   t_{\nu
  \mathscr{R}} \int\limits_{\psi_\mu'}  d\omega_\nu )\\ 
& = \sum   (C_{i\mu}  p_{\mu \nu}   t_{\nu \mathscr{R}} + d_{i\mu}  
q_{\mu \nu}   t_{\nu \mathscr{R}}) 
\end{align*}

Rewritten in terms of the matrices, these relations give
$$
P^* = (p^*_{\mu \nu}) = (AP + BQ) T, \; Q^* = (q^*_{\mu \nu}) = (CP +
DQ) T. 
$$

As is well known, a differential of the first kind is uniquely
determined by its periods $\int\limits_{\psi'_\mu}   d\omega    (\mu =
1,2 \ldots n)$ so that $| Q | \neq 0$. The choice $T = Q^{-1}$ is
therefore permissible and this leads to the relations $P^* = APQ^{-1}
+ B = AZ + B$, where $Z = PQ^{-1}$, and $Q^* = CPQ^{-1} + D = CZ +n$. 

If we change only the basis of the differentials and keep the
canonical basis\pageoriginale unchanged, the above relations lead to
the normalized 
period matrices $P^* = PQ^{-1} = Z$; $Q^* = E^{(n, n)}$, the unit
square matrix of order $n$, as in this case we have 
$$
M =\begin{pmatrix} 
A & B \\ 
C & D
 \end{pmatrix} 
= \begin{pmatrix}
  E^{(n,n)} & o\\
 o & E^{(n,n)}
 \end{pmatrix} 
= E^{(2n,2n)}. 
$$

We denote by $X, Y$, the real and imaginary parts of the matrix $Z$
and by $Z'$ the transpose of $Z$. From the theory of algebraic
functions it is known that $Z$ is a symmetric matrix with a positive
imaginary part (i.e. the quadratic form $\mathscr{Y}'$ $Y\mathscr{Y}$,
for any real vector $\mathscr{Y}$ is always 
positive). In symbols, these mean 
\begin{equation*}
\fbox{Z = Z' ; Y, 0.}\tag{2}\label{eq2}
\end{equation*}

It is obvious that in the general case ($M$ arbitrary, unimodular; $T$
arbitrary, non-singular) we have  
$$
Z^* = P^* Q^{*^{-1}} = (AP + BQ)  (CP + DQ)^{-1} = (AZ + B)   (CZ +
D)^{-1} 
$$
and $Z^*$ also satisfies the same properties as $Z$, viz. it is
symmetric with a positive imaginary part. 

In the sequel we shall denote by $\mathscr{Y}$ the set of all
symmetric matrices with a positive imaginary part. 

In order to obtain the typical relations for the coefficients of the
matrix $M= \begin{pmatrix} A & B \\ C & D  \end{pmatrix}$ we consider
the intersection properties of the canonical system $\psi_\nu,
\psi'_\nu    (\nu = 1,2 \ldots n)$, described by means of the
notion of the \textit{ characteristic } (Kronecker). For every pair of
closed curves $\mathscr{L}_1$, $\mathscr{L}_2$ on $\mathfrak{f}$, there
exists a number $\mathscr{S}(\mathscr{L}_1, \; \mathscr{L}_2)$ called the
characteristic of $\mathscr{L}_1$ with regard to $\mathscr{L}_2$,
which satisfies the following conditions. 
\begin{enumerate}[1)]
\item $\mathscr{S}(\mathscr{L}_1,\mathscr{L}_2)$ is an integer
  depending only on the homology classes of $\mathscr{L}_1$ and
  $\mathscr{L}_2$. 

We may\pageoriginale now define $\mathscr{S}(\mathscr{L}_1 + \mathscr{L}_2,
\mathscr{L}_3)$  as $\mathscr{S}(\mathscr{L},\mathscr{L}_3)$ where
$\mathscr{L}   \sim   \mathscr{L}_1 + \mathscr{L}_2$. 

\item $\mathscr{S}(\mathscr{L}_1, \mathscr{L}_2) = -
  \mathscr{S}(\mathscr{L}_2,\mathscr{L}_1)$. This implies the
  additivity of $\mathscr{S}$, viz. 

\begin{equation*}
\mathscr{S}(\mathscr{L}_1 + \mathscr{L}_2, \mathscr{L}) =
\mathscr{S}(\mathscr{L}_1,\mathscr{L}) +
\mathscr{S}(\mathscr{L}_2,\mathscr{L})\tag{3}\label{eq3} 
\end{equation*}

\item For every canonical basis $\psi_\nu, \psi_\nu
  (\nu = 1,2 \ldots n)$ of the homology classes we have 
\begin{align*}
\mathscr{S}(\psi_\mu , \psi_\nu ) &  =
\mathscr{S}(\psi_\mu' ,\psi'_{\nu}) = 0\\ 
\mathscr{S}(\psi_\mu, \psi'_\nu) &  = \delta_{\mu \nu}
\quad \text{ where } \tag{4}\label{eq4} 
\end{align*}
the $\delta_{\mu \nu}'\mathscr{S}$ are the Kronceker $\delta'
\mathscr{S}$. 

\item If $\mathscr{L}_1$ crosses $\mathscr{L}_2$, $r$ times from the
  right side to the left, and $l$ times from the left to the right,
  then  
\begin{equation*}
\mathscr{S} (\mathscr{L}_1, \mathscr{L}_2) = r -\ell \tag{5}\label{eq5}
\end{equation*}
as far as $r$ and $\ell$ are computable.
\end{enumerate}

As we only intend to sketch the way which leads to the typical
relations for $M$, we do not prove here the existence of a
characteristic (cf:H.Weyl, Die Idee der Riemanns chen Flanche). 

From the above properties it is clear that the characteristic defines
a bilinear form on the class of all closed curves and that the matrix
associated with this bilinear form with respect to a canonical basis
is $I = \begin{pmatrix}
  o & E^{(n,n)}\\ 
-E^{(n,n)} &  o
  \end{pmatrix}$. 

If we transform this basis by means of the matrix $M$, the matrix
associated with the above bilinear form with respect to the
transformed basis is clearly $MIM'$. But $M$ is so chosen that the
transformed basis is again canonical so that the matrix associated
with the bilinear form with respect to the transformed basis should
again be $I$. Hence we conclude that  
\begin{equation*}
M   I   M' = I \tag{6}\label{eq6}
\end{equation*}

This\pageoriginale clearly characterises $M$, since the matrix
associated with the 
above bilinear form with respect to a certain basis can be equal to
$I$ if and only if that basis is canonical. As a consequence of (\ref{eq6})
we obtain the desire characteristic relations as  
\begin{equation*}
AD' - BC' = E ; AB' - BA' = 0; CD' - DC' = 0 \tag{7}\label{eq7}
\end{equation*}

A matrix $M = \begin{pmatrix} A & B \\ C & D \end{pmatrix}$ is said to
be \textit{symplectic} if it satisfies the relation (\ref{eq6}) or
equivalently the relation (\ref{eq7}). We denote by $S = S_n$ the class of
all symplectic matrices. In view of the relations  
\begin{align*}
I^{-1} & = -I\\
M^{-1} & = -I M' I \tag{8}\label{eq8}
\end{align*}
it is easily seen that $S$ is a group, called the \textit{symplectic
  group of degree $n$}. The integral matrices $M   \in   S$ form a
sub-group, called the \textit{Modular Group of degree $n$}. We
denote the modular group by $M = M_n$. We may note that if one
replaces $M$ by $M'$ in (\ref{eq7}) one obtains 
\begin{equation*}
A'D - C'B = E; A'C - C'A = 0; B'D - D'B = 0 \tag{9}\label{eq9}
\end{equation*}

These relations are equivalent to (\ref{eq7}). For, from (\ref{eq8}) we have 
\begin{align*}
M' & = I^{-1} M^{-1} I^{-1}\\
& = I^{-1}M^{-1}I \tag*{$(8)'$}\label{eq8'}
\end{align*}

Clearly $I   \in   S$. Since $S$ is a group, the relation \ref{eq8'}
signifies that $M   \in   S$ if and only if $M'   \in   S$ which is
precisely the content of our claim that the relations (\ref{eq7}) and
(\ref{eq9}) are equivalent. In view of (\ref{eq8}) we also obtain 
\begin{equation*}
M^{-1} = 
\begin{pmatrix}
 D' & -B' \\ 
-C' & A'  
\end{pmatrix} \tag{10}\label{eq10}
\end{equation*}

With a view to fixing our ideas, we notice the following aspect
(cf. Siegel, \"Uber die analytische Theorie der quadratischen Formen,
Ann.Math., Vol. 36(1935)).\pageoriginale We define two points $Z$, $Z^* \in
\mathscr{Y}$ to be equivalent (with regard to $M$) if  
\begin{equation*}
Z^* = M \langle Z \rangle = (AZ + B) (CZ + D)^{-1} \tag{11}\label{eq11}
\end{equation*}
for a suitable matrix $M = \begin{pmatrix} A & B \\ C &
  D \end{pmatrix} \in M$

Then our earlier discussion shows that to every closed Riemann
surface, there corresponds a uniquely determined set of equivalent
points in $\mathscr{Y}$. The converse is not always true. Let
$\mathscr{Y}_0$ be the submanifold of $\mathscr{Y}$ consisting of all
sets of equivalent points $Z$, which appear from period matrices in
the manner described above. As Riemann has shown, $\mathscr{Y}_0$ is a
complex analytic manifold depending on $3   n - 3 $ independent
complex variables when $n > 1$, while $\mathscr{Y}$ depends on
$n(n+1)/2$ independent complex variables. By a \textit{module} of
$\mathfrak{f}$ we shall understand a complex valued function
$\mathfrak{f}(\mathfrak{f})$ defined on all closed Riemann surfaces of
genus $n$ and possessing certain analytic properties. For instance,
every function $\mathfrak{f}(Z)$ meromorphic in $\mathscr{Y}$ and
invariant under $M$ is such a modul. We call such an $\mathfrak{f}$ a
modular function of degree $n$, but later we shall give a precise
definition of this concept. The modular functions of degree $n$
constitute a function field. More over we shall prove that this
function field is generated by a special set of modular function 
\begin{equation*}
\mathfrak{f}_o (Z),\mathfrak{f}_1(Z),\ldots
\mathfrak{f}_{\mathscr{R}}(Z),\mathscr{R} = n(n+1)/2 \tag{12}\label{eq12} 
\end{equation*}

The degree of transcendence of this function field is $k$,
that is to say, the functions (\ref{eq12}) satisfy one irreducible algebraic
equation 
\begin{equation*}
A_o (\mathfrak{f}_o, \mathfrak{f}_1 \ldots \mathfrak{f}_k) = 0
\tag{13}\label{eq13}  
\end{equation*}

It is possible to find a system of algebraic equations
\begin{equation*}
A_\nu (\mathfrak{f}_o, \mathfrak{f}_1 \ldots \mathfrak{f}_\mathscr{R}) =
0,   \nu = 1,2 \quad (n-2)(n-3)/2 \tag{14}\label{eq14} 
\end{equation*}
which give a necessary and sufficient condition for $Z$ to belong to
$\mathscr{Y}_o$. The field of modular functions will be changed into
the field of the moduls by these relations. The field of moduls has
the degree of transcendence\pageoriginale  $(n(n+1)/2) -
((n-2)(n-3)/2) = 3 n 
-3$. Since every closed Riemann surface of genus $n$ is biuniquely
determined by the values of $\mathfrak{f}_o,\mathfrak{f}_1,\ldots
\mathfrak{f}_n$, the manifolds of all closed Riemann surfaces of genus
$n$ represent an algebraic manifold by (\ref{eq13}) and
(\ref{eq14}). The explicit 
computation of these equations is still on unsolved problem. 

We proceed to investigate the arithmetic properties of the matrices $M
\in M$. First we consider the more general group $S$. Let $M \in S$
and $M = \begin{pmatrix} A & B \\ C & D \end{pmatrix}$. We call $(A
B)$ the first matrix row of $M$ and $(C, D)$, the second. Two complex
matrices $P$, $Q$ are said to from a \textit{symmetric pair} if we
have $PQ' = QP'$. We note that  if $(P, Q)$ is a symmetric pair and $|
Q | \neq 0$, then $Q^{-1}P$ is a symmetric matrix. 

A matrix $A$ will be called \textit{integral} if all the elements of
$A$ are integers. A pair of $n$-rowed matrices $A$, $B$  will be called
\textit{coprime} if the matrix products $GA$, $GB$ are integral when
and only when the matrix $G$ is integral. This in particular implies
that both $A, B$ are integral if $(A, B)$ is coprime. 

Let $(C, D)$ be a coprime pair of matrices and let $\mathscr{U}_1$, a
$n$-rowed and $\mathscr{U}_2$, a $2n$-rowed unimodular matrix. Then the
matrices $C_1$, $D_1$ defined by $(C_1   D_1) = \mathscr{U}_1(C,
D)\mathscr{U}_2$ are coprime too.  

Indeed, $GC_1$, $GD_1$ are integral if and only if $(G  \mathscr{U}_1 C, G
\mathscr{U}_1   D) \mathscr{U}_2$ are integral which in turn is true
if and only if $G \mathscr{U}_1   C$, $G   \mathscr{U}_1   D$ are
integral as 
$\mathscr{U}_2$ is unimodular. Since by assumption $(C, D)$ is a
coprime pair, 
the above holds if and only if $G  \mathscr{U}_1$ and consequently $G$ are
integral, and this proves our 
contention. 

We will now choose $\mathscr{U}_1$ and $\mathscr{U}_2$ so that $D_1 =
0$ and $C_1$ becomes 
a diagonal matrix with non-negative integers as diagonal elements. The
choice is possible by the elementary divisor theorem. Then, of
necessity $C_1 = E$, as otherwise we can always find a non integral
matrix $G$ such that $GC_1$\pageoriginale  is integral, contradicting
the fact the 
pair $(C_1, 0)$ is coprime. Thus we obtain 
$$
(C   D) \mathscr{U}_2 = \mathscr{U}^{-1}_1 (C_1 , D_1) =
\mathscr{U}^{-1}_1 (E,0) = (\mathscr{U}^{-1}_1,0) 
$$
which shows that the matrix $(C, D)$ is of rank $n$. This says even
more, viz. that there exist integral matrices $X$, $Y$ satisfying the
relation  
\begin{equation*}
CX + DY = E \tag{15}\label{eq15}
\end{equation*}

Indeed, we have only to define $X$, $Y$ by 
$$
\begin{pmatrix} X \\ Y \end{pmatrix} = \mathscr{U}_2 \begin{pmatrix}
  \mathscr{U}_1 
  \\ 0 \end{pmatrix} 
$$
and then 
$$
CX + DY = (C   D) \begin{pmatrix} X \\ Y \end{pmatrix} = (C   D) 
\mathscr{U}_2 \begin{pmatrix} \mathscr{U}_1 \\ 0  \end{pmatrix} = E. 
$$

The converse is true too, viz. if $(C, D)$ is a pair of integral
matrices for which the equation (\ref{eq15}) is solvable for integral
$X$, $Y$, then 
$(C, D)$ is a coprime pair. For, in this case if $G$ is integral then
$GC$, $GD$ are trivially integral, while if $GC$, $GD$ are integral, then
$GCX$, $GDY$ are integral which in turn implies that $GCX + GDY = GE =
G$ is integral. 

We now note that matrix rows of a matrix $M = \begin{pmatrix} A & B
  \\ C & D\end{pmatrix} \in \mathcal{M}$ consist of coprime pairs in view of our
  above inference and relations (\ref{eq7}). They are trivially symmetric
  pairs, again a consequence of (\ref{eq7}). The partial converse is also
  true, viz. every symmetric coprime pair $(C, D)$ is a second matrix
  row of some matrix $\mathcal{M} \in \mathcal{M}$. For, we can determine
  integral $X$, $Y$ such that $C   X + D   Y = E$ and then we have
  only to set  
$$
A = Y' + X'YC, B = - X' + X' Y   D.
$$

Then
\begin{align*}
AB' - BA' & = (Y'+ X' YC)(-X + D'Y'X) - (-X' + X' Y D)(Y + C' Y' X)\\
& = (X'Y - Y'X) + (X'C' + Y'D')Y'X -X'Y(C X + DY)\\
& = (X'Y - Y'X) + Y'X -X'Y = 0
\end{align*}
and\pageoriginale  
\begin{align*}
AD' - BC' & = (Y' + X'YC) D' -(-X' + X' Y D)C'\\
&  = Y'D' + X'C' = (CX +
DY)' = E 
\end{align*}
which show that $A$, $B$, $C$, $D$ satisfy (\ref{eq7}) and consequently
$$
M = \begin{pmatrix} 
A & B \\
C & D  
\end{pmatrix} \in \mathcal{M}.
$$

Let us investigate how far $(C, D)$ determines $\mathcal{M}$
uniquely. Let $\mathcal{M}$, 
$\mathcal{M}_1$ be two modular matrices with the same second matrix
row, say $\mathcal{M}  
= \begin{pmatrix} A & B \\ C & D  \end{pmatrix}$,   $\mathcal{M}_i   
= \begin{pmatrix} A_1 & B_1 \\ C & D \end{pmatrix}$. Then 
$$ 
\mathcal{M}_1\mathcal{M}^{-1} = \begin{pmatrix} A_1 & B_1 \\ C & 
  D \end{pmatrix} \begin{pmatrix} D' & -B' \\ -C  & A' \end{pmatrix}
= \begin{pmatrix} E & S \\ 0 & R \end{pmatrix} 
$$
where $R = - CB' + DA'$, $S = - A_1B' + B_1A'$. 

Since $\mathcal{M}_1 \mathcal{M}^{-i} \in \mathcal{M}$ ($\mathcal{M}$
being a group), it is necessary that $R 
= E$ and $S = S'$. Thus $\mathcal{M}_1 = \begin{pmatrix} E & S \\ 0 &
  E  \end{pmatrix}\mathcal{M}$ where $S$ is a symmetric matrix
conversely also, 
if $\mathcal{M}, \mathcal{M}_1 \in \mathcal{M}$ and $\mathcal{M}_1
= \begin{pmatrix} E & S \\ 0 & 
  E  \end{pmatrix}\mathcal{M}$ for a symmetric matrix $S$, then
$\mathcal{M}$ and $\mathcal{M}_1$ 
have the same second matrix row as is seen by direct
multiplication. Let $T$ be the Abelian sub-group of $\mathcal{M}$ consisting of
all matrices of the form $\begin{pmatrix} E & S \\ 0 &
  E \end{pmatrix}$ where $S = S'$. Then the class of all modular
matrices with different second rows provides a complete representative
system of the set of all right cosets of $T$ in $\mathcal{M}$. 

We proceed to consider another sub-group $A$ of $\mathcal{M}$ of which
$T$ will 
be a normal sub-group. This group $A$ consists precisely of all 
elements $\mathcal{M} \in \mathcal{M}$ of the form $\mathcal{M}
- \begin{pmatrix} A & B \\ 0 & 
  D \end{pmatrix}$. The conditions (\ref{eq7}) will then imply that $AD'  =
E$ and $AB' = BA'$. The first condition means that $D' =
A^{-1}$. Since $A$, $D'$ are integral matrices it is now immediate that
they are unimodular, say $A' = u$ and $D = u^{-1}$. Let $S = Bu = BA'
= AB'$. The second condition then implies that $S$, defined as above,
is symmetric. Thus $A$ is the sub-group of $\mathcal{M}$ consisting
precisely of 
the matrices of the form $\mathcal{M}_0 = \begin{pmatrix} u' & su^{-1} \\ 0 &
  u^{-1} \end{pmatrix}$ where $S$ is symmetric and $u$ is
unimodular. It is easily seen that $T$ is a normal sub-group of
$A$. Then since the mapping which taken the matrix $\begin{pmatrix} A
  & B \\ 0 & D \end{pmatrix} \in \mathcal{M} $ to $\begin{pmatrix} A &
  0 \\ 0 & 
  D  \end{pmatrix}$ is a\pageoriginale  homomorphism of which $T$ is
the kernel, it 
follows that $A/T$ is isomorphic to the group of matrices 
\begin{equation*}
\begin{pmatrix} 
u' & 0 \\
0 & u^{-1} 
\end{pmatrix}
,\qquad u -\text{ unimodular}. \tag{16}\label{eq16}
\end{equation*}

Let us decompose $\mathcal{M}$ into right cosets modulo $A$. The
matrices $\mathcal{M} 
= \begin{pmatrix} A & B \\ C & D \end{pmatrix}$ and $\mathcal{M}_1
= \begin{pmatrix} A_1 & B_1 \\ C_1 & D_1 \end{pmatrix}$ belong to the
same right coset of $A$ if and only if $\mathcal{M}_o\mathcal{M} =
\mathcal{M}_1$ for some $\mathcal{M}_o \in 
A$. 

This implies that
\begin{equation*}
u(C_1,D_1) = (C,D), \qquad u -\text{ unimodular }, \tag{17}\label{eq17}
\end{equation*}
and hence $CD'_1 = u   C_1   D'_1 = u   D_1   C'_1 = D   C'_1$

Thus we have
\begin{equation*}
C   D'_1 = D   C'_1. \tag{18}\label{eq18}
\end{equation*}

On the other hand, if $\mathcal{M}$, $\mathcal{M}_1 \in \mathcal{M}$
be any two matrices for which 
(\ref{eq18}) holds, then $\mathcal{M}\mathcal{M}^{-1}_1 = \begin{pmatrix} * &
  * \\ 0 & 
  * \end{pmatrix}$ so that $\mathcal{M}\mathcal{M}^{-1}_1 \in A$; in
other words $\mathcal{M}$ and 
$\mathcal{M}_1$ determine the same right coset of $A$. Thus (\ref{eq17}) and
(\ref{eq18}) are equivalent and either of them gives a necessary and sufficient 
condition that $\mathcal{M}$ and $\mathcal{M}_1$ belong to the same
right coset modulo 
$A$. Let us now define two pairs $(C, D)$, $(C_1, D_1)$ to be
\textit{associated} if   
\begin{enumerate}
\item Both of them are symmetric co-prime pairs and 
\item $CD'_1 = DC'_1$
\end{enumerate}

This relation of being associated pairs is reflexive, symmetric and
transitive - in other words, and equivalence relation. We can now say
that in the coset $AM$ lie exactly those matrices whose second matrix
rows are associated with the second matrix row of $\mathcal{M}$. 

Let $\{ C, D \}$ denote the class of all coprime symmetric pairs of
matrices associated with $(C, D)$. Let $C$ be of rank $r$, $O < r \le
n$. By the elementary divisor theorem, we can can determine unimodular
matrices $u_1, u_2$ such that  
\begin{equation*}
u_1 C =
\begin{pmatrix}
C_1 & 0 \\
 0 & 0  
\end{pmatrix}
 u'_2,   | C_1|   \neq 0 \tag{19}\label{eq19} 
\end{equation*}

Here\pageoriginale  $C_1$ is a $r \times r$ square matrix.

In analogy with (\ref{eq19}) we write
\begin{equation*}
u_1 D = \begin{pmatrix} D_1 & D_2 \\ D_3 & D_4 \end{pmatrix}
u^{-1}_2 \tag*{$(19)'$}\label{eq19'} 
\end{equation*}
and determine the nature of $D_i$, $i = 1,2,3,4$. 

Since $CD' = DC'$, we have
$$
\begin{pmatrix}C_1 & 0 \\ 0 & 0 \end{pmatrix}   \begin{pmatrix}D_1 &
  D_3 \\ D_2 & D_4 \end{pmatrix}  = \begin{pmatrix} D_1 & D_2 \\ D_3 &
  D_4 \end{pmatrix}   \begin{pmatrix} C_2' & 0 \\ 0 & 0 \end{pmatrix} 
$$
which gives $C_1 D_1' = D_1 C'_1$ and $D_3 = 0$

We now contend that $D_4$ is unimodular.  For, in view of (\ref{eq15}) we
need only verify that the pair $(D_4, 0)$ is coprime. Let then $GD_4$
be integral. Of necessity, $G$ is a matrix with $(n-r)$ columns. We
complete $G$ with zeros to a matrix $(O \;  G)$ with $n$ columns. From
(\ref{eq19}) and (\ref{eq19'}) we obtain that $(O \;  G)u_1 C $ and
$(O   G)u_1 D$ 
are integral. But $(C, D)$ is a coprime pair and consequently $(u_1 C,
u_1 D)$ is also coprime, so that $G$ should be integral. It is now
immediate that $(D_4, 0)$ is coprime and consequently $D_4$ is
unimodular. Let now $u_1 = \begin{pmatrix} E & D_2 \\ 0 &
  D_4 \end{pmatrix}u^*_1$. Then $u^*_1$ is unimodular. Also 
\begin{align*}
u^*_1 C = \begin{pmatrix}E & D_2 \\ 0 & D_4\end{pmatrix}^{-1}   u_1 C
  & = \begin{pmatrix}E & -D_2D^{-1}_4 \\ 0 &
    D^{-1}_4 \end{pmatrix} \begin{pmatrix}C_1 & 0 \\ 0 &
    0 \end{pmatrix} u'_2\\ 
& = \begin{pmatrix} C_1 & 0 \\ 0 & 0\end{pmatrix} u'_2
\end{align*}
and 
\begin{align*}
u^*_1 D = \begin{pmatrix}E & -D_2D^{-1}_4 \\ 0 &
  D^{-1}_4 \end{pmatrix} u_1 D & = \begin{pmatrix}E & -D_2D^{-1}_4
  \\ 0 & D^{-1}_4 \end{pmatrix} \begin{pmatrix} D_1 & D_2 \\ 0 &
  D_4\end{pmatrix} u^{-1}_2\\ 
& = \begin{pmatrix} D_1 & 0 \\ 0 & E\end{pmatrix} u^{-1}_2
\end{align*}

We will now show that the pair $C_1, D_1)$ is coprime. Let $GC_1,
GD_1$ be integral. Then $(G \;  O) u^*_1 C$ and $(G \;  O) u^*_1 D$ are
integral. But $(C, D)$ is a coprime pair and hence also $(u^*_1 C,
\; u^*_1 D)$. Hence we conclude that $G$ is integral. It is now immediate
that $(C_1, D_1)$ is coprime too. We may summarise our results into
the following statement: 

If $(C, D)$\pageoriginale  is a coprime symmetric pair with rank $C =
r$, $O < r \le 
n$, there exist square matrices $C_1$, $D_1$ of order $r$ which again
form a coprime symmetric pair, and unimodular matrices $u_1$, $u_2$ such
that  
\begin{equation*}
u_1 C = \begin{pmatrix}C_1 & 0 \\ 0& 0 \end{pmatrix} u'_2 ,    u_1 D
= \begin{pmatrix}D_1 & 0 \\ 0 & E \end{pmatrix} u^{-1}_2 \tag{20}\label{eq20} 
\end{equation*}

We will now fix $u_2$ more precisely. If we replace $u_2$ in (\ref{eq20}) by
a matrix $u_2 \begin{pmatrix} u_3 & 0 \\ 0 & E\end{pmatrix}$ where
  $u_3$ is an $r \times r$ unimodular matrix, we still obtain the same
  form. This replacement amounts to changing $Q$ to $Q  u_3$ where
  $Q$ consists of the first $r$ columns of $u_2$. We shall call two
  matrices $Q$, $Q_1$ \textit{right associated} if $Q_1 = Qu_3$ for some
  unimodular matrix $u_3$. Let $\{ Q\}$ denote the class of all
  matrices right associated with $Q$. A change of $u_1$ to $u_1^*
  = \begin{pmatrix}u_4 & 0 \\ 0 & E  \end{pmatrix}u_1$ where $u_4$ is
  unimodular carries the pair $(C_1,D_1)$ into $(u_4   C_1,u_4   D_1)
  \in \{ C_1,D_1\}$. A proper choice of $u_3$ and will transform $Q$
  into a fixed matrix in the class $Q$ and $u_4$ the pair $(C_1,D_1)$ into a
  fixed pair in $\{C_1,D_1 \}$. This settles how for $C_1$, $D_1$ and
  $u_1$ characterise $C$, $D$ in (\ref{eq20}). We may note that the matrices
  $Q$ which enter into our discussion are precisely those that can be
  completed to a unimodular matrix $(Q \; R)$. A matrix with this
  property will be called \textit{primitive}. The elementary divisors
  of a primitive matrix are all equal to 1 as $u^{-1}_2 Q
  = \begin{pmatrix}E \\ 0 \end{pmatrix}$ shows. Conversely, if all the
  elementary divisors of $Q$ are equal to 1, then $Q$ is
  primitive. For, then we can determine unimodular matrices $u_2$, $u_3$
  so that $Q = u_2\begin{pmatrix}E \\ 0 \end{pmatrix}u_3$. We write
  $u_2 = (Q_1,R_1)$ where $Q_1$ is r-columned and obtain 

$(Q \;  R_1) = (Q_1   u_3   R_1) = u_2 \begin{pmatrix}u_3 & 0 \\ 0 & 
    E \end{pmatrix}$ 

We use the notation $Q = Q^{(n,n)}$ to signify that $Q$ is a matrix
with $n$ rows and $r$ columns. We will denote $Q^{(n,n)}$ simply by
$Q^{(n)}$. A parametric representation of all classes $\{C,D \}$ of
coprime symmetric pairs $C$, $D$, with rank $C = r$,  $0 < r \le n$ is
give by 

\begin{lem}\label{chap1:lem1}% lemma 1
Let\pageoriginale  $Q = Q^{(n, r)}$ run through a complete
representative system 
  of all primitive classes $\{ Q\}$ of $n \times r $  matrices and let
  $(C_1,D_1)$ run independently through a complete representative
  system of all classes $\{ C_1,D_1 \}$ of coprime symmetric pairs of
  matrices $C_1 = C_1^{(r)}$ and $D_1 = D^{(r)}_1$ with rank $C_1 =
  r$. To every $Q$ we make correspond only one of the matrices, say
  $u_2$, obtained by completing $Q$ arbitrarily to a unimodular
  matrix. Then we obtain a complete representative system of all
  classes $\{C,D \}$ of coprime symmetric pairs $(C,D)$ where $C =
  C^{(n)}$, $D = D^{(n)}$ and rank $C = r$, in the form 
\begin{equation*}
C =  \begin{pmatrix}C_1 & 0 \\ 0 & 0 \end{pmatrix}u'_2   ,   D
= \begin{pmatrix}D_1 & 0 \\ 0 & E \end{pmatrix} u^{-1}_2, \tag{21}\label{eq21} 
\end{equation*}
different choices of $C_1$, $D_1$, $Q$ Leading to different classes.
\end{lem} 


\begin{proof}% proof
Only the last part of the Lemma remains to be proved.
$$
\text{ Let }\quad C =  \begin{pmatrix}C_1 & 0 \\ 0 &
  O \end{pmatrix}u'_2,  D = \begin{pmatrix}D_1 & 0 \\ 0 &
  E \end{pmatrix} u^{-1}_2 
$$
and  
$$
C^* =  \begin{pmatrix}C_1^* & 0 \\ 0 & 0 \end{pmatrix}u^{*'}_2   ,
D^* = \begin{pmatrix}D_1^* & 0 \\ 0 & E \end{pmatrix} u^{*^{-1}}_2 
$$
and let $\{ C, D\} = \{ C^* , D^* \}$ i.e. to say 
\begin{equation*}
C^* D' - D^* C' = 0 \tag{22}\label{eq22} 
\end{equation*}
\end{proof}

Then we have to show that $\{ C_1, D_1 \} =\{C^*_1, D^*_1 \}$ and $\{Q 
\} = \{ Q^* \}$  in an obvious notation. 

From (\ref{eq22}) we get
$$
\begin{pmatrix}C^*_1 & 0 \\ 0 & 0  \end{pmatrix} u^{*'}_{2}
u^{-1}_2 \begin{pmatrix}D'_1 & 0 \\ 0 & E \end{pmatrix}
= \begin{pmatrix}D^*_1 & 0 \\ 0 & E \end{pmatrix} u^{*^{-1}}_2
u_2 \begin{pmatrix}C'_1 & 0 \\ 0 & 0 \end{pmatrix} 
$$
or with the notation
$$
u^{*'}_2   u^{'{-1}}_2 = \begin{pmatrix}v_1 & v_2 \\ v_3 &
  v_4 \end{pmatrix},   u^{*^{-1}}_2   u_2 = \begin{pmatrix}W_1 & W_2 
  \\ W_3 & W_4 \end{pmatrix} 
$$

We have $\begin{pmatrix}C^*_1 V_1 D'_1 & C^*_1 V_2 \\ 0 &
  0 \end{pmatrix}$ = $\begin{pmatrix}D^* W_1 C'_1 & 0 \\ W_3 C_1 &
  0\end{pmatrix}$ 
from which\pageoriginale it follows that $C^*_1   V_2$ and $W_3   C'_1$  and
consequently $V_2$ and $W_3$ are zero (as $C_1$, $C^*_1$ are
nonsingular). 

Since $u^{*^{-1}}_2   u_2$ is unimodular, the above implies that $W_1$
is unimodular. Let $u_2 = (Q \;  R)$ and $u^*_2 = (Q^*   R^*)$ where $Q
= Q^{(n,r)}$ and $Q^* = Q^{*^{(n,r)}}$. Then 
$$ 
(Q \; R) = u_2 = u^*_2 
\begin{pmatrix}
W_1 & W_2 \\ 
W_3 & W_4 
\end{pmatrix}
= (Q^*   R^*) 
\begin{pmatrix}
W_1 & W_2 \\ 
0 & W_4 
\end{pmatrix} 
= (Q^* W_1   *) 
$$

So that $Q = Q^* W_1$. This in turn implies that $\{Q \} = \{Q^* \}$
which is one of the two results we were after. Since $Q$ has been
assumed to run through a representative system of the classes $\{Q\}$,
it follows that $Q = Q^*$ and therefore $u_2 = u^*_2$. This in its
implies that $W_1 = E$ and $V_1 = E$. Hence $C^*_1   D'_1 = D^*_1
C'_1$ or $\{ C_1, D_1\} = \{ C^*_1, D^*_1 \}$. If we assume that just
one element is taken from each class $\{C , D \}$, then of course we
should have $C_1 = C^*_1$ and $D_1 = D^*_1$. 

It remains to prove that the class $\{ C, D \}$ does not depend on the
manner in which $Q$ is completed to a unimodular matrix. For this, we
suppose in the above that $C_1 = C^*_1$ and $D_1 = D^*_1$ and $Q =
Q^*$. Then if follows that $W_1 = E$, $W_3 = 0$, $V_1 = E$ and $V_2 =
0$. We have to deduce that $\{ C, D \} = \{ C^*, D^* \}$, whatever
$R$, $R^*$ be. Now 
\begin{align*}
C^* D' - D^* C' & = 
\begin{pmatrix}
C^*_1 & 0 \\ 
0 &  0 
\end{pmatrix} \begin{pmatrix}V_1 & V_2 \\ V_3 &
  V_4 \end{pmatrix} \begin{pmatrix}D'_1 & 0 \\ 0 & E \end{pmatrix}\\[5pt]
& \qquad 
- \begin{pmatrix} D^1 & 0 \\ 0 & E \end{pmatrix} \begin{pmatrix} W_1 &
  W_2 \\ W_3 & W_4 \end{pmatrix} \begin{pmatrix}C' & 0 \\ 0 &
  0 \end{pmatrix}\\[5pt] 
& = \begin{pmatrix}C^*_1   D'_1 & 0 \\ 0 & 0 \end{pmatrix}
- \begin{pmatrix}D^*_1   C'_1 & 0 \\ 0 & 0 \end{pmatrix} = 0 
\end{align*}
This completes the proof.


