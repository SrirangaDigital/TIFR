
\chapter{Parametrization of sets of integral submanifolds}\label{chap1} %chap 1

\section{}\label{chap1:sec1.1} % section 1.1

In\pageoriginale order to illustrate the problem with which we will be concerned in
this chapter, let us consider an ordinary differential equation,  for
instance,  
$$
\frac{du}{dx} = F(x)
$$
where $F$ is defined and real analytic in a neighbourhood of $x =
0$. Then there exists a unique function $u(x,  w)$,  real analytic in
$x$,  depending real analytically on a parameter $w$ such that for
sufficiently small fixed $w,  u(x,  w)$ is a solution of the
differential equation and $u(o,  w) = w$. Thus the solutions are
parametrized by the parameter $w$. 

More generally,  in order to consider the situation independent of the
coordinate systems,  we shall use the following terminology. We say
that a real analytic function $v(x,  w_1,  \ldots,  w_h)$ is a
parametrisation of solutions of the equation,  when,  for any fixed
$(w^o_1, \ldots,  w^o_h)$,  with $w^o_i$ small,  $v(x,  w^o_1,
\ldots,  w^o_h)$ is a solution of the differential equation and
conversely any solution which is sufficiently small at the origin is
obtained by choosing $(w^o_1,  \ldots,  w^o_h)$ suitably. Then,  for
any parametrization,  the number of parameters is the same and is a
constant determined by the equation (being equal to $1$ in the above
instance). 

In\pageoriginale the case of partial differential equations,  the solutions are
often parametrized by arbitrary functions. Take as a simple example,
the partial differential equation 
$$
\frac{\partial u}{\partial x}  =0, 
$$
where $u$ is an unknown function of the variables $(x,  y)$. Then,
for any real analytic function $f(y),  u = f(y)$ is a solution of the
above differential equation and any real analytic solution is so
obtained. In such a case the solutions of the partial differential
equation are said to depend on one arbitrary function in two
variables. However,  no strict definition of this notion is known. As
a consequence,  the number of arbitrary functions on which the
solutions of the equation depend may not be an invariant of the
equation. For instance,  we can give another parametrization of the
solution of the above partial differential equation,  in which the
solutions depend on two arbitrary functions. Namely,  for any two real
analytic functions $f = \sum a_n ~ y^n,  g = \sum b^n ~ y^n$,  we
associate a solution $u = \sum (a_n ~ y^{2n} + b_n ~ y^{2n+1})$. The
main purpose of this chapter is to introduce a notion of
parametrization of a set of submanifolds by arbitrary functions. This
notion will be used to define systems of partial differential equations
or an exterior differential system,  the solutions of which depend on
certain number of arbitrary functions. In this definition the number
of arbitrary functions and the number of variables will be invariants
of the system. 

\section{}\label{chap1:sec1.2} % section 1.2

Let\pageoriginale $H_p$ denote the vector space of power series,  in $p$ variables
$x_1,  \ldots,  x_p$,  which are convergent on a neighbourhood of the
origin and with coefficients in the field $C$ of complex numbers. We
set $H_o = C$. If $u > 0,  v > 0$ are real numbers,  let $H_p(u,  v)$
denote the subset of $H_p$ consisting of all power series $\xi$
satisfying the following conditions: On a polydisc $\{ x; | x_r | < u
+ \varepsilon\},  \xi$ converges and $| \xi (x) | < v - \varepsilon$,
where $\varepsilon > 0$ depending on $\xi$. In particular $H_o (u, v)
= \{ z \in C; | z | < v \}$. Let $H^s_p$ denote the direct sum of $s$
copies of the vector spaces $H_p$. 

\begin{defi*}% definition
  By a system $S$ of characters we mean an ordered set of non-negative
  integers $s_o,  s_1,  \ldots ,  s_p$. Denote by $H(S)$ the direct
  sum of $H^{s_o}_o,  H^{s_1}_1,  \ldots,
  H^{s_p}_p. \sum\limits_{q=o}^p ~(H_q(u,  v))^{s_q}$ can be naturally
  identified with a subset of $H(S) = \oplus \sum\limits_{q=o}^p ~
  (H_q)^{s_q}$. We denote the subset by $H(S; u,  v)$. It is clear
  that  
  \begin{enumerate}[\rm (1)]
  \item $H(S; u,  v) \subseteq H(S; u,  v')$ \quad if   $v \le v'$
  \item $H(S; u,  v) \subseteq H(S; u',  v)$ \quad if $u \ge u'$
  \end{enumerate}
  and therefore $(3) ~ H(S; u,  v) \subseteq H(S; u',  v')$ if $v \le
  v'$ and $u \ge u'$. 
\end{defi*}

Let $K(a)$ denote the open disc in $C$ of radius a about the origin.

\begin{defi*}% definition
  A mapping $\mathscr{C}$ of $K(a)$ into $H(S;u,  v)$ is called a
  regular curve in $H(S; u,  v)$ if each component
  $\mathscr{C}(z)_\lambda (x_1,  \ldots,  x_q)$ is an analytic
  function in $(z,  x_1,  \ldots,  x_q)$ for $| z | <  a,  | x_i | >
  u$. 
\end{defi*}

Let $S' = (s'_{o},  s'_{1},  \ldots,  s'_{p'})$ be another system of
characters. ($p'$ may be different from $p$). 

\begin{defi*}% definition
  A\pageoriginale mapping $F$ of $H(S; u,  v)$ into $H(S'; u',  v')$ is said to be regular if 
  \begin{enumerate}[\rm (i)]
  \item $f(0) = 0 \in H(S')$, 
  \item for any regular curve $\mathscr{C}$ in $H(S; u,  v) f \circ
    ~\mathscr{C}$ is a regular curve in $H(S'; u',  v')$. 
  \end{enumerate}
\end{defi*}

\begin{proposition}\label{chap1:sec1.2:prop1}% proposition 1
  If $F$ is a regular mapping of $H(S; u,  v)$ into $H(S'; u',  v')$
  then for any real number $b$ with $0 < b \le  1$ and for any
  $\varepsilon > 0$ 
  $$
  F[H(S; u,  bv)] \subseteq H(S',  u',   bv')
  $$
\end{proposition}

\begin{proof}% proof
  Take a $\xi$ in $H(S; u,  v)$. For sufficiently small $\varepsilon',
  z\xi$ is in $H(S; u, v)$ if $| z | \le 1 + \varepsilon'$. Hence $z
  \rightarrow z \xi$ is a regular curve in $H(S; u,  v)$. $F$ being
  regular,  $F(z \xi)$ is a regular curve in $H(S'; u',  v')$. The
  component $F(z \xi)_\lambda (x_1,  \ldots,  x_q)$ is an analytic
  function $f(z)$ for any fixed $(x_1,  \ldots,  x_q)$ with $| x_r | <
  u'$ and $| z | \le  1 + \varepsilon'. f(z)$ satisfies the following
  two conditions: 
  $$
  \displaylines{\hfill 
  f(0) = 0 ~ \text{ since } ~ F(0) = 0 \hfill \cr
  \text{and}\hfill  
  | f(z) | \le v' ~ \text{ for } ~ | z | \le 1 + \varepsilon'\hfill }
  $$
  Hence by Schwarz's lemma it follows that $| f(z)| \le \dfrac{b}{1 +
    \varepsilon'} v'$ for $| z | < b < 1$. Therefore the image of $H(S;
  u,  bv)$ is contained in $H(S'; u',  bv')$. 
\end{proof}

\begin{proposition}\label{chap1:sec1.2:prop2}% proposition 2
  If $F$ and $G$ are two regular maps of $H(S; u,  v)$ into $H(S'; u',
  v')$ and $H(S'; u',  v')$ into $H(S''; u'',  v'')$ respectively,\pageoriginale  then
  $GoF$ is a regular map of $H(S; u,  v)$ into $H(S''; u'',  v'')$. 
\end{proposition}

This follows immediately from the definition of regular maps.

\medskip
\noindent 
\textbf{Germs of regular maps.} We remark that $H(S''; u'',  v'')$ is
contained in $H(S; u,  v) \cap H(S'; u',  v')$ whenever $u'' > u,  u'$
and $v'' < v,  v'$. Let $F_r$ be regular maps of $H(S; u_r,  v_r)$ into
$H(S'; u_r',  v_r') (r = 1,  2)$. We shall introduce an equivalence
relation,  denoted by $\sim$,  in the set of all regular maps as
follows. $F_1$ is said to be \text{equivalent} to $F_2$ (denoted by
$F_1 \sim F_2$) if there exist $u > u_1,  u_2$ and $v < v_1,  v_2$
such that the restrictions of $F_1$ and $F_2$ to $H(S; u,  v)$ are
equal. Clearly $\sim$ is an equivalence relation. 

\begin{defi*}% definition
  An equivalence class of regular maps under $\sim$ is called a
  {\em{germ of regular maps}} of $H(S)$ into $H(S')$. 
\end{defi*}

A germ of regular maps containing a representative $F$ is denoted by
$\mathscr{F}$. 

Let us introduce the following notations. For any $\xi \in H_p$ and $z
\in C$ we define $[\xi. z] \in H_p$ by setting $[\xi.z] ~ (x_1,
\ldots,  x_p) = \xi (z ~ x_1,  \ldots,  z ~ x_p)$ and for any $\xi \in
H(S)$ we define $[\xi. z]$ in $H(S)$ by setting $[\xi ~ z]_\lambda =
[\xi_\lambda ~z]$. Then clearly 
\begin{enumerate}[(i)]
\item if $\xi \in H_p (u,  v) ~ ,  ~ [\xi,  z] \in H_P (| z |^{-1} ~
  u,  v)$ and hence 
\item if $\xi \in H(S; u,  v),  [\xi,  z] \in H(S; | z |^{-1} u,  v)$.
\end{enumerate}

Further,  for $w \in \mathbb{C}$ and for any $\xi \in H(S; u,  v)$,
we define $w \xi$ in $H(S; u,  wv)$ by the usual multiplication by
$w$. 

\begin{proposition}\label{chap1:sec1.2:prop3}% proposition 3
  If\pageoriginale $F_r (r = 1,  2)$ are two regular maps of $H(S; u,  v)$ into
  $H(S'; u'_r,  v_r')$ such that $F_1 \sim F_2$ then $F_1 = F_2$. 
\end{proposition}

\begin{proof}% proof
  $F_1$ and $F_2$ being equivalent there exist $u^* \geq u$ and $v^*
  \leq v$ such that $F_1$ and $F_2$ coincide on $H(S; u^*,
  v^*)$. Take $\xi$ in $H(S; u,  v)$. There exists an $\varepsilon >
  0$ such that $\xi_\lambda$ is convergent in the polydisc of radius
  $(1+2 \varepsilon)u$ and $(1+ 2 \varepsilon) | \xi_\lambda (x_1,
  \ldots,  x_q) | < v$ for $| x_i | < u (i = 1,  \ldots,  q)$. Let $f$
  be the mapping of $K(1 + \varepsilon)$ defined by $f(z) = z [\xi
    . z]. f$ is a regular curve in $H(S; u,  v)$ and  $f(z)$ is in
  $H(S; u^*,  v^*)$ for $z$ sufficiently near the origin,  say for
  example $| z | < \delta$. Then $F_1 (f(z)) = F_2(f(z))$ for $| z | <
  \delta$. 
\end{proof}

  Hence by the theorem of uniqueness $F_1(f(1)) = F_2(f(1))$
  which means $F_1(\xi) = F_2(\xi)$. A germ $\mathscr{F}$ of regular
  maps of $H(S)$ into $H(S')$ is said to be defined at an element $\xi$
  in $H(S)$ if there exists a representative $F$ of  $\mathscr{F}$
  defined on $H(S; u,  v)$ with  $ \xi \in H(S; u,  v)$. The value
  $F(\xi)$ is said to be the value of the germ $\mathscr{F}$ at
  $\xi$. 
  The Proposition \ref{chap1:sec1.2:prop3} shows that the value of the germ $\mathscr{F}$
  at an element $\xi$ in $H(S)$ is uniquely defined and independent of
  the choice of the representatives. 

\noindent 
\textbf{Composition of regular maps.} Let $\mathscr{F}$ and
$\mathscr{G}$ be two germ of regular maps of $H(S)$ into $H(S')$ and
$H(S')$ into $H(S'')$ respectively. We say that the composition of
$\mathscr{F}$ and $\mathscr{G}$ are defined whenever there exist
representatives $F$ of $\mathscr{F}$ and $G$ of $\mathscr{G}$ such
that $F$ is a regular map of $H(S; u,  v)$ into $H(S'; u',  v')$ and
$G$ is a regular map of $H(S'; u',  v')$ into $H(S'', u'',  v'')$. 

It\pageoriginale is clear that the composition of any two germs need not always be
defined. Whenever the composition of  two germs $\mathscr{F}$ and
$\mathscr{G}$ is defined,  the germ of regular maps of $H(S)$ into
$H(S'')$ containing $GoF$ as a representative is called the composition
of $\mathscr{F}$ and $\mathscr{G}$ and is denoted by $\mathscr{G} o
\mathscr{F}$. It is clear that $\mathscr{G} o \mathscr{F}$ is uniquely
defined. 

\begin{defi*}% definition
  A mapping $F$ of $H(S_1; u_1, v_1)$ into $H(S_2; u_2,  v_2)$ is said
  to be {\em{infinite analytic }} if  
  \begin{enumerate}[\rm (i)]
  \item $F$ is regular and 
  \item there exist strictly positive real numbers $u_*,   v,   v',
    w$ and an integer $k$ such that for every $u$ with $0 < u < u_*$ there
    exists a regular map $F_u$ of $H(S_1; u,  u^k ~v)$ into $H(S_2;
    wu,  v')$ which is equivalent to $F$. 
  \end{enumerate}
\end{defi*}

$k$ is called a \textit{degree} of the infinite analytic map $F$.

\begin{remarks*}% remark
  Let $F$ be an infinite analytic map of $H(S_1; u_1,  v_1)$ into
  $H(S_2$; $u_2,  v_2)$ and let $u_*,  v,  v',  w$ and $k$ satisfy the
  conditions of the above definition. 
  \begin{enumerate}[(i)]
  \item Any $w' \le w$ also satisfies the requirement because obviously
    \break 
    $H(S_2; wu,  v')$ is contained in $H(S_2; w'u,  v')$. 
  \item Any $k' \ge k$ also satisfies the requirement. Hence we can
    without loss of generality assume $k$ to be non-negative. 
  \end{enumerate}
\end{remarks*}

\begin{defi*}% definition
  A germ of regular maps is called a germ of {\em{infinite analytic
      maps}}  if every representative of it is an infinite analytic
  map. 
\end{defi*}

We\pageoriginale shall give two simple examples of infinite analytic maps.

\begin{example}\label{chap1:sec1.2:exp1}% example 1
  The mapping $F$ defined by $\xi (x) \rightarrow \dfrac{d \xi }{d x
  }(x)$ of $H_1(u,  v_1)$ into $H_1 \left(\dfrac{u}{2},  \dfrac{2v_1}{u}\right)$
  is an infinite analytic map as can be seen using Cauchy's integral
  formula. Here we can take $k =1,  w = \dfrac{1}{2},  v = v_1$  and
  $v' = 2v_1$. 
\end{example}

\begin{example}\label{chap1:sec1.2:exp2}% example 2
  The mapping $F$ defined by $\xi (x) \rightarrow \int_0^x \xi (y) dy$
  of $H_1(u,  v_1)$ into $H_1(u,  uv_1)$ is infinite analytic. Here we
  can take $k = - 1,  w = 1 ,  v = v_1 = v'$. 
\end{example}

\begin{proposition}\label{chap1:sec1.2:prop4}% proposition 4
  If $\mathscr{F}$ and $\mathscr{G}$ are germs of infinite analytic
  maps of $H(S)$ into $H(S')$ and $H(S')$ into $H(S'')$ then the composition
  $\mathscr{G} ~o ~ \mathscr{F}$ is always defined and is a germ of
  infinite analytic maps of $H(S)$ into $H(S'')$. 
\end{proposition}

\begin{proof}% proof
  Let $F$ (\resp  $G$) be a representative of $\mathscr{F}$
  (\resp  $\mathscr{G}$). Then there exist $(u_*,  v,  v',  w,  k)$ and
  $(\bar{u}_*,  \bar{v},  \bar{v}',  \bar{w},  \bar{k})$ as in the
  definition of an infinite analytic map and $F_a \in \mathscr{F}$ and
  $G_b \in \mathscr{G}$ for any $0 < a < u_*$ and $0 < b <
  \bar{u}_*$. According to remark $1$,  we assume $k > 0,  k' >0$ and
  by remark $2$,  we can choose a $w$ so small that $(u_* w)^{\bar{k}}
  \bar{v}(2v')^{-1} < 1$ and $wa < \bar{u}_*$ for any $0 < a <
  u_*$. Let $v_2 = w^{\bar{k}}\bar{v} ~ v ~ (2v')^{-1}$. For an $a$
  with $0 < a < u_*,  F_a$ is a regular map of $H(S; a,  a^k v)$ into
  $H(S' ; wa,  v')$. From Proposition \ref{chap1:sec1.2:prop1} it follows that for a
  constant $c$ with $0 < c < 1$ the image of $H(S; a,  ba^k v)$ by
  $F_a$ is contained in $H(S'; wa,  bv')$. Taking $b =
  \dfrac{a^{\bar{k}} v_2}{v}$ the image of $H(S; a,  a^{k + \bar{k}}
  v_2)$ by  $F$ is contained in $H(S'; wa,
  (wa)^{\bar{k}}\bar{v}_1)$.\pageoriginale Now since $wa < \bar{u}_*$ we have that
  the image of $H(S'; wa,  (wa)^{\bar{k}}\bar{v})$ by $G_{wa}$ is
  contained in $H(S''; \bar{w}wa,  \bar{v}^1)$. Thus the composite of
  $G_{wa}$ and $F_a$ is defined and this completes the proof. 
\end{proof}

If $1 \geq 0$ is an integer let $H^{(1)}_p$ denote the vector subspace
of $H_p$ consisting of all $\xi \in H_p$ such that the first non-zero
term,  in the expansion of $\xi$ in terms of homogeneous polynomials,
is of degree $\geq 1$. In other words $\xi$ can be put in the form  
$$
\displaylines{\hfill 
\xi = \sum_{k \geq 1} A_k (x_1,  \ldots,  x_p)\hfill \cr
\text{where}\hfill A_k(x_1,  \ldots,  x_p)\hfill }
$$ 
denotes a homogeneous polynomial in
$x_1,  \ldots,  x_p$ of degree $k$. If $S = (s_o,  \ldots,  s_p)$ is a
system of characters,  we can define the vector subspace $H(S)^{(1)}$
of $H(S)$ by  
$$
H(S)^{(1)} = \{ \xi \in H(S) : \xi_\lambda \in H^{(1)}_q\}
$$
For any subset $A$ of $H(S)$ we denote by $H(S; u,  v)^{(1)}$ the set
$H(S; u,  v) \cap H(S)^{(1)}$. 

\begin{defi*}% definition
  For an element $\xi $ in $H_p(u,  v)$ the norm of $\xi $ in $H_p(u,
  v)$,  denote by $| \xi |_u $,  is defined by  
  $$
  | \xi |_u = \sup_x  ~ \{ | \xi (x) | : | x_r | < u (r = 1,  \ldots,  p)\}
  $$
  and for any $\xi$ in $H(S; u,  v)$ the norm of $\xi$ in $H(S; u,
  v)$,  denoted again by $| \xi |_u$,  is defined by  
  $$
  | \xi |_u = \max_{\lambda} | \xi_\lambda |_u
  $$
  In\pageoriginale a phrase like ``$\xi$ is in $H(S; u,  v)$ and $| \xi|_u$'' we
  shall often omit ``$\xi$ is in $H(S;u,  v)$ and'' when there is no
  possible confusion. 
\end{defi*}

The following proposition is an immediate consequence of the above
definitions.

\begin{proposition}\label{chap1:sec1.2:prop5}% proposition 5
  An element $\xi$ in $H(S)$ belongs to $H(S)^{(1)}$ if and only if
  there exists a constant $C > 0$ such that for sufficiently small $u$
  we have the inequality $| \xi |_u \le c.u^1$. 
\end{proposition}

\begin{proposition}\label{chap1:sec1.2:prop6}% proposition 6
  Let $F$ be a regular map of $H(S; u,  v)$ into $H(S'; u',
  v')$. Then for any $\xi,  \eta \in H(S; u,  v/4)$,  
  $$
  | F(\xi) - F(\eta)|_{u'} \leq K | \xi - \eta |_u
  $$
  where $K$ is a positive constant.
\end{proposition}

\begin{proof}
  Suppose $\zeta = \xi -\eta$,  the function $f(z) = \xi - z \zeta$ is
  in $H(S; u,  v)$ for $| z | < R = \dfrac{3v}{4} | \zeta |^{-1}_u (1
  + \varepsilon)$. The function $g(z) = F[f(0)]_\lambda (x) -
  F[F(z)]_\lambda (x)$ is a holomorphic function for $| z | < R$ and
  for $x$ in $| x_r | < u'.  g(z)$ satisfies the conditions: $(i) |
  g(z) | < 2 ~ v'$ and $g(0) = 0$. By Schwarz's lemma $|g(z) | <
  \dfrac{2v' ~ | z |}{R}$ and hence we have 
  $$
  | g(1) | = | F(\xi) - F(\eta) |_{u'} < K \frac{v'}{v} | \xi - \eta |_u
  $$
  if we take $K \geq \dfrac{8}{3}$.
\end{proof}

The Propositions \ref{chap1:sec1.2:prop1} and \ref{chap1:sec1.2:prop5}
together imply the following. 

\begin{proposition}\label{chap1:sec1.2:prop7}% proposition 7
  If $\mathscr{F}$ is a germ of infinite analytic maps (of degree $k$)
  of $H(S)$ into $H(S')$ and if $\xi \in H(S)$ then there exists\pageoriginale a
  positive real number $a_1$ depending on $\xi$ such that for any $a <
  a_1,  ~ \xi$ is defined at $a \xi$; moreover if $\xi$ is in
  $H(S)^{(1+k)}$ (with $1 \geq 0,  1 + k \geq 0$) then $\mathscr{F}
  (a\xi)$ is in $H(S')^{(1)}$.  
\end{proposition}

\section{Regular linear maps}\label{chap1:sec1.3}

Let $S,  S'$ be two systems of characters.

\begin{defi*}% definition 
  A regular map $F$ of $H(S; u,  v)$ into $H(S'; u',  v')$ is said to
  be linear if the following condition is satisfied: for every $\xi,
  \eta$ in $H(S; u,  v)$ such that $\alpha \xi,  \beta \eta$ and
  $\alpha \xi + \beta \eta$ are in $H(S; u,  v)$ where $\alpha,  \beta
  \in C$ 
  $$
  F(\alpha \xi + \beta \eta) = \alpha F(\xi) + \beta ~ F(\eta).
  $$
\end{defi*}

For a strictly positive real number $u$ let $H(S; u) =
\bigcup\limits_{v > 0} ~ H(S; u,  v)$. Clearly $H(S; u)$ is a
subvector space of $H(S)$. Let $F$ be a regular linear map of $H(S; u,
v)$ into $H(S'; u',  v')$. Then,  for any $\xi \in H (S; u)$ and
$\alpha \in C$ such that $\alpha \xi \in H(S; u,  v),  \alpha^{-1}
F(\alpha \xi)$ does not depend on the choice of such an $\alpha$. 

In fact,  if $\beta \in C$ such that $\beta \xi$ is also in $H(S; u,
v)$,  we  see that $F(\alpha \xi) = \beta^{-1} \alpha F(\beta
\alpha^{-1} \alpha \xi) = \beta^{-1} \alpha ~ F(\beta \xi)$ by the
linearity of $F$. 

Setting $F'(\xi) = \alpha^{-1} F(\alpha \xi)$ with $\alpha \in C$ such
that $\alpha \xi$ is in $H(S; u,  v)$ we obtain a map $F'$ of $H(S;
u)$ into $H(S'; u')$. The restriction of $F'$ to $H(S; u,  v)$ is
equal to $F$ because of the linearity of $F$.  Also the restriction of
$F'$ to $H(S; u,  v')$ for any $v_1 > 0$ is a  regular map and is
equivalent to $F$. Further $F'$ is  a\pageoriginale linear map of the vector space
$H(S; u)$ into the vector space $H(S'; u')$. 

In fact let $\xi,  \eta \in H(S; u)$. Then $F' (\alpha \xi + \beta
\eta) = \gamma^{-1}_1 ~ F(\gamma_1 (\alpha \xi + \beta \eta))$ for
$\alpha,  \beta \in C$ and any $\gamma_1 \in C (\gamma_1 \neq 0)$ such
that $\gamma_1 (\alpha \xi +\beta \eta)$ is in $H(S; u,  v)$. Let
$\gamma_2,  \gamma_3 \in C (\gamma_2,  \gamma_3 \neq 0)$ such that
$\gamma_2 \alpha \xi,  \gamma_3 \beta \eta \in H(S; u,  v)$. If
$\gamma \in C$ is such that $| \gamma | = \min ( | \gamma_1 |,  |
\gamma_2 |,  | \gamma_3 | )$ it follows by linearity of $F$ that  
\begin{align*}
  F' (\alpha \xi + \beta \eta) & = \gamma^{-1} F(\gamma(\alpha \xi +
  \beta \eta)) = \gamma^{-1}\{  F(\gamma \alpha \xi) + F(\gamma\beta
  \eta)\\ 
  & = \alpha (\gamma \alpha)^{-1} F(\gamma \alpha \xi) +
  \beta(\gamma\beta)^{-1}  F(\gamma \beta \eta)\\ 
  & = \alpha F' (\xi) + \beta F' (\eta).
\end{align*}

\begin{defi*}% definition
  A germ $\mathscr{F}$ of regular maps of $H(S)$ into $H(S')$ is said
  to be linear if $\mathscr{F}$ contains a representative $F$ which is
  linear. 
\end{defi*}

\begin{proposition}\label{chap1:sec1.3:prop8} % proposition 8
  If $\mathscr{F}$ is a germ of linear infinite analytic maps then
  $\mathscr{F}$ is defined everywhere and linear mapping of the vector
  space $H(S)$ into $H(S')$. 
\end{proposition}

\begin{proof}
  Every $\xi$ in $H(S)$ is an element of some $H(S; u)$ for
  sufficiently small $u > 0$. There exists a representative $F$ of
  $\mathscr{F}$ defined at $\xi \in H(S; u,  v_1)$ for sufficiently
  small $v_1 > 0$ and this $F$ can be extended into $H(S; u)$. The
  linearity of the germ $\mathscr{F}$ is  clear from the above
  remark. 
\end{proof}

Now\pageoriginale we introduce the following terminology: if $S = (s_o,  \ldots,
s_p)$ with $s_p \neq 0$ is a system of characters then $p$ is called
the degree of $H(S)$ and $s_p$ is called the multiplicity of $H(S)$. 

\begin{defi*}% definition
  Two vector spaces $H(S)$ and $H(S')$ are said to be isomorphic if
  there exist germs of linear infinite analytic maps $\mathscr{F}$ and
  $\mathscr{G}$ of $H(S)$ into $H(S')$ and $H(S')$ into $H(S)$
  respectively such that $\mathscr{G} ~ o ~\mathscr{F}$ and
  $\mathscr{F} ~ o ~ \mathscr{G}$ are germs of identity maps on $H(S)$
  and $H(S')$ respectively. 
\end{defi*}

This is denoted by $H(S) \cong H(S')$.

\begin{proposition}\label{chap1:sec1.3:prop9} % proposition 9
  Two vector spaces  $H(S)$ and $H(S')$ are isomorphic if and only if
  $H(S)$ and $H(S')$ have the same degree and multiplicity. 
\end{proposition}

\begin{proof}% proof
  We shall first prove that $H(S) \cong H(S')$ implies that their
  degrees and multiplicities are the same. 
\end{proof}

We observe that for any integer $1 > 0$ the dimension of the quotient
space $\dfrac{H_r} {H^{(1)}_r}$ is $\begin{pmatrix}  r + 1 -1
  \\ r\end{pmatrix} = \frac{1}{r!} ~1^r + $ (lower powers of $1) = f_r
  (1)$ where $f_r(X)$ denotes the polynomial $\dfrac{X^r}{r!} + ~  *
  ~ X^{r -1} + \cdots$ of degree $r$. Then 
  $$
  \displaylines{\hfill  
    \dim \left(\dfrac{H(S)}{H(S)^{(1)}}\right) = \sum\limits_{r=0}^p ~
    s_r ~ f_r ~ (1)\hfill \cr 
    \text{and}\hfill 
    \dim\left(\frac{H(S')}{H(S')^{(1)}}\right) = \sum_{r = 0}^q ~ t_r ~f_r
    ~(1)\hfill } 
  $$
where $S = (s_o,  \ldots,  s_p)$ and $S' = (t_o,  \ldots,  t_q)$ with
$s_p \neq 0,  t_q \neq 0$. Let $\mathscr{F}$ and $\mathscr{G}$ be the
germs of linear infinite analytic maps defining the isomorphism. We
can,  without loss of generality assume that $\mathscr{F}$ has a
degree $k > 0$. The map $\mathscr{F}$ of $H(S)$ into $H(S')$ is
surjective and $\mathscr{F}[H(S)^{(1+k)}] \subseteq H(S')^{(1)}$
(Proposition \ref{chap1:sec1.2:prop5}). Hence there exists\pageoriginale an induced
surjective map of $ \dfrac{H(S)}{H(S)^{(1+k)}}$ onto
$\dfrac{H(S')}{H(S')^{(1)}}$ and therefore 
$$
\sum_{r=0}^p ~ s_r ~ f_r ~ (1) \geq \sum_{r =0}^q ~ t_r  ~ f_r(1).
$$

For large 1,  the comparison of dominant factors on either side of
the inequality shows that $p \geq q$. Similarly,  we show that $q \leq
p$ and hence $p = q$. Then,  by the same reasoning we show that $s_p =
s_q$. 

The converse is proved in two steps.
\begin{case}\label{chap1:sec1.3:case1}% case 1
  Consider the particular case where $t_\nu = s_\nu + s_{\nu+1} +
  \cdots + s_p$ for $\nu = 0,  \ldots,  p$. Let $\xi_r$ be the
  component of $\xi \in H(S)$ in $H^{s_r}_r$. Then $\xi_r$ has $s_r$
  components which are functions of $(x_1,  \ldots,  x_r)$ and we
  denote them by $\xi^1_r, \ldots, \xi^{s_r}_r$ when $s_r \neq
  0$. Similarly $\eta^\sigma_r$ (for $\sigma = 1,  \ldots,  s_r +
  \cdots + s_p$) denote the components of an element $\eta$ in
  $H(S')$. 
\end{case}

Now we define a linear map $F$ of $H(S)$ into $H(S')$ as follows: for
any $\xi \in H(S)$ set 
$$
F(\xi)_o^{s_o + \cdots + s_{q-1} + \sigma} = \xi^\sigma_q (0) \text{ for
} q = 0,  \ldots,  p ; \sigma = 1,  \ldots,  s_q \text{ if } s_q \neq
0 
$$
where $s_o + \cdots + s_{q-1} + \sigma$ means $\sigma$ when $q = 0$ and 
$$
F(\xi)^{s_r + \cdots + s_{q-1} + \sigma}_r = \frac{\partial ~
  \xi^\sigma_q}{\partial ~ x_r} \text{ for }^{ r = 1, \ldots,  p; q =
  r,  \ldots,  p}_{\sigma = 1,  \ldots,  s_q \text{ if } s_q \neq 0} 
$$
where $s_r + \cdots + s_{q-i} + \sigma$ means $\sigma$ when $q =
r$. Clearly $F$ is infinite analytic (see ex.\ref{chap1:sec1.2:exp1}
of \S  \ref{chap1:sec1.2}) and
linear because of the linearity of  partial\pageoriginale derivation. The inverse
map $G$ of $H(S')$ into $H(S)$ is defined as follows: for any $\eta
\in H(S')$ set  
\begin{multline*}
  G(\eta)^\lambda_r = \eta_o^{s_o + \cdots + s_{r-1} + \lambda} +
  \int_o^{x_1} \eta_1^{s_1 + \cdots + s_{r-1} + \lambda} dx_1\\ 
  +\int_o^{x_2} \eta^{s_2 + \cdots + s_{r-1} + \lambda}_2 + \cdots +
  \int_o^{x_r} \eta^\lambda_r ~ d ~ x_r 
\end{multline*}
for $r =1,  \ldots,  p; \lambda = 1,  \ldots,  s_r$ if $s_r \neq
0$. Again $G$ is infinite analytic (ex.\ref{chap1:sec1.2:exp2} of
\S  \ref{chap1:sec1.2}) and linear
because of the linearity of integration. Obviously $F$ and $G$ are
surjective. It is easy to verify that $F$ and $G$ define germs of
infinite analytic maps $\mathscr{F}$ and $\mathscr{G}$,  and that
$\mathscr{G} ~o ~ \mathscr{F}$ (\resp  $\mathscr{F} ~ o ~ \mathscr{G}$)
is identity on $H(S)$ (\resp  $H(S')$). The assertion follows in this
special case. 

\begin{case}\label{chap1:sec1.3:case2}% case 2
  To prove the assertion in the general case we shall write $H(S)$
  explicitly as $H(s_o,  \ldots,  s_p)$. It is sufficient to prove
  that $H(S) \cong H^{s_p}_p$. In view of the Case $1$ proved above we
  can without any loss of generality assume that $s_o \neq 0,  \ldots,
  s_p \neq 0$ because $H(S)$ is isomorphic to $H(s_o + \cdots + s_p,
  \ldots,  s_{p-1} + s_p,  s_p)$. Then it can be seen without much
  difficulty that our assertion is a consequence of the following
  statement: if $s_r > 0,  \ldots,  s_p > 0,  H(0,  \ldots,  0,  s_r,
  s_{r+1},  \ldots,  s_p) \cong H(0,  \ldots,  0,  s_r - 1,  s_{r+1},
  \ldots,  s_p)$. Now since $H_r \cong H_o \oplus H_1 \oplus \cdots
  \oplus H_r$  we can write 
\begin{align*}
  H & (0,  \ldots,  0,  s_r,  s_{r+1},  \ldots,  s_p)\\ 
  &\cong H(0,  \ldots,
  0,  s_r -1,  s_{r+1},  \ldots,  s_p) \oplus H_r \\
    & \cong ~ H(0, \ldots, 0, s_r -1,  s_{r+1},  \ldots,  s_p) \oplus
    (H_o \oplus \cdots \oplus H_r)\\ 
    & \cong ~ H(0,  \ldots,  0,  s_r -1,  s_{r+1}-1,  \ldots,  s_p)
    \oplus H_{r+1} \oplus (H_o \oplus \cdots \oplus H_r)\\ 
    & \cong ~ H(0,  \ldots,  0,  s_r -1,  s_{r+1}-1,  \ldots,  s_p)
    \oplus H_{r+1}\\ 
    & \cong ~ H(0,  \ldots,  0,  s_r -1,  s_{r+1},  \ldots,  s_p).
  \end{align*}
\end{case}\pageoriginale

This is obtained by successive usage of Case \ref{chap1:sec1.3:case1}.

\section{}\label{chap1:sec1.4}% section 1.4

Hitherto we had restricted our attention to the case of convergent
power series with coefficients in the field of complex numbers. We
can,  without much difficulty,  extend all the notions and results
proved in the previous sections to the case of convergent power series
with real coefficients (which we call,  hereafter,  as the real
analytic case). Let $S = (s_o,  \ldots,  s_p)$ be a system of
characters. Let $H^R(S)$ denote the (real) subvector space of $H(S)$
consisting of $\xi$ in $H(S)$ with all its components $\xi^\lambda_r$
to be real analytic. Set $H^R(S; u,  v) = H^R(S) \cap H(S; u,  v)$. A
mapping $F_R$ of $H^R(S; u,  v)$ into $H^R(S'; u',  v')$ is said to be
a (real) regular map if there exists a regular map $F_C$ of $H(S; u,
v)$ into $H(S'; u',  v')$ such that $F_R$ is the restriction of $F_C$
to $H^R(S; u, v). F_C $ is called the complexification of $F_R$ and by
the theorem of identity it can be shown that such an $F_C$ is
unique. Two (real) regular maps are said to be equivalent when their
complexifications are equivalent. Thus we can define germs of (real)
regular maps of $H^R(S)$ into $H^R(S')$. Similarly we can define the
notion of germs of infinite analytic maps of $H^R(S)$ into $H^R(S')$
when there exist germs of infinite analytic map of $H(S)$ into $H(S')$
mapping $H^R(S)$ into $H^R(S^1)$.\pageoriginale 

\section{Examples}\label{chap1:sec1.5}% section 1.5


\begin{enumerate}[(1)]
\item Consider a system of functions
  $$
  A_\lambda (y_1,  \ldots,  y_s,  x_{p+1},  \ldots,  x_{p+1}) \quad
  (\lambda = 1,  \ldots,  s') 
  $$
  defined and analytic in the domain $| y_\sigma | \leq v,  | x_{p +
    \mu } | \leq u^*$  $(\sigma = 1,\break  \ldots,  s ; \mu = 1,  \ldots,  1)$
  satisfying the following conditions:  
  \begin{enumerate}[(i)]
  \item $A_\lambda (0,  \ldots,  0) = 0$
  \item $| A_\lambda  (y_1,  \ldots,  y_s ; x_{p+1},  \ldots,  x_\mu)
    | < v' - \varepsilon$ for small $\varepsilon > 0$ in the above
    domain. 
  \end{enumerate}
  
  Then we define a regular map $F_u$ of $H^s_p(u, v)$ into
  $H^{s'}_{p+1}(u,  v')$ for every $0 < u < u^*$ by setting 
  \begin{multline*}
  [F_u ~ (\xi)]_\lambda (x_1,  \ldots,  x_{p+1})\\ 
  = A_\lambda (\xi_1(x_1,
  \ldots,  x_p),  \ldots,  \xi_s (x_1,  \ldots,  x_p),  x_{p+1},
  \ldots,  x_{p+1}) 
  \end{multline*}
  for every $\lambda$ with $1 \leq \lambda \leq s'$. The germ of $F_u$
  can be verified to be a germ of infinite analytic maps. 
\item Consider a system of functions
  $$
  A_\lambda (x_1,  \ldots,  x_{p+1},  y_1,  \ldots,  y_s,  \ldots,
  y^r_\mu,  \ldots) \quad (\lambda = 1,  \ldots,  s) 
  $$
  defined and analytic in the domain $| x_i | < v^*,  | y_\mu | < v^*,
  | y^r_\mu | < v^*$\break  $(i=1,  \ldots,  p+1; r=1,  \ldots,  p; \mu =1,
  \ldots,  s)$. Consider the system of partial differential equations 
  $$
  (\mathscr{U}) \frac{\partial y_\lambda}{\partial x_{p+1}} =
  A_\lambda \left(x_1,  \ldots,  x_{p+1},  y_1,  \ldots,  y_s,  \ldots,
  \frac{\partial ~y_\mu}{\partial ~ x_r},  \ldots\right). 
  $$
\end{enumerate}

Such\pageoriginale a system of partial differential equations is called a system of
partial differential equations of \text{Cauchy-Kowalewski type}. 
 
Now we make the following definitions.

\begin{defi*}% definition
  If $\xi \in H^s_p$ such that $| \xi_\lambda (0) | < v^* $ and $\Big|
  \dfrac{\partial \xi_\lambda }{\partial x_r} (0) \Big| < v^*$ for
  $\lambda = 1,  \ldots,  s$  and $r =1,  \ldots,  p$ then an element
  $\eta \in H^s_{p+1}$ is called a {\em{solution}} of Cauchy-Kowalewski
  system $(\mathscr{U})$ with the initial condition $\xi$ if 
  \begin{enumerate}[(i)]
  \item $\eta_\lambda (x_1,  \ldots,  x_p,  0) = \xi_\lambda(x_1,
    \ldots,  x_p) \quad (\lambda = 1,  \ldots,  s)$ 
  \item $y_\lambda = \eta_\lambda$ is a solution of $(\mathscr{U})$.
  \end{enumerate}
\end{defi*}

Let $u,  v,  u',  v'$ be strictly positive  real numbers.
\begin{defi*}% definition
  A mapping $F$ of $H^s_p(u, v)$ into $H^s_{p+1} (u',  v')$ (where $v,
  u^{-1} v < v^*)$ is called a solution mapping of the
  Cauchy-Kowalewski system $(\mathscr{U})$ if,  for every $\xi \in
  H^s_p (u,  v) ~ F(\xi)$ is a solution of $(\mathscr{U})$ with the
  initial condition $\xi$. 
\end{defi*}

We remark that there is no ambiguity in this definition since the
conditions $u,  u^{-1} v < v^*$ imply that $| \xi_\lambda (0) | <
v^*$ and $\Big| \dfrac{\partial ~ \xi_\lambda}{\partial ~x_r}(0) \Big|
< v^* $. 

Now the classical theorem of Cauchy-Kowalewski on the existence and
uniqueness of solutions of Cauchy problems can be generalised as
follows. 

\begin{theorem}\label{chap1:sec1.5:thm1}% theorem 1
~

\begin{enumerate}[\rm (i)]
\item Given an element $\xi$ in $H^s_p$ with $\Big| \xi_\lambda (0)
  \Big| < v^*$ and $\Big| \dfrac{\partial ~\xi_\lambda}{\partial
    ~x_r}(0) \Big| < v^* $,  the solution of the system $(\mathscr{U})$
  with the initial condition $\xi$ is (locally) unique if it exists; 
\item there exists a solution mapping; and\pageoriginale 
\item when $A_\lambda (x, 0) = 0$ for all $\lambda = 1,  \ldots,  s$,
  any solution mapping is a regular map and all solution mappings are
  equivalent to each other. 
\end{enumerate}
\end{theorem}

Assume $A_\lambda (x,  0) = 0 $ for all $\lambda = 1,  \ldots,
s$. Then the solution mappings of the system $(\mathscr{U})$ define a
germ of regular maps called the \textit{solution germ} of the system
$(\mathscr{U})$. 

\begin{theorem}\label{chap1:sec1.5:thm2}% theorem 2
  The solution germ of $(\mathscr{U})$ is a germ of infinite analytic
  maps of $H^s_p$ into $H^s_{p+1}$. 
\end{theorem}

The proofs of these two theorems will be given in
sections \ref{chap1:sec1.7} and \ref{chap1:sec1.9}
respectively. Section \ref{chap1:sec1.6} deals with some preliminaries
required in the following sections. 

\section{}\label{chap1:sec1.6}% section 1.6

We will adopt the following notations in the course of the proofs of
the above two theorems and the preliminary propositions: 
$$
x = (x_1,  \ldots,  s_{p+1}),  y = (y_1,  \ldots,  y_s).
$$

Let $(r,  r',  \ldots,  r_i,  \ldots)$ denote integers running
independently between $1$ and $p$ and $(\lambda,  \lambda',  \ldots,
\lambda_i,  \ldots)$ and $(\mu,  \mu',  \ldots,  \mu_i,  \ldots)$
denote integers running independently between $1$ and $s$. $\beta$
denotes an ordered set of integers $(r_1,  \ldots,  r_h)$ where,  by
the above convention $1 \le r_i \le p$ and $h = | \beta |$ denotes the
length of $\beta$. $k$ or $k_i$ denote integers $0,  1,  2,
\ldots$. When we consider a finite set of $\beta'$s we often use the
notation $\beta(1),  \ldots,  \beta(h)$. We include the case where
$\beta$ is the empty set $(\beta = \phi)$ and in this case $| \beta |
= 0$. For a function $g = g(x_1,  \ldots,  x_{p+1})$,  sufficiently
differentiable,\pageoriginale  we set  
\begin{align*}
  \partial^\beta g & = \frac{\partial^{|\beta|}g}{\partial ~ x_{r_1}
    \cdots \partial ~ x_{r_h}} &&\text{ if  }~ \beta  = (r_1,  \ldots,
  r_h) \neq \phi\\ 
  & = g &&\text{ if }~ \beta = \phi.
\end{align*}

Let us introduce a set of indeterminates $w^{\beta k_1 \cdots k_s}_k$
and $w^{\beta k_1 \cdots k_s;r}_k \lambda \mu$. Let $P$ denote the
ring of polynomials in these indeterminates over the field of rational
numbers. A polynomial $\Phi$ in $P$ is said to be positive whenever
all its coefficients are non-negative. For any system of functions 
$$
C = (\cdots,  C_\lambda (x, y),  \ldots,  C^r_{\lambda\mu}(x, y),  \ldots ), 
$$
sufficiently differentiable and for any $\Phi$ in $P$ we denote by
$\Phi C$ the function,  in $(x, y)$,  obtained in substituting 
\begin{align*}
  & w^{\beta k_1 \cdots k_s;r}_k \lambda =
  \frac{\partial^{k_1}}{\partial y^{k_1}_1} \cdots
  \frac{\partial^{k_s}}{\partial y^{k_s}_s} ~
  \frac{\partial^k}{\partial ~ x^k_{p+1}} ~ (\partial^\beta
  C_\lambda), \\ 
  & w^{\beta k_1 \cdots k_s;r}_k \lambda\mu =
  \frac{\partial^{k_1}}{\partial y^{k_1}_1} \cdots
  \frac{\partial^{k_s}}{\partial y^{k_s}_s} ~
  \frac{\partial^k}{\partial ~ x^k_{p+1}} ~ (\partial^\beta
  C^r_\lambda\mu) \text{ in } \Phi. 
\end{align*}

First we consider the following special case of the system of
equations $(\mathscr{U})$ where $A_\lambda = B_\lambda (x, y) +
B^r_{\lambda\mu}(x, y) \dfrac{\partial ~ y_\mu}{\partial ~
  x_r}$. Consider the system of equations 
\begin{equation}
  \frac{\partial ~y_\lambda}{\partial ~ x_{p+1}} = B_\lambda B(x, y) +
  B^r_{\lambda \mu} (x, y) \frac{\partial ~ y_\mu}{\partial ~ x_r}
  \tag{$\mathscr{L}$} 
\end{equation}
where\pageoriginale the usual tensor summation convention is used. Let  $B = (\cdots,
B_\lambda\break (x, y),  \ldots,  B^r_{\lambda\mu}(x, y), \ldots)$. The
proof of the theorem is given in the following lemmas,  in this
special case. Let $1$ denote an integer $\ge 1$. 

\begin{defi*}% definition
  Given the equation $(\mathscr{L})$,  the property $\{ 1,  \beta\}$
  is said to hold if the following condition is satisfied: there exist
  positive polynomials $^1\Phi^{\beta\mu_1 \cdots \mu_h}_{\lambda \beta(1)
    \cdots \beta(h)}$ in $P$ depending only on $p, s$ and the
  indicated indices (for any $| \beta (1) | + \cdots + | \beta (h) |
  \le | \beta | + 1)$ such that the following equation holds: 
  $$
  (\mathscr{L})^{1, \beta} \frac{ \partial^1(\partial^\beta
    y_\lambda)}{\partial ~ x^1_{p+1}} = \left({^1{\Phi}}^{\beta \mu_1
    \cdots \mu_h}_{\beta(1) \cdots \beta(h)}  ~ B\right)
  \left(\partial^{\beta(1)} y_{\mu_1}\right) \cdots
  \left(\partial^{\beta(h)} y_\mu\right ) 
  $$
  where the usual summation convention is used. ($h=0$ is also included).
\end{defi*}

\begin{lemma}\label{chap1:sec1.6:lem1}% lemma 1
  Given the equation $(\mathscr{L})$,  the property $\{ 1, \beta \}$
  holds. Moreover $^1{\Phi}^{\beta \mu_1 \cdots \mu_k}_{\beta(1)
    \cdots \beta(k)}$ depends only on $p$ and $s$. 
\end{lemma}

\begin{proof}% proof
  The proof follows by induction argument in two steps.  
  \begin{enumerate}[(i)]
  \item As for $\{ 1, \phi\}$ is concerned $^1{\Phi}^{\phi\mu_1 \cdots
    \mu_h}_{\beta(1) \cdots \beta(h)} = 0$ unless $h = 0, 1$. Also
    $^1\Phi^\phi_\lambda = w_o^{\beta 0 \cdots 0; \lambda}$ and when
    $\beta = \{ r\},  ^{1}\Phi^{\phi\mu}_{\lambda \beta} = w^{\beta o,
    \ldots,  o; r}_o \lambda \mu$. Therefore  $(\mathscr{L})^{1,
    \phi}$ is nothing but the system $(\mathscr{L})$. Then we proceed
    by induction on $ | \beta |$ to prove $\{ 1,  \beta\}$. by
    formally differentiating both sides of $(\mathscr{L})^{1, \beta'}$
    with respect to $x_r$ where $\beta'$ is chosen such that
    $\dfrac{\partial}{\partial ~ x_r} (\partial^{\beta'} y) =
    \partial^{\beta} ~ y$. 
  \item Assuming the assertion of the lemma to have been proved for
    $\{ 1', \beta\}$\pageoriginale for any $\beta$ and all $1' < 1$ we can show that
    $\{ 1, \beta\}$ holds on the same lines as above. 
  \end{enumerate}
\end{proof}

\begin{lemma}\label{chap1:sec1.6:lem2}% lemma 2
  Whenever there exists a solution of the system of equations
  $(\mathscr{L})$ exists,  the solution is unique for a given initial
  condition. 
\end{lemma}

\begin{proof}% proof
  Let $\xi ~ \in ~ H^s_p$ and $\eta = (\eta_\lambda)$ be a solution of
  $(\mathscr{L})$ with \break $\eta_\lambda (x_1,  \ldots,  x_p,  0) =
  \xi_\lambda(x_1,  \ldots,  x_p)$. By Lemma \ref{chap1:sec1.6:lem1} 
  \begin{multline*}
    \{ 1,  \phi\} \frac{\partial^1 ~ y_\lambda}{\partial ~ x^1_{p+1}} =
    \left(^1 \Phi^{\phi \mu_1 \cdots \mu_h}_{\beta(1) \cdots \beta(h)} ~ B\right) ~
    \left(\partial^{\beta(1)} ~ y_{\mu_1}\right)\cdots\\ 
    \cdots (\partial^{\beta(h)}
    y_{\mu_h}) ~( | \beta(1) | + \cdots + | \beta (h) | \le 1) 
  \end{multline*}
  is a consequence of the system of equations $(\mathscr{L})$. On the
  other hand $\partial^{\beta(k)} ~ (k =1,  \ldots,  h)$ contain only
  partial derivatives $\dfrac{\partial }{\partial x_r} ~(r =1,
  \ldots,  p)$. Hence if $\xi = (\xi_1(x),  \ldots,  \xi_s(x))$ is the
  given initial condition $(\xi \in H^s_p)$,  the solution $\eta =
  (\eta_\lambda)$ must be of the form 
\begin{multline*}
  \eta_\lambda = \xi_\lambda (x) + \sum_{1=1}^{\infty}
  \frac{(x_{p+1})^1}{1 ~ !} ~ \left(^1 B^{\mu_1 \cdots
    \mu_h}_{\lambda\beta(1) \cdots \beta(h)} ~ (x,  \xi)\right)\\
  (\partial^{\beta(1)} ~ \xi_{\mu_1}) \cdots (\partial^{\beta(h)}
  \xi_{\mu_h}) ~ (| \beta(1) | + \cdots + | \beta (h) | \le 1)
\end{multline*}
  $$
  \displaylines{
  \text{where}\hfill  
  ^1 B^{\mu_1 \cdots \mu_h}_{\lambda\beta(1) \cdots \beta(h)} ~ (x, y)
  = \left(^1 \Phi^{\phi \mu_1 \cdots \mu_h}_{\lambda\beta(1) \cdots
    \beta(h)}. B\right) (x_1,  \ldots,  x_p,  0, y_1,  \ldots,  y_s)\hfill } 
  $$
\end{proof}

The right hand side of the above expansion being unique for a given
$\xi \in H^s_p$,\pageoriginale  it follows that the solution is unique whenever
there exists one. $\quad q.e.d$. 

Let us denote the formal power series representing $\eta_\lambda$ by
$T^{\mathscr{L}}_\lambda (\xi)$. It only remains to prove that
$T^{\mathscr{L}}_\lambda (\xi)$ is convergent in a
neighbourhood of $x_{p+1} = 0$. For this purpose we introduce the
following notation. 

Let $f(v_1, \ldots,  v_h)$ and $g(v_1, \ldots,  v_h)$ be formal power
series in \break $(v_1,  \ldots,  v_h)$.  A power series $g$ is said to be
positive if each coefficient of $g$ is $\ge 0$. A power series $f$ is
said to be majorized by a positive power series $g$ if the absolute
value of each coefficient of $f$ is majorized by the corresponding
coefficient of $g$. This we denote by $f \alpha g$. 

Now consider the system of linear differential equations
\begin{equation}
  \frac{\partial ~ y_\lambda}{\partial ~ x_{p+1}} = C_\lambda ~ (x, y)
  + \sum_{\mu, r} ~ C^r_{\lambda\mu} ~ (x, y) \frac{\partial ~
    y_\mu}{\partial ~ x_r} \tag{$\mathscr{L}$} 
\end{equation}
where $C_\lambda ~ (x, y),  C_\lambda,  ^r_\mu(x, y)$ are
positive. Assume that $B_\lambda \alpha C_\lambda$ and $B^r_{\lambda
  \mu}\break  \alpha
C^r_{\lambda\mu}$. Let $\xi,  \zeta \in H^s_p$ with $\zeta_\lambda$
positive for all $\lambda= 1,  \ldots,  s$. In $\{ \ell,  \phi\},
^\ell\break \Phi^{\phi\mu_1 \cdots \mu_h}_{\lambda\beta(1) \cdots
  \beta(h)}$ are all positive and are independent of $\beta_\mu,
B^r_{\lambda\mu},  C_\mu,  C^r_{\lambda\mu}$.\break Hence it follows from
lemmas (\ref{chap1:sec1.6:lem1}) and (\ref{chap1:sec1.6:lem2}) that,  if $\xi_\lambda \alpha \zeta_\lambda ~
(\lambda = 1,  \ldots,  s)$, $T^{\mathscr{L}}_\lambda (\zeta)$ is
positive and $T^{\mathscr{L}}_\lambda (\xi) \alpha T^{\mathscr{L}}_\lambda
(\zeta)$. Therefore it is 
sufficient to prove the convergence of
$T^{\mathscr{L}}_\lambda(\zeta)$. 

\begin{lemma}\label{chap1:sec1.6:lem3}% lemma 3
  There exists a solution of the system $(\mathscr{L})$ when the
  initial condition $\xi \in H^s_p$ is given. 
\end{lemma}

\begin{proof}%proof
  First\pageoriginale we shall construct a system $\mathscr{L}$ satisfying the above
  requirements. Assume that  
  $$
  B_\lambda,  B^r_{\lambda\mu} \alpha \left[  b \Big/ 1 - \frac{1}{a}
    \left(\frac{^x p+1}{z} + x_1 + \cdots + x_p + y_1 + \cdots +
    y_s\right)\right] 
  $$
  for any $0 < z < 1$. we prove that the system $(\mathscr{L}(a, b, z))$
  \begin{multline*}
  \frac{\partial ~ y_\lambda}{\partial ~ x_{p+1}} = \left[  b \Big/ 1 -
    \frac{1}{a} \left(\frac{^x p+1}{z} + x_1 + \cdots + x_p + y_1 + \cdots
    + y_s\right)\right]\\ 
  \left(1 + \sum_{\mu, r} \frac{\partial ~ y_\mu}{\partial
    ~ x_r}\right) 
  \end{multline*}
 has a solution $f_{a, b, z}(x)$ with positive coefficients. Then it
 follows that $\Big[ T^{\mathscr{L}}_\lambda\Big] ~ (x) \alpha f_{a, b,
   z}(x)$. Now we prove that $f_{a, b, z}(x)$ exists for $\dfrac{1}{z}
 > $ bsp as follows: Consider a function $h(t)$ of the variable $t$
 such that $y_\lambda = h(x_1 + \cdots + x_p + \frac{^x p+1}{z})$ is a
 solution of  $(\mathscr{L}(a, b, z))$. This is possible if and only
 if 
 $$
 \frac{1}{z} \left( \frac{dh}{dt}\right)  = \left[ b \Big/ 1 - \frac{1}{a}
   (t + sh)\right] \left[  1 + sp \frac{dh}{dt}\right] 
 $$
 or equivalently if and only if 
 $$
 \left(  \frac{1}{z} - \frac{bsp}{1- \frac{t + sh}{a}}\right)
 \frac{dh}{dt} - \frac{b}{1- \frac{t + sh}{a}}. 
 $$
\end{proof}

This equation takes the form $\dfrac{dh}{dt} = F(h, t)$ where $F(h,
t)$ is a positive convergent power series in $h, t$,  if $\dfrac{1}{z}
> bsp$ . But it is known that there exists a positive solution $h(t)$
and so $(\mathscr{L}(a, b, z))$ has a positive solution if
$\dfrac{1}{z} >bsp$. This shows that $(\mathscr{L})$ has a solution
with the initial condition $0$. Now,  $\eta$ is a solution of
$(\mathscr{L})$ with the initial condition $\xi$ if and only if
$(w_\lambda) = (\eta_\lambda - \xi_\lambda )$ is a solution of the
system 
\begin{equation}
  \frac{\partial ~w_\lambda}{\partial ~ x_{p+1}} = B^\xi_\lambda ~ (x,
  w) + B^{\xi r}_{\lambda \mu} (x, w) \frac{\partial ~
    w_\lambda}{\partial ~ x_r} \tag*{$(\mathscr{L})^\xi$} 
\end{equation}\pageoriginale
with the initial condition $0$,  where
\begin{align*}
  B^\xi_\lambda (x, w) & =  B_\lambda ~ (x, w + \xi (x)) +
  B^r_{\lambda\mu} (x, w + \xi (x)) \frac{\partial ~ \xi_\mu}{\partial ~
    x_r}, \\ 
  & \quad  B^{\xi r}_{\lambda\mu} (x, w) = B^r_{\lambda\mu} (x, w + \xi (x))
\end{align*}

Therefore,  $(\mathscr{L})$ has a solution with the initial condition
$\xi$.  

\begin{lemma}\label{chap1:sec1.6:lem4}% lemma 4
  There exists a solution mapping for the system of equations $(\mathscr{L})$.
\end{lemma}

\begin{proof}% proof
  Let $B_{\lambda} (x, y),  B^r_{\lambda\mu} (x, y)$  be defined for
  $| x_k |,  | x_{p+1} |,  | y_\sigma | < v^*  ~ (k = 1,  \ldots,  p;
  \sigma = 1,  \ldots,  s)$ and let $| B_\lambda |,  |
  B^r_{\lambda\mu} | < v' - \varepsilon$ on this domain. Then,  if
  $\xi \in H^s_p (v^*,  v^*/2)$,  we have that $B^\xi_\lambda (x, y),
  B^{\xi r}_{\lambda\mu} (x, y)$ are defined for $| x_k |$,\break $|x_{p+1}|$,
  $| y_\sigma | < v^* /2$ and on this domain $| B^\xi_\lambda |,  |
  B^{\xi r}_{\lambda \mu} | < (1 + sp) v' - \varepsilon$ (since
  $\frac{\partial ~ \xi_\lambda}{\partial ~ x_r } \in H_p
  (\frac{v^*}{2},  1)$). Hence for large $b > 0$ and small $a >0$ 
  $$
  B^\xi_\lambda,  B^{\xi ~ r}_{\lambda \mu} \alpha b\Big/ \Big[ 1 -
    \frac{1}{a} (2 ~ spb ~ x_{p+1} + x_1 +  \cdots + x_p + y_1 +
    \cdots + y_s)\Big] 
  $$
  whenever $\xi \in H^s_p (v^*,  v^* /2)$. Hence,  by
  Lemma \ref{chap1:sec1.6:lem3},
  $T_\lambda^{(\mathscr{L})} (\xi)$ converges for any $\xi \in H^s_p
  (v^*, v^*/2)$ and is majorized by $f_{a, b, } \dfrac{1}{2spb}$. Thus
  there exists a solution mapping $T$ of $H^s_p(v^*, v^*/2)$ into
  $H^s_{p+1}(u', v')$ for suitable $u', v'$. 
\end{proof}

If $T$ is any solution mapping of $H^s_{p}(u, v)$ into $H^s_{p+1}(u',
v')$ for the system $(\mathscr{L})$ then $T_\lambda(\xi)$ must have
the expression $T^{\mathscr{L}}_\lambda (\xi)$ in Lemma \ref{chap1:sec1.6:lem2} and hence
$T$ must be regular. It is also clear that the solution\pageoriginale mappings are
equivalent to each other. Thus the Lemmas \ref{chap1:sec1.6:lem1},
\ref{chap1:sec1.6:lem2}, \ref{chap1:sec1.6:lem3},
\ref{chap1:sec1.6:lem4}  together 
complete the proof of Theorem \ref{chap1:sec1.5:thm1} in our special
case of the system $(\mathscr{L})$. 

\section{}\label{chap1:sec1.7} % 1.7

To simplify the expressions we adopt the following notation. If in
$\beta = (r_1 \cdots r_h)$ the integers $1,  \ldots,  p$ occur $a_1,
\ldots,  a_p$ times respectively we shall denote by $C^\beta$ the
constant $\dfrac{1}{a'_1 \cdots a'_p}$. Then,  if $\xi \in H_p(u, v)$,
we have $| C^\beta ~ \partial^\beta ~ \xi (0) | <
\left(\dfrac{u}{2}\right)^{-|\beta |} v$. We rewrite the expansion of
$T_\lambda (\xi)$ in the form: 
\begin{multline*}
  T_\lambda (\xi) = \sum_{n=o}^\infty ~ x^n_{p+1} ~ {}^n B^{\mu_1 \cdots
    \mu_h}_{\lambda\beta(1) \cdots \beta(h)} (x, \xi)
  \left(c^{\beta(1)}\partial^{\beta(1)} \xi_{\mu_1}\right)\\ 
  \cdots \left(c^{\beta(h)}
  ~\xi_{\mu_h}\right) ~(| \beta (1) | + \cdots + | \beta (h) | \le n). 
\end{multline*}

The new $ ^nB^{\mu_1 \cdots \mu_h}_{\lambda\beta(1) \cdots \beta(h)}$
differs from the previous one by a constant factor. 

\begin{lemma}\label{chap1:sec1.7:lem5}% lemma 5
  The solution mapping $T$ of $H^s_p(u, v)$ into $H^s_{p+1} (u', v')$,
  for the system $(\mathscr{L})$,  defined by $\xi \rightarrow (T_1
  (\xi),  \ldots,  T_s(\xi))$ is an infinite analytic map,  provided
  $T_\lambda (0) = 0$. 
\end{lemma}

\begin{proof}% 
  \begin{enumerate}[(i)]
  \item Given $u, v > 0$ there exist $\tilde{u},  v_1 > 0,  u_1 \ge 1$
    with the following property: For any $| x^o_r | < \tilde{u}$ and
    $| y^\beta_\lambda | < v_1 (u_1)^{-| \beta |}$. There is a $\xi $
    in $H^s_p (u, v)$ such that $(c^\beta \partial^\beta \xi_\lambda)
    (x^o) = y^\beta_\lambda$. For,  given $x^o_r,  y^\beta_\lambda$,
    set $\xi_\lambda = \sum\limits_{\beta} ~ y_\lambda^\beta (x_{r_1}
    - x^o_{r_1}) \cdots (x_{r_h} - x^o_{r_h}),  (\beta = (r_1,
    \ldots,  r_h)) $. Then 
    $$
    | \xi_\lambda |_u \le \sum v_1 \frac{1}{u_1^{| \beta |}} ~(u +
    \tilde{u})^{| \beta |} = v_1 \sum_{\beta} ~ _t| \beta | 
    $$
    where $t = \dfrac{u + \tilde{u}}{u_1}$. Choose $\tilde{u},  v_1$
    sufficiently small and $u_1$ sufficiently\pageoriginale large so that $v_1  ~
    \sum\limits_\beta ~ _t| \beta | < v$. Then $\xi \in H_p (u, v)$
    and $(c^\beta \partial^\beta \xi_\lambda) (x^o) = y^\beta_\lambda$. 

  \item For any $\xi \in H^s_p (u, v)$ we have,  by Cauchy's formula,
    that 
{\fontsize{10pt}{12pt}\selectfont
    $$
\left| {}^n {B_{\lambda\beta(1) \cdots \beta(h)}}^{\mu_1 \cdots
      \mu_h}  ~ (x,  \xi) (c^{\beta(1)} \partial^{\beta(1)}
    \xi_{\mu_1}) \cdots (c^{\beta(h)} ~\partial^{\beta(h)}
    \xi_{\mu_h}) \right| < (\dfrac{u'}{2})^{-n} ~ v'.
$$}\relax 
     Therefore,  by
    (i), $\Big| {}^n B_{\lambda\beta(1) \cdots \beta(h)}^{\mu_1
      \cdots \mu_h} (x, y) y^{\beta(1)}_{\mu_1} \cdots
    y^{\beta(h)}_{\mu_h} \Big| < (\dfrac{u'}{2})^{-n} ~ v'$
    whenever 
    $| x_r | < \tilde{u},  | y^\beta_\mu | < \dfrac{v_1}{_{u_1} |
      y^\beta_\mu |} ~ (y^\phi_\mu = y_\mu)$. 
    
    Hence,  for $| x_r | < \tilde{u},  | y^\beta_\mu | < v'_1/\sigma |
    \beta | $ for any $v'_1 ~ \sigma > 0$ we have  
    $$
    \left| ^nB^{\mu_1 \cdots \mu_h}_{\lambda\beta(1) \cdots \beta(h)} ~
    (x, y) \prod_{j=1}^h \left( \frac{v_1}{v'_1}
    \left(\frac{\sigma}{u_1}\right)^{| \beta(j) | } y^{\beta(j)}_{\mu_j} \right)
    \right| < \left(\frac{u'}{2}\right)^{-n} ~v'. 
    $$

    More explicitly, 
    \begin{multline*}
    \Big| \sum_{k=0}^n  \sum_{| \beta(1) | + \cdots + | \beta(h) | =
      k} \left(\frac{\sigma}{u_1}\right)^k
    \left(\frac{v_1}{v'_1}\right)^h ~ ^nB^{\mu_1 
      \cdots \mu_h}_{\lambda\beta(1) \cdots \beta(h)}\\ 
    (x, y) y^{\beta(1)}_{\mu_1} \cdots y^{\beta(h)}_{\mu_h} ~ \Big| <
    \left(\frac{u'}{2}\right)^{-n} ~ v' 
    \end{multline*}

    That is
{\fontsize{10pt}{12pt}\selectfont
    $$
    \left| {  \sum_{| \beta(1) | + \cdots + | \beta(h) | =k}  }
    ^nB^{\mu_1 \cdots \mu_h}_{\lambda\beta(1) \cdots \beta(h)} ~ (x,
    y)  ~ y^{\beta(1) }_{\mu_1} \cdots y^{\beta(h)}_{\mu_h} \right| < \left(
    \frac{u'}{2}\right)^{-n} v' \left(\frac{u_1}{\sigma}\right)^k
    \left(\frac{v'_1}{v_1}\right)^h. 
    $$}\relax

  \item Let $0 < t < 1$ be fixed and let $0 < a < \tilde{u}$; then if
    $\xi \in H^s_p (a, v'_1)$,\break  $| c^\beta \partial^\beta \xi_\mu | <
    (ta)^{- | \beta | } v'_1$ in a polydisc of radius $(1- t)a$,  by
    Cauchy's formula. Therefore  
    \begin{align*}
      | T_\lambda (\xi) |_{(1-t)a} & \le \sum_{n=0}^\infty (1-t)^n a^n
      \sum_{k =o}^n \sum_n \left(\frac{u_1}{ta}\right)^k
      \left(\frac{u'}{2}\right)^{-n} ~
      v' ~\left(\frac{v'_1}{v_1}\right)^h\\ 
      & \le v' \sum_{n=o}^\infty ~ (n + 1) ~ \left(\frac{1-t}{t} ~
      \frac{u_1}{u'}. 2\right)^n \sum_h ~
      \left(\frac{v'_1}{v_1}\right)^h  \text{ if } a < 1 
    \end{align*}
  \end{enumerate}
  Choose\pageoriginale $v'_1$ such that $\dfrac{v'_1}{v_1} < 1$ and $t$ such that
  $\dfrac{1 - t}{t} ~ \dfrac{u_1}{u'} < \dfrac{1}{4}$. Then we obtain
  the majorization $| T_\lambda (\xi) |_{(1 -t)a} < k \Big/ \left(1 -
  \dfrac{v'_1}{v_1}\right)$ where $K$ is a constant $> 0$. 
  
  Thus the proof of Theorem \ref{chap1:sec1.5:thm2} is completed in
  the special case of the system of equations $(\mathscr{L})$. 
\end{proof}

\section{ }\label{chap1:sec1.8} % section 1.8

We shall now give the proof of the Theorems \ref{chap1:sec1.5:thm1}
and \ref{chap1:sec1.5:thm2} in the general 
case of the system of equations $(\mathscr{U})$. 

\begin{proof of theorem}% Proof 1
  The system of equations $(\mathscr{U})$ being given we consider the
  following system of equations $(\mathscr{U}')$ with the unknown
  functions $y_1,  \ldots,  y_s,  \ldots,  y^r_\mu,  \ldots$;  
  \begin{multline*}
  (\mathscr{U}') 
  \begin{cases}
    \frac{\partial y_\lambda}{\partial ~x_{p+1}} = A_\lambda (x, y,
    \ldots,  y^r_\mu,  \ldots)  \quad (\lambda =1,  \ldots,  s)\\ 
          {}\\
          \frac{\partial ~ y^r_\lambda}{\partial ~ x_{p+1}} =
          \frac{\partial ~ A_\lambda}{\partial ~ x_r} (x, y,  \ldots,
          y^r_\mu,  \ldots) + \frac{\partial A_\lambda}{ \partial ~ y_\mu} ~
          (x, y,  \ldots,  y^r_\mu,  \ldots) \frac{\partial ~ y_\mu}{
            \partial ~ x_r} 
  \end{cases}\\
  + \frac{ \partial ~ A_\lambda}{ \partial ~ y^{r'}_\mu} ~ (x, y,
  \ldots,  y^r_\mu,  \ldots ) \frac{\partial ~ y^{r'}_\mu}{\partial ~
    x_r} ~ \begin{pmatrix}  \lambda = 1,  \ldots ,  s; \\ r =1,
    \ldots,  p\end{pmatrix} 
  \end{multline*}
    where $A_\lambda$ are analytic functions of all their arguments in the
    domain $| x_k |,  | y_\mu | ,  | y^r_\mu | < v^* (k =1,  \ldots p+1;
    \mu = 1,  \ldots,  s; r=1,  \ldots,  p) $ and  the second system is
    obtained by formally differentiating both the members of the system of
    equations $(\mathscr{U})$ with respect to the variables $ x_r (r=1,
    \ldots,  p)$. Hence we easily see that $y_\lambda = \eta_\lambda (x)$\pageoriginale
    is a solution of $(\mathscr{U})$ with the initial condition $\xi$ if
    and only if $y_\lambda = \eta_\lambda (x),  y^r_\lambda =
    \dfrac{\partial ~ \eta_\lambda}{\partial ~ x_r}$ is a solution of
    $(\mathscr{U}')$ with the initial condition $\left(\xi,  \ldots,
    \dfrac{\partial ~ \xi_\mu}{\partial ~ x_r},  \ldots\right)$. Because the
    system $(\mathscr{U}')$ is of the special type of system
    $(\mathscr{L})$ considered,  it follows therefore that there exists a
    unique solution of the system of equations $(\mathscr{U})$ with a
    given initial condition $\xi$ from lemmas \ref{chap1:sec1.6:lem2}
    and \ref{chap1:sec1.6:lem3}.  
\end{proof of theorem}

Further,  if $\xi $ is in $H^s_p(u, v)$ then clearly $\left(\xi,  \ldots,
\dfrac{\partial ~ \xi_\mu}{\partial ~ x_r},  \ldots\right)$ is in\break 
$H^{s+ps}_p (u/2, 2v/u)$. There exists,  by lemma
\ref{chap1:sec1.6:lem4},  a solution 
mapping $T^*$ of $H^{s+ps}_p(u/2, 2v/u)$ into $H^{s+ps}_{p+1}(u', v')$
for the system of equations $(\mathscr{U}')^p$.  On the other hand the
mapping $F$ of $H^s_{p}(u, v)$ into $H^{s+ps}_p (u/2, 2v/u)$ defined
by $F(\xi) = \left(\xi, \ldots,  \dfrac{\partial ~\xi_\mu}{\partial x_r},
\ldots\right)$ is clearly infinite analytic. (See
Ex. \ref{chap1:sec1.2:exp1} of \S ~ 
\ref{chap1:sec1.2}). Similarly the projection mapping $P$ defined by $p(\eta,
\ldots,\break  \eta^r_\mu,  \ldots)$ $= \eta$ of $H^{s+ps}_{p+1} ~ (u', v')$
onto $H^s_{p+1}(u', v')$ is also infinite analytic. Therefore the
composite mapping $T = p ~ o ~ T^* ~ o ~ F$ is an infinite analytic
map since $T^*$ is so by Lemma \ref{chap1:sec1.7:lem5}. Moreover $T$
is a solution mapping 
because of the remark made in the beginning of this section. This
complete the proof of  Theorem \ref{chap1:sec1.5:thm1}
and \ref{chap1:sec1.5:thm2}.  

\section{}\label{chap1:sec1.9} % section 1.9

In this section we consider the discussion of the case of a system of
differential equations involving certain parameters. We prove in this
case that the solution depends infinite analytically on the initial
condition and the parameters. 

Here in addition to the usual notations,  $z = (z_1,  \ldots,  z_q)$
denotes the set of parameters. Let $A_\lambda (x, y, \ldots,  y^r_\mu,
\ldots,  z)$ be a system of functions defined and analytic (in all
their arguments) in the domain\pageoriginale $| x_k |,  | y_\lambda |,  |y^r_\lambda
|,  | z_\sigma |$ $< v^*_1 ~ (k=1,  \ldots p+1; r=1,  \ldots,  p;
\lambda = 1,  \ldots s; \sigma =1,  \ldots,  q)$. Given a $\zeta \in
H^q_{p+1}$ with $| \zeta (0) | < v^*_1$ consider the system of partial
differential equations 
\begin{equation}
  \frac{\partial ~ y_\lambda}{\partial ~ x_{p+1}} = A_\lambda \left(x, y,
  \ldots,  \frac{y_\mu}{\partial ~ x_r},  \ldots \zeta(x)\right). \tag{
    $\mathscr{U}_\zeta$} 
\end{equation}

\begin{defi*}% definition
  A mapping $T$ of the direct sum $(H^s_p + H^q_{p+1}) (u, v)$ into
  $H^{q+s}_{p+1}$ $(u', v')$ (with $u^{-1} v,  v < v^*_1$) is said to be
  a solution mapping of the system of partial differential equations
  $(\mathscr{U}_\zeta)$ with parameters if $y_\lambda = T_\lambda
  (\xi,  \zeta)$ is a solution of $(\mathscr{U}_\zeta)$ with the
  initial condition $\xi$,  for any $(\xi, \xi)$ in $(H^s_p +
  H^q_{p+1}) (u, v)$.  
\end{defi*}

\begin{theorem}\label{chap1:sec1.9:thm3}% theorem 3
  For any given system of partial differential equations
  $(\mathscr{U}_\zeta)$ with parameters of Cauchy-Kowalewski type there
  exists a solution mapping. If moreover $A_\lambda (x, 0, 0) = 0$
  then the solution mappings are regular and equivalent to each
  other. 
\end{theorem}

The additional condition $A_\lambda (x, 0, 0) = 0$ is imposed to
ensure that the image by any solution mapping of $0$ in $(H^s_p +
H^q_{p+1}) (u, v)$ is the $0$ in $H^{q+s}_{p+1}(u', v')$. The germ of
solution mappings in this case is called the solution germ  of the
system with parameters. 

\begin{theorem}\label{chap1:sec1.9:thm4}% theorem 4
  The solution germ of a system of partial differential equations of
  Cauchy-Kowalewski type with parameters is germ of infinite analytic
  maps. 
\end{theorem}

\begin{proof of theorem}[and 3]% Proof of theorem 3 and 4
  Given the system of equations $(\mathscr{U}_\zeta)$ we construct a
  new system of equations $(\mathscr{U}'')$ by introducing another new
  variable $t$,  with the unknown functions $y_1,  \ldots,  y_s,  z_1,
  \ldots,  z_q$. 
{\fontsize{10pt}{12pt}\selectfont
  \begin{equation*}
    (\mathscr{U}'')
    \begin{cases}
      \frac{\partial ~ y_\lambda}{\partial ~ x_{p+1}} = A_\lambda
      (x_1,  \ldots,  x_p,  t,  t_{p+1},  y_1, \ldots,  y_s,  \ldots,
      \frac{\partial ~ y_\mu}{\partial ~ x_r}, \ldots,  z_1,  \ldots,
      z_q)\\ 
      \frac{\partial ~ z_\sigma}{\partial ~ x_{p+1}} = \frac{\partial
        ~ z_\sigma}{\partial ~ t} \qquad  \qquad \begin{matrix}
        (\lambda = 1,  \ldots, s) \\ (\sigma = 1,  \ldots,
        q) \end{matrix} 
    \end{cases}
  \end{equation*}}\relax\pageoriginale
{\fontsize{10pt}{12pt}\selectfont
  $$
  \displaylines{\text{where}\quad A_\lambda \left(x_1,  \ldots,  x_p,  t,
    x_{p+1},  y_1,  \ldots, 
  y_s,  \ldots,  \dfrac{\partial ~ y_\mu}{\partial ~ x_r},  \ldots,
  z_1,  \ldots,  z_q\right)\hfill \cr 
  \qquad = A_\lambda (x_1,  \ldots,  x_p, x_{p+1},  y_1,  \ldots,  y_s,
  \ldots,  \frac{\partial ~ y_\mu}{\partial ~ x_r},  \ldots,  z_1,
  \ldots,  z_q)} 
  $$}\relax
$(\mathscr{U}'')$ is a system of Cauchy-Kowalewski type treated in
  Theorems \ref{chap1:sec1.5:thm1} and \ref{chap1:sec1.5:thm2}. Now if
  $y_\lambda = \eta_\lambda(x_1,  \ldots, 
  x_p,  t,  x_{p+1}),  z_\sigma = \mu_\sigma (x_1,  \ldots,  x_p,  t,
  x_{p+1})$ is a solution of the system $(\mathscr{U}'')$ then
  $\mu_\sigma (x_1,  \ldots,  x_p, t,  x_{p+1})$ must be equal to
  $\zeta_\sigma (x_1,  \ldots, x_p,  t+x_{p+1})$ where $\zeta_\sigma
  (x_1,  \ldots,  x_p,  t) = \mu_p (x_1,  \ldots,  x_p,  t,  0)$ ,
  because $\dfrac{\partial ~ z_\sigma}{\partial~ x_{p+1}} = \dfrac{
    \partial ~ z_\sigma}{\partial ~ t}$ is of Cauchy-Kowalewski type
  and so the solution must be unique for the initial
  condition. Therefore $y_\lambda = \eta_\lambda (x_1,  \ldots,  x_p,
  0$,  $x_{p+1})$ is a solution of the system
  $(\mathscr{U}_\zeta)$. Then the proof can be carried out by an
  argument similar to the one given when we deduced the
  theorems \ref{chap1:sec1.5:thm1} 
  and \ref{chap1:sec1.5:thm2} from the special case $(\mathscr{L})$. 
\end{proof of theorem}

\begin{remark*}\label{chap1:sec1.9:rem34}% remark % for pageref
  Let $F$ be a solution mapping of $(\mathscr{U}'')$. Then the mapping
  $\xi \rightarrow F(\xi) - F(0)$ is infinite analytic. This follows
  from the fact that $F - F(0)$ is a solution mapping of a system of
  partial differential equations of the same type as
  $(\mathscr{U}'')$. 
\end{remark*}

Next we shall briefly mention the case of a system of partial
differential equations in which the derivatives of the parameters also
occur. Given a $\zeta \in H^q_{p+1} (u, v)$ consider the system of
equations 
\begin{gather*}
  \frac{\partial ~ y_\lambda}{\partial ~ x_{p+1}} = A_\lambda \left(x, y,
  \ldots,  \frac{\partial ~y_\mu}{\partial ~ x_r},  \ldots,  z_1,
  \ldots,  z_q,  \ldots,  \frac{\partial ~ z_\sigma}{\partial ~ x_r},
  \ldots\right) \tag{$\mathscr{U}'_\zeta$}\\
  (\lambda = 1,  2,  \ldots,  s), 
\end{gather*}\pageoriginale
where $A_\lambda (x, y,  \ldots,  y_\mu^r,  \ldots,  z_1,  \ldots,
z_q,  \ldots,  z^r_\sigma,  \ldots)$ are functions defined and
analytic in $| x_k |,  |y_\mu |,  |z_\sigma |,  |y^r_\mu |,
|z^r_\sigma | < v^*_1 ~ (k =1,  \ldots,  p+1,  r=1,  \ldots,  p$,  $\mu
=1,  \ldots,  s,  \sigma =1,  \ldots,  q)$. In this case also it is
not difficult to give definitions of a solution mapping and a solution
germ. 

\begin{defi*}% definition
  A mapping $T$ of the direct sum $(H^s_p + H^q_{p+1}) (u, v)$ (with
  $u^{-1}v$,  $v < v^*_1$) into $H^{q+s}_{p+1} (u',  v')$ is called a
  solution mapping of the system $(\mathscr{U}'_\zeta)$ if $y_\lambda
  = T_\lambda (\xi,  \zeta)$ is a solution of $(\mathscr{U}'_\zeta)$,
  for every $(\xi, \zeta)$ in $(H^s_p + H^q_{p+1}) (u, v)$,  with the
  initial condition $\xi$. 
\end{defi*}

\begin{theorem}\label{chap1:sec1.9:thm5}% theorem 5
  For any system $(\mathscr{U}'_\zeta)$ of Cauchy-Kowalewski type
  there exists a solution mapping. Moreover if $A_\lambda (x, 0,  0) =
  0$,  then the solution mappings are regular and equivalent to each
  other. 
\end{theorem}

The germ of solution mappings is called the solution germ of the given
system $(\mathscr{U}'_\zeta)$. 

\begin{theorem}\label{chap1:sec1.9:thm6}% theorem 6
  The solution germ of $(\mathscr{U}'_\zeta)$ is infinite analytic.
\end{theorem}

\begin{proof}% proof
  First,  consider $(z_\sigma,  z^r_\sigma)$ as independent
  parameters. Then restrict the parameter to $z^r_\sigma = \partial
  z_\sigma/ \partial x_r$. 
\end{proof}

\section{Differentials of regular maps}\label{chap1:sec1.10} % section 1.10

Let $S$ and $S'$ be two systems of characters. Then,  if $F$ is a
regular map of $H(S; u,  v)$ into $H(S',  u',  v')$,  for any $\xi,
\eta$ in $H(S; u,  v/2)$ the mapping of $K(1+ \varepsilon)$ into
$H(S;u,  v)$ defined by $t \rightarrow \xi + t \eta$ is a regular
curve in $H(S; u, v)$. We pose the following definition.\pageoriginale 

\begin{defi*}% definition
  The differential $dF$ of a regular map $F$ of $H(S; u,  v) $ into
  $H(S';  u',  v')$ is defined to be the mapping of the direct sum
  $H\left(S; u,  \dfrac{v}{2}\right)$ $\oplus~ H \left(S; u,
  \dfrac{v}{2}\right)$ into $H(S';
  u',  v')$ by the formula 
  $$
  dF_\lambda ~ (\xi,  \eta) = \left[  \frac{\partial}{\partial ~ t} ~
    F_\lambda ~ ( \xi + t \eta)\right]_{t=0} 
  $$
\end{defi*}

\begin{remarks*}% remarks
  Let $S = (s_0 \,  \ldots,  s_p)$ be a system of characters. Set $S''
  = (2s_0,  \ldots,  2s_p)$; then $dF$ can be identified with a
  mapping of $H(S''; u, v)$ into $H(S'; u',  v')$. It can be seen by
  direct verification that the following are immediate consequences of
  the above definition. 
  \begin{enumerate}[\rm (1)]
  \item For any regular map $F$,  its differential is also a regular map.
  \item For any two regular maps $F_1$ and $F_2$ with $F_1 \sim F_2,
    dF_1 \sim dF_2$. This implies that one can define the differential
    of a germ $\mathscr{F}$ of regular maps of $H(S)$ into $H(S')$ to be
    germ containing the differential of any representative of
    $\mathscr{F}$. The differential of a germ $\mathscr{F}$ of regular
    maps is denoted by $d \mathscr{F}$. This definition is
    unambiguous. 
  \item The differential $dF$ of any infinite analytic map $F$ of $H(S)$
    into $H(S')$ is itself infinite analytic. 
  \end{enumerate}
\end{remarks*}

The differential $d \mathscr{F}$ of any infinite analytic germ
$\mathscr{F}$ is an infinite analytic germ. 

Let $F$ be any regular mapping of $H(S; u, v)$ into $H(S'; u',  v')$,
then\pageoriginale $dF_\lambda ~ (0,  \xi) = \left[  \dfrac{\partial}{\partial ~ t }
  ~ F_\lambda ~ ((t,  \xi)\right]_{t =0}$ and this is denoted by
$(dF)_{o, \lambda} (\xi)$. Define $(dF)_0 = ((dF)_{o, \lambda});
(dF)_o$ is called the differential of $F$ at the origin. 

For any regular map $F$ of $H(S; u, v)$ into $H(S'; u',  v')$,  it is
easy to see that $(dF)_0$ is a linear map of $H(S; u, v)$ into $H(S';
u',  v')$. 

Now if $F$ is a regular map of $H(S; u, v)$ into $H(S'; u',  v')$,
since the mapping of $K(1+ \varepsilon)$ into $H(S; u, v)$ defined by
$ t \rightarrow t ~ \xi$ is a regular curve,  the mapping of $K(1+
\varepsilon)$ into $H(S'; u',  v')$ defined by $t \rightarrow F(t ~
\xi)$ is a regular curve in $H(S'; u', v')$. Hence $F_\lambda (t ~
\xi)$ has an expansion in powers of $t$ in a neighbourhood of $t = 0$
as follows: 
\begin{align*}
  F_\lambda (t \xi) & = \left[ \frac{\partial}{\partial ~ t} ~
    F_\lambda ~ (t \xi)\right]_{t =0} ~ t + \text{ higher powers of } ~
  t\\ 
  & = (dF)_{o, \lambda} ~ (\xi) ~ t + \text{ higher powers of } ~ t.
\end{align*}

Now supposing $G$ is a regular map of $H(S'; u',  v')$ into $H(S'';
u'',  v'')$ then we have  
$$
(GoF)_\lambda ~ (t ~ \xi) = d(GoF)_{o, \lambda} ~ (\xi) ~ t + \text{
  higher powers of } ~ t. 
$$

But on the other hand
\begin{align*}
  (GoF)_\lambda ~ (t ~ \xi) & = G_\lambda \Big[  t ~ (dF)_{o, \mu} ~
    (\xi) + \text{ higher powers of } t\Big]\\ 
  & = G_\lambda \Big[  t ~ (dF)_{o, \mu} ~ (\xi)\Big] + G_\lambda
  (\text{ higher powers of } t)\\ 
  & = (dG)_{o, \lambda} ~ o ~ (dF)_{o, \lambda} ~ (\xi) ~ t + \text{
    higher powers of } t. 
\end{align*}

Comparing\pageoriginale the coefficients of $t$ in the right members of the two
equalities we obtain the formula 
$$
d(GoF)_{o, \lambda} = (dG)_{o, \lambda} ~ o ~ (dF)_o
$$
and hence we can write $(d(GoF))_0 = ((dG)_{o, \lambda} ~ o ~ (dF)_o)$.

\begin{proposition}\label{chap1:sec1.10:prop10}% proposition 10
  If $\mathscr{F}$ and $\mathscr{G}$ are two germs of infinite
  analytic maps of $H(S)$ into $H(S')$ and $H(S')$ into $H(S)$
  respectively such that $\mathscr{G} ~ o ~ \mathscr{F}$ and
  $\mathscr{F} ~ o ~ \mathscr{G}$ are germs of identity maps on $H(S)$
  and $H(S')$ respectively then $H(S)$ and $H(S')$ have same degree
  and multiplicity. 
\end{proposition}

\begin{proof}% proof
  The germs $(d\mathscr{F})_o,  (d \mathscr{G})_o$ are linear infinite
  analytic and are such that $(d(\mathscr{G}_o ~ \mathscr{F}))_o =
  (d\mathscr{G})_o ~ o ~ (d \mathscr{F})_o$ and $(d(\mathscr{F} ~ o ~
  \mathscr{G}))_o = (d ~ \mathscr{F}_o ~ o ~ (d \mathscr{G})_o$ are
  germs of identity maps on $H(S)$ and $H(S')$ respectively. Hence,
  by Proposition \ref{chap1:sec1.3:prop9},  $H(S)$ and $H(S')$ have
  same degree and multiplicity. 
\end{proof}

All our considerations above are in the case of complex analytic
maps. The whole discussion can,  without much difficulty,  be carried
out to the case of real analytic maps. 

\section{Germs of submanifolds of a manifold}\label{chap1:sec1.11}% section 1.11

Let  $M$ denote a real analytic manifold and $z$ a point in $M$. Let
$(N, z)$ denote a real locally closed analytic submanifold of $M$
passing through the point $z$. $(N, z)$ is called a real analytic
submanifold with center. Two real analytic submanifolds with centers
$(N, z)$ and $(N, z')$ passing through $z$ and $z'$ respectively are
said to be equivalent if 
\begin{enumerate}[(i)]
\item $z = z'$ and\pageoriginale 
\item there exists a neighbourhood $U$ of $z$ such that $U \cap N = U
  \cap N'$. 
\end{enumerate}

This is denoted by $(N, z) \sim (N',  z')$. Clearly $\sim$ is an
equivalence relation. 

\begin{defi*}% definition
  An equivalence class of real analytic submanifolds with centers
  under $\sim$ is called a germ of real analytic submanifolds. 
\end{defi*}

A germ of real analytic submanifolds of $M$ is denoted by
$\vartheta$. An element $(N, z) \in \vartheta$ is called a
representative of $\vartheta,  z$  is called the origin of the germ
$\vartheta$,  and $\vartheta$ is called a germ at $z$. 

Let $M$ and $M'$ be two real analytic manifolds and $\varpi$ be a real
analytic mapping of $M$ onto $M'$. $\varpi$ is said to be locally
trivial when,  for any $z \in M,  d ~ \varpi$ maps the tangent vector
space to $M$ at $z$ onto the tangent vector space to $M'$ at
$\varpi(z)$. A triple consisting of $M.M'$ and a locally trivial real
analytic map $\varpi$ of $M$ onto $M'$ is called of \text{ fibred
  manifold} and is denoted $(M,  M',  \varpi)$. For any point $z$ in
$M$,  a coordinate system $(x_1,  \ldots,  x_n,  y_1,  \ldots,  y_m)$
at $z$ in $M$ is called a coordinate system at $z$ in the fibred
manifold $(M,  M',  \varpi)$ when there exists a coordinate system
$(x'_1,  \ldots,  x'_n)$ at $\varpi(z)$ in $M'$ such that $x_i = x'_i
~ o ~ \varpi$. It is clear that there exist coordinate systems in $(M,
M',  \varpi)$ at any point of $M$. 

\begin{defi*}% definition
  A germ $\vartheta$ of real analytic submanifolds at $z$,  of $M$ is
  called a germ of cross - sections of $(M,  M',  \varpi)$ at
  $z$ if $\vartheta$ contains a representative $(N, z)$ such that
  $\varpi$ induces a real analytic homeomorphism\pageoriginale of $N$ onto an open
  neighbourhood of $\varpi(z)$. 
\end{defi*}

$\vartheta$ is also called a germ of cross - sections of $(M,  M',
\varpi)$ over $\varpi(z)$. 

Again all these notions can be carried over without any change to the
complex analytic case. 

Let $(M,  M',  \varpi)$ be a fibred manifold. Then for any fixed point
$z' \in M'$,  let $T(\varpi,  z')$ denote the set of all germs
$\vartheta$ of cross-sections of \break $(M,  M',  \varpi)$ over $z'$. Let
$\vartheta^0 \in T (\varpi,  z')$. 

A coordinate system $(x_1,  \ldots,  x_n,  y_1,  \ldots,  y_m)$ around
the origin of \break $\vartheta^o$ in $(M, M',  \varpi)$ is called a
coordinate system with 
centre $\vartheta^0$ if the germ $\vartheta^0$ can be expressed by
$(x_1,  \ldots,  x_n,  0,  \ldots,  0)$. Let us fix an germ
$\vartheta^o$ in $T(\varpi,  z')$ and a coordinate system $(x, y)$ in
$(M, M',  \varpi)$ with centre $\vartheta^0$. Denote by $U$ the open
set of $M$ where $(x, y)$ is defined and by $T(\varpi,  U,  z')$ the
set of germs $\vartheta'$ in $T(\varpi, z')$ with their origins in
$U$. If $\vartheta$ is in $T(\varpi,  U,  z'), \vartheta$ can be
expressed parametrically by $y_\lambda = \xi_\lambda (x_1,  \ldots,
x_n) ~ (\lambda =1,  \ldots,  m)$,  where $\xi_\lambda (x_1,  \ldots,
x_n)$ is a real analytic function defined in a neighbourhood of the
point $x_1 = \cdots = x_n = 0$. Thus $\xi_\lambda (x_1,  \ldots,
x_n)$ can be regarded as a convergent power series in $x_1,  \ldots,
x_n$ and hence as an element of $H_n$. We set $\tauup_\lambda
(\vartheta) = \xi_\lambda \in H_n$,  and $\tauup (\vartheta) =
(\tauup_1(\vartheta),  \ldots,  \tauup_m (\vartheta)) \in H^m_n$. The
mapping $\tauup$ defined by this is an injective mapping of $T(\omega,
U,  z')$ into $H^m_0$. For strictly positive real numbers $u, v,
T(\omega,  \vartheta^0,  u, v)$ denotes the set of  germs $\vartheta$
in $T(\omega,  U, z')$ such that $\tauup(\vartheta)$ is in $H^m_n(u,
v)$. For sufficiently\pageoriginale small $v,  \tauup$ induces a bijective mapping
of $T(\varpi,  \vartheta^0,  u, v)$ into $(H^m_n)^R (u, v)$. Evidently
$T(\varpi,  \vartheta^0,  u, v)$ depends on the choice of the
coordinate system chosen with centre $\vartheta^0$. The idea behind
this definition is to follow the analogy of coordinate systems in
point sets for sets of germs of cross sections. Namely,  the
coordinate neighbourhoods and coordinate functions in the case of
manifolds are replaced respectively by $T(\varpi,  \vartheta^0,  u,
v)$ and $\tauup$ respectively in the case of the set of germs of cross
sections. We observe that,  when $M'$ is reduced to a point,
$T(\varpi,  \vartheta^0,  u, v)$ and $\tauup$ are respectively the
coordinate neighbourhood and coordinate function in $M$. Because of
this analogy we call $\tauup$ the coordinate mapping with centre
$\vartheta^0$ induced by the coordinate system $(x, y)$ in $(M, M',
\varpi)$. 

Because we will only consider,  henceforth,  real analytic manifolds
and real analytic mappings,  we will drop the usage $R$ in  $H^R_n,
(H^m_n)^R$ and $H^R(S)$ etc. Thus $H_n$ is the vector space of
convergent power series with real coefficients,  and $H_n(u, v)$ is
the subset of $H_n$,  which was denoted by $H^R_n \cap H_n (u, v)$ so
far. A germ of  infinite analytic maps of $H(S)$ into $H(S')$ means a
germ of real infinite analytic maps. 

\section{}\label{chap1:sec1.12} % section 1.12

\begin{defi*}% definition
  Let $\sigma$ be a set of germs of cross-sections of a fibred
  manifold $(M,  M',  \varpi)$. Denote by $(n+m),  n$ the dimensions
  of $M,  M'$ respectively. $\sigma$ is said to depend on $s$
  functions in $p$ variables around a fixed germs $\vartheta^0$ of
  $\sigma$ if there exist germs $\mathscr{F}$ and $\mathscr{G}$ of
  infinite analytic maps of $H(S)$ into $H^m_n$ and of $H^m_n$ into
  $H(S)$,  respectively,\pageoriginale  satisfying the following conditions: 
  \begin{enumerate}[\rm (i)]
  \item $S$ has degree $p$ and multiplicity $s$;
  \item there exist strictly positive numbers $\tilde{u},  \tilde{v}$
    and an integer $\tilde{k}$ such that for every $a$ (with $0 < a <
    \tilde{u}) \mathscr{F}$ has a representative which maps $H(S; a,
    a^{\tilde{k}} \tilde{v}$ into $\tauup (T(\varpi,  \vartheta^0,
    u',  v') \cap \sigma)$ where $\tauup$ is the coordinate mapping
    induced by a suitable coordinate system in $(M,  M',  \varpi)$
    around $\vartheta^0$; 
  \item $\mathscr{G} ~o ~ \mathscr{F}$ is the germ of the identity mapping.
  \end{enumerate}
\end{defi*}

Then we see easily that the germ $\mathscr{F} ~ o ~ \mathscr{G}$ has a
representative defined on $H^m_n (a,  a^\ell,  v)$ and which is
identity on $\tauup (T(\varpi,  \vartheta^0,  a,  a^\ell \tilde{v}))
\cap \sigma)$ for sufficiently large $\ell$ and small $a$. 

We remark that if the conditions $(ii)$ and $(iii)$ are satisfied by a
coordinate system,  then the same conditions hold for any arbitrary
coordinate system of $(M,  M',  \varpi)$ with centre $\vartheta^0$ by
suitably changing $\tilde{u},  \tilde{v},  \mathscr{F}$ and
$\mathscr{G}$. 

\begin{proposition}\label{chap1:sec1.12:prop11}% proposition 11
  If $\sigma$ is a set of germs of cross - sections of $(M,  M',
  \varpi)$ depending on $s$ functions in $p$ variables and also on
  $s'$ functions in $p'$ variables,  then $p = p'$ and $s = s'$. 
\end{proposition}

\begin{proof} % proof
  By the above remark,  we can,  without loss of generality,  assume
  that the coordinate systems in both the cases are the same. Then the
  germs $\mathscr{G}' ~ o ~ \mathscr{F}$ (\resp  $\mathscr{G} ~ o ~
  \mathscr{F}$) of $H(S)$ (\resp  $H(S'))$ onto $H(S')$ (\resp \break  $H(S)$)
  are germs of infinite analytic maps and both $(\mathscr{G}'  ~ o `
  \mathscr{F}) ~ o ~ (\mathscr{G} ~ o ~ \mathscr{F}$ and $(\mathscr{G}
  ~ o ~ \mathscr{F}) ~o ~ (\mathscr{G}' ~o ~ \mathscr{F}')$ are germs
  of  identity mappings. Hence,  by Proposition
  \ref{chap1:sec1.10:prop10},  $S$,  $S'$ have the 
  same degree and multiplicity. 
\end{proof}
