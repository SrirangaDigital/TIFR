\chapter{Chapter 1}\label{chap1}

\section{Introduction: Abelian Functions}\pageoriginale 

In this course of lectures, we shall be concerned with a systematic
study of Riemann matrices which arise in a natural way from the theory
of abelian functions. This introductory article will be devoted to
explaining this connections.

Let $u_1,\ldots,u_n$ be $n$ independent complex variables and 1et
$u=\left(\begin{smallmatrix}u_{1}\\ \vdots\\ u_{n}\end{smallmatrix}\right)$. We
shall denote by $\mathbb{C}^n$, the $n$-dimensional complex euclidean
space and by $\mathbb{C}$, the field of complex numbers. Let $f(u)$ be
an {\em abelian function} of $u$; in other words, $f(u)$ is a
complex-valued function defined and meromorphic in $\mathbb{C}^n$ and
having 2n periods $\omega_1,\ldots,w_{2n}$ linearly independent over
the field of real numbers (i.e. for $1\leq i \leq 2n$,
$f(u+\omega_i)=f(u)$). We suppose further that $f(u)$ is a
{\em non-degenerate} abelian function i.e. there does not exist any
complex linear transformation of the variables $u_1,\ldots, u_n$ such
that $f(u)$ can be brought to depend on strictly less than $n$ complex
variables. 

The periods of $f(u)$ form a lattice $\Gamma$ in $\mathbb{C}^n$, which
we may assume, without loss of generality, to be generated by
$\omega_1,\ldots, \omega_{2n}$ over the ring $\mathbb{Z}$ of rational
integers. The matrix $P=(\omega_1\omega_2\ldots \omega_{2n})$ of $n$
rows and $2n$ columns is called a {\em period-matrix} of the
lattice $\Gamma$. Any other period-matrix $P_1$ of $\Gamma$ is of the
form $P$ $U$ where $U$ is unimodular (i.e. $U$ is a rational integral
matrix of determinant $\pm 1$).

The abelian functions admitting all elements of $\Gamma$ as periods,
form a field $\mathbb{G}$. It is known that there exist $n+1$ abelian
functions $f_0(u)$, $f_1(u),\ldots, f_n(u)$\pageoriginale in
$\mathbb{G}$ such that $f_1(u),\ldots,f_n(u)$ are algebraically
independent over $\mathbb{C}$ (and, in fact, even analytically
independent), $f_0(u)$ depends algebraically upon
$f_1(u),\ldots,f_n(u)$ and further

$\mathbb{G}=\mathbb{C} (f_0(u), \ldots, f_n(u))$. In other words,
$\mathbb{G}$ is an algebraic function field of $n$ variables over
$\mathbb{C}$. 

Let now $\mathscr{L}$ be another field of abelian functions of the form
$g(u)=f(K^{-1}u)$ for $f(u)\in\mathbb{G}$ and fixed complex
nonsingular matrix $K$. Let us further, suppose that $\mathscr{L}$ has
period-lattice $\Delta$ contained in $\Gamma$. Then it is easy to show
that $\mathscr{L}$ is an algebraic extension of
$\mathbb{G}$. Moreover, if $Q$ is a period-matrix of $\Delta$, then,
on the one hand, $Q=KPU$ for a unimodular $U$ and, on the other hand,
$Q=PG_1$ for a nonsingular rational integral matrix $G_1$. Thus we
have
\begin{equation*}
K P=PG \tag{1}\label{eq1}
\end{equation*}
with complex nonsingular $\ub{K}$ and rational integral $G$. We call
any such $K$, a {\em complex multiplication} of $P$ and $G$, a
{\em multiplier} of $P$. Our object is to study the nature of the
set of $K$ and $G$ satisfying the matrix equation~\eqref{eq1}. To this end, we
first relax our conditions and ask for all rational $2n$-rowed square
matrices $M$ satisfying the condition
\begin{equation*}
K P = P M \tag{2}\label{eq2}
\end{equation*}
with a suitable complex matrix $K$. It is easy to verify that the set
of such $M$ is an algebra $\mathfrak{M}$ of finite rank over the field
$\mathbb{Q}$ of rational numbers. We denote this abstract algebra by
$\mathfrak{M}$, while the set of matrices $M$ give a matrix
representation of $\mathfrak{M}$ which we denote by ($\mathfrak{M}$).

For the period-matrix $P$, there exists a rational $2n$-rowed
alternate\pageoriginale non-singular matrix $A$ such that
\begin{align*}
& i) \; P A^{-1} P' = 0\tag{3}\label{eq3}\\
{\rm and}\qquad  & ii) \; H = \sqrt{-1} P A^{-1} \ob{P}' >0 \text{
  (i.e. positive hermitian)}
\end{align*}
We call $A$, a {\em principal matrix} for $P$.

\begin{defi*}
Any complex matrix $P$ of $n$ rows and $2n$ columns\break satisfying \eqref{eq3} for
some principal matrix $A$ is called a ($n$-rowed) {\em Riemann
  matrix}. 
\end{defi*}

Conditions \eqref{eq3} are known as Riemann's {\em period relations}. In
the case when $A=\left(\begin{smallmatrix} 0 & E \\ -E &
  0 \end{smallmatrix} \right)$ ($E$ being the $n$-rowed identity
matrix), conditions \eqref{eq3} were given by Riemann \cite{16} as precisely the
conditions to be satisfied by the periods of a normalized complete
system of abelian integrals of the first kind on a Riemann surface of
genus $n$. It was shown by Poincare that conditions \eqref{eq3} are necessary
and sufficient for $P$ to be a period-matrix of a nondegenerate
abelian function.

Let now $Q=\left(\dfrac{P}{P}\right)$. Then conditions \eqref{eq3} may be
rewritten as 
\begin{equation*}
i \;Q \;A^{-1} \;\ob{Q}' \left(\begin{smallmatrix} H & 0 \\ 0 &
  -\ob{H} \end{smallmatrix} \right) \quad (i=\sqrt{-1}) \tag{4}\label{eq4}
\end{equation*}
with $H$ positive hermitian. If $W=i Q A^{-1}\ob{Q}'$ , then $W$ and
therefore $Q$ are nonsingular. We may now reformulate \eqref{eq2} as
\begin{equation*}
T Q = Q M \tag{5}\label{eq5}
\end{equation*}
where $T = \left(\begin{smallmatrix} K & 0 \\ 0 &
  \ob{K} \end{smallmatrix} \right)$.

Following $H$. Weyl, we introduce the $2n$-rowed matrix
$L=\left(\begin{smallmatrix} -iE & 0 \\ 0 & iE \end{smallmatrix}
\right)$  and consider, instead of $P$, the matrix
\begin{equation*} 
R = Q^{-1} L Q. \tag{6}\label{eq6}
\end{equation*}\pageoriginale
Under the transformation $P\rightarrow D P$ or equivalently
$Q\rightarrow \left(\begin{smallmatrix} D & 0 \\ 0 &
 \ob{D} \end{smallmatrix} \right) Q$ (with arbitrary complex
nonsingular $D$), $R$ remains unchanged. If $P$ is a period-matrix,
this has the significance that $R$ as defined by \eqref{eq6} is independent of
the choice of the differentials $du_1,\ldots, du_n$ of the first kind
on the abelian variety associated with $P$.

The advantage in working with $R$ is that in the first place $R$ is
real as we shall see presently and, further, that equation \eqref{eq5} may be
written simpler as
\begin{equation*}
R M = M R \tag{7}\label{eq7}
\end{equation*}
using the fact that $L T = T L$. Thus $M$ has to be just a $2n$-rowed
rational matrix commuting with $R$. Conversely, if $M$ is such a
matrix, then defining $T=Q M Q^{-1}$, we have $L T = T L$. But, from
the form of $L$, we see that $T= \left(\begin{smallmatrix} K & 0 \\ 0 &
 K_1\end{smallmatrix} \right)$ with $n$-rowed square matrices $K$ and
 $K_1$. But $T Q = Q M$ gives $K P = P M = \ob{\ob{P}M} = \ob{K}_1 P$
 which, in turn, leads to $K=\ob{K}_1$ (since $P$ is of rank
 $n$). Thus the rational solutions $M$ of \eqref{eq7} are the same as those of
 \eqref{eq5}.

\begin{proposition}\label{chap1:prop1}
The matrix $R$ defined by \eqref{eq6} has the following properties:
\begin{align*} 
(i) & \; \; R \text{ is real}\\
(ii) & \;\; R^2 = -E (E \text{ being the $2n$-rowed identity matrix})\\
\text{and }\qquad  (iii) & \;\; S = A R \text{ is positive
  symmetric}.  
\end{align*}

Conversely, any $2n$-rowed rational matrix having properties (i), (ii)
and (iii) leads to a Riemann matrix $P$ which is uniquely
determined\pageoriginale upto a left-sided nonsingular factor and $P$
has $A$ for a principal matrix.
\end{proposition}

\begin{proof}
Let $V= \left(\begin{smallmatrix} 0 & E \\ E &
 0 \end{smallmatrix} \right)$ with $E$ being the $n$-rowed identity
matrix. Since $\ob{Q}=VQ$ and $V^{-1}\ob{L} V = L$, we have
$\ob{R}=\ob{Q}^{-1} \ob{L} \; \ob{Q}=Q^{-1} V^{-1} \ob{L}VQ=R$, which
proves (i). From $L^2=-E$, (ii) follows. To prove (iii), we set $F=i Q
A^{-1} \ob{Q}'= \left(\begin{smallmatrix} H & 0 \\ 0 &
 -\ob{H} \end{smallmatrix} \right)$. Then $F=\ob{F}'$ and $S=A R = A
Q^{-1} L Q = i \ob{Q}' F^{-1} L Q = \ob{Q}' F^{-1}
\left(\begin{smallmatrix} E & 0 \\ 0 & -E \end{smallmatrix}
\right) Q$. But $F^{-1} \left(\begin{smallmatrix} E & 0 \\ 0 &
 -E \end{smallmatrix} \right) = \left(\begin{smallmatrix} H^{-1} & 0 \\ 0 &
 \ob{H}^{-1} \end{smallmatrix} \right)$ is positive hermitian and so
is its transform $S$. Since $S$ is real, our assertion (iii) is
proved.

Conversely, let $R$ have the properties (i), (ii) and (iii). From
(ii), the eigen-values of $R$ are $+i$ and $-i$ and they occur with
the same multiplicity $n$, since the characteristic equation of $R$ is
of degree $2n$ and has real coefficients. Thus it may be seen that
there is a complex non-singular matrix $C$ such that $R=C^{-1}LC$. If
$C_0$ also satisfies $C^{-1}_0 L C_0 = R$, then $C_0=
\left(\begin{smallmatrix} B_1 & 0 \\ 0 & B_2 \end{smallmatrix}
\right) C$ with complex $n$-rowed non-singular matrices $B_1$ and
$B_2$. Now from (i), $C^{-1}L C = \ob{C}^{-1} \ob{L} \; \ob{C} = (V
\ob{C})^{-1} L (V\ob{C})$ since $\ob{L}=V^{-1}LV$ so that $V\ob{C}=
\left(\begin{smallmatrix} B_1 & 0 \\ 0 & B_2 \end{smallmatrix} \right) 
C $ or $\ob{C} = \left(\begin{smallmatrix} 0 & B_2 \\ B_1 &
  0\end{smallmatrix} \right) C$. Splitting up $C$ as
  $\left(\begin{smallmatrix} C_1 \\C_2 \end{smallmatrix} \right)$ with
  $n$-rowed $C_1$, we have $\ob{C}_1 = B_2 C_2$ and $\ob{C}_2 =
  B_1C_1$. We may now choose $Q = \left(\begin{smallmatrix} C_1
    \\ B_2 & C_2 \end{smallmatrix} \right) = \left(\begin{smallmatrix}
   E & 0 \\ 0 & B_2 \end{smallmatrix} \right)
  \left(\begin{smallmatrix} C_1 \\ C_2 \end{smallmatrix} \right)
  $. Then $Q^{-1}LQ = R$ and if we denote $C_1$ as $P$,
  $Q=\left(\dfrac{P}{P}\right)$. We shall prove that $P$ is a 
Riemann\pageoriginale matrix having $A$ for a principal matrix. In
fact, from (iii), we have $A Q^{-1} LQ=\ob{Q}'
F^{-1}\left(\begin{smallmatrix} E & 0 \\ 0 & -E \end{smallmatrix} 
\right) Q$ is positive hermitian, where $F=i Q A^{-1}\ob{Q}'$. But
then this means $F^{-1} \left(\begin{smallmatrix} E & 0 \\ 0 &
 -E \end{smallmatrix} \right) $ is hermitian and positive. Therefore,
$\left(\begin{smallmatrix} E & 0 \\ 0 & -E \end{smallmatrix} 
\right) F$ is again positive hermitian. Writing $F=
\left(\begin{smallmatrix} F_1 & F_2 \\ \ob{F}'_2 & F_3\end{smallmatrix} 
\right) $ we have $\left(\begin{smallmatrix} F_1 & F_2 \\ -\ob{F}'_2 &
 -F_3 \end{smallmatrix} \right) = \left(\begin{smallmatrix} \ob{F}'_1 & -F_2
  \\ \ob{F}'_2 & -\ob{F}'_3 \end{smallmatrix} \right) $. Thus $F_2=0$
and $F_1$, $-F_3$ are positive hermitian. Writing $F=
\left(\begin{smallmatrix} H & 0 \\ 0 & H_1 \end{smallmatrix} \right) $
where $H$, $- H_1$ are positive hermitian, it is trivial to see $H_1 =
-\ob{H}$. Thus our proposition is completely proved.

For the sake of brevity, we shall call a matrix $R$ having properties
(i), (ii) and (iii) mentioned in Proposition~\ref{chap1:prop1}, a
$R$-{\em matrix}. A real matrix satisfying just condition (iii) is
referred to by $H$. Weyl \cite{27} as a ``generalized Riemann matrix''. We
shall call the matrix $A$, a `principal matrix' for $R$, too.

\end{proof}

\section{The commutator-algebra of a $R$-matrix}

In the last section we reduced the problem of finding the set of
rational matrices $M$ satisfying \eqref{eq2} for a suitable complex $K$, to
that of finding all $2n$-rowed rational matrices $M$ which commute
with a $2n$-rowed $R$-matrix $R$. We may now forget the period matrix
$P$ which gave rise to $R$ and work with $R$ instead. As we remarked,
the set of such commutators $M$ of $R$ is an algebra $(\mathfrak{M})$
of finite rank over $\mathbb{Q}$.

We shall now see that in $(\mathfrak{M})$, we have an involution $M
\rightarrow M^{\ast}$; this involution is known as the {\em Rosati
  involution}. Further, it is a {\em positive
  involution}\pageoriginale in the sense that for any $M
\in(\mathfrak{M})$, the trace $\sigma(M M^{\ast})$ of $M M^{\ast}$ is
a positive rational number unless $M=0$.

(For a complex square matrix $X$, we denote the {\em trace} by
$\sigma(X)$ and the {\em determinant} by $|X|$).

\begin{proposition}\label{chap1:prop2}
We have in $\mathfrak{M}$, a positive involution.
\end{proposition}

\begin{proof}
For any $2n$-rowed complex square matrix $W$, define
$$
W^{\ast} = A^{-1} W' A.
$$
Then it is easy to verify that
\begin{align*}
(W_1\pm W_2)^{\ast} & = A^{-1} (W_1 \pm W_2)' A = W^{\ast}_1 \pm
  W^{\ast}_2\\
(cW_1)^{\ast} & = c W^{\ast}_1 \text{ for any } c \in
  \mathbb{C}. \tag{8}\label{eq8}\\
(W_1 W_2)^{\ast} & = A^{-1} W'_2 W'_1 A = W^{\ast}_2 W^{\ast}_1\\
(W^{\ast})^{\ast} & = A^{-1}(A^{-1}W'A)' A=W
\end{align*}

If $M\in(\mathfrak{M})$, then $MR = RM$ and by \eqref{eq8}, $M^{\ast}
R^{\ast}=R^{\ast} M^{\ast}$. But $R^{\ast}= A^{-1} R' A = -
A^{-1}S=-R$. Thus $M^{\ast} R = R M^{\ast}$. Further $M^{\ast}$ is a
rational matrix and therefore $M^{\ast} \in (\mathfrak{M})$. We now
obtain from \eqref{eq8} that the mapping $M\rightarrow M^{\ast}$ of
$(\mathfrak{M})$ is an anti-automorphism of order 2, i.e. an
involution.

Clearly $\sigma(M M^{\ast}) = \sigma(M A^{-1} M' A)= \sigma (MRS^{-1}
M'SR^{-1}) =\break \sigma(RMS^{-1}M' S R^{-1})=\sigma(MS^{-1}M'S)= \sigma (M
C^{-1} C'^{-1} M' C' C)=\break \sigma (CMC^{-1} C'^{-1} M' C')$ where $C$ is
a real nonsingular matrix such that $S=C'C$. Now setting $G=C M
C^{-1}$, we have $\sigma(M M^{\ast}) = \sigma (G G')$ which is
strictly positive for $G\neq 0$ and zero for $G=0$. Equivalently,
$\sigma(M M^{\ast}) > 0$ for $M \neq 0$ in $(\mathfrak{M})$ and
$\sigma(M M^{\ast})=0$ for M=0.

We shall\pageoriginale see later that the property of $(\mathfrak{M})$
mentioned in Proposition \ref{chap1:prop2} serves to characterise the algebra of
multiplications of a Riemann matrix. More precisely, we shall prove
that, except in some very special cases, any matrix algebra over
$\mathbb{Q}$ carrying a positive involution can be realized as the
algebra of multiplications of a Riemann matrix. To this end, we need
to prove some preliminary results.

A $2n$-rowed $R$-matrix $R$ is said to be {\em reducible}, if there
exists a rational $2n$-rowed non-singular matrix $C_1$ such that
\begin{equation*}
C^{-1}_1 R C_1 = \left(\begin{smallmatrix} R_1 & R_{12} \\ 0 &
 R_2 \end{smallmatrix} \right) \tag{9}\label{eq9}
\end{equation*}
where $R_1$ is a matrix of $n_1(<2n)$ rows and $n_1$
columns. Otherwise, we say that $R$ is {\em irreducible}.

Let us remark that if $R$ is a reducible $R$-matrix, it is not a
priori obvious from the form \eqref{eq9} of $C^{-1}_1 R C_1$ whether $R_1$ and
$R_2$ are again $R$-matrices and whether atleast $n_1$ is even. We
obtain clear information about this from
\end{proof}

\begin{thm}[Poincare, \cite{12}]\label{chap1:thm1} 
If $R$ is a $2n$-rowed reducible
  $R$-matrix then there exists a rational non-singular matrix $C$ such
  that
\begin{equation*}
C^{-1} R C = \left(\begin{smallmatrix} R_1 & 0 \\ 0 &
 R_2 \end{smallmatrix} \right) \tag{10}\label{eq10}
\end{equation*}
where $R_1$ and $R_2$ are again $R$-matrices of $2r$ and $2(n-r)$ rows
respectively. 
\end{thm}

\begin{proof}
We may take $R$ already in the form $\left(\begin{smallmatrix} R_1 &
  R_{12} \\ 0 & R_2 \end{smallmatrix} \right)$, without loss of
generality. Let $n_1$ and $n_2(=2n-n_1)$ be the number of rows of
$R_1$ \pageoriginale and $R_2$ respectively and let $E_i$ be the
identity matrix of $n_i$ rows $(i=1,2)$. It is then enough to find 
first a suitable rational $X$ such that for $C = \left(\begin{smallmatrix} E_1 & X
  \\ 0 & E_2 \end{smallmatrix} \right) $, we have $C^{-1} R C$ in the
form $(10)$. Now 
$$
C^{-1} R C = \left(\begin{smallmatrix} E_1 & -X_2 \\ 0 &
 E_2 \end{smallmatrix} \right) \left(\begin{smallmatrix} R_1 & R_{12}
  \\ 0 &  R_2 \end{smallmatrix} \right) \left(\begin{smallmatrix} E_1
  & X \\ 0 & E_2 \end{smallmatrix} \right) = \left(\begin{smallmatrix}
  R_1 & R_{12}+R_1X-X R_2 \\ 0 & R_2 \end{smallmatrix} \right) .
$$ 
If $X$ is rational and satisfies $R_{12}+R_1X-X R_2=0$, we will be
through. Breaking up $A$ as $\left(\begin{smallmatrix} A_1 & A_{12}
  \\ -A'_{12} & A_2 \end{smallmatrix} \right)$ and $S$ as
$\left(\begin{smallmatrix} S_1 & S_{12} \\ S'_{12} &
  S_2 \end{smallmatrix} \right)$ in a similar way, we see that $A_1$
is a nonsingular alternate matrix, since $S_1=A_1 R_1$ is positive
symmetric. Thus $n_1$ is even and let $n_1=2r$ (say). Further from
$A_1 R_1=S_1=-R'_1 A_1$, we have 
$$
A_1 R_{12} + A_{12} R_2 = S_{12} = (-A'_{12} R_1)' = - R'_1 A_{12} =
A_1 R_1 A^{-1}_1 A_{12}.
$$
Setting $X=-A^{-1}_1 A_{12}$, we have a rational matrix $X$ satisfying
$R_{12}+R_1X-X R_2 = 0$. 

To  complete the proof, we first remark that if $R$ is replaced by
$C^{-1}\break RC$, then $A$, $S$ and $M$ have respectively to be replaced by
$C' A C$, $C' S C$ and $C^{-1}MC$. Now
$$
C'AC = \left(\begin{smallmatrix} E_1 & 0 \\ X' &
 E_2 \end{smallmatrix} \right) \left(\begin{smallmatrix} A_1 & A_{12}
  \\ -A'_{12} & A_2 \end{smallmatrix} \right)
\left(\begin{smallmatrix} E_1 & X \\ 0 & E_2 \end{smallmatrix} \right)
=\left(\begin{smallmatrix} A_1 & 0 \\ 0 &
  A_3 \end{smallmatrix} \right) 
$$
where $A_3 = A_2 - A'_{12} A^{-1}_1 A_{12}$ is again an alternate
$2(n-r)$-rowed nonsingular matrix. Further from the form of $C' A C$
and $C^{-1}RC$, it is clear that $C'SC=\left(\begin{smallmatrix} S_1&
 0\\ 0 & S_2 \end{smallmatrix} \right)$ where $S_1$ and $S_2$ are
positive symmetric matrices of $2r$ and $2(n-r)$ rows
respectively. From $R^2=-E$, we \pageoriginale have $R^2_1 = -E_1$,
$R^2_2=-E_2$ and 
from $C' S C >0$, we see that $A_1 R_1 = S_1$ and $A_3 R_2 = S_2$ are
again positive symmetric. Thus $R_1$ and $R_2$ are again $R$-matrices.
\end{proof}

\begin{remarks*}
\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item Theorem \ref{chap1:thm1} was proved by Poincare only in the special case when
  the underlying abelian variety is the Jacobian of a Riemann surface
  of genus $n$ (see also p.133, \cite{26}).

\item If $R$ is reducible and $MR=RM$, then although $C^{-1}MC$
  commutes with $C^{-1}RC$, it is not necessary that $C^{-1}MC$ should
  reduce to the form $(10)$.

\item In terms of period matrices, the transformation $R\rightarrow
  C^{-1} RC$ corresponds to the transformation $P\rightarrow PC$ where
  $P$ is a period matrix associated to $R$ (by Proposition 1). From
  \eqref{eq10}, we can prove that $PC= \left(\begin{smallmatrix} P_1 & 0
    \\ 0 & P_2 \end{smallmatrix} \right)$ where $P_1$ and $P_2$ are
  period matrices of $r$ and $n-r$ rows respectively. The field of
  abelian functions having $PC$ for a period matrix is the composite
  of the fields of abelian functions having $P_1$ and $P_2$ for period
  matrices respectively.
\end{enumerate}

Applying the reduction above successively, we can split $R$ into
irreducible $R$-matrices.

If $A, B, C, \ldots$ are finitely many square matrices, then
$[A,B,C,\ldots]$ shall stand for the direct sum of $A, B, C,\ldots
$. With this notation, we can find by Theorem \ref{chap1:thm1}, a rational $2n$-rowed
non-singular matrix $C$ such that
\begin{equation*}
C^{-1} R C = [R_1,R_2,\ldots], \tag{11}\label{eq11}
\end{equation*}
and correspondingly
$$
C' A C = [A_1, A_2,\ldots].
$$\pageoriginale
If two of the matrices $R_i$ occurring on the right hand side in \eqref{eq11}
are equivalent, say $R_2 = C^{-1}_1 R_1 C_1$, for a rational
non-singular $C_1$, then, replacing $C$ by $C
\left(\begin{smallmatrix} E_1 & 0 &0 \\ 0 & C^{-1}_1 & 0\\ 0 & 0 &
  E \end{smallmatrix} \right)$ , we could suppose that already
$R_1=R_2$. In this process of changing $C$, $A_2$ gets replaced by
$C'_1 A_2 C_1$. Now if $C'_1A_2 C_1$ is not equal to $A_1$, we could
change the matrix $A$ we started from suitably so that this would be
true. Thus grouping the equivalent matrices $R_i$ in \eqref{eq11} together and
choosing $C$ properly, we could suppose that
\begin{equation*}
C^{-1} R C = \begin{matrix} [R_1, & R_2, & \ldots]\\ f_1 & f_2
   & \end{matrix}  \tag{12}\label{eq12}
\end{equation*}
where $R_j$ is a $R$-matrix repeated $f_j$ times in the direct sum. 
Correspondingly we may suppose that 
\begin{equation*}
C' A C = \begin{matrix} [A_1, & A_2, & \ldots] \\ f_1 & f_2 & 
\end{matrix}  \tag{13}\label{eq13}
\end{equation*}
where, again, $A_j$ are repeated $f_j$ times. Now in \eqref{eq12}, $R_j$ is
not equivalent to $R_k$ over $\mathbb{Q}$ for $j\neq k$. On the other
hand, it could happen that $A_j=A_k$ for $j\neq k$, in \eqref{eq13}.

We shall suppose, in the sequel, that $R$ and $A$ are already in the
form given on the right hand side of \eqref{eq12} and \eqref{eq13} respectively.

Let us consider the set of linear equations defined by the single
matrix equation $R M = MR$. This is a system of $4n^2$ linear
equations in $4n^2$ unknowns with roal coeffiecients, namely the
elements of $R$. In order to reduce this to a set a equations with
rational coefficients,\pageoriginale we shall adopt the following
procedure. 

Let $\rho_1$, $\rho_2,\ldots$, $\rho_p$ be a maximal set of elements
$r_{kl}$ of $R$ which are linearly independent over $\mathbb{Q}$. We
may then write 
\begin{equation*}
R = \rho_1 L_1 + \ldots +\rho_p L_p \tag{14}\label{eq14}
\end{equation*}
where $L_1,\ldots L_p$ are rational $2n$-rowed square
matrices. Denote by $\mathscr{T}$ the abstract algebra generated by
$L_1,\ldots, L_p$ over $\mathbb{Q}$ and by $\mathscr{(T)}$, the matrix
representation by the $L_i's$. In other words, $\mathscr{(T)}$ is the algebra
consisting of elements $T$ of the form $T = \sum\limits_{1\leq
  k_1,\ldots, k_m\leq p} a_{k_1} \ldots_{k_m}
L_{k_1}\ldots L_{k_m} (a_{k_1\ldots k_m}\break\in \mathbb{Q})$ and the
$2n$-rowed identity $E$. By definition, $\mathscr{T}$ is uniquely determined by
$R$, since a change of $\rho_1,\ldots, \rho_p$ would merely involve
taking instead of $L_1,\ldots L_p$ matrices $T_1,\ldots T_p$
which are rational linear combinations of $L_1,\ldots, L_p$ and vice
versa.

Incidentally, we remark that the determination of $L_1,\ldots, L_p$ in
\eqref{eq14} is not all that simple as it appears. For, take the the simple
2-rowed $R$-matrix $\left(\begin{smallmatrix} 0 & -1/\sqrt{\gamma}
  \\ \sqrt{\gamma} & 0 \end{smallmatrix} \right)$ with $A =
\left(\begin{smallmatrix} 0 & 1 \\ -1 & 0 \end{smallmatrix} \right)$
and $\gamma$ begin Euler's constant. It is rather ironical that one
does not know whether $\sqrt{\gamma}$ and $1/\sqrt{\gamma}$ are
linearly independent over $\mathbb{Q}$.

The relationship between $(\mathfrak{M})$ and $\mathscr{(T)}$ is given by 
\end{remarks*}

\begin{proposition}
{\em The algebra $(\mathfrak{M})$ is the commutator algebra of
$\mathscr{(T)}$. (Definition.} By the {\em commutator algebra} of
$\mathscr{(T)}$, we mean the set of all $2n$-rowed rational square
matrices $M$ for which $TM=MT$ for all $T\in \mathscr{(T)}$).
\end{proposition}

\begin{proof}
For\pageoriginale each $M \in (\mathfrak{M})$, we have
$$
0 = R M - MR = \sum^p_{j=1} \rho_j (L_j M-ML_j).
$$
But now $\rho_1,\ldots,\rho_p$ being linearly independent over
$\mathbb{Q}$ and since $L_j M - M L_j$, $j=1,2,\ldots,p$ are rational
matrices, we deduce that $L_jM= M L_j$ for $1\leq j \leq p$. Hence
$TM=MT$ for all $T\in\mathscr{(T)}$. The converse is trivial, since if $M$ is
in the commutator algebra of $\mathscr{(T)}$, then $M$ commutes with $L_j$ for
$1 \leq j \leq p$ and hence with $R$ by \eqref{eq14}. Thus $(\mathfrak{M})$ is
precisely commutator algebra of $\mathscr{(T)}$.
\end{proof}

\begin{proposition}
 The algebra $\mathscr{(T)}$ admits the involution $T\rightarrow
T^{\ast} = A^{-1} T' A$.
\end{proposition}

\begin{proof}
First of all, we see that for the basis elements $L_j, j =
1,2,_{\cdotp p}$ of $\mathscr{(T)}$, we have $L^{\ast}_j = -L_j$. For,
from $A'=-A$, $S=S'$, we have $R^{\ast} = -R$ and further
$R^{\ast}=(\rho_1L_1 + \ldots + \rho_p L_p)^{\ast} = \rho_1
L^{\ast}_1+\ldots + \rho_p L^{\ast}_p = - (\rho_1L_1 +
\ldots + \rho_p L_p)$. In other words, we have
$$
\rho_1 (L_1 + L^{\ast}_1) + \ldots + \rho_p(L_p+L^{\ast}_p) = 0.
$$
Again, since $L_j+L^{\ast}_j$, $1\leq j\leq p$ are rational and
$\rho_1,\ldots, \rho_p$ are linearly independent over
$\mathbb{Q}$, we have $L^{\ast}_j = -L_j (1\leq j\leq p)$. And now,
for any $T = \sum\limits_{1\leq k_1,\ldots, k_{m} \leq p} a_{k_1\ldots
  k_m} L_{k_1} \ldots L_{k_m}$, we see that
$$
T^{\ast} = \sum a_{k_1\ldots k_m} L^{\ast}_{k_m} \ldots
L^{\ast}_{k_1} = \sum_{1\leq k_1, \ldots, k_{m} \leq p}
a_{k_1\ldots k_m} L_{k_m} \ldots L_{k_1} \in \mathscr{(T)}.
$$
That the mapping $T \rightarrow T^{\ast}$ is an involution of
$\mathscr{(T)}$ is quite clear.

Let \pageoriginale us remark that the involution $T\rightarrow
T^{\ast}$ of $\mathscr{(T)}$ is not necessarily a positive
involution. The fact that $S=AR$ is symmetric is equivalent to the
fact that $\mathscr{(T)}$ is closed under an involution $T\rightarrow
T^{\ast}$ such that $L^{\ast}_j = -L_j$, $1\leq j\leq p$. Therefore,
the condition that $S$ is positive symmetric is much stronger than
$\mathscr{(T)}$ admitting the special involution $T\rightarrow
T^{\ast}$. 

Since $R= \begin{smallmatrix} [R_1, &  R_2,& \ldots]\\
f_1 & f_2 & \end{smallmatrix}$, it is clear that every $L_i$ is
of the form as $R$, in view of the linear independence of
$\rho_1,\ldots \rho_p$ over $\mathbb{Q}$. Thus, any $T \in
\mathscr{(T)}$ is of the form $\begin{smallmatrix} [T_1, &
    T_2, & \ldots]\\ f_1 & f_2 & \end{smallmatrix} $ with
$T_j$ being repeated $f_j$ times in the direct sum. For fixed $j$, let
us denote by $(\mathscr{T}_j)$ the algebra generated over $\mathbb{Q}$
by such rational matrices $T_j$ and the corresponding abstract algebra
by $\mathscr{T}_j$. None of the $\mathscr{T}_j$ can be the
null-algebra for then we will have $R_j =0$, contradicting $R^2 = -
E$.
\end{proof}

\begin{remark*}\label{rem:p14}
For $k\neq 1$, it could happen that $\mathscr{T}_k$ and
$\mathscr{T}_1$ are isomorphic. But {\em there cannot exist a nonsingular
rational matrix $B$ independent  of $T = \begin{smallmatrix} [T_1, &
 T_2, & \ldots]\\ f_1 & f_2 & \end{smallmatrix} \in
(\mathscr{T})$ such that for $k \neq 1$, $T_k = B^{-1} T_1 B$ for
every $T\in(\mathscr{T})$.} For, if such a $B$ were to exist then $R_k
= B^{-1} R_1 B$ for $k\neq 1$, which is a contradiction.

Each algebra $(\mathscr{T}_j)$ is necessarily irreducible (since, if
$(\mathscr{T}_j)$ were reducible, we would have $R_j$ necessarily
reducible).

By a {\em simple algebra}, we mean an irreducible matrix algebra
\cite{28}. This definition of a simple algebra can be identified with
another \pageoriginale of a simple (matrix) algebra as one having no
proper two-sided ideals. A {\em semi-simple algebra} is, by
definition, a direct sum of simple algebras.
\end{remark*}

\begin{proposition}
The algebra $(\mathscr{T})$ is semi-simple.
\end{proposition}

\begin{proof}
The algebras $(\mathscr{T}_j)$ are simple and if we could show that
$\mathscr{T}$ is the direct sum of the algebras $\mathscr{T}_j$, then
our proposition would be proved. For this, it is sufficient to prove
that if $T_j \in (\mathscr{T}_j)$, then $T =
\begin{smallmatrix} \big[0 & 0 & T_1 & 0 \big]\\ f_1^{,\dots,} &
  f_{1-1}{\; '}& 
  f_1^{\; ,\dots,} &  f_p \end{smallmatrix} \in 
(\mathscr{T})$ for every $l$ with $1\leq l \leq p$. We might suppose,
without loss of generality that $l=1$. Let now, for $1\leq j \leq p$,
$(\mathfrak{N}_j)$ be the set of $T_j\in (\mathscr{T}_j)$ such that
$T= \begin{smallmatrix} \big[\ast, \ldots, &  \ast, & T_j, & 
 0,\ldots, & 0 \big]\\f_1 & f_{j-1} & f_j & f_{j+1} &
  f_p  \end{smallmatrix}$ is in $(\mathscr{T})$. It is easy to 
verify that $(\mathfrak{N}_j)$ is a two-sided ideal in
$(\mathscr{T}_j)$. Now $(\mathscr{T}_j)$ being simple, we have
$(\mathfrak{N}_j) = (\mathscr{T}_j)$ or $(\mathfrak{N}_j)$ is the
null-algebra. If $(\mathfrak{N}_1) = (\mathscr{T}_1)$, we are
through. Otherwise, let $k$ be the smallest positive integer greater
than 1 such that $(\mathfrak{N}_{k-1})$ is the null-algebra and
$(\mathfrak{N}_k)$ is the whole of $(\mathscr{T}_k)$. Then
necessarily, $2\leq k \leq p$ for otherwise $(\mathscr{T}_p)$ will be
the null-algebra which is not true, as we know. We now claim that if
$T= \begin{smallmatrix} \big[T_1,\ldots, & T_{k-1}, &
  T_k,\ldots, & 0 \big]\\ f_1 & f_{k-1} & f_k &
  f_p \end{smallmatrix}$ is in $(\mathscr{T})$, then
corresponding to $T_k\in (\mathscr{T}_k)$, $T_{k-1}$ in
$(\mathscr{T}_{k-1})$ is uniquely determined. For, if 
$T = \begin{smallmatrix}\big[T_1,\ldots, & T_{k-1}, & T_k, &
    0,\ldots, & 0 \big]\\ f_1 & f_{k-1} & f_k & f_{k+1} &
    f_p\end{smallmatrix} \in (\mathscr{T})$ and $M =
\begin{smallmatrix} \big[M_1,\ldots, & M_{k-1}, & 
  T_k, & 0\ldots & 0 \big]\\ f_1 & f_{k-1} & f_k & f_{k+1} &
  f_p \end{smallmatrix} \in (\mathscr{T})$, \pageoriginale then
$T_{k-1} - 
M_{k-1} \in (\mathfrak{N}_{k-1})$ which is the null-algebra, by
definition of $k$. Thus there is a one-one correspondence $T_k
\leftrightarrow T_{k-1}$ between $(\mathscr{T}_{k})$ and
$(\mathscr{T}_{k-1})$ which is actually an algebra isomorphism. But
now $(\mathscr{T}_k)$ and $(\mathscr{T}_{k-1})$ are irreducible, and
therefore there exists a constant non-singular rational matrix $B$
such that if $T= \begin{smallmatrix} \big[\ldots & T_{k=1}, &
    T_k, & \ldotp \big]\\ & f_{k-1} & f_k\end{smallmatrix} \in
(\mathscr{T})$ , then $T_{k-1} = B^{-1} T_k B$, which is a
contradiction to our Remark on p.~\pageref{rem:p14}. Then $(\mathfrak{N}_1) =
(\mathscr{T}_1)$ and similarly we can show $(\mathfrak{N_j}) =
(\mathscr{T}_j)$ for every $j$.

By a theorem on algebras, the commutator algebra of a semi-simple
matrix algebra is again semi-simple. Thus $(\mathfrak{M})$ which is
the commutator-algebra of $(\mathscr{T})$ is semi-simple. (Compare the
proof of Theorem 1.4-A, p.717, \cite{28}). We shall however find the
structure of $(\mathfrak{M})$ explicitly, as follows.

We may write $R = \left[\begin{smallmatrix} R_1, & R_2,\ldots,\\
 f_1 & f_2 \end{smallmatrix}\right]$ as 
$$
R=\left[R^{(1)},  R^{(2)},\ldots,\right]
$$
 and correspondingly $T = \left[\begin{smallmatrix} T_1, & T_2
     \ldots\\ f_1 & f_2 \end{smallmatrix}\right]\in
 (\mathscr{T})$ as  
$$
 T = [T^{(1)}, T^{(2)},\ldots ,]
$$
Again, we decompose $M\in (\mathfrak{M})$ correspondingly as
$(M_{kl})$. From $T M = M T$, \pageoriginale it follows 
\begin{equation*}
 T^{(k)} M_{kl} = M_{kl} T^{(l)} \tag{15}\label{eq15}
\end{equation*}
where $T^{(k)}$, $T^{(l)}$ run over all elements of
$(\mathscr{T}_{j_k})$ and $(\mathscr{T}_{j_l})$ respectively. But now
the algebras $(\mathscr{T}_j)$ are irreducible. Therefore applying
Schur's lemma, we see that either $M_{kl}$ is the zero matrix or if
$M_{kl}$ is a square matrix (different from the zero matrix), then it
is necessarily non-singular. Let us suppose now that $j_k \neq
j_l$. If $M_{kl}$ is a square matrix different from zero, then it is
necessarily a nonsingular (rational) matrix and this contradicts the
remark on p.~\ref{rem:p14}. Thus corresponding to the decomposition
$\left[\begin{smallmatrix} R_1, & R_2 \ldots \\ f_1 &
    f_2 \end{smallmatrix}\right] $ of $R$, the matrix $M \in
(\mathscr{M})$ takes the form $[M_1, M_2, \ldots]$ where $M_k =
(M^{(k)}_{pq})$ and from \eqref{eq15},
\begin{equation*}
T_k M^{(k)}_{pq} = M^{(k)}_{pq} T_k \tag{16}\label{eq16}
\end{equation*}
for every $T_k \in (\mathscr{T}_k)$. Thus $M^{(k)}_{pq}$ belongs to
the commutator algebra of $(\mathscr{T}_k)$ which we may denote by
$(\mathfrak{L}_k)$. By Schur's lemma again, since $(\mathscr{T}_k)$ is
irreducible, we see from \eqref{eq16} that $M^{(k)}_{pq}=0$ or it is
non-singular. Thus the matrix algebra $(\mathfrak{L}_k)$ is indeed a
division-algebra. Conversely, if $M = [M_1,\ldots, M_k, \ldots]$
where $M_k = (M^{(k)}_{pq})$, and $M^{(k)}_{pq} \in (\mathfrak{L}_k)$,
then $M\in (\mathfrak{M})$. Thus $(\mathfrak{M})$ is the direct sum
$(\mathfrak{M}_1) + (\mathfrak{M}_2) + \ldots $ where
$(\mathfrak{M}_k)$ is the complete $f_k$-rowed matrix algebra over the
division algebra $(\mathfrak{L}_k)$. Each $(\mathfrak{M}_k)$ is a
simple algebra,\pageoriginale since it is the complete matrix algebra
over a division algebra. Thus $(\mathfrak{M})$ is semi-simple.

Let us remark that one could prove the fact $(\mathfrak{M})$ is
semi-simple also directly by making use of the positive involution in
$(\mathfrak{M})$. 

In the direct sum decomposition $(\mathfrak{M}) = (\mathfrak{M}_1) +
(\mathfrak{M}_2) + \ldots$ above, each $(\mathfrak{M}_k)$ is a
complete matrix-algebra over a division-algebra $(\mathfrak{L}_k)$
and $(\mathfrak{M}_k)$ carries a positive involution which, when
restricted to $(\mathfrak{L}_k)$ is again a positive involution. Thus,
in our study of the commutator algebra of a $R$-matrix, we are finally
reduced to the case of division algebras of finite rank over
$\mathbb{Q}$, carrying a positive involution.
\end{proof}

\section{Division algebras over $\mathbb{Q}$ with a positive involution}

We now consider a division algebra $(\mathfrak{M})$ with a positive
involution, realised as the commutator algebra of a simple (matrix)
algebra $(\mathscr{T})$. From the theory of algebras, it is known that
the commutator algebra of $(\mathfrak{M})$ is precisely
$(\mathscr{T})$.

Regarding the subalgebra $(\mathfrak{R}) = (\mathscr{T}) \cap
(\mathfrak{M})$, we have 

\begin{proposition}\label{chap1:prop6}
The algebra $(\mathfrak{R})$ coincides with the centre of
$(\mathscr{T})$ as also with the centre of $(\mathfrak{M})$. 
\end{proposition}

\begin{proof}
Let \pageoriginale $K \in (\mathfrak{R})$. Then, since $K \in
(\mathfrak{M})$, $K$ belongs to the centre of
$(\mathscr{T})$. Conversely, if $L$ is in the centre of
$(\mathscr{T})$, then $L\in(\mathfrak{M})$, by our remark on
commutator algebras above and therefore $L\in (\mathfrak{R})$. Again
let $K \in (\mathfrak{R})$. Then $K$ is in the centre of
$(\mathfrak{M})$, since $K \in (\mathscr{T})$. Conversely, if $L$
belongs to the centre of $(\mathscr{T})$, then $L \in (\mathfrak{M})$
clearly.

Since $(\mathfrak{M})$ is a division algebra, its centre
$(\mathfrak{R})$ is a field which is a representation of an algebraic
number field $\mathfrak{R}$ of degree $h$, say, over $\mathbb{Q}$. We
denote the conjugates of $\mathfrak{R}$ by $\mathfrak{R}^{(1)}
(=\mathfrak{R})$, $\mathfrak{R}^{(2)},\ldots, \mathfrak{R}^{(h)}$. For
$\alpha \in \mathfrak{R}$, we denote its conjugates by $\alpha^{(1)}
\ldots \ldots, \alpha^{(h)}$, and the `trace' $\alpha^{(1)} + \ldots +
\alpha^{(h)}$ and `norm' $\alpha^{(1)},\ldots \alpha^{(h)}$
respectively by $tr_{\mathfrak{R}/\mathbb{Q}}(\alpha)$,
$N_{\mathfrak{R}/\mathbb{Q}} (\alpha)$.

The involution $M \rightarrow M^{\ast}$ of $(\mathfrak{M})$, when
restricted to $(\mathfrak{R})$, gives an automorphism $K\rightarrow
K^{\ast}$ of $\mathfrak{R}$, of order 2. We now distinguish between
the following two cases:
\begin{itemize}
\item[{\rm (i)}] for every element $\kappa$ of $\mathfrak{R}$,
  $\kappa^{\ast} = \kappa$.

\item[{\rm (ii)}] there exists at least one $\kappa_{\circ} \in
  \mathfrak{R}$ such that $\kappa^{\ast}_{\circ} \neq \kappa_{\circ}$.
\end{itemize}

In \pageoriginale the case of (i), we say that the {\em involution is
  of the first kind} and in the case of (ii), we say it is {\em of the
second kind}.

The positive involution in $(\mathfrak{R})$ enables us to characterise
the field $\mathfrak{R}$ further, as follows.
\end{proof}

\begin{thm}\label{chap1:thm2}
In the case of positive involutions of the first kind, $\mathfrak{R}$
is totally real. In the case of positive involutions of the second
kind, $\mathfrak{R}$ is a totally complex field which is an imaginary
quadratic extension of a totally real field $\mathscr{L}$.
\end{thm}

\begin{proof}
First, we take the case of a positive involution of the first kind in
$\mathfrak{R}$. To $\kappa \in \mathfrak{R}$, there corresponds $K \in
(\mathfrak{R})$. Now $\sigma (K K^{\ast}) = \sigma (K^2) > 0$ for
every $K$ in $(\mathfrak{R})$, different from $0$. But
$(\mathfrak{R})$ is a multiple of the ``regular representation'' of
$\mathfrak{R}$ over $\mathbb{Q}$ (upto equivalence) and hence
$\sigma(K^2) = m\ldotp tr_{\mathfrak{R}/\mathbb{Q}}(\kappa^2)$ where,
$m$ is a positive rational integer. Thus for $\kappa \neq 0$ in
$\mathfrak{R}$, we have
\begin{equation*}
tr_{\mathfrak{R}/\mathbb{Q}} (\kappa^2) \neq 0 \tag{17}\label{eq17}
\end{equation*}
Suppose now $\mathfrak{R}$ is not totally real; in fact, let
$\mathfrak{R}^{(1)}$ and $\mathfrak{R}^{(2)}$ be a pair of complex
conjugates, without loss of generality. Let $\omega_1, \omega_2,
\ldots, \omega_h$ be a basis of $\mathfrak{R}$ over
$\mathbb{Q}$. Then we have, for $1\leq k \leq h$, $\kappa^{(k)} =
\sum\limits^h_{j=1} x_j \omega^{(k)}_j$ with $x_j \in \mathbb{Q}$. Now
$tr_{\mathfrak{R}/ \mathscr{Q}}(\kappa^2) = F(x_1,\ldots, x_h)$
is a quadratic form in $x_1,\ldots, x_h$ with coefficients in
$\mathbb{Q}$ and it assumes positive (rational) values for rational
$x_1,\ldots, x_h$ not all zero.
Hence,\pageoriginale in the first place $F$ is nondegenerate since if
$F$ is degenerate, there exists a rational column $\ub{x}_0 = 0$ where
$F_1$ is the matrix associated with $F(x_1 \ldots x_h)$ and
then $\ub{x}'_0 F_1 \ub{x}_0 = F (x^{(0)}_1,\ldots x^{(0)}_h) =
0$ for rational $x^{(0)}_1, \ldots x^{(0)}_h$ not all zero. By
continuity, it can be seen that it is, in fact, a positive-definite
quadratic form in $x_1,\ldots, x_h$. Since the matrix
$(\omega^{(j)}_k) (1\leq k, j \leq h)$ is non-singular and
$\mathfrak{R}^{(1)}, \mathfrak{R}^{(2)}$ are complex fields, it is
possible to find real numbers $x^{(0)}_1,\ldots, x^{(0)}_h$
such that 
\begin{align*}
\omega^{(1)}_1  x^{(0)}_1 + \ldots + \omega^{(1)}_h x^{(0)}_h &
= i \\[5pt]
\omega^{(2)}_1 x^{(0)}_1 + \ldots + \omega^{(2)}_h x^{(0)}_h &
= -i\\[5pt]
\omega^{(j)}_1  x^{(0)}_1 + \ldots + \omega^{(j)}_h x^{(0)}_h &
= 0 \text{ for } 2 < j \leq h. 
\end{align*}

Now $F(x^{(0)}_1, \ldots x^{(0)}_h) = i^2 + (-i)^2 + 0 + \ldots + 0 =
-2 < 0$. We can, by continuity of $F(x_1,\ldots, 
x_h)$ again, find rational numbers $x'_1,\ldots, x'_h$
sufficiently close to $x^{(0)}_1, \ldots, x^{(0)}_h$ such that
$F(x'_1, \ldots, x'_h)<0$, which is a contradiction. Thus
$\mathfrak{R}$ is necessarily totally real.

We now take the case of involutions of the second kind. Let
$\mathscr{L}$ be the fixed field of the involution, viz. the set of
all $\kappa \in \mathfrak{R}$ such that $\kappa^{\ast} =
\kappa$. Since the involution is of the second kind, there exists
$\kappa_0 \in \mathfrak{R}$ such that $\kappa_0 \neq \kappa^{\ast}_0$;
clearly $\kappa_0 \not\in \mathscr{L}$. We now claim that
$\rho = \kappa_0 - \kappa^{\ast}_0(\neq 0 !)$ generates $\mathfrak{R}$
over $\mathscr{L}$. For $\rho=-\rho^{\ast}$ and $\rho^2 = \delta
\in\mathscr{L}$, $\delta \neq 0$. An arbitrary $\kappa \in
\mathfrak{R}$ can be written as $\dfrac{1}{2}(\kappa + \kappa^{\ast})
+ \dfrac{1}{2\rho}(\kappa-\kappa^{\ast}) = \lambda+\mu \rho$
(say). Obviously $\lambda$, $\mu \in \mathscr{L}$ and further, 
\begin{equation*}
\text{if } \kappa = \lambda + \mu \rho \text{ with } \lambda, \mu \in
\mathscr{L}, \text{ then } \kappa^{\ast} = \lambda - \mu
\rho. \tag{18}\label{eq18} 
\end{equation*}\pageoriginale
(It is trivial that any $\lambda + \mu \rho$ with $\lambda$, $\mu \in
\mathscr{L}$ belongs to $\mathfrak{R}$). Now if $K \in (\mathfrak{R})$
corresponds to $\kappa \in \mathfrak{R}$, then $\sigma(K K^{\ast})>0$
for $K\neq 0$ implies that $tr_{\mathfrak{R}/\mathbb{Q}}((\lambda+\mu
\rho)(\lambda-\mu \rho)) = tr_{\mathfrak{R}/\mathbb{Q}} (\lambda^2 -
\mu^2 \delta)>0$ for all $\lambda$, $\mu \in \mathscr{L}$ not both
zero. (Recall that $\sigma(KK^{\ast}) = m\ldotp
tr_{\mathfrak{R}/\mathbb{Q}} (\kappa\kappa^{\ast})$ for a positive
integer $m$). But we know that
$tr_{\mathfrak{R}\mathbb{Q}}(\kappa\kappa^{\ast}) =
tr_{\mathscr{L}/\mathbb{Q}} (tr_{\mathfrak{R}/\mathscr{L}} (\kappa
\kappa^{\ast})) = 2 tr_{\mathscr{L}/\mathbb{Q}} (\lambda^2)-2
tr_{\mathscr{L}/\mathbb{Q}} (\mu^2 \delta)$. In particular, for
$\lambda \neq 0$ in $\mathscr{L}$,
$tr_{\mathscr{L}/\mathbb{Q}}(\lambda^2)>0$ which implies, by the
foregoing arguments that $\mathscr{L}$ is totally real. We now claim
that $\delta$ is necessarily totally negative. For, if one particular
conjugate, say $\delta^{(j)}>0$, then we can find an element
$\varepsilon$ in $\mathscr{L}$ such that $|\varepsilon^{(j)}|$ is
large and $|\varepsilon^{(k)}|\leq 1$ for $k\neq j$ so that
$tr_{\mathscr{L}/\mathbb{Q}} (-\varepsilon^2\delta)<0$ which gives a
contradiction. Thus $\mathfrak{R} = \mathscr{L}(\sqrt{\delta})$
i.e. $\mathfrak{R}$ is an imaginary quadratic extension of the totally
real field $\mathscr{L}$  .
 \end{proof}

\begin{remark*}
\label{rem:p22}
In the case of positive involutions of the second kind, the {\em
  involution is uniquely determined} by \eqref{eq18}, viz. if $\kappa =
\lambda + \mu \sqrt{\delta}$ with $\lambda$, $\mu \in \mathscr{L}$,
then $\kappa^{\ast} = \lambda - \mu \sqrt{\delta}$ which is just the
complex conjugate of $\kappa$. If the involution is not positive, then
it is not uniquely determined, in general. Take the biquadratic field
generated by $\sqrt{2}$ and $\sqrt{3}$ over $\mathbb{Q}$; we have two
distinct involutions, $\sqrt{2}\rightarrow - \sqrt{2}$ and $\sqrt{3}
\rightarrow - \sqrt{3}$.

Having known the structure of the centre $\mathfrak{R}$ of the algebra
$\mathfrak{M}$, we wish to remark that the division algebra
$\mathfrak{M}$ can be considered as an algebra even over
$\mathfrak{R}$, or, in the notation of the theory of algebras, a {\em
  central algebra}. For, let $\kappa_1,\ldots, \kappa_h$ be a
basis of $\mathfrak{R}$ over \pageoriginale $\mathbb{Q}$ and let us
denote the identity in $\mathfrak{M}$ by $\gamma_1$. Then $\kappa_1
\gamma_1, \ldots \ldots \ldots , \kappa_h \gamma_1$ are linearly
independent over $\mathbb{Q}$. If there exists $\gamma_2$ in
$\mathfrak{M}$ linearly independent of these $h$ elements, then it is
easy to see that $\kappa_1 \gamma_1, \ldots\kappa_h \gamma_1, \kappa_1
\gamma_2, \ldots, \kappa_h \gamma_2 $ are linearly independent over
$\mathbb{Q}$. In this way, we can find $\gamma_1,\gamma_2,\ldots
\gamma_m $ in $\mathfrak{M}$ such that $\kappa_1 \gamma_1,\ldots,
\kappa_h \gamma_1, \kappa_1 \gamma_2,\ldots, \kappa_h \gamma_2,
\ldots, \kappa_h \gamma_m$ form a basis of $\mathfrak{M}$ over
$\mathbb{Q}$ and $\gamma_1,\ldots, \gamma_m$ form a basis of
$\mathfrak{M}$ over $\mathfrak{R}$. Thus $\mathfrak{M}$ is a central
algebra of rank $m$ over $\mathfrak{R}$. It is known from the theory
of algebras that $m=s^2$, where $s$ is a rational integer.

In connection with the problem of determining the division algebras
over $\mathbb{Q}$ with a positive involution, occurring as the
complete commutator algebra of a $R$-matrix, we shall find, as a first
step, all division algebras over $\mathbb{Q}$ carrying a positive
involution. In view of our remark above, it clearly suffices to find
all central division algebras with a positive involution over a given
field of the type mentioned in Theorem~\ref{chap1:thm2}. Then, given one positive
involution therein, we shall obtain all positive involutions in the
algebra. We shall also examine the possibility of expressing the given
positive involution in the specific form $M\rightarrow A^{-1} M' A$
(in the regular representation) with a rational non-singular
skew-symmetric matrix $A$ and then getting from one such $A$, all
other principal matrices for the same involution.

First we proceed to determine all central division algebras
$\mathscr{V}$ with a positive involution, over a given number field. 

Let \pageoriginale $\mathscr{V}$ be commutative. Then, by Theorem~\ref{chap1:thm2},
$\mathscr{V}$ is either a totally complex or a totally real number
field $\mathfrak{R}$, of degree $h$, say, over $\mathbb{Q}$. Let
$\mathfrak{R}^{(1)},\ldots, \mathfrak{R}^{(h)}$ be the conjugates of
$\mathfrak{R}$ and $\omega_1,\ldots, \omega_h$ be a basis of
$\mathfrak{R}$ over $\mathbb{Q}$. We now consider the so-called 
{\em  regular representation} of $\mathfrak{R}$ over $\mathbb{Q}$
(relative to $\omega_1,\ldots, \omega_h$). For any $\delta \in
\mathfrak{R}$, we have $\omega_k \delta = \sum\limits^h_{j=1} x_{kj}
\omega_j$, $1\leq k \leq h$, $x_{kj} \in \mathbb{Q}$.
\begin{equation*}
\omega^{(1)}_k \delta^{(1)} = \sum^h_{j=1} x_{kj} \omega^{(1)}_j,
\quad 1 \leq k, \quad 1 \leq h. \tag{19}\label{eq19}
\end{equation*}
Denoting the matrices $[\delta^{(1)},\ldots, \delta^{(h)}]$,
$(\omega^{(1)}_k)$ and $(x_{kj})$ by $(\delta)$, $\Omega$ and $D$
respectively, we rewrite (19) in matrix form as
\begin{equation*}
\Omega (\delta) = D \Omega \tag*{$(19)'$}\label{eq19'}
\end{equation*}

The mapping $(\delta)\rightarrow D$ gives a faithful and irreducible
rational representation of $\mathfrak{R}$. If $\omega_1, \ldots ,
\omega_h$ are replaced by $\omega'_1,\ldots, \omega'_h$ where
$\left(\begin{smallmatrix} \omega'_1
  \\ \vdots\\ \omega'_h\end{smallmatrix}\right)  = C
  \left(\begin{smallmatrix}
    \omega_{1}\\ \vdots\\ \omega_{h}\end{smallmatrix}\right)$ where
  $C$ is rational and nonsingular, then we have the equivalent
  representation $(\delta) \rightarrow C D C^{-1}$. All irreducible
  representations of $\mathfrak{R}$ are equivalent to the regular
  representation of $\mathfrak{R}$ and an arbitrary `non-degenerate'
  representation of $\mathfrak{R}$ is just a multiple of the same.

The involution $\delta \rightarrow \delta^{\ast}$ in $\mathfrak{R}$
is, in view of our remark on p.~\pageref{rem:p22} given by 
\begin{equation*}
\delta^{\ast} = 
\begin{cases}
\delta, & \text{ if the involution is of the first kind}\\
\ob{\delta}, &  \text{ if it is of the second kind.}
\end{cases}
\end{equation*}
Passing to the transpose conjugate in $(19)'$, we have
$$
(\delta^{\ast}) \ob{\Omega}' = \ob{\Omega}'D'
$$\pageoriginale
But $\Omega (\delta^{\ast}) = D^{\ast}\Omega$. Thus setting $F^{-1} =
\Omega \ob{\Omega}'$, we have 
$$
D^{\ast} = F^{-1} D' F.
$$\label{p25}
Now, observing that the involution $\ast$ commutes with all the
isomorphisms of $\mathfrak{R}$ i.e. $\ob{\omega^{(k)}_j} =
(\ob{\omega}_j)^{(k)}$, we see that $F^{-1}=
(tr_{\mathfrak{R}/\mathbb{Q}}(\omega_i \ob{\omega}_j))$ is a rational
matrix but being positive hermitian, is positive symmetric.

The rank of a central (matrix) division algebra $\mathscr{V}$ over its  
centre is $s^2$, where $s$ is a rational integer. For $s=1$,
$\mathscr{V}$ itself is an algebraic number field $\mathfrak{R}$ and
we have seen in detail, the structure of $\mathfrak{R}$ in order that
it might carry a positive involution.

Now suppose $s=2$. The algebra $\mathscr{V}$ is then a so-called {\em
  quaternion algebra} over the centre $\mathfrak{R}$. Any element
$\delta \in \mathscr{V}$ is of the form $\delta = x + y i + z j + tk$
where $i,j,k$ satisfy the multiplication table given below:
\begin{align*}
i^2 & = a \in \mathfrak{R}, \quad j^2 = b \in \mathfrak{R}, \quad k^2
= -ab, \quad ij = -ji = k,\\  jk &= -bi = - kj
ki  = -aj = - ik.
\end{align*}
It can be verified that if $1,i,j,k$ are linearly independent over
$\mathbb{Q}$ and satisfy the multiplication table above, then they
generate an algebra $\mathscr{V}$ of rank 4, with centre
$\mathfrak{R}$.

When is this algebra a division algebra with a positive involution?
 
Now, in $\mathscr{V}$, we have the mapping
$$
\delta = x + yi + zj + tk \rightarrow \widetilde{\delta} = x - yi - zj - tk
$$\pageoriginale
and it is easy to check that this is an involution of
$\mathscr{V}$. Under the regular representation, $\delta \rightarrow
D$ where $D$ is given by 
\begin{gather*}
\left(\begin{smallmatrix} 1\\ i\\ j \\k\end{smallmatrix}\right) (x+yi
  + zj + tk) = D
  \left(\begin{smallmatrix} 1 \\ i \\ j \\ k \end{smallmatrix}\right)
  \\
\text{i.e. } D =
\left(\begin{smallmatrix} x & y & z & t \\
ay & x & at & z\\
bz & -bt & x & -y \\
-abt & bz & -ay & x
\end{smallmatrix}\right) 
\end{gather*}
Now $\widetilde{D} = \left(\begin{smallmatrix} x & -y & -z & -t \\
-ay & x & -at & -z\\
-bz & bt & x & y \\
abt & -bz & ay & x
\end{smallmatrix}\right)$. Defining $F^{-1} = [1, -a, -b, ab]$ it can
be seen that $\widetilde{D} = F^{-1} D' F$. The representation $\delta
\rightarrow D$ is not a representation over $\mathbb{Q}$, but we can
get one by replacing each $\alpha$ in $D$ by its regular
representation $\Omega (\alpha) \Omega^{-1}$ over $\mathbb{Q}$.

In order that $\mathscr{V}$ is a division algebra, it is necessary and
sufficient that the norm of $\delta \in \mathscr{V}$ over
$\mathfrak{R}$ is different from zero, for $\delta \neq 0$. But the
norm of $\delta = x + yi + zj +tk$ over $\mathfrak{R}$ is just $x^2 -
ay^2 - bz^2 + abt^2$. {\em Hence the necessary and sufficient
  condition for $\mathscr{V}$ to be a division algebra is that the
  quadratic form $f(x,y,z,t) = x^2- ay^2- bz^2 + abt^2$ should not
  represent 0 non-trivially over $\mathfrak{R}$}.

We shall now find conditions under which the involution $\delta
\rightarrow \widetilde{\delta}$ in $\mathscr{V}$ is a positive
involution.

More generally, let us take a central division algebra $\mathfrak{M}$
of \pageoriginale rank $m$ over its centre $\mathfrak{R}$ and let
$\delta \rightarrow \delta^{\ast}$ be an involution in $\mathfrak{M}$,
which is identity on $\mathfrak{R}$. We shall now give the ``regular
representation'' of $\mathfrak{M}$ over $\mathfrak{R}$. Let $\gamma_1,
\ldots, \gamma_m$ be a basis of $\mathfrak{M}$ over $\mathfrak{R}$ and
$\omega_1, \ldots, \omega_h$ be a basis of $\mathfrak{R}$ over
$\mathbb{Q}$. Then, for any $\delta = \sum\limits^m_{j=1} x_j \gamma_j
\in \mathfrak{M}$, we have 
\begin{equation*}
\left(\begin{smallmatrix} \gamma_1 \\ \vdots
  \\\gamma_m \end{smallmatrix}\right) \gamma = D 
\left(\begin{smallmatrix} \gamma_1 \\ \vdots
  \\\gamma_m \end{smallmatrix}\right) \tag{20}\label{eq20}
\end{equation*}
where $D = (d_{pq})$ is an $m$-rowed square matrix with elements in
$\mathfrak{R}$. For getting a rational representation of
$\mathfrak{M}$, we may proceed as follows. Under the regular
representation of $\mathfrak{R}$ with respect to the basis $\omega_1,
\ldots, \omega_h$, we know that $d_{pq} \rightarrow D_{pq} = \Omega
[d^{(1)}_{pq}, \ldots, d^{(h)}_{pq}] \Omega^{-1}$ where $\Omega
= (\omega^{(1)}_g)$, $1 \leq g$, $1\leq h$. Let us now take as a
basis of $\mathfrak{M}$ over $\mathbb{Q}$, the $mh$ elements
$\beta_1,\ldots, \beta_{mh}$ defined by
$\beta_{k+(l-1)h}=\omega_k \gamma_1$ for $1\leq k \leq h$, $1
\leq l \leq m$. Then we have for $\delta \in \mathfrak{M}$, 
\begin{equation*}
\left(\begin{smallmatrix} \beta_1 \\ \vdots
  \\\beta_{mh} \end{smallmatrix}\right) \delta = D_0 
\left(\begin{smallmatrix} \beta_1 \\ \vdots
  \\\beta_{mh} \end{smallmatrix}\right) \tag{21}\label{eq21}
\end{equation*}
where $D_0 = (D_{pq}) (1\leq p, q \leq m)$ is clearly rational. We can
get the relationship between $D_0$ and $D$ as follows. Suppose,
instead of $\beta_1,\ldots, \beta_{mh}$, we take as a basis of
$\mathfrak{M}$ the $mh$ elements $\alpha_1, \ldots, \alpha_{mh}$ where
$\alpha_{1+(k-1)m} = \omega_k \gamma_1(1\leq k \leq h, 1 \leq l \leq
m)$ and suppose $V$ is the $mh$-rowed permutation matrix taking
$(l+(k-1)m)^{\frac{th}{\cdots}}$ row to $(k+(l-1)h)^{\frac{th}{\cdots}}$
row; then with respect to the new basis, $\delta \rightarrow V^{-1}D_0
V$. It \pageoriginale is now easy to verify that 
\begin{equation*}
V^{-1} D_0 V = (\Omega \times E_m) [D^{(1)},\ldots, D^{(h)}] (\Omega
\times E_m)^{-1} \tag{22}\label{eq22}
\end{equation*}
where $D^{(1)} = (d^{(1)}_{pq})$ for $1\leq l \leq h$ and $\Omega
\times E_m$ denotes the $mh$-rowed square matrix $(\omega^{(1)}_j
E_m)$, $E_m$ being the $m$-rowed identity matrix.

Let $\delta = \sum\limits^m_{j=1} x_j \gamma_j \in \mathfrak{M}$ and
$\delta \rightarrow D$, $\delta^{\ast} \rightarrow D^{\ast}$ under
\eqref{eq20}. Then $\sigma(D D^{\ast}) = f(x_1, \ldots, x_m)$ is a quadratic
form in $x_1, \ldots x_m$ with coefficients in $\mathfrak{R}$.
\end{remark*}

\begin{proposition}\label{chap1:prop7}
The involution $\delta \rightarrow \delta^{\ast}$ in $\mathfrak{M}$ is
positive if and only if $\mathfrak{R}$ is totally real and $f(x_1,
\ldots, x_m)$ is totally positive-definite (i.e. $f(x_1,\ldots,\break x_m)$
as well as its conjugates over $\mathbb{Q}$ are positive-definite
quadratic\break forms).
\end{proposition}

\begin{proof}
By definition, the involution is positive, if, for every $\delta \in
\mathfrak{M}$ we have $\sigma(D_0 D^{\ast}_0)$ positive for the image
$D_0$ of $\delta$ under \eqref{eq21}. Now, by \eqref{eq22}, $\sigma(D_0 D^{\ast}_0) =
\sum\limits^h_{j-1} \sigma (D^{(j)} (D^{\ast})^{(j)}) =
\sum\limits^h_{j=1} \sigma (D^{(j)} (D^{(j)})^{\ast})$ (defining
$(D^{(j)})^{\ast} = (D^{\ast})^{(j)}$). We have thus
$$
\sigma (D_0 D^{\ast}_0) = tr_{\mathfrak{R}/\mathbb{Q}} (\sigma (D D^{\ast}))
$$
Thus we should have, in particular,
$tr_{\mathfrak{R}/\mathbb{Q}}(\lambda^2)>0$ for $\lambda \neq 0$ in
$\mathfrak{R}$. Therefore $\mathfrak{R}$ should be totally real, using
the arguments in the proof of Theorem \ref{chap1:thm2}.

By the foregoing, the involution is positive if and only if
$tr_{\mathfrak{R}/\mathbb{Q}} (f(x_1,\break\ldots, x_m)) >0$ for
$x_1,\ldots, x_m$ in $\mathfrak{R}$ not all zero. Now, for
\pageoriginale $u\neq 0$ in $\mathfrak{R}$, we have $f(x_1 u, \ldots,
x_m u) = u^2 f(x_1,\ldots, x_m)$. If, for $x^{(0)}_1,\ldots,
x^{(0)}_m$ not all zero, some conjugate of $\kappa = f(x^{(0)}_1,
\ldots, x^{(0)}_m)$ is negative, then, by choosing $u\in \mathfrak{R}$
suitably, we can make $tr_{\mathfrak{R}/\mathbb{Q}}(\kappa u^2) <0$,
which is a contradiction. Moreover, no conjugate of $f(x_1, \ldots,
x_m)$ over $\mathbb{Q}$ can be degenerate, for then there will exist
$x'_1,\ldots, x'_m$ not all zero in $\mathfrak{R}$ such that
$f(x'_1,\ldots, x'_m) = 0$ and
$tr_{\mathfrak{R}/\mathbb{Q}}(f(x'_1.\ldots, x'_m)) = 0$. Thus the
conjugates of $f(x_1,\ldots, x_m)$ are all nondegenerate and represent
only totally positive numbers in the respective conjugates of
$\mathfrak{R}$ which implies that they are positive definite. Our
proposition is thus completely proved.

Going back to the quaternion division algebra $\mathscr{V}$ over
$\mathfrak{R}$, we deduce that {\em the involution $\delta \rightarrow
  \widetilde{\delta}$ is positive if and only if $\mathfrak{R}$ is
  totally real and further, the quaternary form $x^2-ay^2 - bz^2 +
  abt^2$ is totally positive-definite; in other words, $-a$, $-b$
  should both be totally positive numbers in $\mathfrak{R}$.}

If $-a$, $-b$ are not both totally positive, then the involution
$\delta \rightarrow \widetilde{\delta}$ is not positive. We shall, in
this case, look for other involutions in $\mathscr{V}$ which might be
positive. We shall first find the relationship between any two
involutions in $\mathscr{V}$, which have the same effect on
$\mathfrak{R}$.

\end{proof}

\begin{thm}[Albert, \cite{1}]\label{chap1:thm3}
Let $\delta \rightarrow \widetilde{\delta}$ and $\delta \rightarrow
\delta^{\ast}$ be two involutions in a central division algebra
$\mathfrak{M}$ with centre $\mathfrak{R}$ and let, for $\alpha \in
\mathfrak{R}$, $\widetilde{\alpha} = \alpha^{\ast}$. Then there exists
$\lambda \neq 0$ in $\mathfrak{M}$ such that for $\delta \in \mathfrak{M}$,
$$
\delta^{\ast} = \lambda^{-1} \widetilde{\delta}\lambda, \quad
\widetilde{\lambda} = \pm \lambda.
$$ \pageoriginale
\end{thm}

\begin{proof}
The mapping $\delta \rightarrow (\widetilde{\delta^{\ast}})$, being
the composite of two involutions, is an automorphism of $\mathfrak{M}$
and further it in identity on $\mathfrak{R}$. By a theorem of
T. Skolem \cite{23}, every automorphism of a central simple algebra
which is identity on its centre, is an inner automorphism of the
algebra. Therefore, there exists $\lambda\neq 0$ in $\mathfrak{M}$
such that
\begin{align*}
(\widetilde{\delta}^{\ast}) & = \lambda \delta \lambda^{-1} \tag{23}\label{eq23}\\
\text{i.e. } \delta^{\ast} & = \widetilde{\lambda}^{-1}
\widetilde{\delta} \widetilde{\lambda}.
\end{align*}
Replacing $\delta$ by $\delta^{\ast}$ in \eqref{eq23}, we get
\begin{align*}
\widetilde{\delta} & = \lambda \delta^{\ast} \lambda^{-1}\\
& = \lambda \widetilde{\lambda}^{-1} \widetilde{\delta} \lambda^{-1}
\widetilde{\lambda}. 
\end{align*}
In other words, $\lambda^{-1} \widetilde{\lambda}$ commutes with all
elements of $\mathfrak{M}$ and hence $\widetilde{\lambda} = \kappa
\lambda$ for a $\kappa \in \mathfrak{R}$. Further $\lambda =
\widetilde{\kappa} \widetilde{\lambda} = \widetilde{\kappa} \kappa
\lambda$. Since $\mathfrak{M}$ is a division algebra,
$\widetilde{\kappa} \kappa = 1$.

Now, suppose that $\kappa = -1$; then
$\widetilde{\lambda}=-\lambda$. If $\kappa \neq -1$, then setting
$\gamma = \kappa +1$, we have $\gamma \neq C$ and $\widetilde{\gamma}
\kappa = (\widetilde{\kappa}+1) \kappa = \gamma$. Further
$\widetilde{\lambda \gamma} = \widetilde{\gamma} \kappa \lambda = \gamma
\lambda = \lambda \gamma$. Now $\delta^{\ast} = \lambda^{-1}
\widetilde{\delta}\lambda = (\lambda \gamma)^{-1} \widetilde{\delta}
\lambda \gamma$ for $\kappa \neq -1$, we have
$$
\delta^{\ast} = \lambda^{-1}_1 \widetilde{\delta} \lambda_1 \text{
  with } \widetilde{\lambda}_1 = - \lambda_1 \text{ or }
\widetilde{\lambda}_1 = \lambda_1.
$$

Let now $\delta \rightarrow \delta^{\ast}$ be a positive involution of
the first kind in $\mathscr{V}$; then $\mathfrak{R}$ is totally
real. We know that the involution $\delta \rightarrow
\widetilde{\delta}$ in $\mathscr{V}$ is also of the first kind. Thus,
by Theorem~\ref{chap1:thm3}, there exists $\lambda \neq 0$ in $\mathscr{V}$ with
$\widetilde{\lambda} = \pm \lambda$, such that for $\delta \in
\mathscr{V}$. 
\begin{equation*}
\delta^{\ast} = \lambda^{-1} \widetilde{\delta} \lambda. \tag{24}\label{eq24}
\end{equation*}\pageoriginale
If $\widetilde{\lambda} = \lambda$, then $\lambda \in \mathfrak{R}$
and then $\delta^{\ast} = \widetilde{\delta}$ i.e. {\em the involution
$\delta \rightarrow \delta^{\ast}$ coincides with the involution
  $\delta \rightarrow \widetilde{\delta}$.}

Let us suppose now that $\widetilde{\lambda} = - \lambda$. We shall
construct $\rho \in \mathscr{V}$ such that $\rho^{\ast} = -
\widetilde{\rho} \neq 0$ i.e. $\lambda^{-1} \widetilde{\rho} \lambda =
- \widetilde{\rho}$ which means $\lambda \widetilde{\rho} +
\widetilde{\rho} \lambda = 0$. But then applying the involution
$\sim$, we have $\rho\lambda + \lambda\rho =0$. This again gives
$(\rho+\widetilde{\rho})\lambda+\lambda(\rho+\widetilde{\rho})=0$. But
$\rho+\widetilde{\rho} \in \mathfrak{R}$. Therefore
$\rho+\widetilde{\rho} = 0$, since $\lambda \neq 0$. The condition
$\rho^{\ast} = - \widetilde{\rho}$ implies
$\widetilde{\rho}=-\rho$. Expressing $\rho$ as $x+yi+zj+tk$, the
condition $\widetilde{\rho}=-\rho$ means that $x=0$. Further since
$\widetilde{\lambda} = - \lambda$, $\lambda = pi + qj+rk$ with $p$,
$q$, $r\in \mathfrak{R}$. Thus to find $\rho\in \mathscr{V}$ such that
$\rho^{\ast} = -\widetilde{\rho}$, we have only to find numbers $y$,
$z$, $t$ in $\mathfrak{R}$ satisfying $\lambda \rho+\rho \lambda =0$,
i.e. $apy+bqz-abrt =0$. But this last equation is a linear equation in
three unknowns over the field $\mathfrak{R}$ and therefore admits of
infinitely many solutions. Thus, there exists $\rho_0=y_0 i + z_0 j +
t_0 k \in \mathscr{V}$ such that $\rho^{\ast}_0 = -
\widetilde{\rho}_0$ and $\rho_0 \neq 0$.

We now observe that the involutions $\delta \rightarrow
\widetilde{\delta}$ and $\delta \rightarrow \delta^{\ast}$ related by
\eqref{eq24}, with $\widetilde{\lambda}=-\lambda$, cannot both be
positive. For, $tr_{\mathfrak{R}/\mathbb{Q}}(\rho_0 \rho^{\ast}_0) = -
tr_{\mathfrak{R}/\mathbb{Q}}(\rho_0 \widetilde{\rho}_0)$. In the case
when the involution $\delta \rightarrow \widetilde{\delta}$ is
positive, we thus conclude that no involution $\delta \rightarrow
\delta^{\ast}$ with $\delta^{\ast}= \lambda^{-1}
\widetilde{\delta}\lambda$ can be positive unless $\widetilde{\lambda}
= \lambda$ in which case both the involutions coincide.

Let us suppose that the involution $\delta \rightarrow
\widetilde{\delta}$ is not positive. Then, in the first place,
$f(x,y,z,t)$ cannot be totally positive definite \pageoriginale and if
the involution $\delta\rightarrow \delta^{\ast}(=\lambda^{-1}
\widetilde{\delta} \lambda$ with $\widetilde{\lambda} = - \lambda)$
is to be positive, then no conjugate of $f(x,y,z,t)$ over $\mathbb{Q}$
can be negative definite either, since for $\lambda \neq 0$ in
$\mathbb{Q}$, $h\lambda^2 = h\ldotp f(\lambda,
0,0,0)=tr_{\mathfrak{R}/\mathbb{Q}}(\lambda^2) =
tr_{\mathfrak{R}/\mathbb{Q}}(\lambda \lambda^{\ast})$ must be
positive. Now, we claim that no conjugate of $f(x,y,z,t)$ can be
positive-definite either. For, $tr_{\mathfrak{R}/\mathbb{Q}}(\rho_0
\rho^{\ast}_0 u^2)$ must be positive for all $u\neq 0$ in
$\mathfrak{R}$, i.e. $tr_{\mathfrak{R}/\mathbb{Q}}(-f(0,y_0,z_0,
t_0)u^2)$ must be positive for all $u \neq 0$ in $\mathfrak{R}$. But,
now, if some conjugate of $f(x,y,z,t)$ were positive-definite, we
could choose $u$ suitably so that $tr_{\mathfrak{R}/\mathbb{Q}}
(-f(0,y_0,z_0,t_0)u^2)<0$. We know already that no conjugate of
$f(x,y,z,t)$ can be negative definite. Thus $f(x,y,z,t)$ and all its
conjugates must be indefinite, if the involution $\delta \rightarrow
\delta^{\ast} (=\lambda^{-1} \widetilde{\delta} \lambda$ with
$\widetilde{\lambda} = - \lambda)$ were to be positive. We are thus
led to
\end{proof}

\begin{proposition}
If the quadratic form $f(x,y,z,t) = x^2 - ay^2 - bz^2 + abt^2$ is
totally positive-definite, then the only positive involution in
$\mathscr{V}$ is the involution $\delta \rightarrow
\widetilde{\delta}$; otherwise, in order that there might exist
positive involutions in $\mathscr{V}$, $f(x,y,z,t)$ should be totally
indefinite. 
 \end{proposition}

In the case when the form $x^2-ay^2-bz^2 + abt^2$ is totally
indefinite, we remark that by means of a linear transformation in
$x,y,z,t$ with coefficients in $\mathfrak{R}$, it can be brought to
the form $x^2 - a_1y^2 - b_1 z^2+a_1 b_1 t^2$ where $a_1$ and $-b_1$
are totally positive. First, we note that the ternary form
$\varphi(y,z,t) = - ay^2 - bz^2 + abt^2$ is necessarily totally
indefinite (This is because the three numbers $-a^{(j)}, -b^{(j)},
a^{(j)}, b^{(j)}, 1 \leq j \leq h$ cannot all be of the same sign, in
\pageoriginale view of the fact that $a^{(j)}$ and $b^{(j)}$ are not
both negative). We can find a linear transformation over
$\mathfrak{R}$ which takes $\varphi(y,z,t)$ to the form $-\alpha y^2 - bz^2+
\alpha bt^2$ where $-\alpha$ is any totally negative number
represented by $\varphi(y,z,t)$ in $\mathfrak{R}$. Again noticing that
the binary form $-bz^2 + \alpha bt^2$ is totally indefinite, we can
eventually transform $\varphi(y,z,t)$ to the form $-a_1y^2 -b_1z^2 +
a_1b_1t^2$ where $a_1$ and $-b_1$ are totally positive. Thus we could
suppose that the totally indefinite form $x^2 - ay^2 - bz^2 + ab t^2$
has already the property that $a,-b$ are totally positive.

For $\alpha \in \mathfrak{R}$, $\alpha > 0$ means $\alpha$ is totally
positive. Now since $a>0$, the element $i=\sqrt{a}$ generates in
$\mathscr{V}$, a real field $\mathfrak{R}$ (i). Any $\delta = x+ yi +
zj + tk$ can be written as $\xi +\eta j$ with $\xi = x + yi$ and $\eta
= z + ti$. For $\alpha = a+ bi$ in $\mathfrak{R}(i)$, we denote
$\ob{\alpha}=a-bi$. Then we have $j \xi = \ob{\xi} j$. Then we obtain a
representation of $\mathscr{V}$ (as a vector-space over $\mathfrak{R}$
(i)) given by $\delta = \xi +\eta j \rightarrow D_1 =
\left(\begin{smallmatrix} \xi & & \eta\\ \mathfrak{b}\ob{\eta} & & 
  \ob{\xi} \end{smallmatrix}\right) $;
\begin{equation*}
\left(\begin{smallmatrix} 1 \\ j \end{smallmatrix}\right) \delta = D_1
\left(\begin{smallmatrix} 1 \\ j \end{smallmatrix}\right) \tag{25}\label{eq25}
\end{equation*}
Now
\begin{equation*}
\ob{D}_1 = \left(\begin{smallmatrix}\ob{\xi} & \ob{\eta}
  \\ \mathfrak{b} \eta & \xi \end{smallmatrix}\right) = \mathscr{F}
D_1 \mathscr{F}^{-1} \tag{26}\label{eq26}
\end{equation*}
where $\mathscr{F} = \left(\begin{smallmatrix} 0 & 1 \\ b &
  0 \end{smallmatrix}\right)$ corresponds to $\delta=j$ 
under~\eqref{eq25}. Further 
\begin{equation*}
\widetilde{D}_1 = \left(\begin{smallmatrix}\xi & -\eta
  \\ -\mathfrak{b} \ob{\eta} & \xi \end{smallmatrix}\right) =
\mathsf{J} D'_1 \mathsf{J}^{-1} \tag{27}\label{eq27}
\end{equation*}
where \pageoriginale $\widetilde{D}_1$ corresponds under~\eqref{eq25} to
$\widetilde{\delta} = x-yi-zj-tk = \ob{\xi} - \eta j$ and $\mathsf{J}
= \left(\begin{smallmatrix} 0 & 1 \\ -1 &
  0\end{smallmatrix}\right)$. The regular representation $\delta
  \rightarrow D$ of $\mathscr{V}$ may be seen to be equivalent to the
  representation $\delta \rightarrow \left(\begin{smallmatrix} D_1 & 0\\
   0 & \ob{D}_1\end{smallmatrix}\right)$ as below, viz. 
\begin{equation*}
D = W \Omega_2 \left(\begin{smallmatrix} D_1 & 0 \\ 0 &
  \ob{D}_1\end{smallmatrix}\right) \Omega^{-1}_2 W^{-1} \tag{28}\label{eq28}
\end{equation*}
where $\Omega_2 = \left(\begin{smallmatrix} E_2 & E_2 \\ iE_2 & -
  iE_2\end{smallmatrix}\right)$, $E_2$ is the 2-rowed identity and $W$
  is a permutation matrix.

Let $\delta \rightarrow \delta^{\ast} = \lambda^{-1}_1
\widetilde{\delta} \lambda_1$ with $\widetilde{\lambda_1} = -
\lambda_1$ be an involution in $\mathscr{V}$ and let $\delta^{\ast}
\rightarrow D^{\ast}_1$, $\widetilde{\delta} \rightarrow
\widetilde{D}_1$ and $\lambda_1 \rightarrow L_1$ under~\eqref{eq25}. Now
$D^{\ast}_1 = L^{-1}_1 \widetilde{D}_1 L_1$ where $L_1 =
\left(\begin{smallmatrix} \mu & \gamma \\ \mathfrak{b} \ob{\gamma} &
  \ob{\mu}\end{smallmatrix}\right) =-\widetilde{L}_1 = - \mathsf{J}
L'_1 \mathsf{J}^{-1}$, by \eqref{eq27}. Thus setting $F_1 = \mathsf{J}^{-1}
L_1$, we see $F_1$ is real symmetric which is equivalent to saying
that $\mu = - \ob{\mu}$. Further
\begin{equation*}
D^{\ast}_1 = F^{-1}_1 D'_1 F_1 \tag{29}\label{eq29}
\end{equation*}

Now $\sigma(D D^{\ast}) = 2 \sigma(D_1 D^{\ast}_1)$, in view of 
\eqref{eq28} and \eqref{eq26}. Hence, for the involution $\delta \rightarrow \delta^{\ast}$
to be positive, the quadratic form $\sigma(D_1 D^{\ast}_1)$ should be
totally positive-definite over $\mathfrak{R}$. This again implies, by
\eqref{eq29}, that $\sigma (D_1 F^{-1}_1 D'_1 F_1)$ should be totally
positive. A necessary and sufficient condition for this is given by 

\begin{lem}
Let $X=(x_{kl})$, $1 \leq k \leq g$, $1 \leq l \leq h$, be a real
matrix and let $P$, $Q$ be real square matrices of $h$ and $g$ rows
respectively. Then the quadratic form $\sigma(X P X' Q)$ in $x_{kl}$
is positive-definite if and only if $P$ and $Q$ are both
positive-definite or both negative-definite.
\end{lem}

\begin{proof}
There \pageoriginale exist real non-singular matrices $C$ and $B$ such
that $B P B' = [p_1, \ldots, p_h]$ and $C' Q C =[q_1,\ldots,
  q_g]$. Replacing $X$ by $C X B$, we can suppose that already $P$
and $Q$ are in the diagonal form. Now\break $\sigma (X P X' Q) =
\sum\limits^g_{k=1} \sum\limits^h_{l=1} p_l q_k x^2_{kl}$ is positive-definite if
and only if $p_l q_k$ are all positive. Thus either $p_1,\ldots, p_h$,
$q_1,\ldots, q_g$ are all positive or all negative. In other words,
the necessary and sufficient condition is that $P$, $Q$ should be both
positive-definite or both negative-definite.

The passage from $\xi(=x+yi)$, $\eta (=z + ti)$, $\ob{\xi}$,
$b\ob{\eta}$ to $x, y, z, t$ is a nonsingular real linear
transformation and we can thus look upon the elements of $D_1$ as
independent variables. Thus taking $D_1$ for $X$ in lemma 1, the
criterion for $\sigma (D_1 F^{-1}_1 D'_1 F_1)$ to be totally positive
is that each conjugate of $F_1$ is either positive-definite or
negative-definite. Now we can find $\kappa \in \mathfrak{R}$ such that
the conjugates of $\kappa$ have prescribed signs and if we choose
$\kappa \lambda_1$ instead of $\lambda_1$, then we get $\kappa F_1$
instead of $F_1$. Thus, without changing the $\ast$ involution, we
might require that $F_1 = \left(\begin{smallmatrix} -\mathfrak{b}
  \ob{\gamma} & - \ob{\mu}\\ \mu & \gamma \end{smallmatrix}\right)$ is
totally positive-definite. Now $\mu = -\ob{\mu}$ and therefore $\mu =
p i$ with $p \in \mathfrak{R}$ and let $\gamma = q + r i$. The
conditions for $F_1$ to be totally positive are 
$$
\gamma > 0 , \quad -b \ob{\gamma} > 0, \quad \mu \ob{\mu} - b \gamma 
\ob{\gamma} > 0
$$
But $-b >0$ and therefore, these may be rewritten as
$$
\gamma >0, \quad \ob{\gamma} >0, \quad \gamma + \ob{\gamma} = 2q >0, \quad
-p^2 a - b(q^2-ar^2)>0
$$
It is easy to check that all these conditions can be compressed as 
\begin{equation*}
q > 0, \quad -b(q^2-ar^2) > a p^2 \tag{30}\label{eq30}
\end{equation*}\pageoriginale
It is possible to find $p,q,r$ in $\mathfrak{R}$ satisfying \eqref{eq30} (for
example, take $q=1$, $p=r=0$) and therefore, the existence in
$\mathscr{V}$ of positive involutions $\delta \rightarrow
\delta^{\ast}(= \lambda^{-1}_1 \widetilde{\delta} \lambda_1)$ with
$\widetilde{\lambda}_1 =- \lambda_1$ is assured.

Let now $\delta \rightarrow \lambda^{-1} \widetilde{\delta} \lambda$
with $\widetilde{\lambda} = - \lambda \in \mathscr{V}$ be another
positive involution (of the first kind). Setting
$\rho=\lambda^{-1}_1\lambda$, we have $\rho^{-1}\delta^{\ast}\rho =
\lambda^{-1} \widetilde{\delta}\lambda$. Now, $\rho^{\ast} =
\lambda^{\ast} (\lambda^{\ast}_1)^{-1} = (\lambda^{-1}_1
\widetilde{\lambda} \lambda_1) (\lambda^{-1}_1 \widetilde{\lambda}_1
\lambda_1)^{-1}=\rho$. Conversely, if $\rho=\rho^{\ast}$, then
$\lambda = \lambda_1 \rho$ satisfies $\widetilde{\lambda} = -
\lambda$. Thus all such involutions $\delta \rightarrow \lambda^{-1}
\widetilde{\delta} \lambda$ are connected with $\delta \rightarrow
\delta^{\ast}$ by $\lambda^{-1} \widetilde{\delta} \lambda = \rho^{-1}
\delta^{\ast} \rho$ for a $\rho \in \mathscr{V}$ satisfying
$\rho^{\ast}=\rho$.

Suppose $\delta \rightarrow \lambda^{-1}_k \widetilde{\delta}
\lambda_k$, $k=1,2$ are two positive involutions of $\mathscr{V}$,
with $\widetilde{\lambda}_k = - \lambda_k$. If $\lambda_k \rightarrow
L_k$ under \ref{eq25}, then $L_2 = L_1 R_1$ where $R_1$ corresponds to $\rho
= \lambda^{-1}_1 \lambda_2$. Since $\rho^{\ast} =\rho$ by the
foregoing, we have $R^{\ast}_1 = R_1$. Then $F_k = \mathsf{J}^{-1} L_k
(k=1,2)$ should be totally positive. Further $F_2 = F_1
R_1$. Conversely, given $R_1 = R^{\ast}_1$ such that $F_2 = F_1 R_1$
is totally positive, then the element $\lambda_2 \in \mathscr{V}$
corresponding to $L_2 = L_1 R_1$ under \eqref{eq25}, given a positive
involution $\delta \rightarrow^{-1}_z \widetilde{\delta} \lambda_2$ in
$\mathscr{V}$. 
\end{proof}

\begin{lem}\label{chap1:lem2}
If $F$ is a real $m$-rowed positive-definite matrix and $R$ is a real
matrix such that $F R$ is symmetric, then $F R$ is positive-definite
if and only if all the eigenvalues of $R$ are positive.
\end{lem}

\begin{proof}
Since $F$ is positive-definite and $FR=R'F'$, we can find real
non-singular $C$ such that $F=C'C$ and $FR=C'BC$ with $B=[b_1,\ldots,
  b_m]$. \pageoriginale Then $C'BC = C'CR$ i.e. $B=CRC^{-1}$. Thus the
eigenvalues of $B$ and $R$ are the same. Our lemma easily follows.

Choosing the rational representation $\delta \rightarrow D_0$ given
by \eqref{eq21}, we may conclude as follows. {\em If $D_0 \rightarrow
  D^{\ast}_0$ is a positive involution of the first kind in
  $\mathscr{V}$, then all other positive involutions can be obtained
  in the form $D_0 \rightarrow R^{-1}_0 D^{\ast}_0 R_0$ where $R_0 =
  R^{\ast}_0$ in $\mathscr{V}$ and further all the eigen-values of
  $R_0$ are positive. The quadratic form $x^2-ay^2-bz^2 + abt^2$ is
  either totally definite or totally indefinite over $\mathfrak{R}$.}

From \eqref{eq29} and \eqref{eq28}, it can be verified that
\begin{equation*}
D^{\ast} = F^{-1} D' F \tag{31}\label{eq31}
\end{equation*}
where $F=W'^{-1} \Omega'^-{1}_2
\left(\begin{smallmatrix} F_1 & 0 \\ 0 &
  \ob{F}_1 \end{smallmatrix}\right) \Omega^{-1}_2 W^{-1}
$. Now 
\begin{align*}
F^{-1} &= W \Omega_2
\left(\begin{smallmatrix} F^{-1}_{1} & 0 \\ 0 &
  \ob{F}^{-1}_1\end{smallmatrix}\right)  \Omega'_2 W' = W \Omega_2 
\left(\begin{smallmatrix} L^{-1}_1 & 0 \\0 &
  \ob{L}^{-1}_1\end{smallmatrix}\right) (W\Omega_2)^{-1} W \Omega_2\\
&\qquad\qquad\times 
\left(\begin{smallmatrix} \mathsf{J} & 0 \\ 0 &
  \mathsf{J} \end{smallmatrix}\right) (W\Omega_2)^{-1} W \Omega_2
\Omega'_2 W'.
\end{align*}
Further since $W\Omega_2
\left(\begin{smallmatrix} L^{-1}_1 & 0 \\ 0 &
  \ob{L}^{-1}_1 \end{smallmatrix}\right) \Omega^{-1_W -1}_2 $
corresponds to $\lambda^{-1}_1$ under the regular representation of
$\mathscr{V}$, it is a matrix with elements in $\mathfrak{R}$;
moreover it is easy to verify that the matrices $W\Omega_2
\left(\begin{smallmatrix} \mathsf{J} & 0 \\ 0 &
  \mathsf{J} \end{smallmatrix}\right) (W\Omega_2)^{-1}$ and $W\Omega_2
\Omega'_2W'$ have again their elements in $\mathfrak{R}$. Thus
$F^{-1}$ has elements in $\mathfrak{R}$ and moreover being $a$
transform of the totally positive matrix
$\left(\begin{smallmatrix} F^{-1}_1 & 0 \\ 0 &
  \ob{F}^{-1}_1 \end{smallmatrix}\right)$ is itself totally positive
over $\mathfrak{R}$. Going to the rational representation
\pageoriginale $\delta \rightarrow D_0$ again, we have, from \eqref{eq31},
that 
\begin{equation*}
D^{\ast}_0 = F^{-1}_0 D'_0 F_0 \tag{32}\label{eq32}
\end{equation*}
where $F_0$ is a rational positive symmetric matrix. The relation \eqref{eq32}
is analogous to what we obtained on p.~\ref{p25} for the case of fields, in
terms of the regular representation over $\mathbb{Q}$. 

An important theorem due to Albert (p.161, \cite{1}) says that any
division algebra over $\mathbb{Q}$ admitting an involution of the
first kind is either an algebraic number field $\mathfrak{R}$ or a
quaternion division algebra over $\mathfrak{R}$. We have discussed
precisely for these two cases, all the involutions of the first
kind. We may now proceed to study division algebras carrying
involutions of the second kind. Such algebras, again, have been
studied by Albert \cite{1}.

We have, in this connection, to deal with an important class of
algebras called {\em cyclic algebras} first introduced by
L.E. Dickson in 1906.
\end{proof}

\section{Cyclic algebras}\pageoriginale
Let $\mathfrak{Z}$ be a cyclic extension of degree $s(>1)$ over an
algebraic number field $\mathfrak{R}$ of degree $h$ over
$\mathbb{Q}$. Let $\tau, \tau^2,\ldots \tau^{s-1}$, $\tau^s$
(=identity) be the distinct automorphisms of $\mathfrak{Z}$ over
$\mathfrak{R}$. For $\eta \in \mathfrak{Z}$, we denote by
$\eta^{(r)}$, the effect of $\tau^r$ on $\eta$; particular,
$\eta^{(s)} = \eta=\eta^{(0)}$.

Let $\mathfrak{M}$ be the set of elements $\delta = \xi_0 + \xi_1 j +
\ldots + \xi_{x-1} j^{s-1}$ where $\xi_0, \xi_1,\ldots, \xi_{x-1}$ are
in $\mathfrak{Z}$ and $j$ satisfies
\begin{equation*}
j \xi = \xi^{(1)} j \tag{33}\label{eq33}
\end{equation*}\pageoriginale
for $\xi \in \mathfrak{Z}$. By iteration, we get from \eqref{eq33}
$$
j^k \xi^{(1)} = \xi^{(k+l)} j^k.
$$
This relation may be seen to be valid for all rational integers $k\geq
0$ and $l$, defining $j^0=1$. In particular,
$$
j^s \xi = \xi^{(s)} j^s = \xi j^s.
$$
We now stipulate that $1, j, j^2, \cdot, j^{s-1}$ are linearly
independent over $\mathfrak{Z}$ and 
\begin{equation*}
j^s = b \tag{34}\label{eq34}
\end{equation*}
for some $b(\neq 0) \in \mathfrak{R}$. Under conditions \eqref{eq33} and \eqref{eq34},
it can be verified that $\mathfrak{M}$ is an algebra of rank $s^2$
over its centre $\mathfrak{R}$. A central algebra $\mathfrak{M}$ over
$\mathfrak{R}$, constructed as above with an auxiliary cyclic
extension $\mathfrak{Z}$ of $\mathfrak{R}$ is called a {\em cyclic
  algebra}. The field $\mathfrak{Z}$ is called a {\em splitting field}
for $\mathfrak{M}$. The quaternion algebra is a special case of a
cyclic algebra, when $s=2$.

It is known that every cyclic algebra is a simple algebra. Conversely,
by a theorem of Brauer-Hasse-Noether \cite{7}, every simple algebra
over $\mathbb{Q}$ can be realised as a cyclic algebra over its centre.

For $\delta = \xi_0+\xi_1 j + \ldots + \xi_{s-1} j^{s-1}
\in\mathfrak{M}$, we have the representation $\delta \rightarrow D$ of
$\mathfrak{M}$ in $\mathfrak{Z}$ given by 
$$
\left(\begin{smallmatrix}1 \\ j \\\cdot
  \\ j^{s-1}\end{smallmatrix}\right) \delta = D
\left(\begin{smallmatrix} 1 \\ j \\ \cdot
  \\ j^{s-1}\end{smallmatrix}\right) 
$$
where
\begin{equation*}
D = \left(\begin{smallmatrix}\xi_0 & \xi_1 \ldots & \xi_{s-1}\\
b\xi^{(1)}_{s-1} & \xi^{(1)}_{0} \ldots & \xi^{(1)}_{s-2}\\
\cdotp & \cdotp &\cdotp  \\
\cdotp &\cdotp &\cdotp  \\
b\xi^{(s-1)}_1 & b\xi^{(s-1)}_2 \ldots & \xi^{(s-1)}_0 
\end{smallmatrix}\right) \tag{35}\label{eq35}
\end{equation*}\pageoriginale
Let us observe that all the terms below the diagonal of $D$ involve
$b$. The regular representation of $\mathfrak{M}$ over $\mathfrak{R}$
is given by
$$
\delta \rightarrow (\Omega \times E_s) [D^{(1)}, \ldots D^{(s)}]
(\Omega \times E_s)^{-1}
$$
where $\Omega = (\gamma^{(1)}_k)$, with $\gamma_1 \ldots \gamma_s$
being a basis of $\mathfrak{Z}$ over $\mathfrak{R}$ and for
$D=(d_{pq})$, $D^{(i)}= (d^{(i)}_{pq})$.

The algebra $\mathfrak{M}$ is a division algebra if and only if
$|D|\neq 0$ for every $\delta \neq 0$ in $\mathfrak{M}$. For, we know
$\mathfrak{M}$ is a simple algebra containing 1 and the condition
$|D|\neq 0$ for $\delta \neq 0$ would imply that $\mathfrak{M}$ is
free from divisors of zero and therefore, the ideal generated by any
$\delta \neq 0$ would be the whole of $\mathfrak{M}$. Conversely, if
$\mathfrak{M}$ is a division algebra and $\delta \neq 0$ in
$\mathfrak{M}$, it is trivial to see that $|D|\neq 0$.

Writing every $\xi_k \in \mathfrak{Z}$ as $\sum\limits^s_{l=1} x_{kl}
\gamma_l$, we see that corresponding to $\delta =
\sum\limits^{s-1}_{k=0} \xi_k j^k$, $|D|$ is a homogeneous form
$f(\ldots, x_{kl}, \ldots)$ of degree $s$ in the variables $x_{kl}$,
with coefficients in $\mathfrak{R}$. The necessary and sufficient
condition for $\mathfrak{M}$ to be a division algebra may thus be
reformulated as follows, viz. the form $f(\ldots x_{kl},\ldots)$
should \pageoriginale not represent $0$ nontrivially over
$\mathfrak{R}$. 

In the case of the quaternion algebra $\mathscr{V}$ over
$\mathfrak{R}(s=2)$, for $\delta = \xi + \eta j$, $\xi = x + y i, \eta
= z + t i$, we have $|D| = \xi \ob{\xi} - b \eta \ob{\eta} =
f(x,y,z,t) = x^2 - ay^2 - bz^2 + abt^2$. We know that $\mathscr{V}$ is
a division algebra if and only if $f(x,y,z,t)$ does not represent zero
nontrivially in $\mathfrak{R}$. Clearly, for $\eta=0$,
$|D|=x^2-ay^2\neq 0$, since $\ub{a}$ is not a square in
$\mathfrak{R}$. We may then suppose that, for given $\delta = \xi +
\eta j $ in $\mathscr{V}$ $\eta \neq 0$. The condition that $|D|=0$ is
equivalent to the fact that $\ub{b}$ is the norm of an element $\xi
\eta^{-1}$ in $\mathfrak{R}(i)$ over $\mathfrak{R}$. Thus the
quaternion algebra $\mathscr{V}$ is a division algebra if and only if
$\ub{b}$ is not the norm of any element of $\mathfrak{R}(i)$. In the
case $s>2$, we shall find conditions analogous to this, which shall be
{\em sufficient} for the cyclic algebra $\mathfrak{M}$ to be a
division algebra.

\begin{thm}[Wedderburn, \cite{24}]\label{chap1:thm4}
Let $\mathfrak{M}$ be a cyclic algebra constructed as above with
$1,j,\ldots, j^{s-1}$ as basis over the splitting field $\mathfrak{Z}$
and let $j^s=b$ belong to the centre $\mathfrak{R}$. If for every
integer $r$ satisfying $0< r \leq s-1$, $b^r$ is not the norm of an
element $\xi$ of $\mathfrak{Z}$ over $\mathfrak{R}$ then
$\mathfrak{M}$ is a division algebra. 
\end{thm}

\begin{proof}
Let $\delta = \xi_0 + \xi_1 j +\ldots + \xi_k j^k$ where $0\leq k \leq
s-1$, be an arbitrary element of $\mathfrak{M}$. If $k=0$ and $\delta
\neq 0$, then, trivially, $\delta$ has an inverse. Let then $k>0$ and
let us suppose $\xi_k \neq 0$. We may, in fact, assume that $\xi_k =
1$, without loss of generality.

We shall first find $\eta_0, \eta_1,\ldots, \eta_{s-k} \in
\mathfrak{Z}$ such that $(\eta_0 + \eta_1 j + \ldots + \eta_{s-k}
j^{s-k}) (\xi_0 + \xi_1 j + \ldots + j^k)$ is of the form $\rho_0 +
\rho_1 j + \ldots + \rho_{k-1} j^{k-1}$ and is different from $0$. By
iteration of this process, we can eventually obtain an inverse for
$\delta$, under the hypotheses of the theorem. Now, $(\eta_0+\eta_1 j
+ \ldots + \eta_{s-k} j^k)\delta = \rho_0+\rho_1 j+\ldots + \rho_{s-1}
j^{s-1}$ where
\begin{align*}
\rho_0 & = \eta_0 \xi_0 + \eta_{s-k} b\\
\rho_1 & = \eta_0 \xi_1 + \eta_1 \xi^{(1)}_0\\
\rho_{k-1} & = \eta_0 \xi_{k-1} + \eta_1 \xi^{(1)}_{k-2} + \eta_2
\xi^{(2)}_{k-3} + \cdots + \eta_{k-1} \xi^{(k-1)}_0\\
\rho_k & = \eta_0 + \eta_1 \xi^{(1)}_{k-1} + \cdots + \eta_k
\xi^{(k)}_0\\
\rho_{k+1} & = \eta_1 + \eta_2 \xi^{(1)}_{k-1} + \cdots\\
&\ldots \ldots \ldots \ldots \\
\rho_{s-1} & = \eta_{s-k-1} + \eta_{s-k} \xi^{(s-k)}_{k-1}
\end{align*}
Taking $\eta_{s-k}=1$, we can find $\eta_{s-k-1}$,
$\eta_{s-k-2},\ldots, \eta_0$ inductively such that $\rho_{s-1}=0$,
$\rho_{s-2} = 0,\ldots, \rho_k =0$.

If, now, it turns out that $\rho_0 = \rho_1 = \cdots = \rho_{k-1} =0$,
we shall see that we arrive at a contradiction to the hypotheses.

Replacing $j$ by $j_0$ where, analogous to \eqref{eq33} and \eqref{eq34}, $j_0$
satisfies
\begin{align*}
j_0 \xi & = \xi^{(1)} j_0 \text{ for } \xi \in \mathfrak{Z}\\
1, j_0 & \cdots j^{s-1}_0 \text{ are linearly independent over
  $\mathfrak{Z}$, and }\\
j^s_0 & = x \text{ (an indeterminate)},
\end{align*}
we \pageoriginale can verify easily that
\begin{equation*}
(\eta_0 + \eta_1 j_0 + \cdots + j^{s-k}_0) (\xi_0 + \xi_1 j_0 + \cdots
  + j^k_0) = \eta_0 \xi_0 + j^s_0 = x-b. \tag{36}\label{eq36}
\end{equation*}
Now, for any $\mu = \mu_0+\mu_1 j_0+\cdots + \mu_{s-1} j^{s-1}_0$ with
$\mu_0, \mu_1,\ldots, \mu_{s-1}$ in $\mathfrak{Z}$, we have
\begin{equation*}
\left(\begin{smallmatrix}1
  \\ j_0\\ \vdots\\ j^{s-1}_0\end{smallmatrix}\right) \mu = M 
\left(\begin{smallmatrix}1
  \\ j_0\\ \vdots\\ j^{s-1}_0\end{smallmatrix}\right) \tag{37}\label{eq37}
 \end{equation*}
where $M =
\left(\begin{smallmatrix} A & B\\C & D \end{smallmatrix}\right), C$
is a $k$-rowed square matrix with $x$ on the diagonal and the factor
$\ub{x}$ only up to the first power below and zeros above, $B$ is a
$(s-k)$-rowed square matrix with 1 on the diagonal and zeros above
and further the matrices $A,B,D$ are free from $x$. Let $M_1$, $M_2$
correspond respectively to $\eta_0 + \eta_1 j_0 + \cdots + j^{s-k}_0$
and $\xi_0 + \xi_1 j_0 + \cdots + j^k_0$ under~\eqref{eq37}. Further noting
that 
$$
\left(\begin{smallmatrix} 1 \\ j_0
  \\\vdots\\ j^{s-1}_0\end{smallmatrix}\right) (x-b) = (x-b) E_s
\left(\begin{smallmatrix} 1
  \\ j_0\\ \vdots\\j^{s-1}_0\end{smallmatrix}\right) 
$$
where $E_s$ is the $s$-rowed identity matrix, we have, from \eqref{eq36}, \eqref{eq37}
by taking determinants, that
$$
|M_1||M_2| = |(x-b)E| = (x-b)^s.
$$
But, by using Laplace's expansion of the determinant of $M_2$ along
$k$-rowed minors of the first $k$ columns of $M_2$, we observe that
$$
|M_2| = (-1)^{s-k} x^k + \cdots + N (\xi_0)
$$
where $N(\xi_0)$ is the norm of $\xi_0$ over $\mathfrak{R}$. Since
$|M_2|$ divides the polynomial \pageoriginale $(x-b)^s$, it follows
that it is necessarily equal to $(-1)^{s-k} (x-b)^k$. Comparing the
constant terms, we have
\begin{align*}
N (\xi_0) & = (-b)^k (-1)^{s-k}\\
\text{ i.e. } \hspace{4cm} b^k & = N (-\xi_0)\hspace{4cm}
\end{align*}
which is a contradiction to the hypothesis that no $b^r(0<r<s)$ is the
norm of an element of $\mathfrak{Z}$. Our theorem is therefore proved.
\end{proof}

\begin{rem}\label{rem1}
In the hypotheses of Theorem~\ref{chap1:thm4}, it is sufficient to require that {\em
  for divisors} $t$ of $s$ satisfying $1\leq t < s$, $b^r$ {\em shall
  not be the norm of any element of $\mathfrak{Z}$ over
  $\mathfrak{R}$}. For, let $0< r < s$ and $t=g\cdot c\cdot d$ of $r$
and $s$. Further, let $b^r = N (\xi)$ for $\xi \in \mathfrak{Z}$. Now
there exist rational integers $p$, $q$ such that $pr + qs = t$. Then
$b^t = N (\xi^p b^q)$. In particular, if $s$ is a prime, then all
these $(s-1)$ conditions reduce to the single condition that $\ub{b}$
should not be the norm of an element $\xi$ in $\mathfrak{Z}$. In this
case, $\mathfrak{M}$ is a division algebra. Conversely, as we shall
see presently, if $\mathfrak{M}$ is a cyclic division algebra over
$\mathfrak{Z}$ with $s$, a prime, then necessarily $b$ cannot be the
norm of any element of $\mathfrak{Z}$ over $\mathfrak{R}$. It has been
shown by Hasse \cite{9} that the conditions on $b$ in Theorem~\ref{chap1:thm4} are
also {\em necessary} for $\mathfrak{M}$ to be a division algebra. The
proof by Hasse involves the use of `factor systems' in the theory of
algebras. We give, in simple cases, a proof of the necessity of
Wedderburn's conditions.
\end{rem}

\begin{proposition}
With the notation of Theorem~\ref{chap1:thm4}, let for a divisor $r$ of $s$, $b^r= N
(\xi)$ for $\xi \in \mathfrak{Z}$ and let $r$ and $\dfrac{s}{r}$
be coprime. Then $\mathfrak{M}$ cannot be a division algebra.
\end{proposition}

\begin{proof}
It is \pageoriginale sufficient to show that $\mathfrak{M}$ contains
divisors of zero, under the given conditions.

Let $s_1 = \dfrac{s}{r}$ and let $\mathfrak{Z}_{s_1}$ be the fixed
field of the group of the automorphisms $1, \sigma^{s_1},\ldots,
\sigma^{(r-1)s_1}$ of $\mathfrak{Z}$ over $\mathfrak{R}$. Then the set
$\mathfrak{M}_{s1}$ of elements of the form $\delta = \eta_0 + \eta_1
j^r + \eta_2 j^{2r} + \cdots + \eta_{s_1} j^{(s_1-1)r}_{-1}$ with
$\eta_i \in \mathfrak{Z}_{s_1}$ is again an algebra. Now
$\mathfrak{M}_{s_1} \subset \mathfrak{M}$ and we shall show that
$\mathfrak{M}_{s_1}$ contains divisors of zero.

The element $\alpha=\xi \xi^{s_1} \cdots \xi^{(rs_1-r)}$ lies in
$\xi_{s_1}$ and further $b^r$ is the norm of $\alpha$ in $\xi_{s_1}$
over $\mathfrak{R}$. Moreover if $j_0 = j^{r^2} \alpha^{-1}$, then
$1,j_0,\ldots , j^{s_1-1}_0$ is a basis of $\mathfrak{M}_{s_1}$ over
$\mathfrak{Z}_{s_1}$ and $j_0$ satisfies a minimum polynomial equation
of degree $s_1$. Now
$$
j^{s_1}_0 = b^r(N_{\mathfrak{Z}_{s_{1/\mathfrak{R}}}}(\alpha))^{-1} = 1
$$
This gives us a factorization of $0$ in $\mathfrak{M}_{s_1}$, viz.
$$
0=(j_0-1)(j^{s_1-1}_0 + j^{s_1-2}_0+\cdots +1)
$$
and neither of the factors can be zero, since the minimum polynomial
of $j_0$ is of degree $s_1$.
\end{proof}

\begin{coro*}
If $s$ is a product of distinct primes, then the conditions of
Wedderburn in Theorem~\ref{chap1:thm4} are also necessary for $\mathfrak{M}$ to be a
division algebra.
\end{coro*}

\section[Division algebras over $\mathbb{Q}$...]{Division algebras over $\mathbb{Q}$ with involutions of the
  second kind}%%% sec 5 
Let $\mathscr{V}$ be a division algebra over $\mathbb{Q}$. Then by a
theorem due to Brauer-Hasse-Noether, it is known that $\mathscr{V}$ is
a cyclic algebra over \pageoriginale its centre $\mathfrak{R}$, with a
certain cyclic extension over $\mathfrak{R}$ as splitting field.

Conditions necessary and sufficient for $\mathscr{V}$ to have an
involution of the second kind have been given by Albert
\cite{1}. First, let $\delta \rightarrow \widetilde{\delta}$ be such
an involution in $\mathscr{V}$. If $\mathscr{L}$ is the fixed field of
the involution contained in the centre $\mathfrak{R}$ of
$\mathscr{V}$, then $\mathfrak{R} = \mathscr{L}(c)$ is a quadratic
extension over $\mathscr{L}$, with a suitable $c$ in $\mathfrak{R}$
satisfying $\widetilde{c}=-c$. In this case, Albert has shown (Chap
$X$, \cite{1}) that one can find a cyclic extension $\xi_0 =
\mathscr{L}(\varsigma)$ of degree $\ub{s}$ over $\mathscr{L}$ such
that 
\begin{itemize}
\item[{\rm i)}] the involution is identity on $\mathfrak{Z}_0$, and

\item[{\rm ii)}] the algebra $\mathscr{V}$ is a cyclic algebra having
  for its 
\end{itemize}
splitting field the field $\mathfrak{Z}= \mathfrak{Z}_0
  (c)=\mathscr{L}(c,\varsigma)$ which is abelian of degree $2s$ over
  $\mathscr{L}$. 

Following our earlier notation, let $1$, $j, j^2,\ldots, j^{s-1}$
generate $\mathscr{V}$ over $\mathfrak{Z}$ and let $j^2 = b \in
\mathfrak{R}$. Now we claim that $\widetilde{j} j$ commutes with all
elements of $\mathfrak{Z}$. For, first of all, any $\xi \in
\mathfrak{Z}$ is of the form $\xi = \eta_0 + \eta_1 c$ with $\eta_0,
\eta_1 \in \mathfrak{K}_0$ and $\widetilde{\xi} = \eta_0 - \eta_1 c $ . Hence
the mapping $\xi \rightarrow \widetilde{\xi}$ is an automorphism of
$\mathfrak{Z}$. Denote by $\sigma$ the generating automorphism of
$\mathfrak{Z}$ over $\mathfrak{R}$ and by $\eta^{(1)}$, the effect of
$\sigma^1$ on $\eta \in \mathfrak{Z}$. Using the fact that
$\mathfrak{Z}$ is abelian over $\mathscr{L}$, we have, for $\xi =
\eta_0 + \eta_1 c$ (with $\eta_0, \eta_1 \in \mathfrak{K}_0$),
\begin{equation*}
\widetilde{\xi^{(l)}} = (\widetilde{\eta^{(l)}_0 + \eta^{(l)}_1 c}) =
\eta^{(l)}_0 - \eta^{(l)}_1 \cdot c = (\widetilde{\xi})^{(1)} \tag{38}\label{eq38}
\end{equation*}
Now, for $\eta \in \mathfrak{Z}$, we have
\begin{equation*}
\tilde{\eta} \tilde{j} = \widetilde{j\eta} = \widetilde{\eta^{(1)}j} =
\tilde{j} \widetilde{\eta^{(1)}} = \tilde{j} (\tilde{\eta})^{(1)}
\tag{39}\label{eq39} 
\end{equation*}\pageoriginale
and therefore, for $\xi \in \mathfrak{Z}$, we obtain
$$
\tilde{j}j \xi = \tilde{j} \xi^{(1)} j = \xi \tilde{j}j
$$
using \eqref{eq39} with $\tilde{\eta}= \xi$. Now $\mathfrak{Z}$ is a maximal
commutative system in $\mathscr{V}$ and it follows immediately that
\begin{equation*}
\tilde{j} j = a \in \mathfrak{Z}. \tag{40}\label{eq40}
\end{equation*}
Moreover $\widetilde{\tilde{j}j} = \tilde{j}j$ and therefore $a\in
\mathfrak{Z}_0$, from \eqref{eq38}. Now $j^s=b$ and $\tilde{j}^s = \tilde{b}$
and $b \tilde{b} = \tilde{j}^s j^s = a a^{(1)} \cdots a^{(s-1)}$
. Thus we arrive at the important condition
\begin{equation*}
N_{\mathfrak{R}/\mathscr{L}}(b) = N_{\mathfrak{Z}_0/\mathscr{L}}
(a). \tag{41}\label{eq41} 
\end{equation*}
(See Theorem~18, p.160, \cite{1})

Conversely, if $\mathscr{V}$ is a cyclic algebra generated by
$1,j,j^2,\ldots j^{s-1}$ over its splitting field $\mathfrak{Z}$ and
if $\mathfrak{Z}$ is realisable as a field $\mathscr{L}(c,\varsigma)$
as above and further, if $j^s=b$ in $\mathfrak{R}$ satisfies (41) for
a suitable $a\in \mathscr{L}(\varsigma)$, then we can define an
involution of the second kind in $\mathscr{V}$ as follows. For $\xi =
\eta_0 + \eta_1 c$ in $\mathfrak{Z}$ with $\eta_0, \eta_1 \in
\mathfrak{Z}_0$, we have only to define
\begin{align*}
\widetilde{\xi} & = \eta_0 - \eta_1 c \\
\widetilde{j} & = a j^{-1} \tag{42}\label{eq42}
\end{align*}
Extending \eqref{eq42} to all elements of $\mathscr{V}$ in the obvious way, we
have an involution of the second kind.

We shall now show that Wedderburn's conditions sufficient for a cyclic
algebra $\mathscr{V}$ to be a division algebra are not incompatible
with condition \eqref{eq41} which is necessary and sufficient for a cyclic
(division) algebra\pageoriginale to carry an involution of the second
kind. 

Let us take $\mathscr{L}= \mathbb{Q}$, $c=\sqrt{-1}$, $\mathfrak{R} =
\mathbb{Q} (\sqrt{-1}),p$ an odd prime and $\varepsilon$, a primitive
$p^{\rm th}$ root of unity. The field $\mathfrak{Z}_0 = \mathbb{Q}
(\varsigma)$ with $\varsigma = \varepsilon + \varepsilon^{-1}$ is
cyclic of degree $s=\dfrac{1}{2}(p-1)$ over $\mathscr{L}$. If now $q$
is a prime $q\equiv 1(\mod 4)$, then $q=\kappa \ob{\kappa}$ for
$\kappa$ in $\mathfrak{R}$, since $\left(\dfrac{-1}{q}\right) =1$. Let
us further suppose that $q$ is a primitive root modulo $p$ (There
exist infinitely many such $q$). If now $\mathfrak{Z}=
\mathfrak{R}(\varsigma)$, it is clear that the integral ideal
$(\kappa)$ generated by $\kappa$ in $\mathfrak{Z}$ is prime; similarly
$(\ob{\kappa})$ is prime in $\mathfrak{Z}$ and $(\kappa) \neq
(\ob{\kappa})$. Now let us define $b=\kappa\ob{\kappa}^{s-1}$. Then
$N_{\mathfrak{R}/\mathscr{L}}(b) =
N_{\mathfrak{Z}_0/\mathscr{L}}(q)$. Moreover, we claim that for
$0<r<s$, $b^r \neq \xi \xi^{(1)}\cdots \xi^{(s-1)}$ for $\xi \in
\mathfrak{Z}_0$. For, otherwise, let $b^r=\xi\xi^{(1)}\cdots \xi^{(s-1)}$ for
$\xi \in \xi_0$ and let $\xi = \kappa^t\cdot \lambda$ where in the
prime factor decomposition of $(\lambda)$, $(\kappa)$ does not
occur. Now $\xi^{(1)}=\kappa^t \lambda^{(l)}$ and therefore
$$
\kappa^r \cdot \ob{\kappa}^{(s-1)r} = \kappa^{st} \lambda \cdot
\lambda^{(1)} \cdots \lambda^{(s-1)}
$$
As a consequence $r=st$, which is a contradiction, since $0<r<s$ and
$t$ is a rational integer.

Thus the cyclic algebra generated by $1,j,\ldots, j^{s-1}$ over
$\mathfrak{Z}$ as splitting field (where $j^s=b$) is, in fact, a
division algebra with an involution of the second kind.

\begin{example*}
$p=7$, $s=\dfrac{p-1}{2}=3$, $q=17=(4+i)(4-i)$, $\kappa = 4+i$,
  $b=(4+i)(4-i)^2$, $a=17$, $j^3=b$, $\mathfrak{Z} =
  \mathbb{Q}\left(\cos \dfrac{2\pi}{7}, i\right)$.

Let $\delta \rightarrow D$ be the representation of the division
algebra over \pageoriginale its splitting field, where $D$ is given
by (35). Under this representation, we have
$$
j \rightarrow \mathscr{F} =
\left(\begin{smallmatrix} 0 & E_{s-1} \\ b &
  0\end{smallmatrix}\right), E_{s-1} \text{ being the $(s-1)$-rowed
    identity matrix}
$$
and for $\xi \in \mathfrak{Z}$,
$$
\xi \rightarrow [\xi, \xi^{(1)}, \ldots \xi^{(s-1)}]
$$

Let now $\delta \rightarrow \widetilde{\delta}$ be an involution of
the second kind in $\mathscr{V}$ and let $\widetilde{D}$ correspond to
$\widetilde{\delta}$ under the representation above. The restriction
of the involution in $\mathscr{V}$ to $\mathfrak{Z}$ is an
automorphism $\xi \rightarrow \ob{\xi}$ of $\mathfrak{Z}$. Let us
denote for any matrix $M = (m_{kl})$ with $m_{kl} \in \mathfrak{Z}$,
the matrix $(\ob{m}_{kl})$ by $\ob{M}$. Then the connection between
$D$ and $\widetilde{D}$ is given by 
\end{example*}

\begin{proposition}
There exists an $s$-rowed nonsingular symmetric matrix $F$ with
elements in $\mathfrak{Z}_0$ such that, for any $\delta\in
\mathscr{V}$, we have
\begin{equation*}
\widetilde{D} = F^{-1} \ob{D}' F \tag{43}\label{eq43}
\end{equation*}
\end{proposition}

\begin{proof}
Since $\mathscr{V}$ has an involution of the second kind, we have, by
\eqref{eq41} an element $a \in \mathfrak{Z}$ such that
\begin{equation*}
b \ob{b} = aa^{(1)}\cdot a^{(s-1)} \tag{44}\label{eq44}
\end{equation*}
Now to $\widetilde{j} = a j^{-1}$ corresponds $\widetilde{\mathscr{F}}
= [a,a^{(1)},\ldots  a^{(s-1)}]  
\left(\begin{smallmatrix} 0 & b^{-1}\\ E_{s-1} &
  0\end{smallmatrix}\right)$  
We shall find elements $x_0, x_1,\ldots, x_{s-1}$ in $\mathfrak{Z}_0$
different from zero, such that 
$$
[x_0, x_1, \ldots, x_{s-1}]
\left(\begin{smallmatrix} 0 & \ob{b} \\ E_{s-1} &
  0 \end{smallmatrix}\right) [x_0,x_1,\ldots, x_{s-1}]^{-1} =
\widetilde{\mathscr{F}} =
\left(\begin{smallmatrix}
 0 & \frac{a}{b} \\ 
[a^{(1)},\ldots,  a^{(s-1)}] & 0 
\end{smallmatrix}\right) 
$$
This \pageoriginale matrix equation is equivalent to the conditions
\begin{equation*}
\frac{x_1}{x_0} = a^{(1)},\ldots, \frac{x_{s-1}}{x_{s-2}} = a^{(s-1)},
\quad \frac{x_0 \ob{b}}{x_{s-1}} = \frac{a}{b} \tag{45}\label{eq45}
\end{equation*}
If we set for $1\leq i \leq s-1$, $x_i = a a^{(1)}\cdots a^{(i)}$ and
$x_0=a$, then they satisfy \eqref{eq45} and the last condition in \eqref{eq45} is
nothing but \eqref{eq44}. Thus if we set $ F=[a^{-1}, (aa^{(1)})^{-1}, \ldots,
  (a a^{(1)}\cdots a^{(s-1)})^{-1}]$ then $\widetilde{\mathfrak{F}} =
F^{-1} \ob{\mathfrak{F}}'F$ and by iteration, we have
\begin{equation*}
\widetilde{\mathfrak{F}}^r = F^{-1} \ob{\mathfrak{F}}'^r F \tag{46}\label{eq46}
\end{equation*}
For $\xi \in \mathfrak{Z}$, $\xi \rightarrow [\xi,
  \xi^{(1)},\ldots, \xi^{(s-1)}]$ and it is trivial to verify that
\begin{equation*}
[\widetilde{\xi}, \widetilde{\xi^{(1)}}, \ldots,
  \widetilde{\xi^{(s-1)}}] = [\ob{\xi}, \ob{\xi^{(1)}},\ldots,
  \ob{\xi^{(s-1)}}] = F^{-1} [\ob{\xi}, \ob{\xi^{(1)}},\ldots,
  \ob{\xi^{(s-1)}}]'F \tag{47}\label{eq47}
\end{equation*}
From \eqref{eq46} and \eqref{eq47}, follows \eqref{eq43} for any $\delta \in 
\mathscr{V}$. Let us note that $F$ itself does not correspond in general to an element
of $\mathscr{V}$ under the representation $\delta \rightarrow D$.

The relationship \eqref{eq43} between $\widetilde{D}$ and $D$ will be useful
in examining the positivity of the involution $\delta \rightarrow
\widetilde{\delta}$. Our next object will be to find all involutions
of the second kind in $\mathscr{V}$ and to investigate the existence
of a positive involution. Results in this direction are again due to
Albert \cite{1}.

If $\delta \rightarrow \delta^{\ast}$ is any other involution in
$\mathscr{V}$ having the same effect on $\mathfrak{R}$ as the
involution $\delta \rightarrow \widetilde{\delta}$, then we know that,
for a \; $\lambda \neq 0$ in $\mathscr{V}$ with $\widetilde{\lambda}=\pm
\lambda$, $\delta^{\ast} = \lambda^{-1}
\widetilde{\delta}\lambda$. Since the involutions are of
\pageoriginale the second kind, we can suppose without loss of
generality that $\widetilde{\lambda}= + \lambda$, by taking, if
necessary $c\lambda$ instead of $\lambda$. Now if $\lambda \rightarrow
L$ under the representation $\delta \rightarrow D$, then this means
that $D^{\ast}=L^{-1} \widetilde{D}L= L^{-1} F^{-1} \ob{D}' F
L$. Setting $G=FL$, we have from $\widetilde{L}=L$ that
$G=\ob{G}'$. Thus we have
\begin{equation*}
D^{\ast} = G^{-1} \ob{D}' G, \quad G = FL = \ob{G}'. \tag{48}\label{eq48}
\end{equation*}
\end{proof}

\section{Positive involutions of the second kind in division algebras}

Let $\delta \rightarrow \widetilde{\delta}$ be an involution of the
second kind in a division algebra $\mathscr{V}$ over $\mathbb{Q}$,
with centre $\mathfrak{R} \supset\mathbb{Q}$. Then we know
from \S 5 that $\mathscr{V}$ has a splitting field $\mathfrak{Z}$
which can be realised as an abelian extension
$\mathscr{L}(c,\varsigma)$ where $\mathscr{L}$ is the fixed field of
the involution in $\mathfrak{R}$, $\mathfrak{Z}_0 =
\mathscr{L}(\varsigma)$ is cyclic of degree $s$ over $\mathscr{L}$ and
$c=\sqrt{-d}=-\widetilde{c} \in \mathfrak{R}$ for an element $d\in
\mathscr{L}$. 

For the involution $\delta \rightarrow \widetilde{\delta}$ to be
positive, we should have necessarily that $\mathscr{L}$ is totally
real and $-d>0$; thus $\mathfrak{R}$ should be a totally complex
quadratic extension of the totally real field $\mathscr{L}$. For
$\xi\in \mathfrak{Z}$, $\widetilde{\xi}$ is just the complex conjugate
of $\xi$. Further $\mathfrak{\xi}_0$ is totally real and the
involution is identity on $\mathfrak{Z}_0$.

From the representation $\delta \rightarrow D$ of $\mathscr{V}$ given
by \eqref{eq35} we first get a representation $\delta \rightarrow D_0$ of
$\mathscr{V}$ over $\mathfrak{R}$ by taking $D_0 = (\Omega \times E_s)
[D, D^{(1)},\ldots,\break D^{(s-1)}](\Omega \times E_s)^{-1}$ where if $D=
(d_{pq})$, $D^{(k)} = \left(d^{(k)}_{pq}\right)(1\leq k \leq s-1)$, $E_s$ is the
$s$-rowed identity and $\Omega=\left(\gamma^{(1)}_k\right)(1\leq k, 1\leq s)$,
$\gamma_1,\ldots, \gamma_s$ being a basis of $\mathfrak{Z}_0$ over
\pageoriginale $\mathscr{L}$ and serving also as a basis of
$\mathfrak{Z}$ over $\mathfrak{R}$. Now let $\omega_1, \ldots
\omega_h$ be a basis of $\mathfrak{R}$ over $\mathbb{Q}$ and let
$\Omega^{\ast} = \left(\omega^{(q)}_p\right) (1\leq p, q \leq h)$. If $D_0 =
(\delta_{kl})$, denote by $D_{0i}$ the corresponding matrix
$\left(\delta^{(i)}_{kl}\right)$ for $1\leq i \leq h$. Then setting $\ub{D} =
(\Omega^{\ast} \times E_{s^2})[D_{01},\ldots, D_{oh}](\Omega^{\ast}
\times E_{s^2})^{-1}$ where $E_{s^2}$ is the $s^2$-rowed identity
matrix, we see that the mapping $\delta \rightarrow \ub{D}$ is a
representation of $\mathscr{V}$ over $\mathbb{Q}$ by $hs^2$-rowed
matrices. Throughout this section, we shall denote by $(\mathscr{V})$,
the image of $\mathscr{V}$ under the representation $\delta\rightarrow
D$ over $\mathfrak{Z}$. 

Let us define, analogously, $F_0$ by $F^{-1}_0 = (\Omega \times E_s)
\times [F,F^{(1)},\ldots,\break F^{(s-1)}] (\Omega \times E_s)'$ and denote
by $F_{0i}(1\leq i \leq h)$ the matrix $\left(f^{(i)}_{kl}\right)$
corresponding to $F_0=(f_{kl})$. Introducing $\ub{F}$ by the
definition $\ub{F}^{-1} = (\Omega^{\ast} \times E_{s^2})
[F^{-1}_{01},\ldots, F^{-1}_{oh}](\ob{\Omega^{\ast} \times
  E_{s^2}})'$, we see that $\ub{F}$ is a $hs^2$-rowed rational
symmetric matrix and the relation \eqref{eq43} in terms of $\ub{D}$ and
$\ub{F}$ goes over into
\begin{equation*}
\ub{\widetilde{D}} = \ub{F}^{-1} \ub{D}' \ub{F}, \;\; \ub{F} = \ub{F}'
\tag{49} \label{eq49}
\end{equation*} 
Defining $\ub{G} =\ub{F}\;\;\ub{L}$, we see that (48) goes over into
\begin{equation*}
\ub{D}^{\ast} = \ub{G}^{-1} \ub{D}' \ub{G} \text{ with } \ub{G} =
\ub{F} \;\; \ub{L} = \ub{G}' \tag{50}\label{eq50}
\end{equation*}

For the involution $\delta \rightarrow \delta^{\ast}$ to be positive
we must require that for $\delta \neq 0$, $\sigma (\ub{D}\;\;
\ub{D}^{\ast}) = \sigma (\ub{D}\;\ub{G}^{-1} \ub{D}' \ub{G})>0$. Now 
$\sigma (\ub{D} \; \ub{D}^{\ast}) = \sum\limits^h_{i=1} \sigma (D_{0i}
D^{\ast}_{0i}) = tr_{\mathfrak{R}/\mathbb{Q}} (\sigma (D_0
D^{\ast}_0))$ (By defining $(D_{0i})^{\ast} = D^{\ast}_{oi}$). Further
$\sigma (D_0 D^{\ast}_0) =\break s\sigma (D D^{\ast})$ by using the fact
that $D^{(1)} = \mathscr{F} D \mathscr{F}^{-1}$ $\left(where \mathscr{F} =
\left(\begin{smallmatrix} 0 & E_{s-1}\\ b &
  0\end{smallmatrix}\right)\right)$ and hence, by iteration,
\begin{equation*}
D^{(k)} = \mathscr{F}^k D \mathscr{F}^{-k} \tag{51}\label{eq51}
\end{equation*}\pageoriginale
Now $\ob{\sigma(D D^{\ast})} = \sigma (\ob{D}\; \ob{G}^{-1} D' \ob{G})
= \sigma(G \ob{D}'G^{-1}D) = \sigma (D D^{\ast})$ and therefore for
$D\in\mathscr{V}$, $\sigma(D \; D^{\ast})$ is real. The elements
$x_{kl}$ of $D$ are linearly independent over $\mathfrak{R}$ and
looking upon them as independent complex variables, we see that
$\sigma(D D^{\ast})$ is a hermitian form $f({\phantom{w}},\ldots,
x_{kl}\ldotp, \ob{x}_{kl},\ldotp)$ in the $s^2$ complex variables
$x_{kl}$. On the other hand, by using the arguments of 
Proposition~\ref{chap1:prop7} the necessary and sufficient condition for
$tr_{\mathfrak{R}/\mathbb{Q}}(\sigma(D_0 D^{\ast}_0))$ to be positive
is that $\sigma (D_0 D^{\ast}_0) = s\sigma(D G^{-1} \ob{G}' G)$ should
be a totally positive-definite hermitian form. Analogously to Lemma 2,
the necessary and sufficient condition for this may be seen to be that
the hermitian matrix $G$ must be totally positive-Definite over
$\mathfrak{Z}$. We have thus proved

\begin{proposition}
In terms of the representation $\delta \rightarrow D$ of $\mathscr{V}$
over $\mathfrak{Z}$, any positive involution of the second kind in
$(\mathscr{V})$ is of the form $D\rightarrow G^{-1} \ob{D}'G$ where $G
= FL$ is totally positive-definite hermitian and $L=F^{-1}\ob{L}'F$
corresponds to a $\lambda \neq 0$ in $\mathscr{V}$.
\end{proposition}

In particular, for the involution $D \rightarrow \widetilde{D} =
F^{-1} \ob{D}'F$ to be positive, the necessary and sufficient
condition is that $F=[a,a a^{(1)},\ldots a a^{(1)}\cdots a^{(s-1)}]$
is totally positive-definite i.e. $a>0$.

Suppose a is not totally positive i.e. the involution $\delta
\rightarrow \widetilde{\delta}$ is not positive. Then we claim that
the involution $\delta \rightarrow \delta^{\ast}$ in $\mathscr{V}$
defined by $j^{\ast}=\lambda^{-1}\widetilde{j}\lambda$ and $\xi^{\ast}
= \widetilde{\xi}$ for $\xi \in \mathfrak{Z}$ is positive for suitably
chosen $\lambda$ in $\mathfrak{Z}_0$. In fact, if we set $\theta =
\dfrac{\lambda^{(s-1)}}{\lambda}$, then for $\delta \in \mathscr{V}$,
we \pageoriginale have $D^{\ast} = G^{-1} \ob{D}'G$ where
$$
G^{-1} = [a \theta, a a^{(1)} \theta \theta^{(1)},\ldots, a
  a^{(1)}\ldots a^{(s-1)} \theta \theta^{(1)}\ldotp\ldotp
  \theta^{(s-1)}]. 
$$
Now $G=\ob{G}'$, $N_{\mathfrak{Z}_0/\mathscr{L}}(\theta)=1$ and we
have only to choose $\lambda$ in $\mathfrak{Z}_0$ such that $G$ is
totally positive. But we see that
$$
G^{-1} = \left[a \frac{\lambda^{(s-1)}}{\lambda}, aa^{(1)}
  \frac{\lambda^{(s-1)}}{\lambda^{(1)}}, \ldots, a a^{(1)} \cdots
  a^{(s-1)} \frac{\lambda^{(s-1)}}{\lambda^{(s-1)}}\right]. 
$$
Certainly we can find $\lambda \in \mathfrak{Z}_0$ such that the
numbers
\begin{equation*}
\frac{a}{\lambda}, \frac{a a^{(1)}}{\lambda^{(1)}}, \ldots,
\frac{aa^{(1)} \ldotp\ldotp a^{(s-1)}}{\lambda^{(s-1)}} \tag{52}\label{eq52}
\end{equation*}
are all positive, since this merely involves choosing $\lambda \in
\mathfrak{Z}_0$ such that $\lambda, \lambda^{(1)},\ldots,
\lambda^{(s-1)}$ have prescribed signs. Further this entails that
$\lambda^{(s-1)}>0$ , since by \eqref{eq44}, $a a^{(1)}\ldots a^{(s-1)} = b
\ob{b}>0$. Multiplying all the numbers in \eqref{eq52} by $\lambda^{(s-1)}>0$,
we see that the numbers $a\theta, a\theta(a\theta)^{(1)}$,
$a\theta(a\theta)^{(1)} \cdots\break (a\theta)^{(s-1)}$ are positive and
hence $G$ is positive-definite. In a similar way, by properly choosing
the signs of the other conjugates of $\lambda$ over $\mathbb{Q}$, we
can actually ensure that $a\theta >0$ and hence $G$ is totally
positive-definite. Thus the existence of positive involutions of the
second kind in $\mathscr{V}$ is ensured.

We have seen that any positive involution in $(\mathscr{V})$ (with
$\mathscr{L}$ as the fixed field in $\mathfrak{R}$) is given by $D
\rightarrow G^{-1} \ob{D}' G$ where $G=FL$ is totally
positive-definite hermitian and $L = \widetilde{L}=F^{-1} \ob{L}' F
\in (\mathscr{V})$. We shall now find that the real dimension of the
linear closure $\mathscr{C}$ of the corresponding $\ub{G}$ in the
space of $(hs^2)$-rowed real square matrices is $gs^2$, where
$g=\dfrac{h}{2}$. For, $\ub{L}$ is equivalent over the field of
complex numbers to $[L_{01},\ldots, L_{0h}]$ and $L_{01}$ is equivalent
to $[L, L^{(1)},\ldots, L^{(s-1)}]$. From \eqref{eq51}, we know that $L,
L^{(1)},\ldots, L^{(s-1)}$ are all equivalent to one
another. \pageoriginale Looking
at the form of a general $L$ in $(\mathscr{V})$, we see that its
elements are linearly independent over $\mathfrak{R}$ and are of the
form $\eta + \varsigma\sqrt{d}$ where $\eta$, $\varsigma$ are in
$\mathfrak{Z}_0$. Pairing off the $h$ conjugates of $\mathfrak{R}$
over $\mathbb{Q}$ as $\mathfrak{R}^{(1)}, \mathfrak{R}^{(2)} (=
\ob{\mathfrak{R}^{(1)}}),\ldots, \mathfrak{R}^{(h-1)},
\mathfrak{R}^{(h)} (=\ob{\mathfrak{R}^{(h-1)}})$, we observe that
$L_{02} = \ob{L}_{01},\ldots, L_{0h} = \ob{L}_{0(h-1)}$. Expressing
$\eta$, $\varsigma$ in terms of a basis $\gamma_1,\ldots, \gamma_s$ of
$\mathfrak{Z}_0$ over $\mathscr{L}$, we can thus conclude that the
complex dimension of the linear closure of $\ub{L}$ and hence of
$\ub{G} = \ub{F}\;\ub{L}$ is $gs^2$. The condition $G=\ob{G}'$ means
that the real dimension of $\mathscr{C}$ is precisely $gs^2$; the
positivity of $G$ is expressed in terms of a finite number of
inequalities. Using the fact that the rational numbers are dense in
the reals, we can find $L \in (\mathscr{V})$ such that the
corresponding $\ub{G} = \ub{F} \; \ub{L}$ is sufficiently close to an
element of $\mathscr{C}$ and to secure $G=\ob{G}'$, we have only to
take $\dfrac{1}{2}(L+\widetilde{L})$ instead of $L$.

We shall, without risk of confusion, denote till the end of this
section, the rational representations $\ub{D}$, $\ub{F}$, $\ub{L}$,
$\ub{G}$ etc. by $D, F, L, G$ etc. respectively. Let $D\rightarrow
D^{\ast}$ be a positive involution in $(\mathscr{V})$; then
$D^{\ast}=G^{-1} D'G$ with rational $G=G'>0$. Now, any other positive
involution in $(\mathscr{V})$ is of the form $D \rightarrow
L^{-1}D^{\ast}L$ where $L=L^{\ast}$ is in $(\mathscr{V})$ and further
$GL$ is positive symmetric. By using Lemma~\ref{chap1:lem2}, this is equivalent to
saying that the eigenvalues of $L$ are real and positive. Such an
element $L$ in $(\mathscr{V})$ may be called a {\em positive element}
in $(\mathscr{V})$. A nice characterisation of positive elements is
given by the following.

\begin{proposition}[Albert \cite{6}]\label{chap1:prop12}
Given a positive involution $D \rightarrow D^{\ast}$ of
$(\mathscr{V})$, any \pageoriginale other positive involution in
$(\mathscr{V})$ is of the form $D \rightarrow L^{-1} D^{\ast}L$ where
$L=\sum\limits^{p}_{k=1}L_k L^{\ast}_k$  with $L_k$ in $(\mathscr{V})$
not all equal to $0$.
\end{proposition}

\begin{proof}
First, let, for $L \in (\mathscr{V}), D \rightarrow L^{-1} D^{\ast}L$
be a positive involution. Then, from above, we know that all the
eigenvalues of $L$ are real and positive. Let $r$ be a root of the
characteristic equation $|(xE-L)|=0$. Then $r$ is a totally positive
algebraic number and let $\mathscr{F}$ be the field generated by $r$
over $\mathbb{Q}$. By a theorem of Siegel \cite{19},
$r=r^2_1+r^2_2+r^2_3+r^2_4$, where, for $1\leq k \leq 4$, $r_k =
\sum\limits^{N-1}_{l=0} a_{kl} r^l$, $(a_{kl}\in\mathbb{Q})$ and $N$
is the degree of $\mathscr{F}$ over $\mathbb{Q}$. Denoting, for $1\leq
k \leq 4$, the polynomial $\sum\limits^{N-1}_{l=0}a_{kl} t^l$ by
$p_k(t)$ and $p^2_1(t) + \cdots + p^2_4(t)-t$ by $p(t)$, we see that
$p(r)=0$. Since $r$ is an eigenvalue of $L$, $p(r)=0$ is an eigenvalue
of $p(L)$. But $p(L)$, being an element of the division-algebra
$(\mathscr{V})$, must consequently be 0. i.e. $L = L^2_1 + L^2_2 +
L^2_3 + L^2_4$ where $L_k = p_k(L) (1\leq k \leq 4)$ are in
$(\mathscr{V})$. Now $L=L^{\ast}$ implies that $L^{\ast}_k = L_k$ i.e.
$$
L=L_1 L^{\ast}_1 + \cdots + L_4 L^{\ast}_4.
$$
Clearly at least one $L_k$ is different from $0$.

Conversely, let, in fact, $L=\sum\limits^p_{k=1}L_k L^{\ast}_k \neq 0$
with $L_k \in (\mathscr{V})$. Then we claim that the mapping $D
\rightarrow L^{-1}D^{\ast}L$ is a positive involution of
$\mathscr(V)$. That it is an involution is clear. What remains to be
shown is that $\sigma (DL^{-1}D^{\ast}L)>0$ for $D\neq 0$ in
$(\mathscr{V})$. But now
\begin{align*}
\sigma(DL^{-1} D^{\ast}L) & = \sigma(DL^{-1} L L^{\ast^{-1}}
D^{\ast}L) (\text{since } L=L^{\ast})\\
& = \sigma (D_1 L D^{\ast}_1 L) (\text{setting } D_1 = D L^{-1})\\
& = \sum^p_{k,l-1} \sigma (D_1 L_k L^{\ast}_k D^{\ast}_1 L_l
L^{\ast}_l)\\
& = \sum_{k,l} \sigma (L^{\ast}_l D_1 L_k (L^{\ast}_l D_1
L_k)^{\ast}).
\end{align*}\pageoriginale
Since $L\neq 0$, at least one $L_k\neq 0$ and hence at least one
$L^{\ast}_k D_1 L_k \neq 0$ in $(\mathscr{V})$ and by the positivity
of the involution $D \rightarrow D^{\ast}$, we see that the new
involution is also positive.
\end{proof}

\section{Existence of $R$-matrices with given commutator-algebra}
Let $\mathscr{V}$ be a division algebra over $\mathbb{Q}$ with an
involution and let $(\mathfrak{M})$ be a rational representation of
$\mathscr{V}$. Then $(\mathfrak{M})$ is equivalent to a multiple, say
$q$ times, of the regular representation $(\mathscr{V})$ of
$\mathscr{V}$ over $\mathbb{Q}$. If $M \in (\mathfrak{M})$, then we
can suppose $M =[\underset{q \text{ times}}{D,\ldots D}]=
\underset{q}{D}$ (abbreviating $[\underset{q\text{ times}}{G,\ldots G}]$
as $\underset{q}{G}$). The involution $\delta \rightarrow
\widetilde{\delta}$ in $\mathscr{V}$ can be described as $M
\rightarrow \underset{q}{F^{-1}}M' F_q$ where $\underset{q}{F}$ is
rational symmetric and $M \in (\mathfrak{M})$. In terms of
$(\mathfrak{M})$, the involution $\delta \rightarrow \delta^{\ast} -
\lambda^{-1}\widetilde{\delta} \lambda$ (for $\lambda \neq 0$ in
$\mathscr{V}$) is described as $M \rightarrow M^{\ast} =
\underset{q}{G}^{-1}M'\underset{q}{G}$ where $\underset{q}{G} =
\underset{q}{F} \underset{q}{L}$, $\underset{q}{L}=
\widetilde{\underset{q}{L}} \in (\mathfrak{M})$ and $\underset{q}{G} =
\underset{q}{G'}$. If the involution $\delta\rightarrow \delta^{\ast}$
is positive, then $G$ is positive.

In connection with the existence of an $R$-matrix with the property
that $RM = MR$ for every $M \in (\mathfrak{M})$, we shall first look
for a rational nonsingular skew-symmetric matrix $A$ such that for all
$M \in (\mathfrak{M})$, we have 
\begin{equation*}
M^{\ast} = A^{-1}M' A \tag{53}\label{eq53}
\end{equation*}\pageoriginale
and then ask for all $A$ for which \eqref{eq53} is true. But since $M^{\ast} =
\underset{q}{G^{-1}}M'\underset{q}{G}$, \eqref{eq53} gives
$A\underset{q}{G}^{-1} M' = M' A \underset{q}{G^{-1}}$
i.e. $(A\underset{q}{G^{-1}})' \in (\mathscr{F})$ , the
commutator-algebra of $(\mathfrak{M})$. Setting $T_0 =
(\underset{q}{G^{-1}})' A$, we see that $\underset{q}{G'} T_0 = A =
-A' = - T'_0 \underset{q}{G}$ i.e. 
\begin{equation*}
T_0 = - \underset{q}{G^{-1}} T'_0 \underset{q}{G} \tag{54}\label{eq54}
\end{equation*}
Now, for $T \in (\mathscr{F})$, we can show that $\underset{q}{G^{-1}}
T' \underset{q}{G} \in (\mathscr{F})$ and the mapping $T \rightarrow
\underset{q}{G^{-1}} T'\underset{q}{G}$ is, in fact, an involution of
$(\mathscr{F})$. Actually, for $T \in (\mathscr{F})$,
$$
\widetilde{T} = \underset{q}{F^{-1}} T' \underset{q}{F} =
\underset{q}{L} \underset{q}{G^{-1}} T' \underset{q}{G}
\underset{q}{L^{-1}} = \underset{q}{G^{-1}} T' \underset{q}{G}
$$
since elements of $(\mathscr{F})$ commute with $\underset{q}{L}$. Thus
all the involution in $(\mathscr{V})$ induce the same involution $T
\rightarrow \widetilde{T}$ in $(\mathscr{F})$. Now, from \eqref{eq54}, we have
$T_0 = - \widetilde{T}_0$. The problem then is to find non-singular
$T$ in $(\mathscr{F})$ such that $\widetilde{T}=-T$. Given $T$ in
$(\mathscr{F})$, $T_1 = \dfrac{1}{2}(T-\widetilde{T})$ always
satisfies $\widetilde{T}_1 = - T_1$. But if we can ensure that $T_1$
is also non-singular, then we will be through. Let now $T \in
\mathscr{F}$ be of the form $(T_{kl})(1\leq k, l\leq q)$ with $T_{kl}$
being $hs^2$-rowed rational square matrices. Then, for every $D \in
(\mathscr{V})$, we have $DT_{kl} = T_{kl}D$ i.e. $T_{kl}$ belongs to
$(\mathscr{V})^{\ast}$, the commutator-algebra of $(\mathscr{V})$. If
$\widetilde{T}$ should be equal to $-T$, then we must have, in
particular, $\widetilde{T}_{kk} = -T_{kk}$ for $1\leq k \leq q$. If we
can find nonsingular $T_{11}$ in $(\mathscr{V})^{\ast}$ with
$\widetilde{T}_{11} = - T_{11}$, then $T = [T_{11},\ldots, T_{11}]$
will meet our requirements. \pageoriginale Now if
$(\mathscr{V})^{\ast}$ is not commutative, there always exists at
least one $T_2 \neq 0$ (and hence non-singular) for which
$\widetilde{T}_2 \neq T_2$ and then we can take $T_{11}= T_2 -
\widetilde{T}_2$ ($\neq 0$, since $(\mathscr{V})^{\ast}$ is a
division-algebra). (If, for every $T_3 \in (\mathscr{V})^{\ast}$ we
have $\widetilde{T}_3 = T_3$, then for any two elements $T_4, T_5 \in
(\mathscr{V})^{\ast}$, we would have $T_4 T_5 = \widetilde{T}_4
\widetilde{T}_5 = \widetilde{T_5 T_4} = T_5 T_4$) 

Taking a basis $\lambda_1,\ldots, \lambda_n$ of $\mathscr{V}$ over
$\mathbb{Q}$, for any $\delta\in \mathscr{V}$, we have two
representations,
\begin{align*}
\delta & \rightarrow D \text{ where }
\left(\begin{smallmatrix} \lambda_1
  \\ \vdots\\\lambda_n\end{smallmatrix}\right) \delta = D
 \left(\begin{smallmatrix} \lambda_1
   \\ \vdots\\ \lambda_{n}\end{smallmatrix}\right) \\
\delta & \rightarrow B \text{ where } \delta (\lambda_1,\ldots
\lambda_n) = (\lambda_1,\ldots, \lambda_n) B
\end{align*}
and further $B =C^{-1}DC$ for a fixed rational nonsingular matrix
$C$. The matrices $B'$ give a regular representation of
$(\mathscr{V})^{\ast}$. Now $B' = C' D' C'^{-1} = (C'G)D^{\ast} (C'
G)^{-1}$. Hence the matrices $D^{\ast}$ for $\delta \in D$ give an
equivalent representation of $(\mathscr{V})^{\ast}$. Denoting this
equivalent representation itself by $(\mathscr{V})^{\ast}$, we see
that $(\mathscr{V})$ and $(\mathscr{V})^{\ast}$ coincide as sets and
their multiplicative structure coincides on their centre
(cf. Proposition~\ref{chap1:prop6}). If the involution in $\mathscr{V}$ is of the
second kind, then there exists already in $\mathfrak{R}$, an element
$c$ with $c\neq \widetilde{c}$. 

If finally the involution is of the first kind and further
$\mathscr{V} = \mathfrak{R}$, then the commutator algebra of
$\mathscr{V}$ is itself and if $\delta \rightarrow D$ is an
irreducible representation of $\mathfrak{R}$ over $\mathbb{Q}$, then,
by $(19)'$, $D= \Omega[\sigma^{(1)},\ldots
  \delta^{(h)}]\Omega^{-1}$. Taking $T_{kl} = \Omega
[\delta^{(1)}_{kl},\ldots \delta^{(h)}_{kl}]\Omega^{-1}$ with
$\delta_{kl} \in \mathfrak{R} (1\leq k, l \leq q)$ for which the
matrix $(\delta_{kl})$ is non-singular and skew-symmetric, we have
then that the matrix $A=(\Omega_q \Omega')^{-1}(T_{kl}) =
(\Omega'^{-1} [\delta^{(1)}_{kl},\ldots,
  \delta^{(h)}_{kl}]\Omega^{-1})$ \pageoriginale is clearly rational,
non-singular and skew-symmetric. A necessary and sufficient condition
for such a non-singular skew-symmetric matrix $(\delta_{kl})$ over
$\mathfrak{R}$ to exist is that $q$ is even. It is easy to verify that
$\text{dett}\ldotp A=N_{\mathfrak{R}/\mathbb{Q}}
(\text{dett}(\delta_{kl}))/ (\text{dett}\ldotp\Omega)^{-2q}\neq 0$
i.e. $A$ is non-singular. If $q=2p$, for example, we can choose
$(\delta_{kl}) =
\left(\begin{smallmatrix} 0 & E_p \\ -E_p & 0\end{smallmatrix}\right),
  E_p $ being the $p$-rowed identity matrix. 

Having found a rational skew-symmetric matrix $A$ satisfying (53), we
proceed to look for an $R$-matrix $R$ having $(\mathfrak{M})$ for its
commutator-algebra. The following proposition prompts us to look for
$R$ in the linear closure $(\ub{\mathscr{F}})$ with respect to the
reals of the algebra $(\mathscr{F})$.

\begin{proposition}
Any real matrix $T$ for which $TM=MT$ for all $M \in (\mathfrak)$
belongs to $(\ub{\mathscr{F}})$. 
\end{proposition}

\begin{proof}
Writing $T = \sigma_1 T^0_1 + \cdots + \rho_k T^0_k$ with
$T^0_1,\ldots, T^0_k$ rational and $\rho_1,\ldots, \rho_k$ being real
numbers linearly independent over $\mathbb{Q}$, we see from $TM=MT$
for $M \in (\mathfrak{M})$, that $\sum\limits^k_{p-1}\rho_p(T^0_p M -
M T^0_p)=0$. By the linear independence of the $\rho_p$ over
$\mathbb{Q}$, we obtain that
$$
T^0_p \in (\mathscr{F}) \text{ for } 1 \leq p \leq k \text{ i.e. } T
\in (\ub{\mathscr{F}}).
$$

Denoting by $(\ub{\mathfrak{M}})$ the linear closure of
$(\mathfrak{M})$ with respect to the reals, we deduce from Proposition
13 that $(\mathscr{F})$ is precisely the set of all real matrices
commuting with all elements of $(\ub{\mathfrak{M}})$.

Our object is then to find $R \in (\ub{\mathscr{F}})$ such that
\begin{itemize}
\item[{\rm 1)}] $R^2 = - E$ ($E$ being the identity matrix)

\item[{\rm 2)}] $AR = S$ is positive-definite symmetric, and

\item[{\rm 3)}] Any \pageoriginale rational $M$ for which $MR=RM$
  belongs to $(\mathfrak{M})$. 
\end{itemize}
For the moment, we shall agree to ignore condition 3) and look for $R$
satisfying only conditions 1) and 2). 

A necessary condition for $R$ to exist is that the involution $M
\rightarrow M^{\ast} = A^{-1}M' A$ in $(\mathfrak{M})$ is positive. In
particular, $(\mathscr{V})$ should admit a positive involution
\begin{equation*}
D \rightarrow D^{\ast} = G^{-1}D'G, \text{ where } G = G'>0 \tag{55}\label{eq55}
\end{equation*}
and hence $\mathscr{V}$ has to be one of the following four types:
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\theenumi)}
\item $\mathscr{V} = \mathfrak{R}$, a totally real algebraic number
  field of degree $h$ over $\mathbb{Q}$.

\item $\mathscr{V} = \mathcal{G}$, a totally indefinite quaternion
  algebra over $\mathfrak{R}$ of $1^{\rm st}$ kind 

\item $\mathscr{V} = \mathscr{F}$, totally definite quaternion algebra
  over $\mathfrak{R}$ of $1^{\rm st}$ kind 

\item $\mathscr{V}$ is a cyclic algebra with a positive involution
  (55) of the second kind, with centre $\mathfrak{R}$ which is a
  totally imaginary quadratic extension of the fixed field
  $\mathscr{L}$ of the involution, $\mathscr{L}$ being totally real
  and of degree $g$ over $\mathbb{Q}$. Further $\mathscr{V}$ has a
  splitting field $\mathfrak{Z}$ of degree $s\geq 1$ over
  $\mathfrak{R}$, with $\mathfrak{Z}$ being realisable as indicated at
  the beginning of \S 6.
\end{enumerate}
\end{proof}

For the construction of $R$, we shall deal with these four cases
separately. We shall first find a simple normal form for elements of
$(\ub{\mathfrak{M}})$ and then, for elements of
$(\ub{\mathscr{F}})$. 

\medskip
\noindent
{\rm \bf{Case} (i)} $\mathscr{V} = \mathfrak{R}$.

For $\mathfrak{R}$, we have the regular representation $\delta (\in
\mathfrak{R}) \rightarrow D =
\left(\omega^{(l)}_k\right)[\delta^{(1)},\break\ldots, \delta^{(h)}]
\left(\omega^{(l)}_k\right)^{-1}$ with respect to a basis
$\omega_1,\ldots, \ldots, \omega_h$ of $\mathfrak{R}$ over
$\mathbb{Q}$. The linear closure $(\ub{\mathscr{V}})$ of
$(\mathscr{V})$ with respect to the real number field $\mathbb{R}$
consists of all matrices of the form
$\left(\omega^{(1)}_k\right)[\delta_1,\ldots \delta_h]
\left(\omega^{(l)}_k\right)^{-1}$ \pageoriginale where
$\delta_1,\ldots,\delta_h$ are arbitrary real numbers. Taking an
$\mathbb{R}$-equivalent representation for $(\ub{\mathscr{V}})$
(i.e. a representation equivalent over the reals), we may suppose that
$(\ub{\mathfrak{M}})$ consists of all real matrices of the form
$\ub{R}=[\overset{1}{\underset{q}{R_{1}}},\ldots,
  \overset{1}{\underset{q}{R_{h}}} ]$ where $R_1,\ldots, R_h$ are
independent one-rowed real square matrices occurring with multiplicity
$q$. The commutator-algebra $(\ub{\mathscr{F}})$ of
$(\ub{\mathfrak{M}})$ consists exactly of real matrices $\ub{T} = [
  \overset{q}{\underset{1}{T_1}},\ldots,
  \overset{q}{\underset{1}{T_{h}}} ]$ where $T_1,\ldots, T_h$ are
arbitrary $q$-rowed real square matrices occurring with multiplicity
$1$. In passing to the new representation $\delta \rightarrow \ub{D} =
[\delta_1,\ldots, \delta_h]$ of $(\ub{\mathscr{V}})$, the positive
symmetric matrix $G$ in \eqref{eq55} goes over into the $h$-rowed indentity
matrix $E_h$. The positive involution in $(\mathfrak{M})$ is just
$\ub{R} \rightarrow (\ub{R})^{\ast} = \underset{q}{E_h}(\ub{R})'
\underset{q}{E_h} = R'$ and the induced involution in
$(\ub{\mathscr{F}})$ is just $\ub{T}\rightarrow \ub{T}'$.

\medskip
\noindent
{\rm \bf{Case} (ii)} $\ub{\mathscr{V} = \mathscr{G}}$

Any element $\delta\in\mathscr{G}$ is of the form $x+yi+zj+tk$ where
$x$, $y$, $z$, $t\in\mathfrak{R}$, $i^{2}=a>0$, $j^{2}=b$, $-b>0$,
$a$, $b\in \mathfrak{R}$, $\xi=x+yi$, $\ob{\xi}=x-yi$, $\eta=z+ti$,
$\ob{\eta}=z-ti$. For $\mathscr{G}$, we have the representation  over
$\mathfrak{R}$ given by $\delta=\xi+\eta j\to D$ where
$D=\left(\left(\begin{smallmatrix} 1 & 1\\ i & -i 
\end{smallmatrix}\right)\times E_{2}\right)[D_{1},\ob{D}_{1}]\times
\left(\left(\begin{smallmatrix} 1 & 1 \\ i & -i
\end{smallmatrix}\right)\times E_{2}\right)^{-1}$,
$D_{1}=\left(\begin{smallmatrix} \xi & \eta\\ b\ob{\eta} & \xi 
\end{smallmatrix}\right)$, $\ob{D}_{1}=\left(\begin{smallmatrix}
  \ob{\xi} & \ob{\eta}\\ b\eta &
  \xi \end{smallmatrix}\right)=\left(\begin{smallmatrix} 0 & 1\\ b & 0
\end{smallmatrix}\right)D_{1}\left(\begin{smallmatrix} 0 & 1\\ b & 0
\end{smallmatrix}\right)^{-1}$. Further $\mathscr{G}$ has a rational
representation $\delta\to K[D^{(1)},\ldots,D^{(h)}]K^{-1}$, $K$ being
a certain fixed matrix. Going over to an $\mathbb{R}$-equivalent
representation, we see that $(\ub{\mathscr{M}})$ consists of all real
matrices of the form
$\ub{D}=[\overset{2}{\underset{2q}{R_{1}}},
  \ldots,\overset{2}{\underset{2q}{R_{h}}}]$  
where $R_{1},\ldots,R_{h}$ are arbitrary 2-rowed real square
matrices occurring with multiplicity $2q$. Any real matrix commuting
with\pageoriginale all real matrices of the form $\underset{2q}{R}$
where $R$ is a real 2-rowed square matrix, is of the form $(T_{kl})$
$(1\leq k,l\leq 2q)$ with $T_{kl}=t_{kl}E_{2}$,
$t_{kl}\in\mathbb{R}$. Thus $(\mathscr{F}_{1})$ consists of all the
matrices of the form $\ub{T}= [\overset{2q}{T_{1}}\times E_{2},\ldots,
  \overset{2q}{T_{h}}\times E_{2}]$ where $T_{1},\ldots,T_{h}$ are
arbitrary $2q$-rowed real square matrices. The positive involution in
$\mathscr{G}$ is given by $D_{1}\to
D^{\ast}_{1}=G^{-1}_{1}D'_{1}G_{1}$ where $G_{1}$ is symmetric and
totally-positive over $\mathfrak{R}$. This involution goes over in
$(\ub{\mathscr{M}})$ to the involution 
$\ub{D}\to \ub{D}^{\ast} =
\left[\underset{2q}{G_{1}^{(1)^{-1}}} \underset{2q}{\overset{2}{R'_{1}}}
   \;\;  \underset{2q}{G_{1}^{(1)}},\ldots, \underset{2q}{G_{1}^{(h)^{-1}}}
  \underset{2q}{\overset{2}{R'_{h}}} \;\; \underset{2q}{G_{1}^{(h)}}\right]  
$. 
For each $G^{(k)}_{1}(l\leq k\leq h)$, there exists a real
non-singular matrix $C_{k}$ such that
$G^{(k)}_{1_{2}}=C'_{k}C_{k}$. Taking for $(\mathscr{M})$, the
equivalent representation
$\ub{D}=\left[\underset{2q}{C_{1}} \; \underset{2q}{R_{1}}
 \; \underset{2q}{C^{-1}_{1}},\ldots, \; \underset{2q}{C_{h}}
  \; \underset{2q}{\overset{2}{R_{h}}} \; \underset{2q}{C^{-1}_{h}} \right]$, 
we see that $(\mathscr{M})$ still consists of the same set of matrices
as above but, in terms of the new representation, the given positive
involution is more simply expressed by $\ub{D}\to \ub{D}'$ and the
induced involution in $(\ub{\mathscr{F}})$ is just $\ub{T}\to
\widetilde{\ub{T}}=\ub{T}'$. 

\medskip
\noindent
{\bf Case~(iii)} $\mathscr{V}=\mathfrak{R}$


For $\delta=x+yi+zj+tk\in\mathscr{V}$, we have the $4$-rowed\label{p63} 
representation 
$$
\text{over } \mathfrak{R}, \text{ viz. } \delta\to D=
\begin{pmatrix}
x & y & z & t\\
ay & x & at & z\\
bz & -bt & x & -y\\
-abt & bz & -ay & x
\end{pmatrix}
\text{ and a rational}
$$
representation given by $\delta\to
K_{1}[D^{(1)},\ldots,D^{(h)}]K^{-1}_{1}$ with a constant matrix
$K_{1}$. It is easy to see after passing to an equivalent
representation that $(\mathscr{M})$ consists precisely of all matrices
of the form $\ub{D}=[\underset{q}{D_{1}},\ldots,\underset{q}{D_{h}}]$,
where\pageoriginale $D_{1},\ldots D_{h}$ are matrices of the same form
as $D$ above, except that now $x$, $y$, $z$, $t$ are arbitrary real
numbers. Let 
$$
C_{k}=\left[1,\sqrt{-a^{(k)}}, \sqrt{-b^{(k)}},
  \sqrt{a^{(k)}b^{(k)}}\right]\text{ \ and \ } C=[C_{1},\ldots,C_{h}].
$$
Taking $C^{-1}\ub{D}C$ instead of $\ub{D}$ and replacing $x$, $y$,
$z$, $t$ by $x$, $\dfrac{y}{\sqrt{-a}}$, $\dfrac{z}{\sqrt{-b}}$,
$\dfrac{t}{\sqrt{ab}}$ respectively, we obtain finally that
$(\mathscr{M})$ consists of all real matrices of the form
$\ub{D}=[\underset{q}{\overset{1}{H_{1}}},\ldots,\underset{q}{\overset{1}{H_{h}}}]$ 
where $H_{1},\ldots,H_{h}$ are independent $4$-rowed real
representations of Hamiltonian quaternions, each occurring with
multiplicity $q$. Let $\mathbb{K}$ denote the algebra of real
Hamiltonian and $(\mathbb{K})$ denote the algebra of $4$-rowed real
matrices 
$$
H=
\begin{pmatrix}
x & y & z & t\\
-y & x & -t & z\\
-z & t & x & -y\\
-t & -z & y & x
\end{pmatrix}
$$
(with real $x$, $y$, $z$, $t$) representing elements of
$\mathbb{K}$. Then the matrices
$$
\widehat{H}=
\begin{pmatrix}
x & y & z & t\\
-y & x & t & -z\\
-z & -t & x & y\\
-t & z & -y & x
\end{pmatrix}
$$
(with $x$, $y$, $z$, $t$ real) give a representation of $\widehat{K}$,
the opposite algebra of $\mathbb{K}$. We denote by $(\mathbb{K})$ the
set of such matrices $\widehat{H}$.


The involution in $\mathfrak{R}$ was, to start with, given by $D\to
F^{-1}D'F$ where $F^{-1}=[1,-a,-b,ab]$ and in terms of the new
representation, $F$ is to be replaced by the identity. Thus the
positive involution in $(\mathscr{M})$ is given by $\ub{D}\to
\ub{D}'$. The commutator-algebra $(\mathscr{F})$ consists of all
matrices $\ub{T}$ of the form
$\ub{T}=[\underset{1}{\overset{q}{\widehat{H}_{1}}},\ldots\underset{1}{\overset{q}{\widehat{H}_{h}}}]$
where, for $1\leq k\leq h$, $\overset{q}{\widehat{H}_{k}}$ is an
arbitrary $q$-rowed square matrix with elements which belong to
$(\mathbb{K})$ and the involution in $(\mathscr{F})$ is just
$\ub{T}\to \ub{T}'$. 


\medskip
\noindent
{\bf Case (iv) $\mathscr{V}$}, {\em a\pageoriginale cyclic algebra
  with a positive 
  involution of the second kind.}

For $\delta\in\mathscr{V}$, we have the regular representation
$\delta\to D$ over $\mathfrak{Z}$ given by \eqref{eq35} and the rational
representation $\delta+\ub{D}$ (see p.~\pageref{eq49}). We arrange the
conjugates of $\mathfrak{R}$ over $\mathbb{Q}$ as
$\mathfrak{R}=\mathfrak{R}^{(1)}$,
$\mathfrak{R}^{(2)}=\overline{\mathfrak{R}^{(1)}},\ldots,\mathfrak{R}^{(h-1)}$,
$\mathfrak{R}^{(h)}=\overline{\mathfrak{R}^{(h-1)}}$. Using~\eqref{eq51}
and passing to an equivalent representation over the field
$\mathbb{C}$ of complex numbers, we see that the linear closure
$(\ub{\mathscr{V}})$ of $(\mathscr{V})$ (with respect to the reals)
consists of all complex matrices $\ub{M}$ of the form
$\ub{M}=[\underset{s}{\overset{s}{D_{1}}}, \underset{s}{\ob{D_{1}}},
  \underset{s}{\overset{s}{D_{3}}},
  \underset{s}{\ob{D_{3}}},\ldots,\underset{s}{\overset{s}{D_{h-1}}},\underset{s}{\ob{D}_{h-1}}]$
where $D_{1}$, $D_{3},\ldots,D_{h-1}$ are $g$ independent $s$-rowed
complex square matrices occurring with multiplicity $s$. The positive
involution $D\to D^{\ast}=G^{-1}\ob{D}'G$ in $(\mathscr{V})$
corresponds exactly to a positive involution $\ub{M}\to
P^{-1}\ob{M}'P$ in $(\mathscr{V})$ where
$P=[\overset{s}{G_{1}},\overset{s}{G_{2}},\ldots\overset{s}{G_{sh}}]$
is positive-definite hermitian. Now, for a complex non-singular
$L_{k}$, we have $G_{k}=\ob{L}'_{k}L_{k}$ for $1\leq k\leq hs$. Let
$L=[L_{1},L_{2},\ldots,L_{sh}]$. Taking the representation
$L\ub{M}L^{-1}$ instead of $\ub{M}$, the given involution in
$(\mathscr{V})$ is expressed simply by $\ub{M}\to \ob{\ub{M}}'$. Now,
every complex matrix $\left(\begin{smallmatrix} \ob{\alpha} & 0\\ 0
 & \ob{\alpha}\end{smallmatrix}\right)$ with
$\alpha=\beta+\sqrt{-1}\gamma(\beta,\gamma \text{ real})$ is
equivalent over $\mathbb{C}$ to $\left(\begin{smallmatrix} \beta &
  \gamma\\ -\gamma & \beta
\end{smallmatrix}\right)$. Thus passing to a suitable equivalent
representation we obtain that $(\mathscr{M})$ consists precisely of
all matrices of the form
$\ub{D}=[\underset{sq}{\overset{s}{C_{1}}},\ldots,\underset{sq}{\overset{s}{C_{g}}}]$ 
where $C_{1},\ldots,C_{g}$ are independent $s$-rowed square matrices
with elements which are $2$-rowed real representations of complex
numbers, each $C_{i}$ occurring with multiplicity $sq$. The positive
involution in $(\mathscr{M})$ is just $\ub{D}\to \ub{D}'$. The
commutator-algebra $(\mathscr{F})$ consists of all matrices
$\ub{T}=[\overset{sq}{T_{1}}\times
  E_{s},\ldots,\overset{sq}{T_{g}}\times E_{s}]$\pageoriginale where
$T_{1},\ldots,T_{g}$ are independent $sq$-rowed square matrices with
elements which are $2$-rowed real representations of complex
numbers. The involution $\ub{T}\to \widetilde{\ub{T}}$ in
$(\mathscr{F})$ induced by the positive involution in $(\mathscr{M})$
is just $\ub{T}\to \ub{T}'$. We have thus proved

\begin{thm}\label{chap1:thm5}
With the notation as above, we have the following normal forms for
elements of $(\mathscr{M})$ and $(\mathscr{F})$, {\em viz.}
{\fontsize{10}{11}\selectfont
\begin{xalignat*}{3}
\text{\bf Case (i) } \mathscr{V} &= \mathfrak{R} &&
D=[\overset{1}{\underset{q}{R_{1}}},\overset{(\mathscr{M})}{\ldots},\overset{1}{\underset{q}{R_{h}}}]
&&
\ub{T}=[\overset{q}{\underset{1}{T_{1}}},\overset{(\mathscr{F})}{\ldots},\underset{1}{\overset{q}{T_{h}}}]\\
\text{\bf Case (ii) }  \mathscr{V} &= \mathscr{G} &&
\ub{D}=[\underset{2q}{\overset{2}{R_{1}}},\ldots,\underset{2q}{\overset{2}{R_{h}}}]
&& \ub{T}=[\overset{2q}{T_{1}}\times
  E_{2},\ldots,\overset{2q}{T_{h}}\times E_{2}]\\
\text{\bf Case (iii) } \mathscr{V} &= \mathfrak{R} &&
\ub{D}=[\underset{q}{\overset{1}{H_{1}}},\ldots,\underset{q}{\overset{1}{H_{h}}}]
&& \ub{T}
=[\underset{1}{\overset{q}{\widehat{H}_{1}}},\ldots,\underset{1}{\overset{q}{\widehat{H}_{h}}}]\\
\text{\bf Case (iv) } \mathscr{V}, & \text{ cyclic algebra} && \ub{D}
=[\underset{sq}{\overset{s}{C_{1}}},\ldots,\underset{sq}{\overset{s}{C_{g}}}]
&& \ub{T}=[\overset{sq}{C_{1}}\times
  E_{s},\ldots,\overset{sq}{C_{g}}\times E_{s}] 
\end{xalignat*}}\relax
In all the four cases, the given positive involution in
$(\mathscr{M})$ is given by $\ub{D}\to \ub{D}'$ and the involution in
$(\mathscr{F})$ is $\ub{T}\to \ub{T}'$. 
\end{thm}

At the beginning of this section, we looked for a rational matrix
$A=\underset{q}{G}$ $T_{0}$ with $\widetilde{T}_{0}=-T_{0}$ in
$(\mathscr{F})$. With the simplification carried out above in
$(\ub{\mathscr{F}})$, we shall reduce $A=T_{0}$ to a simple normal
form by making the real linear transformations in
$(\ub{\mathscr{F}})$. For reducing $A$ to the simplest form, we deal
with each one of the four cases separately. We denote, in the sequel,
the matrix $\left(\begin{smallmatrix} 0 & E_{k}\\ -E_{k} & 0
\end{smallmatrix}\right)$ by $\epsilon_{k}$ ($E_{k}$ being the
$k$-rowed identity) and shall denote $\epsilon_{1}$ by $\epsilon$, for
brevity.

\medskip
\noindent
{\bf Case (i) {\boldmath$\underline{\mathscr{V}=\mathfrak{R}}$.}} We
have seen that a necessary and sufficient condition for such an $A$ to
exist is that $q$ is even, say, $q=2p$. Let
$A=[\overset{q}{T_{1}},\ldots,\overset{q}{T_{h}}]$\pageoriginale where
$T_{1},\ldots,T_{h}$ are arbitrary $2p$-rowed real nonsingular
skew-symmetric matrices. By passing to an equivalent representation of
$(\mathscr{F})$ (which does not disturb the form of the elements of
$(\ub{\mathscr{F}})$, we can suppose that
$A=\underset{h}{\epsilon_{p}}$ already.

\medskip
\noindent
{\bf Case (ii) {\boldmath$\mathscr{V}=\mathscr{G}$}}. As in case (i),
  passing to an equivalent representation of $(\ub{\mathscr{F}})$
  which does not destroy the form of the elements of $(\mathscr{F})$,
  we could suppose that $A=\underset{2h}{\epsilon_{q}}$. 

\medskip
\noindent
{\bf Case (iii) {\boldmath$\mathscr{V}=\mathscr{P}$}}. For the sake of
simplification, we might, to start with, use for the Hamiltonian
quaternions, the representation, as elements of the opposite algebra
$(\widehat{\mathbb{K}})$. Thus, the elements of $(\ub{\mathscr{F}})$
are exactly all matrices of the form
$\ub{T}=[\underset{1}{\overset{q}{H_{1}}},\ldots,\underset{1}{\overset{q}{H_{h}}}]$
where $H_{1},\ldots,H_{h}$ are $q$-rowed square matrices with elements
in $(\mathbb{K})$. We now make a simple transformation in
$(\ub{\mathscr{F}})$ as follows (Of course, we have to make a
corresponding transformation also in $(\ub{\mathscr{M}})$, in order
that $(\ub{\mathscr{F}})$ might continue to be the commutator-algebra
of $(\ub{\mathscr{M}})$, but, for the moment, we can afford to forget
$(\ub{\mathscr{M}})$). If $H\in (\mathbb{K})$ corresponds to the
Hamiltonian quaternion $x+yi+zj+tk=\xi+\eta j$ (where $\xi=x+yi$,
$\eta=z+ti$ are in $\mathbb{C}$ and $x$, $y$, $z$, $t\in \mathbb{R}$),
then $H$ is nothing but $\left(\begin{smallmatrix} \xi &
  \eta\\ -\ob{\eta} & \ob{\xi}
\end{smallmatrix}\right)$ where $\xi$, $\eta$, $\ob{\xi}$,
$-\ob{\eta}$ are just the two-rowed real representations of the
corresponding complex numbers. Passing to an equivalent representation
for $(\ub{\mathscr{F}})$ with a suitable permutation matrix, we can
suppose that the elements of $(\ub{\mathscr{F}})$ are of the form
$$
\ub{T}=\left[
\begin{pmatrix}
C_{1,1} & C_{1,2}\\
-\ob{C}_{1,2} & \ob{C}_{1,1}
\end{pmatrix},\ldots,
\begin{pmatrix}
C_{h,1} & C_{h,2}\\
-\ob{C}_{h,2} & \ob{C}_{h,1}
\end{pmatrix}
\right]
$$
where\pageoriginale $C_{k,l}(1\leq k\leq h,l=1,2)$ are independent
$q$-rowed square matrices with elements which are of the form
$\left(\begin{smallmatrix} x & y\\ -y & x
\end{smallmatrix}\right)$ with $x$, $y\in\mathbb{R}$. Further
$\overline{C_{k,l}}$ is obtained from $C_{k,l}$ by just replacing a
general element $\left(\begin{smallmatrix} x & y\\ -y & x
\end{smallmatrix}\right)$ in $C_{k,l}$ by $\left(\begin{smallmatrix} x
  & -y \\ y & x
\end{smallmatrix}\right)$. Let us consider each one of the $h$ blocks
$\left(\begin{smallmatrix} C_{k,1} & C_{k,2}\\ -\ob{C}_{k,2} &
  \ob{C}_{k,1}
\end{smallmatrix}\right)$ in $\ub{T}$, separately. By applying a
suitable permutation-transformation to $C_{k,l}$ which brings all the
elements $x$ together, all the $y$ together, all the elements $-x$
together and all the elements $-y$ together, we could suppose that
$C_{k,l}=
\left(\begin{smallmatrix} U_{k,l} & V_{k,l}\\ -V_{k,l} & U_{k,l}
\end{smallmatrix}\right)$ where $U_{k,l}$ and $V_{k,l}$ are
independent $q$-rowed real square matrices then
$\ob{C}_{k,l}=\left(\begin{smallmatrix} U_{k,l} & -V_{k,l}\\ V_{k,l} &
  U_{k,l}
\end{smallmatrix}\right)$. To start with $A$ is an element of
$(\ub{\mathscr{F}})$ satisfying $A=-A'$. By means of a transformation
which does not disturb the final form of the elements of
$(\ub{\mathscr{F}})$, we can suppose $A=\underset{h}{\epsilon_{2q}}$
already. 

\medskip
\noindent
{\bf Case (iv) {\boldmath$\mathscr{V}$}}, {\em a cyclic algebra with a
  positive involution of the second kind.}

Let
$T_{0}=[\overset{sq}{\underset{s}{T_{1}}},\ldots,\overset{sq}{\underset{s}{T_{g}}}]$ 
be a non-singular skew-symmetric matrix in $(\ub{\mathscr{F}})$. Now
$\underset{sq}{\epsilon}$ commutes with $T_{k}$ and
$M_{k}=\underset{sq}{\epsilon}T_{k}(1\leq k\leq g)$ considered as a
$sq$-rowed complex matrix is hermitian and non-singular. There exists
a $sq$-rowed complex non-singular matrix $L_{k}$ such that
$\ob{L}'_{k}M_{k}L_{k}=[\underset{a_{k}}{1},\underset{b_{k}}{-1}]$
with $a_{k}+b_{k}=sq$. Let
$L=[\overset{sq}{\underset{s}{L_{1}}},\ldots,\overset{sq}{\underset{s}{L_{g}}}]$
where, now in $L_{i}$, we have replaced the complex elements by their
$2$-rowed real representations. Taking the equivalent representation 
$L(\ub{\mathscr{F}})L^{-1}$\pageoriginale instead of
$(\ub{\mathscr{F}})$, we see that the elements of $(\ub{\mathscr{F}})$
are again of the same form as above but the matrix $T_{0}$ assumes the
very simple form
$\left[[\underset{a_{1}}{\epsilon},-\underset{b_{1}}{\epsilon}],\ldots,
[\underset{a_{h}}{\epsilon},-\underset{b_{h}}{\epsilon}]\right]$. We
make now a simple transformation on $(\ub{\mathscr{F}})$. The elements
of $(\ub{\mathscr{F}})$ are of the form
$[\underset{s}{\overset{sq}{T_{1}}},\ldots,\underset{s}{\overset{sq}{T_{g}}}]$
where each $T_{i}$ is a $sq$-rowed matrix with elements of the form
$\left(\begin{smallmatrix} x & y\\ -y & x
\end{smallmatrix}\right)$, $x$, $y\in\mathbb{R}$. Passing to an
equivalent representation of $(\mathscr{F})$, by clubbing all the
$x$'s together and all the $y$'s together as in case (iii), we may
suppose that each $T_{k}=\left(\begin{smallmatrix} U_{k} &
  V_{k}\\ -V_{k} & U_{k}
\end{smallmatrix}\right)$ where $U_{k}$, $V_{k}$ are arbitrary
$sq$-rowed real square matrices. Thus $T_{0}$ goes over into
$[\underset{s}{T^{0}_{1}},\ldots,\underset{s}{T^{0}_{g}}]$ where
$T^{0}_{k}=\left(\begin{smallmatrix} 0 & P_{k}\\ -P_{k} & 0
\end{smallmatrix}\right)$ $(1\leq k\leq g)$ and
$P_{k}=[\underset{a_{k}}{1},\underset{b_{k}}{-1}]$ with
$a_{k}+b_{k}=sq$. As a further simplification, we take the
representation $B(\ub{\mathscr{F}})B^{-1}$ where
$B=[\underset{s}{B_{1}},\ldots,\underset{s}{B_{g}}]$ and
$B_{k}=[\underset{sq}{1},P_{k}](1\leq k\leq g)$. Thus
$(\ub{\mathscr{F}})$ may be supposed to be the set of all matrices of
the form $[\underset{s}{\overset{sq}{\widehat{C}_{1}}},\ldots,
  \underset{s}{\overset{sq}{\widehat{C}_{g}}}]$ where
$\widehat{C}_{k}=\left(
\begin{smallmatrix} U_{k} & V_{k} & P_{k}\\ -P_{k}V_{k} & P_{k}U_{k} &
  P_{k}
\end{smallmatrix}\right)(1\leq k\leq g)$ and $U_{k}$, $V_{k}$ are
arbitrary $sq$-rowed real square matrices. Our given matrix $T_{0}$
goes over into the simple matrix
$[\underset{s}{\epsilon_{sq}},\ldots,\underset{s}{\epsilon_{sq}}]$. 

Summing up, the elements of $(\ub{\mathscr{F}})$ have the normal form
given in the following table and in each of the four cases, the given
matrix $T_{0}$ in $(\ub{\mathscr{F}})$ assumes the simple form
$\mathsf{J}$. 
{\fontsize{9}{11}\selectfont
\begin{equation*}
\begin{array}{|c|l|c|c|}
\hline
 & \multicolumn{1}{c|}{(\ub{\mathscr{F}})} & \ub{\mathsf{J}_{0}} &
\ub{\mathsf{J}}\\
\hline
\mathscr{V}=\mathfrak{R} &
\ub{T}=[\underset{1}{\overset{2p}{R_{1}}},\ldots,\underset{1}{\overset{2p}{R_{h}}}]
  & \epsilon_{p} & \underset{h}{\mathsf{J}_{0}}\\
\mathscr{V}=\mathscr{G} &
\ub{T}=[\underset{2}{\overset{2q}{R_{1}}},\ldots,\underset{2}{\overset{2q}{R_{h}}}]
& \epsilon_{q} & \underset{2h}{\mathsf{J}_{0}}\\
\mathscr{V}=\mathfrak{R} & \ub{T}
=[\underset{1}{\overset{q}{H_{1}}},\ldots,\underset{1}{\overset{q}{H_{h}}}]
\text{ where } H_{k} \text{ is of the form} & \epsilon_{2q} &
\underset{h}{\mathsf{J}_{0}}\\
 & \begin{pmatrix}
     C_{1} & C_{2}\\
     -\ob{C}_{2} & \ob{C}_{1}
   \end{pmatrix}, C_{1}=
   \begin{pmatrix}
    U & V\\
    -V & U
   \end{pmatrix}\text{ and } \ob{C}_{1}=
   \begin{pmatrix}
     U & -V\\
     V & U
   \end{pmatrix} & & \\[5pt]
\multicolumn{1}{|p{1.2cm}|}{$\mathscr{V}$,~{\em cyclic algebra}} &
\ub{T}=[\underset{s}{\overset{sq}{\widehat{C}_{1}}},\ldots,\underset{s}{\overset{sq}{\widehat{C}_{g}}}]
\text{ where } \widehat{C}_{k} \text{ is of the form} & &\\
 & \begin{pmatrix}
     U_{k}  & V_{k} P_{k}\\
     -P_{k}V_{k} & P_{k}U_{k}P_{k}
   \end{pmatrix}, \text{ and }
P_{k}=[\underset{a_{k}}{1},\underset{b_{k}}{-1}] \text{ with} &
\epsilon_{sq} & \underset{sq}{\mathsf{J}_{0}}\\
 & \hspace{3cm} a_{k}+b_{k}=sq & & \\
\hline
\end{array}\tag{56}\label{eq56}
\end{equation*}}\relax\pageoriginale

We have to find $R\in (\ub{\mathscr{F}})$ such that $R^{2}=-E$ and
$\mathsf{J}R=S=S'>0$. Since $R$ and $\mathsf{J}$ are both in
$(\ub{\mathscr{F}})$, they decompose into similar blocks and therefore
confining ourselves to one of the components at a time, our problem
reduces to finding all real matrices $R$ satisfying
\begin{equation*}
\mathsf{J}_{0}R=S=S'>0,\quad R^{2}=-E\tag{57}\label{eq57}
\end{equation*}
and further $R$ is of the form $\overset{2p}{R_{1}}$,
$\overset{2q}{R_{1}}$, $\overset{q}{H_{1}}$ or
$\overset{sq}{\widehat{C}_{1}}$ as in \eqref{eq56}. We shall call a real
matrix $R$ of the form $\overset{2p}{R_{1}}$, $\overset{2q}{R_{1}}$,
$\overset{q}{H_{1}}$ or $\overset{sq}{\widehat{C}_{1}}$ as in
\eqref{eq56}, an {\em admissible matrix of type} $1$, $2$, $3$ or $4$
respectively.

From \eqref{eq56}, we get $\mathsf{J}^{-1}_{0}S\mathsf{J}^{-1}_{0}S=-E$,
$S=S'>0$. Since $\mathsf{J}^{2}_{0}=-E$, we have
\begin{equation*}
S\mathsf{J}_{0}S=\mathsf{J}_{0},\quad S=S'>0.\tag{58}\label{eq58}
\end{equation*}
Thus we have to look for all {\em admissible positive symmetric
  symplectic} matrices $S$.

Let\pageoriginale us now analyse \eqref{eq58}. First note that $E+S$ is
positive symmetric along with $S$. Let us set $W=2(E+S)^{-1}$; then
$W$ is positive symmetric too and further $S=-E+2W^{-1}$. From
\eqref{eq58}, we get
$$
4W^{-1}\mathsf{J}_{0}W^{-1}-2W^{-1}\mathsf{J}_{0}-2\mathsf{J}_{0}W^{-1}=0,\quad
\text{\ie } 2\mathsf{J}_{0}=\mathsf{J}_{0}W+W\mathsf{J}_{0}. 
$$
Setting $\mathsf{J}_{0}-\mathsf{J}_{0}W=-F$, this means that
$F=F'$. Further $F$ is admissible, of the same type as
$\mathsf{J}_{0}$ and $W$. Let us write
$$
F=
\begin{pmatrix}
G & H\\
H' & K
\end{pmatrix}
\text{ \ with \ } G=G', K=K'
$$
$G$ and $K$ having the same number of rows. Now $W=E-\mathsf{J}_{0}F$,
$W=W'$ together give $\mathsf{J}_{0}F=(\mathsf{J}_{0}F)'$. But
$\mathsf{J}_{0}F=\left(\begin{smallmatrix} H' & K\\ -G & -H
\end{smallmatrix}\right)$. Thus $H=H'$ and $K=-G$. Now
$S=-E+2(E-\mathsf{J}_{0}F)^{-1}=(E+\mathsf{J}_{0}F)(E-\mathsf{J}_{0}F)^{-1}$. Thus
\begin{equation*}
R=\mathsf{J}^{-1}_{0}S=+\mathsf{J}^{-1}_{0}(E+P)(E-P)^{-1}\tag{59}\label{eq59}
\end{equation*}
where
\begin{equation*}
P=
\begin{pmatrix}
H & -G\\
-G & -H
\end{pmatrix},\quad H=H',\quad G=G'.\tag{60}\label{eq60}
\end{equation*}
and $P$ is admissible, of one of the four types. (The parametrization
of $S$ is quite similar to the Cayley parametric representation for
orthogonal matrices).

We now proceed to examine the nature of the set of all admissible $R$
satisfying $R^{2}=-E$ and $\mathsf{J}_{0}R=(\mathsf{J}_{0}R)'>0$,
distinguishing between the various types. For this purpose, we go back
to the Riemann matrices associated with the $R$-matrix $R$. From
\S\ 1, we know that we can find a Riemann matrix
$\mathscr{P}$ uniquely up to a left-sided complex non-singular matrix
factor such that 
\begin{equation*}
R=\binom{\mathscr{P}}{\ob{\mathscr{P}}}^{-1}
\begin{pmatrix}
-iE & 0\\
0 & iE
\end{pmatrix}
\binom{\mathscr{P}}{\ob{\mathscr{P}}},\quad
\mathscr{P}\mathsf{J}^{-1}_{0}\mathscr{P}'=0,\quad
i\mathscr{P}\mathsf{J}^{-1}_{0}\ob{\mathscr{P}}'>0.\tag{61}\label{eq61} 
\end{equation*}\pageoriginale
(Here $i=\sqrt{-1}$). If $\mathscr{P}=(AB)$ with square matrices $A$
and $B$, then we know that $A$, $B$ are both non-singular and hence,
we can assume without loss of generality, that $\mathscr{P}=(Z \; E)$ and
the last two conditions in \eqref{eq61} are, in terms of $Z$, just
\begin{equation*}
Z=X+iY,\quad X=X',\quad Y=Y',\quad Y>0.\tag{62}\label{eq62}
\end{equation*}
From \eqref{eq59}, \eqref{eq60} and the first condition in \eqref{eq61} we
obtain
$$
\binom{\mathscr{P}}{\ob{\mathscr{P}}}\mathsf{J}^{-1}_{0}(E+P)=
\begin{pmatrix}
-iE & 0\\
0 & iE
\end{pmatrix}
\binom{\mathscr{P}}{\ob{\mathscr{P}}}(E-P)
$$
\ie
\begin{equation*}
\begin{split}
& -iZ+iZH -iG=ZG+E+H\\
& -iZG-iE-iH=-Z+ZH-G\\
& \ob{Z}(-iE+iH-G)=E+H+iG\\
& \ob{Z}(iG+E-H)=-G-iE-iH
\end{split}\tag{63}\label{eq63}
\end{equation*}

Let us set $Z_{0}=H+iG$. Then solving for $Z_{0}$ from the third
equation in \eqref{eq63}, we have $E+Z_{0}=Z(-iE+iZ_{0})$, \ie
$(E-iZ)Z_{0}=-(E+iZ)$. (equivalently, $Z=i(E+Z_{0})(E-Z_{0})^{-1}$)
\begin{align*}
Z_{0}=Z'_{0} &= -(E-iZ)^{-1}(E+iZ)\\
            &= -(E+iZ)(E-iZ)^{-1}\tag{64}\label{eq64}
\end{align*}
The condition $S>0$ is equivalent to $Y>0$ and using \eqref{eq63}, this
is equivalent to
\begin{equation*}
E-\ob{Z}_{0}Z_{0}>0.\tag{65}\label{eq65}
\end{equation*}
The mapping $Z\to Z_{0}$ takes the {\em ``generalized upper half-plane
  of degree $n$''} consisting of all $n$-rowed complex $Z$ satisfying
\eqref{eq62} into the ``generalized unit circle'' consisting of all
$n$-rowed $Z_{0}=Z'_{0}$ satisfying\pageoriginale \eqref{eq65}.

Thus $R$ is an admissible matrix of the form
\begin{equation*}
R=\mathsf{J}^{-1}_{0}
\begin{pmatrix}
E+H & -G\\
-G & E-H
\end{pmatrix}
\begin{pmatrix}
E-H & G\\
G & E+H
\end{pmatrix}^{-1}\tag{66}\label{eq66}
\end{equation*}
where $Z_{0}=H+iG$ satisfies 
\begin{equation*}
Z_{0}=Z'_{0},\quad E-\ob{Z}_{0}Z_{0}>0\tag{67}\label{eq67}
\end{equation*}

In case $\mathscr{V}=\mathfrak{R}$ or $\mathscr{V}=\mathscr{G}$, any
$q$-rowed (respectively $2q$-rowed) real square matrix is admissible
and therefore, from \eqref{eq60}, $G$, $H$ can be arbitrary real square
matrices of $\dfrac{q}{2}$ and $q$ rows respectively. Thus
$Z_{0}=H+iG$ is an arbitrary point of the generalized unit circle of
degree $\dfrac{q}{2}$ in case $\mathscr{V}=\mathfrak{R}$ and of degree
$q$, in case $\mathscr{V}=\mathscr{G}$. The matrix $Z$ is then an
arbitrary point of the generalized upper half-plane of the
corresponding degree. Taking into account all the components in the
representation \eqref{eq56} of $(\ub{\mathscr{F}})$, we are led in the
case $\mathscr{V}=\mathfrak{R}$, to a $h$-fold product of the
generalized upper half-plane of degree $\dfrac{q}{2}$ which is a
complex space of complex dimension
$\dfrac{h}{2}\left(\dfrac{q}{2}+1\right)\dfrac{q}{2}$. In the case
$\mathscr{V}=\mathscr{G}$ we arrive at the $h$-fold product of the
generalized upper half-plane of degree $q$, which is of complex
dimension $\dfrac{h}{2}q(q+1)$. 

Let us take the case $\mathscr{V}=\mathfrak{R}$. From the form
\eqref{eq60} of $P$ and from the `admissibility' of $P$, we see that
$H=H'=-\ob{H}$, $G=G'=-\ob{G}$ and both $G$ and $H$ have to be of the
form
$\left(\begin{smallmatrix} U & V\\ -V & U
\end{smallmatrix}\right)$ with $U$, $V$ being arbitrary $q$-rowed real
square matrices. From $H'=-\ob{H}$, $G'=-\ob{G}$, we obtain $H=\left(
\begin{smallmatrix} 0 & X_{1}\\ X'_{1} & 0
\end{smallmatrix}\right)$, $G=\left(\begin{smallmatrix} 0 &
  Y_{1}\\ Y'_{1} & 0
\end{smallmatrix}\right)$ with $X_{1}=-X'_{1}$ and
$Y_{1}=-Y'_{1}$. Now\pageoriginale $Z_{0}=\left(\begin{smallmatrix} 0
  & Z_{1}\\ Z'_{1} & 0
\end{smallmatrix}\right)$ with $Z_{1}=X_{1}+iY_{1}=-Z'_{1}$. Condition
\eqref{eq65} is equivalent to the condition $E-\ob{Z}_{1}Z'_{1}>0$. We
are thus led, in this case, to the set of $q$-rowed complex square
matrices $Z_{1}$ satisfying
\begin{equation*}
Z'_{1}=-Z_{1},\quad E_{q}-\ob{Z}_{1}Z'_{1}>0\tag{68}\label{eq68}
\end{equation*}
This space, like the `generalized unit circle' met before in earlier
cases, is again one of the complex symmetric spaces of E.\@ Cartan. It
is of complex dimension $q(q-1)/2$. If we take into account all the
components in the representation \eqref{eq56} of $(\ub{\mathscr{F}})$,
we are led to a $h$-fold product of the symmetric domain defined by
\eqref{eq68}. We remark that there is no special advantage in
interpreting \eqref{eq68} in terms of $Z$ and in fact, it becomes more
complicated.

We now take up case (iv). Since $P$ is admissible of type 4, it
follows that
$$
H=-DHD, \; G=-DGD\text{ where } D=[\underset{a}{1},\underset{b}{-1}]
\text{ with } a+b=sq.
$$
Breaking up $H$ as $\left(\begin{smallmatrix} H_{1} & H_{2}\\ H_{3} &
  H_{4}
\end{smallmatrix}\right)$ with $a$-rowed square $H_{1}$, we see that
$H=H'=-DHD$ implies $H_{4}=0$, $H_{1}=0$, $H_{2}=H'_{3}$. Thus
$H=\left(\begin{smallmatrix} 0 & X_{2}\\ X'_{2} & 0
\end{smallmatrix}\right)$ and similarly $G=\left(\begin{smallmatrix} 0
  & Y_{2}\\ Y'_{2} & 0\end{smallmatrix}\right)$ with arbitrary real
  $X_{2}$, $Y_{2}$ of $\ub{a}$ rows and $\ub{b}$ columns. Now
  $Z_{0}=H+iG=\left(\begin{smallmatrix} 0 & Z_{2}\\ Z'_{2} & 0
  \end{smallmatrix}\right)$ with $Z_{2}=X_{2}+iY_{2}$ having $\ub{a}$
  rows and $\ub{b}$ columns. Condition \eqref{eq65} is equivalent to the
  two conditions,
\begin{equation*}
E_{a}-\ob{Z}_{2}Z'_{2}>0,\quad E_{b}-\ob{Z}'_{2}Z_{2}>0\tag{69}\label{eq69}
\end{equation*} 
Now\pageoriginale ``$E_{a}-\ob{Z}_{2}Z'_{2}>0$'' is equivalent to the
fact that the matrix 
$$
M=
\begin{pmatrix}
E_{a} & \ob{Z}_{2}\\
Z'_{2} & E_{b}
\end{pmatrix}
=
\begin{pmatrix}
E_{a} & \ob{Z}_{2}\\
0 & E_{b}
\end{pmatrix}
\begin{pmatrix}
E_{a}-\ob{Z}_{2}Z'_{2} & 0\\
0 & E_{b}
\end{pmatrix}
\begin{pmatrix}
E_{a} & 0\\
Z'_{2} & E_{b}
\end{pmatrix}
$$
is positive-hermitian. But $M>0$ if and only if $\ob{M}>0$. On the
other hand 
$$
\ob{M}=
\begin{pmatrix}
E_{a} & 0\\
\ob{Z}'_{2} & E_{b}
\end{pmatrix}
\begin{pmatrix}
E_{a} & 0\\
0 & E_{b}-\ob{Z}'_{2}Z_{2}
\end{pmatrix}
\begin{pmatrix}
E_{a} & Z_{2}\\
0 & E_{b}
\end{pmatrix}
$$
is positive-hermitian if and only if $E_{b}-\ob{Z}'_{2}Z_{2}>0$. Thus
the two conditions in \eqref{eq69} reduce to the single condition
\begin{equation*}
E_{a}-\ob{Z}_{2}Z'_{2}>0\tag*{$(69)'$}
\end{equation*}
The set of complex rectangular matrices $Z_{2}$ of $\ub{a}$ rows and
$\ub{b}$ columns satisfying $(69)'$ is again a bounded symmetric
domain of E.\@ Cartan, of complex dimension $ab$ (Let us recall that
$a+b=sq$). As before, if we take into account all the components in
the representation \eqref{eq56} of $(\ub{\mathscr{F}})$, we are led to a
$g$-fold product of the domain defined by $(69)'$, which is of complex
dimension $\sum\limits^{g}_{k=1} a_{k}b_{k}$.

It is remarkable that in none of the four cases discussed above, we
arrive at the fourth type of E.\@ Cartan's symmetric domains.

The results above may be formulated as

\begin{thm}\label{thm6}
Any $R$-matrix in $(\ub{\mathscr{F}})$ satisfying the conditions
$R^{2}=-E$, $(\mathsf{J}R)=(\mathsf{J}R)'>0$ is of the form \eqref{eq56}
where the component matrices are of the form \eqref{eq59} with
$Z_{0}=H+iG$ belonging to one of the three types of E.\@ Cartan's
bounded symmetric domains mentioned above, the type being determined
by $(\ub{\mathscr{F}})$. 
\end{thm}

In each one of the four cases above, the set of such
$R\in(\ub{\mathscr{F}})$ is non-empty; for example, $R=-\mathsf{J}$ is
always in this set, since $-\mathsf{J}^{2}=E_{hs^{2}}$\pageoriginale
is positive symmetric. In case (iv), if $a_{k}b_{k}=0$ for all $k$,
then $R=-\mathsf{J}$ is the only $R$-matrix occurring in
$(\ub{\mathscr{F}})$ with the normal form \eqref{eq56}.

Let us define $\lambda=h$ in case $\mathscr{V}=\mathfrak{R}$,
$\mathscr{G}$ or $\mathscr{P}$ and $\lambda=g$ in case $\mathscr{V}$
is a cyclic algebra carrying an involution of the second kind. From
the form \eqref{eq56} of elements of $(\ub{\mathscr{F}})$,
$R=[R_{1},\ldots,R_{\lambda}]$ where each component $R_{k}(1\leq k\leq
\lambda)$ occurs with multiplicity $\mu$ equal to $1$ in cases (i) and
(iii) and equal to 2 or $s$ in cases (ii) and (iv) respectively. We
recall that to each $R_{k}(1\leq k\leq \lambda)$ corresponds a Riemann
matrix $\mathscr{P}_{k}$ under the correspondence
$R_{k}=\binom{\mathscr{P}_{k}}{\ob{\mathscr{P}}_{k}}^{-1}L_{0}
\binom{\mathscr{P}_{k}}{\ob{\mathscr{P}_{k}}}$
  where $L_{0}=[-iE_{v}, iE_{v}]$, $2v$ being the numbers of rows of
  $R_{k}$. Now $hs^{2}_{q}$ is an even integer, say $2n$, in all the
  four cases. Let us denote by $L$, the matrix
  $[-iE_{n},iE_{n}]$. Then to the $R$-matrix $R$ in
  $(\ub{\mathscr{F}})$ corresponds the $n$-rowed Riemann matrix
  $\mathscr{P}$ by means of the relation
  $R=\binom{\mathscr{P}}{\ob{\mathscr{P}}}^{-1}L\binom{\mathscr{P}}{\ob{\mathscr{P}}}$. For
  a suitable permutation matrix $V$, we have
  $V^{-1}LV=\underset{\mu\lambda}{L_{0}}$. From this, it is immediate
  that
\begin{equation*}
\mathscr{P}=
\begin{pmatrix}
E_{\mu}\times \mathscr{P}_{1} & 0\ldots & 0\\
0 & E_{\mu}\times \mathscr{P}_{2}\ldots &  0\\
\ldots\ldots & \ldots\ldots & \ldots\ldots\\
0 & \ldots\ldots & E_{\mu}\times\mathscr{P}_{\lambda}
\end{pmatrix}\tag{70}\label{eq70}
\end{equation*}
is a Riemann matrix corresponding to $R$. Each $\mathscr{P}_{k}$ is a
Riemann matrix of $\nu$ rows and $2\nu$ columns with
$\nu=\dfrac{q}{2}$, $q$, $2q$, $sq$ in cases (i), (ii), (iii) and (iv)
respectively. Further each $\mathscr{P}_{k}$ is of the form
\begin{equation*}
\mathscr{P}_{k}=\left(i\dfrac{E_{\nu}+Z_{k}}{E_{\nu}-Z_{k}},E_{\nu}\right)\tag{71}\label{eq71} 
\end{equation*}
where\pageoriginale $Z_{k}=Z'_{k}$ and
$E_{\nu}-\ob{Z}_{k}Z'_{k}>0$. Let us denote by $\mathfrak{H}$, the set
of $\mathscr{P}$ of the form \eqref{eq70} with $\mathscr{P}_{k}$ of the
form \eqref{eq71}, corresponding to all $R$-matrices in
$(\ub{\mathscr{F}})$. As we saw, $\mathfrak{H}$ consists of at least
of one point $\mathscr{P}$ of the form \eqref{eq70} with all
$\mathscr{P}_{k}=(iE_{\nu},E_{\nu})(1\leq k\leq \lambda)$ and consists
exactly of this point when $a_{k}b_{k}=0(1\leq k\leq \lambda)$.

We may now return to the problem of finding $R$-matrices $R$ in\label{p77} 
$(\ub{\mathscr{F}})$ admitting $(\mathscr{M})$ as the {\em exact}
algebra of commutators. For a given $R$-matrix $R$ in
$(\ub{\mathscr{F}})$, let us denote by $(\mathscr{R})$ the algebra of
all real matrices $M$ commuting with $R$. Then clearly,
$(\mathscr{R})\supset (\ub{\mathscr{M}})$. If $R$ were to have at
least one rational commutator not in $(\mathscr{M})$, then the rank of
$(\mathscr{R})$ over $\mathbb{R}$ will be strictly greater than
$hs^{2}$ which is the rank of $(\ub{\mathscr{M}})$ over $\mathbb{R}$,
or what is the same as the rank of $(\mathscr{M})$ over
$\mathbb{Q}$. Thus our problem is to find out $R$-matrices $R$ in
$(\ub{\mathscr{F}})$ for which the corresponding real
commutator-algebra $(\mathscr{R})$ has rank exactly $hs^{2}$ over
$\mathbb{R}$. The advantage in introducing $(\mathscr{R})$ is the
following. Taking first the normal forms given in 
Theorem~\ref{chap1:thm5},
the algebra $(\ub{\mathscr{F}})$ is the commutator-algebra of
$(\ub{\mathscr{M}})$. Let $C$ be a real non-singular matrix such that
the elements of $C^{-1}(\ub{\mathscr{F}})C$ have precisely the normal
form given by \eqref{eq56}. Then
$(\ub{\mathscr{F}})_{1}=C^{-1}(\ub{\mathscr{F}})C$ is the
commutator-algebra of $(\ub{\mathscr{M}})_{1}$. But the rank over
$\mathbb{R}$ of $(\ub{\mathscr{M}})$ and $(\ub{\mathscr{M}})_{1}$ are
the same. Thus, in connection with the problem mentioned at the
beginning of this paragraph, we are free to look among the elements of
$(\mathscr{F})_{1}$ itself, for $R$-matrices for which the
corresponding algebra of all {\em real} commutators has rank exactly
$hs^{2}$ over $\mathbb{R}$. By a ``rational''  commutator of a
$R$-matrix in $(\ub{\mathscr{F}})_{1}$, we shall mean a commutator of
the form $C^{-1}MC$ with rational $M$. The set of
``rational''\pageoriginale commutators is countable. We denote the
algebra $C^{-1}(\mathscr{M})C$ by $(\mathscr{M})_{1}$. 

We know that the equation $RM=MR$ for a $R$-matrix $R$ corresponds,
in terms of the associated Riemann matrix $\mathscr{P}$, to the
equation
\begin{equation*}
\mathscr{P}M=K\mathscr{P}\tag{72}\label{eq72}
\end{equation*}
where $K$ is a complex matrix. Let then, for a
$\mathscr{P}\in\mathfrak{H}$, the equation \eqref{eq72} hold, for a real
$M$. Splitting up $M$ and $K$ as $(M_{kl})$ and $(K_{kl})$
corresponding to the decomposition \eqref{eq70} of $\mathscr{P}$, we
obtain
\begin{equation*}
(E_{\mu}\times \mathscr{P}_{k})M_{kl}=K_{kl}(E_{\mu}\times
  \mathscr{P}_{\ell})\tag{73}\label{eq73} 
\end{equation*}
for $l\leq k$, $1\leq \lambda$. We break up $M_{kl}$ and $K_{kl}$
respectively into $2\nu$-rowed and $\nu$-rowed square matrices
corresponding to the decomposition of $E_{\mu}\times\mathscr{P}_{k}$
and $E_{\mu}\times\mathscr{P}_{1}$ and denoting a typical block by
$M_{0}$ and $K_{0}$ respectively, we have, from \eqref{eq73}
\begin{equation*}
\mathscr{P}_{k}M_{0}=K_{0}\mathscr{P}_{1}\tag{74}\label{eq74}
\end{equation*}
with complex $K_{0}$. Now $\mathscr{P}_{k}$ and $\mathscr{P}_{1}$ are
of the form \eqref{eq71}; splitting up $M_{0}$ as
$\left(\begin{smallmatrix} A & B\\ C & D
\end{smallmatrix}\right)$ with $\nu$-rowed square $A$, we have, from
\eqref{eq74}, 
\begin{align*}
& i\cdot \frac{E_{\nu}+Z_{k}}{E_{\nu}-Z_{k}}A+C=K_{0}\quad i\cdot
  \frac{E_{\nu}+Z_{1}}{E_{\nu}-Z_{1}}\\ 
& i\cdot \frac{E_{\nu}+Z_{k}}{E_{\nu}-Z_{k}}B+D=K_{0}
\end{align*}
Elimination of $K_{0}$ leads to the equations (for $1\leq k$, $l\leq
\lambda$)
\begin{align*}
i(E_{\nu}+Z_{k})A(E_{\nu}-Z_{1})&+(E_{\nu}-Z_{k})C(E_{\nu}-Z_{1})+
(E_{\nu}+Z_{k})B(E_{\nu}\\
&\qquad{}+Z_{1})-i(E_{\nu}-Z_{k})D(E_{\nu}+Z_{1})=0.\tag{75}\label{eq75} 
\end{align*}
Conditions \eqref{eq75} are necessary and sufficient for
$\mathscr{P}\in\mathfrak{H}$ to have $M$ as a multiplier. Referred to
as the ``singular relations they have been studied thoroughly by G.\@
Humbert \cite{10} for $n=2$.

For\pageoriginale any $M\in(\mathscr{M})_{1}$, we know that equations
\eqref{eq75} hold {\em identically} for $\mathscr{P}\in\mathfrak{H}$. If
not $\mathscr{P}\in\mathfrak{H}$ admits a ``rational'' multiplier
$M_{1}$ (\ie if $M_{1}$ is a ``rational'' commutator of the associated
$R$-matrix), then $\mathscr{P}$ necessarily belongs to the quadratic
surface defined in $\mathfrak{H}$ by conditions \eqref{eq75}
corresponding to this $M_{1}$. (Of course, if it turns out that every
$\mathscr{P}\in\mathfrak{H}$ admits this $M_{1}$ as a multiplier, then
this quadratic surface coincides with the whole of
$\mathfrak{H}$). The  number of such surfaces, for $M_{1}\not\in
(\mathscr{M})_{1}$, is, at any rate, countable. The complement of the
union of these countably many surfaces may be seen to be dense in
$\mathfrak{H}$. 

Let us suppose that for {\em all} $\mathscr{P}\in\mathfrak{H}$, a real
matrix $M=(M_{kl})$ is a multiplier. Then, with the same notation as
above, conditions \eqref{eq75} are valid with arbitrary $Z_{k}$, $Z_{l}$
in the ``generalized unit disc'' of degree $\nu$ (such that
$\mathscr{P}\in\mathfrak{H}$). In particular, taking $Z_{k}=0=Z_{1}$,
$\mathscr{P}\in\mathfrak{H}$ and then \eqref{eq75} gives $iA+C+B-iD=0$
\ie $A=D$, $B=-C$. Thus $M_{0}=\left(\begin{smallmatrix} A & B\\ -B &
  A
\end{smallmatrix}\right)$. Now the quadratic terms in \eqref{eq75}
cancel out and we are left with the equation
$$
2iZ_{k}A-2iAZ_{1}+2Z_{k}B+2BZ_{1}=0
$$
Setting $F=A+\ub{iB}$, we have then
\begin{equation*}
Z_{k}\ob{F}=FZ_{1}\tag{76}\label{eq76}
\end{equation*}
We split our further considerations into four parts according as
$\mathscr{V}=\mathfrak{R}$, $\mathscr{G}$, $\mathscr{P}$ or a cyclic
algebra. First, we take up the case
$\underline{\mathscr{V}=\mathfrak{R} \text{ or } \mathscr{G}}$. Here
$\mu=1$ or $2$ respectively. Further, $Z_{k}$, $Z_{l}$ are arbitrary
elements of the ``generalized unit disc'' of degree $\nu$. Taking
$Z_{k}=Z_{l}=tE_{\nu}(0<t<1)$ in \eqref{eq76}, we see that $F=\ob{F}$ or
$B=0$. Now $Z_{k}F=FZ_{l}$\pageoriginale and since $Z_{k}$, $Z_{l}$
are arbitrary elements of the ``generalized unit disc'', we can show
that for $k\neq 1$, we have $A=0$ and for $k=1$, $A=\alpha E_{\nu}$
with arbitrary $\alpha$ in $\mathbb{R}$. Thus when
$\mathscr{V}=\mathfrak{H}$, we see that $M_{kl}=0$ for $k\neq 1$ and
$M_{kk}=\alpha_{k}E_{2\nu}$; with arbitrary real $\alpha$. For
$\mathscr{V}=\mathscr{G}$, again $M_{kl}=0$ for $k\neq 1$, while
$M_{kk}=\left(
\begin{smallmatrix} \alpha_{k}E_{2\nu} &
  k_{\beta_{k}}E_{2\nu}\\ \gamma_{k}E_{2\nu} & \delta_{k}E_{2\nu}
\end{smallmatrix}\right)$ with arbitrary $\alpha_{k}$, $\beta_{k}$,
$\gamma_{k}$, $\delta_{k}$ in $\mathbb{R}$. Thus the rank over
$\mathbb{R}$ of the algebra of real matrices which are multipliers for
{\em every} $\mathscr{P}\in\mathfrak{H}$ is $h$ or
4th according as $\mathscr{V}=\mathfrak{R}$ or $\mathscr{G}$. But the
rank of $(\ub{\mathscr{M}})$ over $\mathbb{R}$ is the same in both
these cases. Thus \eqref{eq75} does not hold identically for
$\mathscr{P}\in\mathfrak{H}$, if $M$ is a ``rational'' multiplier not
in $(\mathscr{M})_{1}$. In fact, if $\mathscr{P}$ does not belong to
the countably many quadratic surfaces in $\mathfrak{H}$ corresponding
to such ``rational'' multipliers not in $(\mathscr{M})_{1}$, then it
has $(\mathscr{M})_{1}$ as its exact algebra of multipliers. Thus {\em
  our problem of finding a $R$-matrix with $(\mathscr{M})$ as exact
  commutator-algebra admits of a solution in these two cases.}

\begin{example*}
If $q=1$ and $\mathscr{V}=\mathbb{Q}$, then any point
$\mathscr{P}=(\tau,1)\in\mathfrak{H}$ (with
$\tau=\alpha+\beta\sqrt{d}$, $\alpha$, $\beta$, $d(<0)$ in
$\mathbb{Q}$) admits all the elements of
$\mathscr{F}=\mathbb{Q}(\sqrt{d})$ as multipliers. Clearly
$\mathscr{F}$ contains $\mathbb{Q}$ properly. Such points $\tau$ are
countably many and constitute a dense set in the complex upper
half-plane; the complement of this set also is dense in the upper half-plane.
\end{example*}

We now take up for consideration the cases (iii) and (iv). In these
two cases, $\lambda=h$ or $g$, $\mu=1$ or $s$, $\nu=2q$ or $sq$
respectively. Further, for $1\leq k\leq \lambda$,
$$
\mathscr{P}_{k}=
\left(i\frac{E_{\nu}+Z_{k}}{E_{\nu}-Z_{k}},E_{\nu}\right)\text{ \ with
  \ } Z_{k}=
\begin{pmatrix}
0 & W_{k}\\
W'_{k} & 0
\end{pmatrix}
$$\pageoriginale
Further in case (iii) $W_{k}=-W'_{k}$ is a $q$-rowed complex matrix
satisfying $E_{q}-\ob{W}_{k}W'_{k}>0$ while, in case (iv), $W_{k}$ is
a complex matrix of $a_{k}$ rows and $b_{k}$ columns such that
$E_{a_{k}}-\ob{W}_{k}W'_{k}>0$ and $a_{k}+b_{k}=sq$. If, in case
(iii), $q=1$, then $W_{k}=0$; in case (iv), if $a_{k}b_{k}=0$, then
$Z_{k}$ is to be taken just as the zero matrix of $sq$ rows and
columns. 

We start from condition \eqref{eq76} and splitting up $F$ as
$\left(\begin{smallmatrix} F_{1} & F_{2}\\ F_{3} & F_{4}
\end{smallmatrix}\right)$ with square $F_{1}$ having the same number
of rows as $W_{1}$, we may rewrite \eqref{eq76} as follows, namely,
\begin{equation*}
\begin{split}
W_{k}\ob{F}_{3} &= F_{2}W'_{1}\\
W_{k}\ob{F}_{4} &= F_{1}W_{1}\\
W'_{k}\ob{F}_{1} &= F_{4}W'_{1}\\
W'_{k}\ob{F}_{2} &= F_{3}W_{1}
\end{split}\tag*{$(76)'$}
\end{equation*}

{\em Let now} $\mathscr{V}=\mathscr{P}$ {\em and} $q\geq 3$. (The
situation when $\mathscr{V}=\mathscr{P}$ and $q=1$ or $2$ is more
complicated and will be dealt with later on). We consider first, for
$k\neq 1$, the equation $W_{k}\ob{F}_{3}=F_{2}W'_{1}$ in $(76)'$. Let
$W_{k}=(u_{\alpha\beta})$, $F_{3}=(a_{\gamma\delta})$,
$F_{2}=(b_{\rho\zeta})$, $W_{1}=(v_{\kappa \omega})$. Comparing the
$(\kappa, \delta)^{\text{th}}$ element on both sides of the matrix
equation, we have
\begin{equation*}
\sum^{q}_{\omega=1}u_{\kappa\omega}\ob{a}_{\omega\delta}=\sum^{q}_{\zeta=1}b_{\kappa\zeta}v_{\delta\zeta}\tag{77}\label{eq77}
\end{equation*}
Here, except for the relations $u_{\kappa\kappa}=0$,
$v_{\kappa\zeta}=-u_{\zeta\kappa}$ and similar relations for the
elements of $W_{1}$, we may regard the elements $v_{\kappa\omega}$ 
of\pageoriginale $W_{k}$ and the elements $v_{\zeta\delta}$ of $W_{1}$
as independent variables. As a consequence, it can be shown that
$F_{2}=0$, $F_{3}=0$, for $k\neq 1$. Similarly using the equation
$W_{k}\ob{F}_{4}=-F_{1}W'_{1}$ in $(76)'$, it can be proved that
$F_{1}=0$, $F_{4}=0$ for $k\neq 1$. Thus, for $k\neq 1$, the matrix
$F$ occurring in \eqref{eq76} is $0$. We may now take up the discussion
of \eqref{eq76} for $\underline{k=1}$. Then we have, in particular, from
$(76)'$
\begin{align*}
W_{k}\ob{F}_{3} &= F_{2}W'_{k}\tag{78}\label{eq78}\\
W_{k}\ob{F}_{4} &= -F_{1}W'_{k}\tag{79}\label{eq79}
\end{align*}
Using the same notation as in \eqref{eq77}, we obtain from \eqref{eq78}
that 
\begin{equation*}
\sum^{q}_{\omega=1}u_{\kappa\omega}\ob{a}_{\omega\delta}=
\sum^{q}_{\zeta=1}b_{\kappa\zeta}u_{\delta\zeta}\tag{80}\label{eq80}
\end{equation*}
We now proceed to show that, for $\varepsilon\neq \delta$,
$a_{\varepsilon\delta}=0$. {\em Since $\underline{\nu=q\geq 3}$,} there
exists $\eta\neq \varepsilon$, $\delta$ such that
$u_{\eta\varepsilon}\not\equiv 0$ and then with $\kappa=\eta$, 
\eqref{eq80} becomes
\begin{equation*}
\sum^{q}_{\omega=1}u_{\eta\omega}\ob{a}_{\omega\delta}-
\sum^{q}_{\zeta=1}b_{\eta\zeta}u_{\delta\zeta}=0\tag{81}\label{eq81} 
\end{equation*}
But the left hand side of \eqref{eq81} is a linear form in the variables
$u_{\alpha\beta}$ with the coefficient of $u_{\eta\varepsilon}$ equal to
$\ob{a}_{\varepsilon\delta}$. Hence $a_{\varepsilon\delta}=0$. Similarly, we
can show that $b_{\eta\zeta}=0$ for $\eta\neq\zeta$. Thus $F_{2}$,
$F_{3}$ are diagonal matrices; using \eqref{eq79}, it would follow again
that $F_{1}$, $F_{4}$ are also diagonal. Now, from \eqref{eq81},
$$
u_{\eta\delta}\left(\ob{a}_{\delta\delta}+b_{\eta\eta}\right)+
\sum^{q}_{\substack{\omega=1\\\omega\neq
\delta}}u_{\eta\omega}\ob{a}_{\omega\delta}-
\sum^{q}_{\substack{\zeta=1\\\zeta\neq
\eta}}b_{\eta\zeta}u_{\delta\zeta}=0 
$$
By the same arguments as above, we have
\begin{equation*}
\ob{a}_{\delta\delta}=-b_{\eta\eta}\tag{82}\label{eq82}
\end{equation*}
Analogous\pageoriginale to \eqref{eq81}, we have
$\sum\limits^{q}_{\omega=1}u_{\eta\omega}\ob{a}_{\omega\varepsilon}-
\sum\limits^{q}_{\zeta=1}b_{\eta\zeta}u_{\varepsilon\zeta}=0$. From  
this we may deduce as above that
\begin{equation*}
\ob{a}_{\varepsilon\varepsilon}=-b_{\eta\eta}\tag{83}\label{eq83}
\end{equation*}
From \eqref{eq82} and \eqref{eq83}, it follows that $F_{3}=a_{11}E_{q}$
and now from \eqref{eq78} we deduce that $F_{2}=-\ob{F}_{3}$. In a
similar manner, we can use \eqref{eq79} to show that
$F_{4}=\ob{F}_{1}=\ob{x}E_{q}$ where $x$ is a complex number. Thus
finally we see that, for $k=1$, the matrix $F$ occurring in \eqref{eq76}
is of the form $\left(\begin{smallmatrix} x & y\\ -\ob{y} & \ob{x}
\end{smallmatrix}\right)\times E_{q}$ where $x$, $y$ are arbitrary
complex. Referring to \eqref{eq72}, if $M$ is a real matrix which is a
multiplier for {\em all} $\mathscr{P}\in\mathfrak{H}$, then
$M=[M_{11},\ldots,M_{kk},\ldots,M_{hh}]$ where each $M_{kk}$ is a real
matrix with 4 independent real parameters. Thus the rank over
$\mathbb{R}$ of the algebra of all real matrices commuting with all
the $R$-matrices in $(\ub{\mathscr{F}})$ is $\underline{4h}$, which is
precisely the rank over $\mathbb{R}$ of $(\ub{\mathscr{M}})$. We may
conclude, as before, that for $\underline{\mathscr{V}=\mathscr{P}}$
and $\underline{q\geq 3}$, {\em there exist $R$-matrices in
  $(\ub{\mathscr{F}})$ admitting $(\mathscr{M})$ as the exact
  commutator-algebra.} 

We now take up case (iv) when $\mathscr{V}$ is a cyclic algebra with
an involution of the second kind and {\em assume} further that
$\underline{qs\geq 3}$ and {\em not all $a_{k}b_{k}$ are equal to
  zero.} We may, without loss of generality, suppose that for $1\leq
k\leq r\leq g$, we have $a_{k}b_{k}>0$. Observe that $a_{k}b_{k}>0$,
$qs\geq 3$ together imply that at least one of $a_{k}$, $b_{k}$ is
greater than $1$. We go back to consider equation \eqref{eq76}. If $k>r$
and $1\leq r$, then we have $FZ_{1}=0$ for all $Z_{1}$ and
consequently $F=0$. Similarly, if $1>r$ and $k\leq r$, we have again
$F=0$ in \eqref{eq76}. Let us now suppose that $1\leq k$, $1\leq
r$. From $(76)'$, we may deduce a relation analogous to
\eqref{eq77}. But now the elements of $W_{k}$ are
independent\pageoriginale complex variables which are again
independent of the elements of $W_{1}$ (for $1\neq k$). Further, at
least one of $a_{k}$, $b_{k}$ is greater than or equal to $2$ and
similarly for $a_{1}$, $b_{1}$. It is easy to deduce as before that
for $k\neq 1$, $1\leq k$, $1\leq r$, we have $F=0$, while for $1\leq
k=1\leq r$, we see that $F=\left(\begin{smallmatrix} x & 0\\ 0 &
  \ob{x}
\end{smallmatrix}\right)\times E_{\nu}$ where $x$ is arbitrary
complex. Thus, referring to \eqref{eq73}, $M_{kk}$ is a real matrix with
$2s^{2}$ real parameters. We may then conclude that any real matrix
$M$ which is a multiplier for all Riemann matrices
$\mathscr{P}\in\mathfrak{H}$ is necessarily of the form
\begin{equation*}
M=[M_{11},\ldots,M_{rr},N]\tag{84}\label{eq84}
\end{equation*}
where $M_{11},\ldots,M_{rr}$ are real matrices with $2s^{2}$
independent real parameters and $N$ is a $2(g-r)s^{2}q$-rowed real
square matrix.

If $r=g$, in other words, $a_{k}b_{k}>0$ for $1\leq k\leq g$ and
$qs\geq 3$, then the rank over $\mathbb{R}$ of the algebra of all such
matrices $R$ is $2gs^{2}=hs^{2}$ which is precisely the rank over
$\mathbb{R}$ of $(\mathscr{M})$. Thus in the case when $\mathscr{V}$
is a cyclic algebra with an involution of the second kind and further
$qs\geq 3$, $a_{k}b_{k}>0(1\leq k\leq g)$, we see that {\em there
  exist $R$-matrices with $(\mathscr{M})$ as the complete
  commutator-algebra.}

If $1\leq r<g$, we know nothing about the nature of the matrix $N$
appearing in \eqref{eq84}. Therefore, before we proceed to discuss the
case when $qs\geq 3$ and $1\leq r<g$, we need to prove

\begin{lem}\label{chap1:lem3}
Let $\mathscr{L}$ be an algebraic numberfield of degree $g$ over
$\mathbb{Q}$, with $\omega_{1},\ldots,\omega_{g}$ as a basis over
$\mathbb{Q}$ and let $\Omega$ stand for the $g$-rowed square matrix
$(\omega^{(1)}_{k})$ $(1\leq k,l\leq h)$. Further, let $Q$ be a
$g$-rowed square matrix of the form $\left(\begin{smallmatrix} a &
  \ast\\ 0 & B\end{smallmatrix}\right)$ with a complex number $a$ and
  let $\Omega Q\Omega^{-1}$ be rational. Then, necessarily
  $a\in\mathscr{Z}$ and furthermore, $Q=[a^{(1)},\ldots,a^{(g)}]$.
\end{lem}

\begin{proof}
Let\pageoriginale $\Omega Q\Omega^{-1}=(p_{kl})(1\leq k,l\leq g)$. Then $\Omega
Q=(p_{kl})\Omega$ and, in particular, we have
$\omega_{k}a=\sum\limits^{g}_{1=1}p_{kl}\omega_{1}(p_{kl}\in\mathbb{Q})$. Hence
$a=\sum\limits^{g}_{1=1}p_{11}\dfrac{\omega_{1}}{\omega_{1}}\in\mathscr{L}$
(since $\omega_{1}\neq 0$) and $a^{(1)}=a$, $a^{(2)},\ldots,a^{(g)}$
are its conjugates over $\mathbb{Q}$. But we know that
$\Omega[a^{(1)},\ldots,a^{(g)}]=(p_{kl})\Omega$ and therefore
$Q=[a^{(1)},\ldots,a^{(g)}]$. 
\end{proof}

\begin{remark*}
We shall use, in the sequel, a generalization of Lemma \ref{chap1:lem3} (the
proof of which is exactly on the same lines) namely, the
following. 
\end{remark*}

{\em Let $d\geq 1$ be a rational integer and $\mathscr{L}$, $\Omega$
as in the hypothesis of Lemma \ref{chap1:lem3}. Let
$Q=\left(\begin{smallmatrix} H & \ast\\ Q & \ast
\end{smallmatrix}\right)$ with a $d$-rowed complex square matrix $H$
and let $(\Omega\times E_{d})Q(\Omega\times E_{d})^{-1}$ be
rational. Then necessarily, the elements of $H$ are in $\mathscr{Z}$
and further, $Q=[H^{(1)},\ldots,H^{(g)}]$.}

We may proceed now to discuss case (iv) with the assumption that
$qs\geq 3$, $1\leq r<g$. In order that the application of the
above-mentioned generalization of Lemma \ref{chap1:lem3} be feasible, we do
not go right up to the eventual normal form \eqref{eq56} of
$(\ub{\mathscr{F}})$ but we stop short somewhat earlier. So let us
start de novo. From the representation $\delta\to D_{0}$ of
$\mathscr{V}$ over $\mathfrak{R}$ given on p.~\pageref{eq51}, we first get a
representation $\delta\to D_{1}$ of $\mathscr{V}$ over $\mathscr{L}$
as follows; namely, if $(1,\sqrt{c})$ is a basis of $\mathfrak{R}$
over $\mathscr{Z}$, then denoting the matrix
$\left(\begin{smallmatrix} 1 & 1\\ \sqrt{c} & -\sqrt{c}
\end{smallmatrix}\right)$ by $\Omega_{1}$, we define $D_{1}$ by
$D_{1}=(\Omega_{1}\times E_{s^2})[D_{0},\ob{D}_{0}](\Omega_{1}\times
E_{s^2})^{-1}$. Now, we can get from this a rational representation
$\delta\to \ub{D}$ of $\mathscr{V}$ by the prescription
$$
\delta\to
\ub{D}=C_{1}\left[\underset{q}{D^{(1)}_{1}},\ldots,
\underset{q}{D^{(g)}_{1}}\right]C^{-1}_{1}
$$
where $C_{1}=\Omega^{\ast}_{1}\times E_{2s^2q}$,
$\Omega^{\ast}_{1}=(\delta^{(1)}_{k})$, $\delta_{1},\ldots,\delta_{g}$
is a basis of\pageoriginale $\mathscr{L}$ over $\mathbb{Q}$ and
$D^{(1)}_{1},\ldots,D^{(g)}_{1}$ are the conjugates of $D_{1}$ over
$\mathbb{Q}$. Thus the elements of the algebra
$(\mathscr{M})_{2}=C^{-1}_{1}(\mathscr{M})C_{1}$ are of the form
$[D^{(1)}_{1},\ldots,D^{(g)}_{1}]$ where $D_{1}$ is a $2s^{2}q$-rowed
square matrix with elements in $\mathscr{L}$. Defining the algebra
$(\ub{\mathscr{F}})_{2}$ by
$(\ub{\mathscr{F}})_{2}=C^{-1}_{1}(\ub{\mathscr{F}})C_{1}$, we shall
look for $R$-matrices in $(\ub{\mathscr{F}})_{2}$ with the required
properties. Applying to $(\ub{\mathscr{F}})_{2}$ the procedure given
earlier to reduce $(\ub{\mathscr{F}})$ to the normal form \eqref{eq56},
we remark that this merely involves going over to the representation
$C^{-1}_{2}(\ub{\mathscr{F}})_{2}C_{2}$ (where
$C_{2}=[C_{2,1},\ldots,C_{2,g}]$ with $2s^{2}q$-rowed real square
matrices $C_{2,k}$). Let $M_{0}$ be a {\em rational} matrix commuting
with all $R$-matrices in $(\ub{\mathscr{F}})$. Then
$M=C^{-1}_{2}C^{-1}_{1}M_{0}C_{1}C_{2}$ commutes with all the
corresponding $R$-matrices in
$C^{-1}_{2}(\ub{\mathscr{F}})_{2}C_{2}$. Since $r\geq 1$, it follows,
by using the same arguments as for the case $r=g$ above, that $M$ and
hence $M_{2}=C_{2}MC^{-1}_{2}$ is of the form \eqref{eq84}. As yet we
know nothing about the number of parameters involved in $N$. But now
$M_{0}=C_{1}M_{2}C^{-1}_{1}$ is rational and appealing to our Remark
on p.\@ \eqref{eq85}, we conclude that $M_{11}$ has elements in
$\mathscr{Z}$ and further $M_{2}$ itself is of the form
\begin{equation*}
M_{2}=[M^{(1)}_{11},\ldots,M^{(g)}_{11}]\tag{85}\label{eq85}
\end{equation*}
If we take $M_{0}$ to be real instead of being rational, and further
if $M_{0}$ commutes with all the $R$-matrices in $(\ub{\mathscr{F}})$
we see, by arguments as in the case $r=g$, that $C_{1}M_{0}C^{-1}_{1}$
is of the form \eqref{eq84} again, with each $M_{kk}(1\leq k\leq r)$
having $2s^{2}$ independent real parameters. Now such $M_{11}$
constitute precisely the linear closure of the matrices $M^{(1)}_{11}$
occurring in \eqref{eq85} with elements in $\mathscr{Z}$. Thus the
matrices $M^{(1)}_{11}$ in \eqref{eq85} form\pageoriginale an algebra of
rank $2s^{2}$ over $\mathscr{Z}$. Let indeed then
$F_{1},\ldots,F_{2s^{2}}$ be a basis of this algebra so that every
such $M^{(1)}_{11}=\sum\limits^{2s^{2}}_{k=1}x_{k}F_{k}$ (with
$x_{k}\in\mathscr{Z}$). Hence, in \eqref{eq85},
$M^{(1)}_{11}=\sum\limits^{2s^{2}}_{k=1}x^{(1)}_{k}F^{(1)}_{k}$. This
enables us to conclude that if $M_{0}$ is a real commutator of all
$R$-matrices in $(\ub{\mathscr{F}})$, then, by virtue of $M_{0}$ lying
in the linear closure of rational commutators of the same kind,
$C_{1}M_{0}C^{-1}_{1}=[M_{11},\ldots,M_{gg}]$ where
$M_{kk}=\sum\limits^{2s^{2}}_{l=1} x_{l}$, $F^{(k)}_{l}$ and
$x_{1},\ldots,x_{2s^{2}}$ are arbitrary real numbers. In other words,
the rank of the algebra of real commutators of {\em all} $R$-matrices
in $(\mathscr{F})$ is $2gs^{2}=hs^{2}$. Thus {\em for $qs\geq 3$,
  $1\leq r\leq g$, there do exist $R$-matrices with $(\mathscr{M})$ as
  the complete commutator algebra.}

We shall now prove that for $qs\geq 2$ and $r=0$ (\ie $a_{k}b_{k}=0$
for all $k$), {\em there cannot exist $R$-matrices with
  $(\mathscr{M})$ as the complete commutator-algebra.} Since
$a_{k}b_{k}=0$ for $1\leq k\leq g$, the only $R$-matrix in
$(\ub{\mathscr{F}})_{1}=C^{-1}(\ub{\mathscr{F}})C$ (referring to the
notation on p.~\pageref{p77}) is
$-\mathsf{J}=[\underset{g \text{ times}}{-\mathsf{J}_{0},\ldots,-\mathsf{J}_{0}}]$. Now the
matrices $P_{k}$ occurring in case (iv) in \eqref{eq56} are all $\pm
E_{sq}$ and therefore all the matrices in $(\ub{\mathscr{F}})_{1}$
commute with $-\mathsf{J}$. Thus all elements of $(\ub{\mathscr{F}})$
commute with the $R$-matrix $R=-C\mathsf{J}C^{-1}$; in particular,
every element of $({\mathscr{F}})$ commutes with $R$. If now $R$ were
to have $(\mathscr{M})$ as its exact commutator algebra, then
$(\mathscr{F})\subset (\mathscr{M})$, necessarily. But, by Proposition
6, $(\mathscr{F})\cap (\mathscr{M})=(\mathfrak{R})$. Hence
$(\mathscr{F})=(\mathfrak{R})$. Now, if $qs\geq 2$, then either $q>1$
or $s>1$. If $q>1$, then $(\mathscr{F})$ is the $q$-rowed
matrix-algebra over the commutator algebra\pageoriginale
$(\mathscr{V})^{\ast}$ of 
$(\mathscr{V})$ and therefore it is not commutative. But then
$(\mathscr{F})=(\mathfrak{R})$ gives a contradiction. Again, if $q=1$
and $s>1$, then $(\mathscr{V})$ is non-commutative and so is
$(\mathscr{F})=(\mathscr{V})^{\ast}$, which contradicts
$(\mathscr{F})=(\mathfrak{R})$. Thus our assertion above is proved.

The exceptional cases which remain to be considered are the following,
namely a)~$\mathscr{V}=\mathscr{P}$, $q=1$ or $2$ and
b)~$\mathscr{V}$, a cyclic algebra with an involution of the second
kind with $qs=1$ or with $qs=2$ and not all $a_{k}b_{k}$ equal to
zero. We shall slightly reformulate our problem of finding
$R$-matrices in $(\ub{\mathscr{F}})$ for which 1)~$AR=S=S'>0$ where
$A=\underset{q}{G}T_{0}$ with $T_{0}=-\widetilde{T}_{0}$ in
$(\mathscr{F})$, 2)~$R^{2}=-E$ and 3)~$(\mathscr{M})$ is the complete
commutator-algebra. Let us set $N=T_{0}R$. Then barring the last
condition, in terms of $N$, these conditions are merely that
1)~$N\in(\ub{\mathscr{F}})$ 
2)~$N=\underset{q}{G^{-1}}N'\underset{q}{G}=L^{-1}\underset{q}{F^{-1}}
\underset{q}{F}L=(\underset{q}{F^{-1}}N'\underset{q}{F})L^{-1}L=\widetilde{N}$  
and 3)~$(T^{-1}_{0}N)^{2}=-E$. On the other hand, by Lemma~\ref{chap1:lem2},
$S=\underset{q}{G}N>0$ and $\underset{q}{G}>0$ together imply that all
the eigenvalues of $N$ are real and positive. Thus our problem reduces
to finding $N\in(\ub{\mathscr{F}})$ for which 
\begin{equation*}
N=\widetilde{N},
T^{-1}_{0}N \; T^{-1}_{0}N=-E; \mbox{ the eigenvalues of } N \mbox{ are real and
positive }\tag{86}.\label{eq86}
\end{equation*}

We shall first take up the case when
$\underline{\mathscr{V}=\mathscr{P}, q=1}$. Choosing for
$(\mathscr{M})$, the 4-rowed representation of the opposite algebra
$\mathscr{P}^{\ast}$ of $\mathscr{P}$ without loss of generality, we
may suppose that $(\mathscr{F})$ is the $4$-rowed representation
$\delta\to D$ of $\mathscr{P}$ over $\mathfrak{R}$, given on p.~\pageref{p63}. We
observe that the involution $T\to\widetilde{T}$ in 
$(\mathscr{F})$ is a positive involution since
$\sigma(T\widetilde{T})= \sigma(T\underset{q}{F^{-1}}T'\underset{q}{F})$ is a
positive definite form over the centre $\mathfrak{R}$ in view of $F$
being positive definite. But now we know that the abstract
totally\pageoriginale definite quaternion algebra $\mathscr{P}$ has a
unique positive involution, viz. $\delta=x+yi+zj+tk\to
\widetilde{\delta}=x-yi-zj-tk$. Thus, for $N\in(\ub{\mathscr{F}})$,
$N=\widetilde{N}$ implies that $N$ is in the center of
$(\ub{\mathscr{F}})$. This gives $R=T^{-1}_{0}N=NT^{-1}_{0}$ \ie
$T_{0}R=N=RT_{0}$. Now suppose that there exists a $R$-matrix $R$ in
$(\ub{\mathscr{F}})$ for which $(\mathscr{M})$ is the exact
commutator-algebra. Then $T_{0}$ being a rational commutator of $R$,
$T_{0}\in(\mathscr{M})$. Since $T_{0}\in(\mathscr{F})$ too, we have
$T_{0}\in(\mathscr{F})\cap (\mathscr{M})=(\mathfrak{R})$. This gives
us $T_{0}=\widetilde{T}_{0}$ but the we have a contradiction to
$T_{0}=-\widetilde{T}_{0}$ from which we started. We may thus conclude
that in the case $\underline{\mathscr{V}=\mathscr{P}, q=1}$, 
{\em there cannot exist $R$-matrices with $(\mathscr{M})$ as the exact
  commutator-algebra.} 

Next we consider the case when $\underline{\mathscr{V}=\mathscr{P}}$
and $\underline{q=2}$. By choosing a suitable representation
$(\mathscr{P})$ of $\mathscr{P}$, we can suppose, without loss of
generality, that for the elements $\delta$ of the commutator-algebra
$(\widehat{\mathscr{P}})$ of $(\mathscr{P})$, we already have the
representation $\delta\to [D^{(1)},\ldots,D^{(h)}]$ over the centre
$\mathfrak{R}$ and its conjugates over $\mathbb{Q}$, as indicated on
p.~\pageref{p63}. Now $T_{0}=\left(\begin{smallmatrix} \alpha_{1} &
  \beta_{1}\\ \gamma_{1} & \delta_{1}
\end{smallmatrix}\right)=-\widetilde{T}_{0}$ with $\alpha_{1}$,
$\beta_{1}$, $\gamma_{1}$, $\delta_{1}$ in
$(\widehat{\mathscr{P}})$. We first remark that there exists a
$2$-rowed non-singular matrix $W$ with elements in
$(\widehat{\mathscr{P}})$ such that
$\widetilde{W}T_{0}W=\left(\begin{smallmatrix} \alpha_{2} & 0\\ 0 &
  \beta_{2}\end{smallmatrix}\right)$ with
$\alpha_{2}=-\widetilde{\alpha}_{2}$,
$\beta_{2}=-\widetilde{\beta}_{2}$ in $(\widehat{\beta})$. If now for
some $p$ in $(\mathfrak{R})$, we have $\beta_{2}=p\alpha_{2}$, then we
can easily find $\lambda\neq 0$ in $(\widehat{\mathscr{P}})$ such that
$\widetilde{\lambda}\beta_{2}\lambda$ does not commute with
$\alpha_{2}$. Therefore, choosing, for example,
$W\left(\begin{smallmatrix} 1 & 0\\ 0 & \lambda
\end{smallmatrix}\right)$ instead of $W$, we could suppose that for no
element $p$ in $(\mathfrak{R})$ do we have
$\beta_{2}=p\alpha_{2}$. The matrix $N\in(\ub{\mathscr{F}})$ has the
properties mentioned in \eqref{eq86}. Now it is trivial to see that
$\widetilde{W}NW$ is again symmetric under the\pageoriginale
involution in $(\ub{\mathscr{F}})$. Moreover, from \eqref{eq86}, we
have, in view of Lemma~\ref{chap1:lem2}, that $\underset{2}{F}N$ is
symmetric and positive-definite. Hence
$W'\underset{2}FNW=\underset{2}{F}\widetilde{W}NW$ is again symmetric
and positive-definite. By Lemma~\ref{chap1:lem2} again, $\widetilde{W}NW$
has its eigen-values real and positive. Thus taking $W^{-1}RW$,
$\widetilde{W}NW$ and $\widetilde{W}T_{0}W$ instead of $R$, $N$ and
$T_{0}$ respectively we could suppose from the beginning that
$T^{-1}_{0}=\left(\begin{smallmatrix} \alpha & 0\\ 0 & \beta
\end{smallmatrix}\right)$ with $\alpha=-\widetilde{\alpha}$,
$\beta=-\widetilde{\beta}$ in $(\widehat{\mathscr{P}})$ and $N=\left(
\begin{smallmatrix}x & \omega\\ \widetilde{\omega} & y
\end{smallmatrix}\right)$ has the properties mentioned in
\eqref{eq86}. Denoting by $(\ub{\mathfrak{R}})$ the centre of
$(\ub{\widehat{\mathscr{P}}})$, we see that
\begin{equation*}
\begin{split}
x &= [\underset{4}{x_{1}},\ldots,\underset{4}{x_{h}}]>0,
y=[\underset{4}{y_{1}},\ldots,\underset{4}{y_{h}}]>0\text{ \ are in
  \ } (\ub{\mathfrak{R}})\\
\omega &= \widetilde{\omega}\in (\ub{\widehat{\mathscr{P}}}), xy
-\omega\widetilde{\omega}>0. 
\end{split}\tag{87}\label{eq87}
\end{equation*}
(The last assertion in \eqref{eq87} is a consequence of the relation 
$$
N=
\begin{pmatrix}
1 & 0\\
x^{-1}\widetilde{\omega} & 1
\end{pmatrix}
\begin{pmatrix}
x & 0\\
0 & y-x^{-1}\widetilde{\omega}\omega
\end{pmatrix}
\begin{pmatrix}
1 & x^{-1}\omega\\
0 & 1
\end{pmatrix}.
$$
Now $R=T^{-1}_{0}N=\left(\begin{smallmatrix} \xi & \eta\\ \zeta & \tau
\end{smallmatrix}\right)$ where $\xi=x\alpha$, $\eta=\alpha \omega$,
$\zeta=\beta\widetilde{\omega}$ and $\tau=y\beta$. The condition
$R^{2}=-E$ may be written as
\begin{equation*}
\begin{split}
& -cx^{2}+\alpha\omega\beta\widetilde{\omega}=-1,
  cx\omega=y\alpha\omega\beta\\
& \beta\widetilde{\omega}\alpha\omega-dy^{2}=-1, dy\widetilde{\omega}=\alpha\beta\widetilde{\omega}\alpha
\end{split}\tag{88}\label{eq88}
\end{equation*}
where $\alpha^{2}=-\alpha\widetilde{\alpha} = -c$,
$\beta^{2}=-\beta\widetilde{\beta}=-d$ with
$c=[\underset{4}{c_{1}},\ldots,\underset{4}{c_{h}}]>0$,
$d=[\underset{4}{d_{1}},\ldots,\underset{4}{d_{h}}]>0$ in
$(\mathfrak{R})$. Writing $x=py$, with $p\in(\mathfrak{R})$, we obtain
from \eqref{eq88},
$$
c(x^{2}-p\omega\widetilde{\omega})=1=d\left(y^{2}-\dfrac{\omega\widetilde{\omega}}{p}\right)
$$
leading to $p^{2}=dc^{-1}$. Thus $p=xy^{-1}$ is the positive square
root of\pageoriginale $dc^{-1}$; \ie
$p_{k}=\left|\sqrt{\dfrac{d^{(k)}}{c^{(k)}}}\right|$. Our problem on
$R$-matrices now reduces to finding $x>0$ in $(\ub{\mathfrak{R}})$ and
$\omega\in(\widehat{\mathscr{P}})$ for which
\begin{gather*}
\alpha\omega\beta=cp\omega\tag{89}\label{eq89}\\
cx^{2}-cp\omega\widetilde{\omega}=1\tag{90}\label{eq90}\\
p^{-1}x^{2}-\widetilde{\omega}\omega>0
\end{gather*}
As a particular solution of \eqref{eq89}, we have
$\omega_{0}=p\alpha-\beta$ (observe that $\omega_{0}\neq 0$). The most
general solution of \eqref{eq89} is given by $\omega=t\omega_{0}$ where
$t\in(\widehat{\mathscr{P}})$ and $t\alpha=\alpha t$. Clearly
$t=u+v\alpha$ with
$u=[\underset{4}{u_{1}},\ldots,\underset{4}{u_{h}}]$,
$v=[\underset{4}{v_{1}},\ldots,\underset{4}{v_{h}}]$ in
$(\ub{\mathfrak{R}})$. Now, the first condition in \eqref{eq90} may be
written as
\begin{equation*}
cx^{2}-cp\omega_{0}\widetilde{\omega}_{0}(u^{2}+cv^{2})=1\tag*{$(90)'$}
\end{equation*}
Equation $(90)'$ defines a ``two-sheeted hyperboloid'' in the $x$,
$u$, $v$-space; the $2h$ components of $u$ and $v$ are independent
real parameters while the $h$ components of $x$ are linearly
independent of the components of $u$, $v$ although quadratically
related to them. We finally arrive at the following parameterization
for the $R$-matrix $R$, namely
\begin{equation*}
R=
\begin{pmatrix}
py\alpha & \alpha(u+v\alpha)\omega_{0}\\
\beta\widetilde{\omega}_{0}(u-v\alpha) & y\beta
\end{pmatrix}\tag{91}\label{eq91}
\end{equation*}
where
$cp^{2}y^{2}-cp\omega_{0}\widetilde{\omega}_{0}(u^{2}+cv^{2})=1$.

Let $\gamma_{1},\ldots, \gamma_{h}$ be a basis of $\mathfrak{R}$ over
$\mathbb{Q}$ and let $\Omega=(\gamma^{(1)}_{k})$ with $1\leq k$,
$1\leq h$. For $\delta\in(\widehat{\mathscr{P}})$, we took the
rational representation $(\Omega\times
E_{4})[D^{(1)},\ldots,D^{(h)}](\Omega\times E_{4})^{-1}$. Let, under
this representation, $\alpha\to(\Omega\times
E_{4})[A^{(1)},\ldots,A^{(h)}](\Omega\times E_{4})^{-1}$ and 
$\beta\to (\Omega\times E_{4})[B^{(1)},\ldots,B^{(h)}](\Omega\times
E_{4})^{-1}$.\pageoriginale It is trivial to verify that $R=(\Omega\times
E_{8})[R_{1},\ldots,R_{h}](\Omega\times E_{8})^{-1}$ where
{\fontsize{10}{12}\selectfont
$$
R_{k}=
\begin{pmatrix}
p_{k}y_{k}A^{(k)} &
A^{(k)}(u_{k}E_{4}+v_{k}A^{(k)})(p_{k}A^{(k)}-B^{(k)})\\
B^{(k)}(p_{k}A^{(k)}-B^{(k)})(v_{k}A^{(k)}-u_{k}E_{4}) & y_{k}B^{(k)}
\end{pmatrix}
$$}\relax
for $1\leq k\leq h$. Let now $M_{0}$ be any $8h$-rowed rational matrix
commuting will all $R$-matrices in $(\ub{\mathscr{F}})$. Then
$M=(\Omega\times E_{8})^{-1}M_{0}(\Omega\times E_{8})$ has to commute
with $[R_{1},\ldots,R_{h}]$ and moreover $(\Omega\times
E_{8})M(\Omega\times E_{8})^{-1}$ has to be rational. From the mutual
independence of the parameters $u_{k}$, $v_{k}$, $y_{k}$ and $u_{1}$,
$v_{1}$, $y_{1}$ (for $k\neq 1$), it is clear that
$M=[M_{1},\ldots,M_{h}]$ and further by our Lemma, $M_{1}$ has
elements in $\mathfrak{R}$ while $M_{k}=M^{(k)}_{1}$ for $1\leq k\leq
h$. In addition $M_{k}$ commutes with $R_{k}$. We proceed to determine
the structure of $M_{1}$, writing $M_{1}=\left(\begin{smallmatrix}
  \rho &\kappa\\ \mu & \nu
\end{smallmatrix}\right)$ with $4$-rowed square matrices with elements
in $\mathfrak{R}$. For the sake of brevity in notation, let us for the
present, agree to understand by $\alpha$, $\beta$ the corresponding
matrices $A^{(1)}$, $B^{(1)}$ and further let us omit the subscript in
$p_{1}$, $y_{1}$, $u_{1}$, $v_{1}$. Then we see that $M_{1}$ has to
commute necessarily with the matrix $\left(\begin{smallmatrix}
  py\alpha \;\; \alpha(u+v\alpha) & \omega_{0}\\ \beta\omega_{0}(v\alpha-u)
  &  y\beta\end{smallmatrix}\right)$. Equivalently $M_{1}$ has to
  commute with $\left(\begin{smallmatrix} p\alpha & 0\\ 0 & \beta
  \end{smallmatrix}\right)$,
$$
\begin{pmatrix}
0 & \alpha\omega_{0}\\
\beta\widetilde{\omega}_{0} & 0
\end{pmatrix}
\text{ \  and \ }
\begin{pmatrix}
0 & -c\omega_{0}\\
-\beta\widetilde{\omega}_{0}\alpha & 0
\end{pmatrix}
=-c
\begin{pmatrix}
0 & \omega_{0}\\
p\widetilde{\omega}_{0} & 0
\end{pmatrix}.
$$
The last matrix is the product of the first two upto a scalar
factor. Hence it suffices to require $M_{1}$ to commute with the two
matrices $\left(\begin{smallmatrix} p\alpha & 0 \\ 0 & \beta
\end{smallmatrix}\right)$ and $\left(\begin{smallmatrix} 0 &
  \alpha\omega_{0}\\ \beta\widetilde{\omega}_{0} & 0
\end{smallmatrix}\right)$. We have now to distinguish between three
cases. 
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item $\underline{p_{k}\not\in \mathfrak{R}^{(k)}}$\pageoriginale {\em
  for some $k$, 
  (say $k=1$).} Since $1$, $p$ are linearly independent over
  $\mathfrak{R}$, it is clear that $M_{1}$ has, of necessity, to
  commute with $\left(\begin{smallmatrix} \alpha & 0\\ 0 & 0
  \end{smallmatrix}\right)$, $\left(\begin{smallmatrix}  0 & 0\\ 0 &
    \beta
  \end{smallmatrix}\right)$ and $\left(\begin{smallmatrix} 0 &
    \alpha\omega_{0}\\ \beta\widetilde{\omega}_{0} & 0
  \end{smallmatrix}\right)$. It follows immediately that
  $M_{1}=\left(\begin{smallmatrix} \rho & 0\\ 0 & \rho
  \end{smallmatrix}\right)$ where $\rho$ commutes with $\alpha$ and
  $\beta$ and hence with all elements of
  $(\widehat{\mathscr{P}})$. Thus $\rho$ is in $(\mathscr{P})$. Since
  $M_{k}=M_{1}^{(k)}(1\leq k\leq h)$, we see that the rank over
  $\mathbb{Q}$ of the algebra of all rational matrices $M_{0}$
  commuting with all $R$-matrices in $(\ub{\mathscr{F}})$ in this case
  is $4h$ which is the same as the rank of $(\mathscr{M})$ over
  $\mathbb{Q}$. Thus, in this case, there exist $R$-matrices admitting
  $(\mathscr{M})$ as the exact commutator-algebra.

\item $\underline{p\not\in (\mathfrak{R})}$ {\em but}
  $\underline{p_{k}=|\tau^{(k)}|(1\leq k\leq h)}$ {\em for}
  $\underline{\tau\in \mathfrak{R}}$.

As in case (i), we know that $M_{1}$ has to commute with
$\left(\begin{smallmatrix} p\alpha & 0\\ 0 & \beta
\end{smallmatrix}\right)$ and $\left(\begin{smallmatrix} 0 &
  \alpha\omega_{0}\\ \beta\widetilde{\omega} & 0
\end{smallmatrix}\right)$. Further, for at least one $k(1\leq k\leq
g)$, $p_{k}=\tau^{(k)}$ and for some $l(1\leq l\leq g)$,
$p_{l}=-\tau^{(l)}$. Thus, from the fact that $M_{k}R_{k}=R_{k}M_{k}$
and $M_{k}=M_{l}^{(k)}$, we see that $M_{l}$ has to commute with 
{\fontsize{10}{12}\selectfont
$$
\begin{pmatrix}
p\alpha &  0\\
0 & \beta
\end{pmatrix},
\begin{pmatrix}
-p\alpha & 0\\
0 & \beta
\end{pmatrix},
\begin{pmatrix}
0 & \alpha\omega_{0}\\
\beta\widetilde{\omega}_{0} & 0
\end{pmatrix}\text{ \  and \ }
\begin{pmatrix}
0 & \alpha(-p\alpha-\beta)\\
\beta(p\alpha+\beta) & 0
\end{pmatrix}.
$$}\relax
Thus, again $M_{l}$ commutes with 
$$
\begin{pmatrix} 
\alpha &  0\\ 
0 & 0
\end{pmatrix},
\begin{pmatrix}
0 & 0\\
0 & \beta
\end{pmatrix},
\begin{pmatrix}
0 & \alpha\beta\\
d & 0
\end{pmatrix}
\text{ \  and \ }
\begin{pmatrix}
0 & -c\\
\beta\alpha & 0
\end{pmatrix}
$$
and therefore $M_{l}$ has to be of the same form as in case (i)
above. We conclude as above that there exist $R$-matrices admitting
$(\mathscr{M})$ as the exact algebra of commutators.

\item $\underline{p\in(\mathfrak{R})}$. In this case $M_{l}$ commutes
  with $P=\left(\begin{smallmatrix} p\alpha & 0\\ 0 & \beta
  \end{smallmatrix}\right)$\pageoriginale and
  $Q=\left(\begin{smallmatrix} 0 &
    \alpha\omega_{0}\\ \beta\widetilde{\omega}_{0} & 0
  \end{smallmatrix}\right)$, as before. But since $P^{2}=-dE_{8}$,
  $Q^{2}=cp\omega_{0}\widetilde{\omega}_{0}E_{8}$, $QP=-PQ$, we see
  that $E_{8}$, $P$, $Q$ generate an abstract quaternion algebra over
  $\mathfrak{R}$ and this $8$-rowed representation contains its
  irreducible representation over $\mathfrak{R}$ exactly twice. Since
  $M_{1}$ commutes with $P$, $Q$ it follows that the rank over
  $\mathfrak{R}$ of the algebra formed by the matrices $M_{l}$ is
  $16$. Thus, in this case, the rank over $\mathbb{Q}$ of the algebra
  of rational matrices commuting with all the $R$-matrices in
  $(\ub{\widehat{\mathscr{P}}})$ is $16h$ which is greater than $4h$,
  the rank of $(\mathscr{M})$ over $\mathbb{Q}$. In other words, there
  do not exist $R$-matrices in $(\ub{\widehat{\mathscr{P}}})$ with
  $(\mathscr{M})$ as the exact algebra of commutators, in this case.
\end{enumerate}

For $\gamma\in(\widehat{\mathscr{P}})$, define the ``reduced norm''
$N_{\mathfrak{R}}(\gamma)$ of $\gamma$ over $\mathfrak{R}$ by
$N_{\mathfrak{R}}(\gamma)=\gamma\widetilde{\gamma}$ (which certainly
belongs to $\mathfrak{R}$) and for $T_{0}=\left(\begin{smallmatrix}
  \alpha^{-1} & 0\\ 0 & \beta^{-1}\end{smallmatrix}\right)$ define the
``reduced norm'' $N_{\mathfrak{R}}(T_{0})$ of $T_{0}$ by
$N_{\mathfrak{R}}(T_{0})=N_{\mathfrak{R}}(\alpha^{-1}\beta^{-1})$. It
is then clear that
$N_{\mathfrak{R}}\left(\dfrac{\alpha}{\beta}\right)$ and
$N_{\mathfrak{R}}(T_{0})$ are the same upto the square of a totally
positive number in $\mathfrak{R}$. We conclude thus, that in the case
when $\mathscr{V}=\mathscr{P}$, $q=2$, {\em there exist $R$-matrices
  with $\binom{\mathscr{P}}{2}$ as exact commutator-algebra except
  when}
$$
N_{\mathfrak{R}}(T_{0})=\tau^{2}\quad\text{for}\quad
\tau>0\quad\text{in}\quad \mathfrak{R}. 
$$

The next case for discussion is when $\mathscr{V}$ is a cyclic algebra
of type (iv) with $qs=1$ i.e.\@ $q=s=1$. Then $\mathscr{V}$ is the
same as its centre $\mathfrak{R}(=\mathscr{Z}(\sqrt{a}))$ which is
totally complex of degree 2 over a totally real subfield $\mathscr{Z}$
of degree $\dfrac{h}{2}$ over $\mathbb{Q}$. Let
$\gamma_{1},\ldots,\gamma_{h}$ be a basis of $\mathfrak{R}$ over
$\mathbb{Q}$ and $\Omega=(\gamma^{(1)}_{k})(1\leq k,l\leq h)$. Then we
have 
\begin{align*}
\Omega T_{0} \Omega^{-1} &=
       [\tau^{(1)},\ldots,\tau^{(h)}]\quad\text{with}\quad
       \tau^{(k)}=-\overline{\tau^{(k)}},\\
\Omega N\Omega^{-1} &= [p_{1},\ldots,p_{h}]>0\\
\Omega R\Omega^{-1} &= [i_{1},\ldots,i_{h}]
\end{align*}\pageoriginale
and necessarily $i_{k}=\pm \sqrt{-1}$ in view of the fact that
$R^{2}=-E_{h}$. Thus $\tau^{(k)}$ and $i_{k}$ are purely imaginary and
lie in opposite half-planes.

Let $R$ be any $R$-matrix in $(\ub{\mathscr{F}})$ which is the same as
$(\ub{\mathfrak{R}})$ and let $(\mathfrak{L})$ denote the algebra of
rational commutators (Of course, by construction, $(\mathfrak{L})$
contains $(\mathfrak{R})$). We know $(\mathfrak{L})$ is semi-simple
but since $(\mathfrak{L})$ is an algebra of $h$-rowed rational
matrices containing an irreducible representation of $\mathfrak{R}$
(of degree $h$ over $\mathbb{Q}$), we see by considering the
characteristic polynomial of a generator over $\mathbb{Q}$ of
$(\mathfrak{R})$, that $(\mathfrak{L})$ is necessarily simple. Let
then $(\mathfrak{L})$ be the total matrix-algebra of order $1$ over a
division algebra $\mathscr{V}_{1}$ and let $\mathscr{V}_{1}$ be a
division algebra with centre $\mathfrak{g}$ which is an algebraic
number field of degree $g$ over $\mathbb{Q}$. By considering the
representation of a generator of $\mathfrak{R}$ over $\mathbb{Q}$ with
respect to a splitting field of $\mathscr{V}_{1}$, we see that
$\mathscr{V}_{1}=\mathfrak{g}$. Now the degree of a maximal
commutative system in $(\mathfrak{L})$ is necessarily $gl$ and it is
easy to deduce that $gl=h$ and $(\mathfrak{g})\subset
(\mathfrak{R})$. Let $\mathfrak{g}^{(1)}(=\mathfrak{g})$,
$\mathfrak{g}^{(2)},\ldots,\mathfrak{g}^{(g)}$ be the conjugates of
$\mathfrak{g}$ over $\mathbb{Q}$. Let
$\mathfrak{R}^{(1)}(=\mathfrak{R})$,
$\mathfrak{R}^{(2)},\ldots,\mathfrak{R}^{(1)}$ be the conjugates of
$\mathfrak{R}$ over $\mathfrak{g}^{(1)}$ and
$\mathfrak{R}^{(1+1)},\ldots,\mathfrak{R}^{(21)}$ the conjugates of
$\mathfrak{R}^{(1+1)}$ over $\mathfrak{g}^{(2)}$ and so on. Taking a
representation over $\mathfrak{g}^{(1)},\ldots,\mathfrak{g}^{(g)}$,
let $T_{0}=[T^{(1)},\ldots,T^{(g)}]$ and
$R=[R_{1},\ldots,R_{g}]$. Since $\mathscr{M}_{1}((\mathfrak{g}))$ is
the complete rational commutator-algebra of $R$, it follows that
$R_{k}=\pm iE_{l}$ (for $1\leq k\leq g$). Thus\pageoriginale
$i\tau^{(kl+1)},\ldots i\tau^{(kl+1)}$ have all the same sign (for
$1\leq k\leq g$). If $\mathfrak{L}=(\mathfrak{R})$, then we must have
necessarily $l=1$ and $g=h$. The criterion for $R$ to have
$(\mathfrak{R})$ an exact commutator-algebra is then clearly that $l$
should not be greater than $1$. We may reformulate this condition as
follows, namely, that there should exist no proper subfield
$\mathfrak{g}$ of $\mathfrak{R}$ such that the conjugates of $\tau$
with respect to each conjugate $\mathfrak{g}^{(k)}$ of $\mathfrak{g}$
should not all lie in the same complex half-plane (lower or upper). In
other words, $\sqrt{a}T_{0}$ {\em should not be totally-definite over
  any proper subfield $\mathfrak{g}$ of $\mathfrak{R}$.}

We proceed to discuss the case when $\mathscr{V}=\mathfrak{R}$ of type
(iv) but $q=2$. Then $\mathfrak{R}=\mathscr{Z}(\sqrt{a})$ with
$\mathscr{Z}$ totally real and $-a>0$ in $\mathscr{Z}$. For
$\alpha=x+y\sqrt{a}$ in $\mathfrak{R}$ with $x$, $y\in\mathscr{Z}$, we
always take the $2$-rowed representation $\left(\begin{smallmatrix} x
  & y\\ ay & x\end{smallmatrix}\right)$ over $\mathscr{Z}$ and denote
  it by $\alpha$ again. For $\alpha=\left(\begin{smallmatrix} x &
    y\\ ay & x  \end{smallmatrix}\right)\in\mathfrak{R}$,
  $\widetilde{\alpha}=\ob{\alpha}=\left(\begin{smallmatrix} x &
    -y\\ -ay & x  \end{smallmatrix}\right)$. In particular, to
  $\sqrt{a}$ corresponds $\varepsilon=\left(\begin{smallmatrix} 0 & 1\\
    a & 0  \end{smallmatrix}\right)$. Any
  $\alpha=x+y\sqrt{a}\in\mathfrak{R}$, is then equal to
  $xE_{2}+y\varepsilon$. If $\mathscr{Z}^{(k)}$ is a conjugate of
  $\mathscr{Z}$ over $\mathbb{Q}$, then corresponding to
  $\alpha=\left(\begin{smallmatrix} x& y \\ ay &
  x  \end{smallmatrix}\right)$ in $\mathfrak{R}$, we define
  $\alpha^{(k)}$ by $\left(\begin{smallmatrix} x^{(k)} &
    y^{(k)}\\ (ay)^{(k)} & x^{(k)}  \end{smallmatrix}\right)$. Let
  $\gamma_{1},\ldots,\gamma_{g}$ be a basis of $\mathscr{Z}$ over
  $\mathbb{Q}$ and $\Omega=(\gamma^{(1)}_{k})(1\leq k,l\leq g)$. For
  elements $T$ of $\mathscr{F}$, we have first a representation
  $T=\left(\begin{smallmatrix} \alpha & \beta\\ \gamma & \delta
  \end{smallmatrix}\right)$ over $\mathscr{Z}$, where $\alpha$,
  $\beta$, $\gamma$, $\delta$ are in $\mathfrak{R}$ and a rational
  representation for $T$ is given by
$$
(\Omega\times E_{4})[T^{(1)},\ldots,T^{(g)}](\Omega\times
  E_{4})^{-1}\text{ \ where \ } T^{(k)}=
\begin{pmatrix}
\alpha^{(k)} & \beta^{(k)}\\
\gamma^{(k)} & \delta^{(k)}
\end{pmatrix}.
$$
We shall in the sequel, use sometimes for the elements $T$ of
$\mathscr{F}$, the representation $(\mathscr{F})$ given
by\pageoriginale $T\to[T^{(1)},\ldots,T^{(g)}]$ over $\mathscr{Z}$ and
its conjugates as mentioned above. We then extend this representation
linearly to the linear closure $(\ub{\mathscr{F}})$ of
$(\mathscr{F})$. When there is no risk of confusion, we shall denote
$T\in(\mathscr{F})$ merely by $\left(\begin{smallmatrix} \alpha &
  \beta\\ \gamma & \delta\end{smallmatrix}\right)$ as above without
  referring to all the $g$ components every time.

Let then $T_{0}=-\widetilde{T}_{0}$ be a nonsingular element of
$(\mathscr{F})$. By the same arguments as in the case
$\mathscr{V}=\mathscr{P}$, $q=2$, we can suppose that
$T^{-1}_{0}=\left(\begin{smallmatrix} \alpha & 0\\ 0 & \beta
\end{smallmatrix}\right)$ with $\alpha=-\widetilde{\alpha}$,
$\beta=-\widetilde{\beta}$ in $\mathfrak{R}$. Let
$N=[N_{1},\ldots,N_{g}]$ with $N_{k}=\left(\begin{smallmatrix}x_{k} &
  z_{k}\\ \ob{z}_{k} & y_{k}\end{smallmatrix}\right)$ be in
$(\ub{\mathscr{F}})$, satisfying $N=\widetilde{N}$ and having all
eigenvalues real and positive. It follows that $x_{k}$, $y_{k}$ are
positive real scalar multiples of $E_{2}$, while
$Z_{k}=c_{k}E_{2}+d_{k}\varepsilon$ with $c_{k}$, $d_{k}\in\mathbb{R}$
and further $x_{k}y_{k}-z_{k}\ob{z}_{k}>0$. 

Let now $R=T^{-1}_{0}N$ be an $R$-matrix in $(\ub{\mathscr{F}})$. Then
\begin{equation*}
R=(\Omega\times E_{4})[R_{1},\ldots,R_{g}](\Omega\times
E_{4})^{-1}\tag{92}\label{eq92} 
\end{equation*}
with $R_{k}=\left(\begin{smallmatrix} \xi_{k} & \eta_{k}\\ \zeta_{k} &
  \tau_{k}
\end{smallmatrix}\right)$ and $\xi_{k}=x_{k}\alpha^{(k)}$,
$\eta_{k}=\alpha^{(k)}z_{k}$, $\zeta_{k}=\beta^{(k)}\ob{z}_{k}$,
$\tau_{k}=y_{k}\beta^{(k)}$. Now $R_{2}=-E$ gives, for $1\leq k\leq
g$,
\begin{equation*}
\xi^{2}_{k}+\eta_{k}\zeta_{k}=-1,\tau^{2}_{k}+\eta_{k}\zeta_{k}=-1,(\xi_{k}+\tau_{k})\eta_{k}=0=(\xi_{k}+\tau_{k})\zeta_{k}\tag{93}\label{eq93} 
\end{equation*}
Now there are two possibilities, namely, either
a)~$\xi_{k}+\tau_{k}\neq 0$, in which case we have necessarily
$\zeta_{k}=0=\eta_{k}$, or b)~$\xi_{k}+\tau_{k}=0$. In case a), we see
that $R_{k}=\left(\begin{smallmatrix} \xi_{k} & 0\\ 0 & \tau_{k}
\end{smallmatrix}\right)$, where, from \eqref{eq93}, $\xi_{k}=\pm
\dfrac{1}{\sqrt{-a^{(k)}}}\varepsilon^{(k)}$,\pageoriginale $\tau_{k}=\pm
\dfrac{1}{\sqrt{-a^{(k)}}}\varepsilon^{(k)}$. Since $\xi_{k}+\tau_{k}\neq
0$, it follows that $\xi_{k}=\tau_{k}=\pm
\dfrac{1}{\sqrt{-a^{(k)}}}\varepsilon^{(k)}$ and thus 
\begin{equation*}
R_{k}=\pm \frac{1}{\sqrt{-a^{(k)}}}
\begin{pmatrix}
\epsilon^{(k)} & 0\\
0 & \epsilon^{(k)}
\end{pmatrix}\tag{94}\label{eq94}
\end{equation*}
Now $\xi_{k}=\tau_{k}$ is equivalent to the fact that
$\dfrac{\alpha^{(k)}}{\beta^{(k)}}=\dfrac{y_{k}}{x_{k}}>0$, which, in
turn, is equivalent to the fact that $a_{k}b_{k}=0$, in our former
notation. Let us now consider case b), when $\xi_{k}+\tau_{k}=0$ or
equivalently
$\dfrac{\alpha^{(k)}}{\beta^{(k)}}=-\dfrac{y_{k}}{x_{k}}<0$. Thus, in
this case, $a_{k}b_{k}=1$. Now $\xi_{k}=-\tau_{k}$ and we have
\begin{equation*}
R_{k}=
\begin{pmatrix}
x_{k}\alpha^{(k)} & \alpha^{(k)}\\
\beta^{(k)}\ob{z}_{k} & -x_{k}\alpha^{(k)}
\end{pmatrix}\tag{95}\label{eq95}
\end{equation*}
From \eqref{eq93}, we obtain
$(x_{k}\alpha^{(k)})^{2}+\alpha^{(k)}\beta^{(k)}x_{k}\ob{z}_{k}=-1$,
\ie
\begin{equation*}
-(\alpha^{(k)})^{2}x^{2}_{k}-\alpha^{(k)}
\beta^{(k)}z_{k}\ob{z}_{k}=1\tag{96}\label{eq96} 
\end{equation*}
Since $\overline{\alpha^{(k)}}=-\alpha^{(k)}$,
$\overline{\beta^{(k)}}=-\beta^{(k)}$, it is clear that
$-(\alpha^{(k)})^{2}>0$ while $-\alpha^{(k)}\beta^{(k)}<0$. Thus
equation \eqref{eq96} defines a two-sheeted hyperboloid in the $x_{k}$,
$z_{k}$-space. As a consequence of \eqref{eq96}, we also have
$-\dfrac{\alpha^{(k)}}{\beta^{(k)}}x^{2}_{k}-z_{k}
\ob{z}_{k}=\dfrac{1}{\alpha^{(k)}\beta^{(k)}}>0$
which means $x_{k}y_{k}-z_{k}\ob{z}_{k}>0$.

We may rule out the possibility that when case a) could occur for all
the $g$ components, since then $a_{k}b_{k}=0$ for all $k$ and this
case has been discussed already.

So then let us assume that at least one component of $R$, say $R_{1}$,
without\pageoriginale loss of generality is of the form
\eqref{eq95}. Let $M$ be a rational matrix commuting with all
$R$-matrices in $(\ub{\mathscr{F}})$. Then using the form \eqref{eq92}
for $R$-matrices, $M_{1}=(\Omega\times E_{4})^{-1}M(\Omega\times
E_{4})$ commutes with $\big[R_{1},\ldots,R_{g}\big]$. We split up $M_{1}$ as
$(M_{kl})(1\leq k,l\leq g)$ with 4-rowed square matrices
$M_{kl}$. Now in $R_{1}$, the three real parameters in $x_{1}$,
$z_{1}$ are linearly independent and therefore
$M_{21}=0=\ldots=M_{gl}$. We are now in a position to apply Lemma
\ref{chap1:lem3} and deduce that all the elements of $M_{11}$ are in
$\mathscr{Z}$ while $M_{kk}=M^{(k)}_{11}$ and $M_{kl}=0$ for $k\neq
1$. Further $M_{kk}R_{k}=R_{k}M_{kk}$. 

Let us now suppose that {\em not all of the components $R_{k}$ are of
  the form \eqref{eq95} \ie neither $a_{k}b_{k}=0$ for all $k$ nor
  $a_{k}b_{k}=1$ for all $k$.} Further, without loss of generality,
let $R_{1}$ be of the form \eqref{eq95} while $R_{2}$ is of the form
\eqref{eq94}. Then writing $M_{22}=\left(\begin{smallmatrix} \lambda &
  \mu\\ \kappa & \nu\end{smallmatrix}\right)$ with $2$-rowed square
  matrices $\lambda$, $\kappa$, $\mu$, $\nu$ having elements in
  $\mathscr{Z}^{(2)}$, we obtain each one of them commutes with
  $\varepsilon^{(2)}=\left(\begin{smallmatrix} 0 & 1\\ a^{(2)} & 0
  \end{smallmatrix}\right)$ and therefore $\lambda$, $\kappa$, $\mu$,
  $\nu$ represent elements in
  $\mathscr{Z}^{(2)}(\sqrt{a^{(2)}})$. Since $M_{kk}=M^{(k)}_{11}$, we
  know $M_{11}$, $M_{22},\ldots, M_{gg}$ are conjugate over
  $\mathscr{Z}$ and hence the elements of $M_{kk}$ are in
  $\mathscr{Z}^{(k)}(\sqrt{a}^{(k)})$ and in particular $M_{11}$ has
  elements in $\mathfrak{R}$.

From $M_{11}R_{1}=R_{1}M_{11}$ for all $R_{1}$ of the form \eqref{eq95}
it follows that $M_{11}$ has to commute with
$\left(\begin{smallmatrix} \alpha & 0\\ 0 & -\alpha
\end{smallmatrix}\right)$, $\left(\begin{smallmatrix} 0 &
  \alpha\\ \beta & 0\end{smallmatrix}\right)$ and
  $\left(\begin{smallmatrix} 0 & \alpha\varepsilon\\ -\beta\varepsilon & 0
  \end{smallmatrix}\right)$ (dropping the suffixes and
  superscripts). Let us now set $\beta\alpha^{-1}=p$; $p$ lies in
  $\mathscr{Z}$, in fact. The matrix $M_{11}$ which already commutes
  with $\left(\begin{smallmatrix} \varepsilon & 0\\ 0 & \varepsilon
  \end{smallmatrix}\right)$ has also to commute with 
\begin{equation*}
A=
\begin{pmatrix}
\varepsilon & 0\\
0 & -\varepsilon
\end{pmatrix},
B=
\begin{pmatrix}
0 & \varepsilon\\
-p\varepsilon & 0
\end{pmatrix}
\quad\text{and}\quad
C=
\begin{pmatrix}
0 & \varepsilon^{2}\\
+p\varepsilon^{2} & 0
\end{pmatrix}
=AB=-BA\tag{97}\label{eq97}
\end{equation*}\pageoriginale
Thus $M_{11}$ has to commute with $\left(\begin{smallmatrix}\varepsilon &
  0\\ 0 & 0\end{smallmatrix}\right)$, $\left(\begin{smallmatrix} 0 &
    0\\
0 & \varepsilon  \end{smallmatrix}\right)$ and $\left(\begin{smallmatrix}
0 & \varepsilon\\ -p\varepsilon & 0  \end{smallmatrix}\right)$. Therefore
  $M_{11}=\left(\begin{smallmatrix}\lambda & 0\\ 0 & \lambda
  \end{smallmatrix}\right)$ with $\lambda\in \mathfrak{R}$. Hence the
  rank over $\mathbb{Q}$ of the algebra of rational matrices commuting
  with all $R$-matrices in $(\ub{\mathscr{F}})$ is $2\cdot g=h$ which
  is exactly the rank of $\mathscr{M}$ over $\mathbb{Q}$. We conclude,
  as before, that in this case, there exist $R$-matrices with
  $(\mathscr{M})$ as exact commutator-algebra.

On the other hand, let all $R_{k}$ be of the form \eqref{eq95}. The
$M_{11}$ is an arbitrary $4$-rowed square matrix with elements in
$\mathscr{Z}$ and commuting with $A$, $B$ and $C$ as defined in
\eqref{eq97}. But from \eqref{eq97} and from $A^{2}=aE$, $B^{2}=-paE$, we
see that $1$, $A$, $B$ generate a quaternion algebra $\Phi$ over
$\mathscr{Z}$ and $M_{11}$ has then to lie precisely in the
commutator-algebra of $\Phi$. Hence the rank of the algebra of all
rational matrices commuting with all the $R$-matrices in
$(\ub{\mathscr{F}})$ is precisely $4g=2h$ which is greater than the
rank of $\mathscr{M}$ over $\mathbb{Q}$. Thus in the case when
$a_{k}b_{k}=1$ for all $k$, there exist no $R$-matrix with
$(\mathscr{M})$ as its exact commutator-algebra. Now $a_{k}b_{k}=1$
for all $k$ or $a_{k}b_{k}=0$ for all $k$ is respectively equivalent
to the fact that $\varepsilon T_{0}$ is totally indefinite or totally
definite over $\mathfrak{R}$. Or, putting it in other words, except
when $|T_{0}|^{-1}=\alpha\beta$ (by definition) is either totally
positive or totally negative in $\mathscr{Z}$, there exist
$R$-matrices with $(\mathscr{M})$ as the exact algebra of commutators.

We now deal with the last of the exceptional cases, namely when
$\mathscr{V}$ is a cyclic algebra of type (iv) and $s=2$, $q=1$. Thus
$\mathscr{V}$ is a quaternion algebra with centre $\mathfrak{R}$ which
is obtained by adjoining $\epsilon=\sqrt{a}$ to a totally
real\pageoriginale field $\mathscr{Z}$ of degree $g$ over $\mathbb{Q}$
and $-a>0$ is in $\mathscr{Z}$. As before, we can find a totally real
field $\mathfrak{Z}_{0}=\mathfrak{Z} (\rho)$ with $\rho = \sqrt{d}$
and $d>0$ in $\mathscr{Z}$ such that $\mathfrak{Z} = \mathfrak{Z}_0
(\varepsilon)$ serves as a 
splitting field for $\mathscr{V}$. There are two automorphisms in
$\mathfrak{g}$ which are identity on $\mathscr{Z}$ and commute with
each other, namely for $\xi\in\mathfrak{Z}$,
\begin{align*}
\xi &= x+y\rho\to \overdot{\xi}=x-y\rho (x,y\in\mathfrak{R})\\
\xi &= p+q\varepsilon \to \ob{\xi}=p-q\epsilon (p,q\in\mathfrak{Z}_{0})
\end{align*}
The algebra $\mathscr{V}$ is generated over $\mathfrak{Z}$ by an
element $\mathfrak{J}$ which satisfies
$\mathfrak{J}^{2}=b\in\mathfrak{R}$ and
$\mathfrak{J}\xi=\overdot{\xi}\mathfrak{J}$. Further, there exists
$c\in \mathfrak{Z}_{0}$ such that $c\overdot{c}=b\ob{b}$. For
$\eta\in\mathfrak{Z}_{0}$, the mapping $\eta\to \overdot{\eta}$ is an
automorphism of $\mathfrak{Z}_{0}$ over $\mathscr{Z}$.

For $\delta=\xi+\eta j\in\mathscr{V}$ with $\xi$,
$\eta\in\mathfrak{Z}$, we have over $\mathfrak{Z}$ the representation
$\delta\to D=\left(\begin{smallmatrix} \xi &
  \eta\\ \mathfrak{b}\overdot{\eta} & \overdot{\xi}
\end{smallmatrix}\right)$. Further
$\overdot{D}=\left(\begin{smallmatrix} \overdot{\xi} &
  \overdot{\eta}\\ \mathfrak{b}\eta &
  \xi\end{smallmatrix}\right)=\mathscr{F}D\mathscr{F}^{-1}$ where
  $\mathscr{F}=\left(\begin{smallmatrix} 0 & 1\\ b & 0
  \end{smallmatrix}\right)$. In terms of $D$, the positive involution
  in $\mathscr{V}$ is expressed as
\begin{equation*}
\begin{pmatrix}
\xi & \eta\\
\mathfrak{b}\overdot{\eta} & \overdot{\xi}
\end{pmatrix}
=D\to\widetilde{D}=F^{-1}\ob{D}'F=
\begin{pmatrix}
\ob{\xi} & \dfrac{\mathfrak{c}}{\mathfrak{b}}\overdot{\ob{\eta}}\\
\overdot{\mathfrak{c}}\ob{\eta} & \ob{\overdot{\xi}}
\end{pmatrix},
F^{-1}=[c,c\overdot{c}]>0\tag{98}\label{eq98}
\end{equation*}
We obtain for $\mathscr{V}$, a representation $\delta\to
D_{0}=(\Omega_{1}\times E_{2})[D\ob{D}](\Omega_{1}\times E_{2})^{-1}$
where $\Omega_{1}=\left(\begin{smallmatrix} 1 & 1\\ \varepsilon &
  -\varepsilon\end{smallmatrix}\right)$. It is clear that
  $D_{0}=\left(\begin{smallmatrix}\xi &
    \eta\\ \mathfrak{b}\overdot{\eta} & \overdot{\xi}
  \end{smallmatrix}\right)$ where now $\xi$, $\eta$,
  $b\overdot{\eta}$, $\overdot{\xi}$ stand for their 2-rowed
  representations over $\mathfrak{Z}_{0}$ with respect to the basis
  1, $\varepsilon$. From this, we pass to a representation of
  $\mathscr{V}$ over $\mathscr{Z}$ given by $\delta\to
  \ub{D}=K_{1}[D_{0}\overdot{D}_{0}]K^{-1}_{1}$ where
  $K_{1}=\Omega_{2}\times E_{4}$,
  $\Omega_{2}=\left(\begin{smallmatrix} 1 & 1\\ \rho & -\rho
  \end{smallmatrix}\right)$ and then to the rational representation
\begin{equation*}
\delta\to (\Omega_{3}\times
E_{8})[\ub{D}^{(1)},\ldots,\ub{D}^{(g)}](\Omega_{3}\times
E_{8})^{-1}\tag{99}\label{eq99} 
\end{equation*}
where\pageoriginale $\Omega_{3}=(\gamma^{(1)}_{k})$,
$\gamma_{1},\ldots,\gamma_{g}$ being a basis of $\mathscr{Z}$ over
$\mathbb{Q}$. 

For the abstract algebra, we may start with the regular representation
of its opposite algebra and assume that for the elements of the
commutator algebra $(\mathscr{F})$ of $(\mathscr{V})$, we already have
the rational representation of the form \eqref{eq99}. (Let us remark
that this arrangement is purely for the sake of convenience in
working. Even if we had started with the regular representation
$(\mathscr{V})$ of the abstract algebra $\mathscr{V}$, the positive
involution in $(\mathscr{V})$ will correspond to the involution $T\to
\widetilde{T}=\overdot{F}^{-1}\ob{T}'\overdot{F}$ in
$(\mathscr{F})$. This is different from \eqref{eq98} only in as much as
$F$ has to be replaced by $\overdot{F}$ but observe that this
involution is again positive).

Let $T_{0}=-\widetilde{T}_{0}$ be a nonsingular element of
$(\mathscr{F})$ and let $T_{0}=(\Omega_{3}\times
E_{8})[\ub{T}^{(1)}_{0},\ldots,\ub{T}_{0}^{(g)}](\Omega_{3}\times
E_{8})^{-1}$ where $\ub{T}_{0}^{(k)}$ are defined as follows. Let
$T_{0}$ have the representation $\left(\begin{smallmatrix}\alpha &
  \beta\\ \mathfrak{b}\overdot{\beta} & \overdot{\alpha}
\end{smallmatrix}\right)$ over $\mathfrak{Z}$ and let
$\mathfrak{Z}^{(k)}=\mathscr{Z}^{(k)}(\sqrt{d^{(k)}} ,
\sqrt{a^{(k)}})$. Define 
$\alpha^{(k)}$, $\beta^{(k)}$, $(b\overdot{\beta})^{(k)}$,
$(\overdot{\alpha})^{(k)}$ to be the images of $\alpha$, $\overdot{\beta}$,
$b\overdot{\beta}$, $\overdot{\alpha}$ respectively under the
isomorphism of $\mathfrak{Z}$ onto $\mathfrak{Z}^{(k)}$ taking
$\mathscr{Z}$ onto $\mathscr{Z}^{(k)}$. Then 
\begin{equation*}
\ub{T}^{(k)}_{0}=
\begin{pmatrix}
\alpha^{(k)} & \beta^{(k)}\\
(b\overdot{\beta})^{(k)} & (\overdot{\alpha})^{(k)}
\end{pmatrix}\tag{100}\label{eq100}
\end{equation*}
where for the elements of $\ub{T}_{0}^{(k)}$, we have taken their
regular representation over $\mathscr{Z}^{(k)}$. From
$\widetilde{T}_{0}=-T_{0}$, we obtain $\alpha=-\ob{\alpha}$,
$\beta=-\dfrac{c}{b}\overdot{\ob{\beta}}$,
$b\overdot{\beta}=-\overdot{c}\ob{\beta}$,
$\overdot{\alpha}=-\ob{\overdot{\alpha}}$ or $\ob{\alpha}-\alpha$,
$b\overdot{\beta}=-\overdot{c}\ob{\beta}$. Let $N=(\Omega_{3}\times
E_{8})[N_{1},\ldots,N_{g}](\Omega_{3}\times E_{8})^{-1}$ be in
$(\mathscr{F})$ having the properties mentioned in \eqref{eq86} and
analogous to \eqref{eq100}, let $N_{k}=\left(\begin{smallmatrix}
  \lambda_{k} & \mu_{k}\\ b^{(k)}\overdot{\mu}_{k} &
  \overdot{\lambda}_{k}\end{smallmatrix}\right)$\pageoriginale where
$\lambda_{k}$, 
$\mu_{k}$ are in the linear closure of $\mathfrak{Z}^{(k)}$ and hence
commute with elements of $\mathfrak{Z}^{(k)}$. Then
$\lambda_{k}=\ob{\lambda}_{k}$ and
$b\overdot{\mu}=\overdot{c}\ob{\mu}$. Further $N_{k}$ has all its
eigenvalues real and positive \ie
$\lambda_{k}\overdot{\lambda}_{k}-b^{(k)}\mu_{k}\overdot{\mu}_{k}>0$. Now
$R=T^{-1}_{0}N$ is a $R$-matrix in $(\ub{\mathscr{F}})$ and let again
$R=(\Omega_{3}\times E_{8})[R_{1},\ldots,R_{g}](\Omega_{3}\times
E_{8})^{-1}$ with $R_{k}=\left(\begin{smallmatrix}\ob{\xi}_{k} &
  \eta_{k}\\ b^{(k)}\overdot{\eta}_{k} & \overdot{\xi}_{k}
\end{smallmatrix}\right)$. From $R^{2}=-E$, we obtain,
\begin{equation*}
\xi^{2}_{k}+b^{(k)}\eta_{k}\overdot{\eta}_{k}=-1=
\overdot{\xi}^{2}_{k}+b^{(k)}\eta_{k}\overdot{\eta}_{k},
\;\; 
(\xi_{k}+\overdot{\xi}_{k})\eta_{k}=0=(\xi_{k}+
\overdot{\xi}_{k})b^{(k)}\overdot{\eta}_{k}\tag{101}\label{eq101} 
\end{equation*}

Let us denote $\alpha\overdot{\alpha}-b\beta\overdot{\beta}$ by
$\delta$. Then $\delta\in\mathscr{Z}$. We know that $\delta^{(k)}$ is
negative or positive according as $a_{k}b_{k}=0$ or $1$ respectively,
in our former notation. On the other hand, taking determinants,
$|R_{k}|\delta^{(k)}=|N_{k}|$ which, in view of \eqref{eq86} is
positive. Further, since $R^{2}=-E$, we have $|R_{k}|=\pm 1$. Now, in
\eqref{eq101}, one of two possibilities arises, namely, either
a)~$\xi_{k}+\overdot{\xi}_{k}\neq 0$, in which case $\eta_{k}=0$,
$\overdot{\eta}_{k}=0$, or b)~$\xi_{k}=-\overdot{\xi}_{k}$.

In case a), using \eqref{eq101}, we see that
$\xi_{k}=\overdot{\xi}_{k}=\pm
\dfrac{1}{\sqrt{a^{(k)}}}\varepsilon^{(k)}$. Since we $|R_{k}|=-1$, we
see that $\delta^{(k)}<0$ and thus case a) corresponds to the
situation when $a_{k}b_{k}=0$ and then
\begin{equation*}
R_{k}=\pm \frac{1}{\sqrt{a^{(k)}}}
\begin{pmatrix}
\varepsilon^{(k)} & 0\\
0 & \varepsilon^{(k)}
\end{pmatrix}\tag{102}\label{eq102}
\end{equation*}
When case b)~occurs, $\xi_{k}=-\overdot{\xi}_{k}$ and therefore
$|R_{k}|=-\xi^{2}_{k}-b^{(k)}\eta_{k}\overdot{\eta}_{k}=1$, in view of
\eqref{eq101}. Thus case b) corresponds precisely to the situation
$\delta^{(k)}>0$ or equivalently $a_{k}b_{k}=1$. Thus in case b),
$R_{k}=\left(\begin{smallmatrix} \xi_{k} &
  \eta_{k}\\ b^{(k)}\overdot{\eta}_{k} & -\xi_{k}
\end{smallmatrix}\right)$ and\pageoriginale $R_{k}$ contains ``a
priori'' eight free real parameters. From
$N_{k}=\ub{T}^{(k)}_{0}R_{k}$, we obtain, dropping the inconvenient
suffix $k$ and the superscript $k$ everywhere without risk of
confusion, that
\begin{equation*}
\lambda=\alpha\xi+b\beta\overdot{\eta},\mu=\alpha\eta-\beta\xi,
\overdot{\lambda}=\mathfrak{b}\overdot{\beta}\eta-\overdot{\alpha}\xi,
b\overdot{\mu}=b(\overdot{\beta}\xi+\overdot{\alpha}\overdot{\eta})\tag{103}\label{eq103}
\end{equation*}
Since $\alpha=-\ob{\alpha}$, $\varepsilon=-\ob{\varepsilon}$,
$\lambda=\ob{\lambda}$ and $\alpha\beta\neq 0$, we can define
$r=\ob{r}$, $s=\ob{s}$ by $\lambda=\alpha\varepsilon r$,
$\overdot{\lambda}=\overdot{\alpha}\varepsilon s$. Then from \eqref{eq103},
we have 
\begin{equation*}
b\overdot{\beta}\eta=\overdot{\alpha}(\xi+\varepsilon s),
b\beta\overdot{\eta}=\alpha(\varepsilon\gamma-\xi)\tag{104}\label{eq104}
\end{equation*}
But, from $N=\tilde{N}$, we have $b\overdot{\mu}=\overdot{c}\ob{\mu}$ and
again, in view of \eqref{eq103},
\begin{equation*}
\overdot{c}(-\alpha\ob{\eta}-\ob{\beta}\ob{\xi})=b(\overdot{\beta}\xi
+\overdot{\alpha}\overdot{\eta})\tag{105}\label{eq105} 
\end{equation*}
Multiplying both sides of \eqref{eq105} by $\beta$ and using
\eqref{eq103}, \eqref{eq104}, we obtain
\begin{align*}
b\beta\overdot{\beta}\xi+\alpha\overdot{\alpha}(\varepsilon r-\xi) &=
-\overdot{c}\beta\ob{\eta}\alpha-\overdot{c}\beta\ob{\beta}\ob{\xi}\\
&=
\ob{b}\ob{\overdot{\beta}}\ob{\eta}\alpha+b\beta\overdot{\beta}\ob{\xi}\\
&= -\alpha\overdot{\alpha}(\overline{\varepsilon
  s+\xi})+b\beta\overdot{\beta}\ob{\xi}\\ 
&= \alpha\overdot{\alpha}(\varepsilon
s-\overline{\xi})+b\beta\overdot{\beta}\ob{\xi} 
\end{align*}
Thus
\begin{equation*}
\frac{1}{2}(\xi-\ob{\xi})=\frac{\alpha\overdot{\alpha}}{\delta}\varepsilon\cdot
\frac{1}{2}(r-s)\tag{106}\label{eq106} 
\end{equation*}
While $r$, $s$ and $t=\frac{1}{2}(\xi+\ob{\xi})$ are free, the
imaginary part of $\xi$ is fixed by \eqref{eq106}. We now set
$$
u=b\frac{\beta\overdot{\beta}}{\delta}\frac{r-s}{2},\quad
v=\frac{r+s}{2},
q=\frac{\alpha\overdot{\alpha}}{b\beta\overdot{\beta}}
$$
Then obviously $q\in \mathscr{Z}$ and furthermore, since
$b\beta\overdot{\beta}=-\overdot{c}\beta\ob{\beta}<0$ and $\delta>0$,
we have
\begin{equation*}
q-1<0\tag{107}\label{eq107}
\end{equation*}
From \eqref{eq106}, we have $\xi=t+q\varepsilon u$. From \eqref{eq104}, we
get 
\begin{equation*}
\frac{b\overdot{\beta}}{\overdot{\alpha}}\eta=\varepsilon
s+\xi=t+\varepsilon(u+v)\tag{108}\label{eq108} 
\end{equation*}\pageoriginale
and similarly
\begin{equation*}
\frac{b\beta}{\alpha}\overdot{\eta}=\varepsilon
r-\xi=-t+\varepsilon(v-u)\tag{109}\label{eq109} 
\end{equation*}
Thus $t$, $u$, $v$ are real parameters which, in view of \eqref{eq108}
and \eqref{eq109} are subject to the conditions
\begin{equation*}
\overdot{t}=-t,\quad \overdot{u}=-u,\quad 
\overdot{v}=v\tag{110}\label{eq110}
\end{equation*}
The relation $\xi^{2}+b\eta\overdot{\eta}=-1$ can be rewritten in
terms of $u$, $v$, $t$ as
$\xi^{2}+\dfrac{\alpha\overdot{\alpha}}{b\beta\overdot{\beta}}\cdot
\dfrac{b\overdot{\beta}\eta}{\alpha}\cdot
\dfrac{b\beta\overdot{\eta}}{\overdot{\alpha}}=-1$ and using
\eqref{eq108} and \eqref{eq109}, we obtain
$$
(t+q\varepsilon u)^{2}+q(t+\varepsilon(v+u))
(-t+\varepsilon(v-u))=-1
$$
\ie
\begin{equation*}
(1-q)t^{2}-aq(1-q)u^{2}+aqv^{2}=-1\tag{111}\label{eq111}
\end{equation*}
In view of \eqref{eq107}, exactly one of $aq$ and $-aq(1-q)$ is a
negative while the coefficient of $t^{2}$ is positive. Further for
$t$, $u$, $v$ satisfying \eqref{eq111}, $r\neq 0$. For, if $r=0$, we
should necessarily have
$(1-q)t^{2}-aq(1-q)\left(\dfrac{b\beta\overdot{\beta}}
{2\delta}\right)^{2}s^{2}+aq\dfrac{s^{2}}{4}=-1$. But
the left hand side is just $(1-q)t^{2}-\dfrac{aq^{2}}{4(1-q)}s^{2}$
which is always non-negative. We thus see that in the $t$, $u$,
$v$-space, equation \eqref{eq111} defines a ``two-sheeted
hyperboloid''. 

Thus in the case when $a_{k}b_{k}=1$, using \eqref{eq108} and
\eqref{eq109}, we have for $R_{k}$ the parametrization
\begin{equation*}
R_{k}=V_{k}
\begin{pmatrix}
t^{k}+qu_{k}\varepsilon^{(k)} & q(t_{k}+\varepsilon^{(k)}(u_{k}+v_{k}))\\
-t_{k}+\varepsilon^{(k)}(-u_{k}+v_{k}) & -t_{k}-qu_{k}\varepsilon^{(k)}
\end{pmatrix}
V^{-1}_{k}\tag{112}\label{eq112}
\end{equation*}
where\pageoriginale $V_{k}=[1,(\dfrac{\alpha}{\beta})^{(k)}]$.

We proceed to discuss the algebra of commutators of the $R$-matrices
$R$. As before, we rule out the occurrence of case a) for all the $g$
components of $R$. Let then at least one component of $R$, say
$R_{1}$, be of the form \eqref{eq112}. If $M$ is a rational matrix
commuting with all $R$-matrices in $(\ub{\mathscr{F}})$, then
$M_{1}=(\Omega_{3}\times E_{8})^{-1}M(\Omega_{3}\times E_{8})$
commutes with $[R_{1},\ldots,R_{g}]$ and by the same arguments as on
p.99, $M_{1}=[M^{(1)}_{11},\ldots,M^{(g)}_{11}]$ where
$M_{11}=M^{(1)}_{11}$ is an $8$-rowed square matrix with elements in
$\mathscr{Z}$ commuting with
\begin{align*}
R_{1}&=\left[
V_{1}
\begin{pmatrix}
t_{1}+q\varepsilon u_{1} & q(t_{1}+\varepsilon(v_{1}+u_{1})\\
-t_{1}+\varepsilon(v_{1}-u_{1}) & -t_{1}-q\varepsilon u_{1}
\end{pmatrix}V^{-1}_{1},\right.\\[5pt]
&\qquad\left.\overdot{V}_{1}
\begin{pmatrix}
-t_{1}-q\varepsilon u_{1} & q(-t_{1}+\varepsilon(v_{1}-u_{1})\\
t_{1}+\varepsilon(v_{1}+u_{1}) & t_{1}+q\varepsilon u_{1}
\end{pmatrix}
\overdot{V}^{-1}_{1}\right]\tag{113}\label{eq113} 
\end{align*}
For the elements of the matrices in \eqref{eq113} of which $R_{1}$ is a
direct sum, we have taken the $Z$-rowed representation over the linear
closure of $\mathfrak{Z}_{0}$ so that $R_{1}$ is an 8-rowed square
matrix. Taking into account the relations $\overdot{t}_{1}=-t_{1}$,
$\overdot{u}_{1}=-u_{1}$ and $\overdot{v}_{1}=v_{1}$ and replacing
$t_{1}$ by $\sqrt{d}t_{1}$, $u_{1}$ by $\sqrt{d}u_{1}$, we see that
$M_{11}$ has to commute with
$[V_{1},\overdot{V}_{1}]A[V_{1},\overdot{V}_{1}]^{-1}$,
$[V_{1},\overdot{V}_{1}]B[V_{1},\overdot{V}_{1}]^{-1}$ and
$[V_{1},\overdot{V}_{1}]C[V_{1},\overdot{V}_{1}]^{-1}$ where 
\begin{align*}
A &= \left[\sqrt{d}
\begin{pmatrix}
1 & q\\
-1 & -1
\end{pmatrix},
-\sqrt{d}
\begin{pmatrix}
1 & q\\
-1 & -1
\end{pmatrix}\right],\\
B&=\left[
\sqrt{d}\varepsilon
\begin{pmatrix}
q & q\\
-1 & -q
\end{pmatrix},
-\sqrt{d}\varepsilon
\begin{pmatrix}
q & q\\
-1 & -q
\end{pmatrix}
\right]\\
C &= \left[\varepsilon
\begin{pmatrix}
0 & q\\
1 & 0
\end{pmatrix},
\varepsilon
\begin{pmatrix}
0 & q\\
1 & 0
\end{pmatrix}
\right]\tag{114}\label{eq114}
\end{align*}
For\pageoriginale the elements in the matrices in \eqref{eq114}. We have
taken the 2-rowed representation over $\mathfrak{Z}_{0}$.

Let us now suppose at least one of the components of $R$ is of the
form \eqref{eq102}. Then we can conclude as on p.99, $M_{11}$ has
elements in $\mathfrak{R}$. But now it is easy to verify that the
matrices $A$, $B$, $C$ defined by \eqref{eq114} satisfy
$$
A^{2}=d(1-q)E,\quad C^{2}=aq E,\quad AC=B=-CA
$$
and therefore generate a quaternion algebra $\Phi$ over
$\mathfrak{R}$. The matrices $M_{11}$ belong to the commutator algebra
of $\Phi$ over $\mathfrak{R}$ and therefore constitute an algebra of
rank $8$ over $\mathscr{Z}$. We may then conclude that the algebra of
all the rational matrices commuting with all the $R$-matrices in
$(\ub{\mathscr{F}})$ is, in this case, exactly $8g=4h$ which is nothing
but the rank of $(\mathscr{M})$ over $\mathbb{Q}$.

Finally, let us suppose that all the components of $R$ are of the form
\eqref{eq112} \ie $a_{k}b_{k}=1$ for all $k$. Then $M_{11}$ is, as
before, an $8$-rowed square matrix with elements in $\mathscr{Z}$
which commutes with the 8-rowed representation of $\Phi$ over
$\mathscr{Z}$. But this latter representation of $\Phi$ contains the
irreducible representation of $\Phi$ over $\mathscr{Z}$ exactly twice
and therefore, it is clear that the matrices $M_{11}$ generate an
algebra of rank 16 over $\mathscr{Z}$. It is now immediate that the
algebra of all rational matrices $M$ commuting with all the
$R$-matrices in $(\ub{\mathscr{F}})$ is, in this case, equal to
$16g=8h$ which is greater than the rank of $(\mathscr{M})$ over
$\mathbb{Q}$.

Thus, in the case when $\mathscr{V}$ is of type (iv) and $s=2$, $q=1$,
there\pageoriginale exist $R$-matrices with $(\mathscr{M})$ as exact
commutator-algebra unless $\varepsilon T_{0}$ is totally - definite
hermitian or totally indefinite hermitian over $\mathfrak{R}$.

We shall say $T_{0}$ is {\em skew-symmetric totally definite} or {\em
  totally indefinite} according as $\varepsilon T_{0}$ is totally
definite hermitian or totally indefinite hermitian.

We have thus completely solved our problem on Riemann matrices and we
may summarize our results in the following theorem. (We remark that
the matrix $T_{0}$, which appears in the statement of Theorem
\ref{chap1:thm7}, is precisely the given non-singular matrix in
$(\mathscr{F})$ which is skew-symmetric for the involution in
$(\mathscr{F})$ and $A=\underset{q}{G}T_{0}$ is a principal matrix for
our $R$-matrices).

\begin{thm}\label{chap1:thm7}
With the notation of Theorem \ref{chap1:thm5}, there always exists a
$R$-matrix with the given $A$ as principal matrix and having
$(\mathscr{M})=(\mathfrak{g})$ as the exact algebra of commutators
except when
\begin{itemize}
\item[\rm a)] $\mathscr{V}=\mathfrak{R}$ with a positive involution of
  the first kind, $q$ is odd

\item[\rm b)] $\mathscr{V}=\mathscr{P}$, $q=1$.

\item[\rm c)] $\mathscr{V}=\mathscr{P}$, $q=2$,
  $N_{\mathfrak{R}}(|T_{0}|)=\tau^{2}$ for $\tau> 0$ in
  $\mathfrak{R}$.

\item[\rm d)] $\mathscr{V}$ is of type {\em (iv)}, $q=s=1$ and there exists a
  proper subfield $\mathfrak{J}$ of $\mathscr{V}=\mathfrak{R}$ over
  which $iT_{0}$ is totally definite.

\item[\rm e)] $\mathscr{V}$ is of type {\em (iv)}, $s=1$, $q=2$ and
  $T_{0}$ is skew-symmetric totally definite or totally indefinite
  over $\mathscr{V}=\mathfrak{R}$, and

\item[\rm f)] $\mathscr{V}$ is of type {\em (iv)}, $s=2$, $q=1$ and
  $T_{0}$ is skew-symmetric totally definite or totally indefinite
  over the centre $\mathfrak{R}$.   
\end{itemize}
\end{thm}

\begin{remarks*}
\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item In\pageoriginale solving our problem on $R$-matrices, we have
  allowed for $A$ the fullest possible generality; we emphasize that
  the transformations which we performed on $(\ub{\mathscr{F}})$
  there, to reduce $A$ to the simple form $\mathsf{J}$, were merely to
  make the discussion easier and constituted no diminution of the
  generality of $A$.

\item Suppose $\mathscr{V}$ is of type (iv), $\mathscr{Z}=\mathbb{Q}$
  and $qs=2$. Then {\em there cannot exist nonsingular}
  $T_{0}=-\widetilde{T}_{0}$ in $(\mathscr{F})$ which are neither
  skew-symmetric totally definite nor skew-symmetric totally
  indefinite over $\mathfrak{R}$ (which is now an imaginary quadratic
  extension of $\mathbb{Q}$), since there cannot exist in $\mathbb{Q}$
  non-zero numbers which are neither positive nor negative!
\end{enumerate}
\end{remarks*}

%%%%%%%%%


\section{Modular groups associated with
 Riemann matrices}\label{chap1-sec8}

In this concluding section, we shall make a close study of the space
$\mathscr{F}$ which we associated on p.~\pageref{p77} with the given division
algebra $\mathscr{V}$. We shall see, for example, how far
$(\mathscr{M})=(\underset{q}{\mathscr{V}})$ determines $\mathscr{M}$
and find all the principal matrices for a general $R$-matrix. Later we
shall define the general modular groups which act on $\mathscr{F}$ as
groups of transformations of $\mathfrak{H}$ onto itself. The scope of
these lectures prevents us from making a function-theoretic study of
these modular groups analogous to some recent work of I. I. Pyatetskii
Shapiro (\cite{13}, \cite{14}). We merely remark that the preparatory
material for this study is contained in \cite{21} and \cite{22}.

We may first briefly recall how $\mathfrak{H}$ was defined. We had
first a division algebra $\mathscr{V}$ of rank $hs^{2}$ over
$\mathbb{Q}$,  with centre $\mathfrak{R}$ of degree $h$ over
$\mathbb{Q}$ and carrying a positive involution. Further
$(\mathscr{M})$ was upto equivalence\pageoriginale over $\mathbb{Q}$,
a $q$-fold multiple of $(\mathscr{V})$, the rational $hs^{2}$-rowed
representation of $\mathscr{V}$. In the algebra $(\mathscr{V})$, we
had an involution $D\to \widetilde{D}=F^{-1}\ob{D}'F$ and the matrix
$\underset{q}{G}$ defined by
\begin{equation*}
\underset{q}{G}=\underset{q}{F}M_{0}>0\tag{115}\label{eq115}
\end{equation*}
was a positive symmetric matrix with $M_{0}$ being in $(\mathscr{M})$
such that $\widetilde{M}_{0}=\underset{q}{F^{-1}}M'_{0}F_{q}$. Further
$T_{0}$ was a given nonsingular element of $(\mathscr{F})$ (the
commutator algebra of $(\mathscr{M})$) for which
\begin{equation*}
\widetilde{T}_{0}=\underset{q}{F^{-1}}T'_{0}\underset{q}{F}=-T_{0}\tag{116}\label{eq116} 
\end{equation*}
The matrix $A$ defined by
\begin{equation*}
A=\underset{q}{G}T_{0}\tag{117}\label{eq117}
\end{equation*}
was a nonsingular rational skew-symmetric matrix defining the Resati
involution $M\to M^{\ast}=A^{-1}M'A$ in $(\mathscr{M})$. Our problem
was first to find $R$-matrices $R$ in $(\ub{\mathscr{F}})$ for which
\begin{equation*}
AR=S=S'>0\tag{118}\label{eq118}
\end{equation*}
(We recall that the matrix $A$ in \eqref{eq118} is a {\em `principal
  matrix'} for $R$). Associated with each such $R$-matrix $R$, we had
defined an $n$-rowed Riemann matrix $\mathscr{P}$ of the form
\eqref{eq70}, uniquely determined by $R$, upto a left sided complex
non-singular factor. We denoted by $\mathfrak{H}$, the set of
$\mathscr{P}$ of the form \eqref{eq70} associated in this way. In the
sequel, however, we shall denote by $\mathfrak{H}$ the set of
$R$-matrices in $(\ub{\mathscr{F}})$ themselves. 

So $\mathfrak{H}$ depends, a priori, on $(\mathscr{M})$,
$M_{0}\in(\mathscr{M})$ given in \eqref{eq115} and
$T_{0}\in(\mathscr{F})$ given in \eqref{eq116}. Given $(\mathscr{M})$,
we shall now see how far $\mathfrak{H}$ is determined by
$(\mathscr{M})$. For our 
subsequent discussion, we shall exclude\pageoriginale $\mathscr{V}$
from being of the type of the six exceptional cases mentioned in the
statement of Theorem \ref{chap1:thm7}. Hence $\mathfrak{H}$ will always
contain a $R$-matrix having $(\mathscr{M})$ as its exact
commutator-algebra. Such a $R$-matrix shall be referred to as a {\em
  generic $R$-matrix}. We now prove

\begin{proposition}\label{chap1:prop14}
Let $\mathfrak{H}$ be the space of $R$-matrices associated with
$(\mathscr{M})$, $M_{0}$ and $T_{0}$ as above and $\mathfrak{H}_{1}$
with $(\mathscr{M})$, $M_{1}$ and $T_{1}$ in a similar manner. Then
$\mathfrak{H}=\mathfrak{H}_{1}$ if $\mathfrak{H}\cap \mathfrak{H}_{1}$
contains a generic $R$-matrix.
\end{proposition}

Before proving the proposition, we remark that if $R$ in
$\mathfrak{H}$ also lies in $\mathfrak{H}_{1}$, then both
$A=\underset{q}{F}M_{0}T_{0}$ and $A_{1}=\underset{q}{F}M_{1}T_{1}$
are principal matrices. The following proposition gives the form of
all principal matrices for a generic $R$-matrix $R\in\mathfrak{H}$. It
is not hard to extend it also to the case when the $R$-matrix is not
necessarily irreducible.

\begin{proposition}\label{chap1:prop15}
If $A$ is a principal matrix for a generic $R\in\mathfrak{H}$, then
any other principal matrix $A_{1}$ of $R$ is of the form $AM$, where
$M$ is a positive element of $(\mathscr{M})$ and conversely,
$A_{1}=AM$ is a principal matrix for $R$, for every such $M\in(\mathscr{M})$.
\end{proposition}

\begin{proof}
From \eqref{eq118}, we obtain $S=AR=-R'A$, $S_{1}=A_{1}R=-R'A_{1}$ and
therefore $A^{-1}A_{1}R=RA^{-1}A_{1}$. But $R$ being generic and
$A^{-1}A_{1}$ being rational, we see that
$A^{-1}A_{1}=M\in(\mathscr{M})$. In the first place,
$M^{\ast}=A^{-1}M'A=-A^{-1}M'A'=-A^{-1}A'_{1}=M$. Further from $S>0$,
and from $SM=\ub{A}RM=\ub{A}MR=S_{1}>0$, we see, by Lemma \ref{chap1:lem2},
that the eigenvalues of $M$ are real positive. In other words,
$A_{1}=AM$ for a positive element $M$ of $(\mathscr{M})$. Conversely,
if $M$ is a positive element of $(\mathscr{M})$
$(M=M^{\ast})$,\pageoriginale then $A_{1}=AM=M'A=-A'_{1}$ and further
$A_{1}R=AMR=ARM$ is symmetric and positive by Lemma \ref{chap1:lem2}. We now
give the
\end{proof}

\setcounter{prfofprop}{13}
\begin{prfofprop}\label{proofofprop14}
Let $R$ be a generic $R$-matrix in $\mathfrak{H}\cap
\mathfrak{H}_{1}$. Then, by proposition \ref{chap1:prop15}, $A_{1}=AM$ for a
positive element $M$ in $(\mathscr{M})$. If $R_{0}\in\mathfrak{H}$,
then $AR_{0}>0$. But now $A_{1}R_{0}=AMR_{0}=AR_{0}M$ is again
positive symmetric, using Lemma \ref{chap1:lem2}. Thus $A_{1}$ is a
principal matrix for $R_{0}$ and so $R_{0}\in\mathfrak{H}_{1}$. Thus
$\mathfrak{H}\subset \mathfrak{H}_{1}$ and similarly
$\mathfrak{H}_{1}\subset \mathfrak{H}$ which proves our proposition.
\end{prfofprop}

In the set of $T_{0}$ of the form \eqref{eq116}, we introduce an
equivalence relation as follows, namely, two such matrices $T_{0}$ are
{\em equivalent} if they differ by a factor $K\in(\mathscr{Z})$ which
is totally positive. We denote by $[K_{0}]$ the equivalence class of
$K_{0}$. 

\begin{proposition}\label{prop16}
If $\mathfrak{H}$ is the space of $R$-matrices associated with
$(\mathscr{M})$, $M_{0}$ and $T_{0}$ as above, then $\mathfrak{H}$
depends essentially only on $(\mathscr{M})$ and $[T_{0}]$.
\end{proposition}

\begin{proof}
Let $\mathfrak{H}_{1}$ be the space of $R$-matrices associated with
$(\mathscr{M})$, $M_{1}$ and $KT_{0}$ where $K$ is in $(\mathscr{Z})$
and has positive eigenvalues. We shall show that
$\mathfrak{H}=\mathfrak{H}_{1}$. Let $R\in\mathfrak{H}$. Then we know
that $\underset{q}{F}M_{0}T_{0}R$ is symmetric and positive. If we
could show that
\begin{equation*}
\underset{q}{F}M_{1}KT_{0}R=(\underset{q}{F}
M_{1}KT_{0}R)'>0\tag{119}\label{eq119} 
\end{equation*}
it would follow that $\mathfrak{H}\subset \mathfrak{H}_{1}$ and then
taking $K^{-1}$ instead of $K$, the reverse inclusion would hold
leading to $\mathfrak{H}=\mathfrak{H}_{1}$. To prove \eqref{eq119}, we
first remark that from
$\underset{q}{F}M_{0}(\underset{q}{F}M_{0})'>0$,
$\underset{q}{F}M_{1}=(\underset{q}{F}M_{1})'=\underset{q}{F}M_{0}M^{-1}_{0}M_{1}<0$,
it follows in view of Lemma \ref{chap1:lem2} that $M^{-1}_{0}M_{1}\in
(\mathscr{M})$ has positive\pageoriginale eigenvalues. Hence the
product $M^{-1}_{0}M_{1}K$ has again positive eigenvalues (since they
commute). Now
$(\underset{q}{F}M_{1}KT_{0}R)'=(T_{0}R)'K'\underset{q}{F}M_{1}=
(T_{0}R)'\underset{q}{F}KM_{1}=(T_{0}R)'\underset{q}{F}M_{0}M^{-1}_{0}
M_{1}K=\underset{q}{F}M_{0}T_{0}RM^{-1}_{0}M_{1}K=\underset{q}{F}M_{1}KT_{0}R$. Further 
since $\underset{q}{F}M_{1}KT_{0}R$ is symmetric and
$\underset{q}{F}M_{0}T_{0}R>0$, it follows that
$\underset{q}{F}M_{1}KT_{0}R=\underset{q}{F}M_{0}T_{0}RM^{-1}_{0}M_{1}K>0$
by Lemma \ref{chap1:lem2}.
\end{proof}

Conversely, if $(\mathscr{M})$, $M_{1}$, $T_{1}$ lead to the same
$\mathfrak{H}$, then we claim $[T_{1}]=[T_{0}]$. For, let $R$ be a
generic $R$-matrix in $\mathfrak{H}$. Then $\underset{q}{F}M_{0}T_{0}$
and $\underset{q}{F}M_{1}T_{1}$ are both principal matrices for $R$
and hence by Proposition \ref{chap1:prop15}, $M_{0}T_{0}=M_{1}T_{1}M$ for a
positive element $M\in(\mathscr{M})$ which means
$M^{-1}_{0}M_{1}M=T_{0}T^{-1}_{1}$. From Proposition \ref{chap1:prop6}, it
follows that $T_{0}T^{-1}_{1}=M^{-1}_{0}M_{1}M=K$ in
$(\mathfrak{R})$. From $T_{0}=-\widetilde{T}_{0}$,
$\widetilde{T}_{1}=-T_{1}$, it follows that
$K\in(\mathscr{Z})$. Further from
$\underset{q}{F}M_{1}=M'_{1}\underset{q}{F}$,
$\underset{q}{F}M_{0}=M'_{0}\underset{q}{F}$, it follows that
$(M^{-1}_{0}M_{1})'\underset{q}{F}M_{0}=\underset{q}{F}M_{0}M^{-1}_{0}M_{1}$
\ie $M^{-1}_{0}M_{1}$ is symmetric under the given positive involution
in $(\mathscr{M})$. Moreover, by the same arguments as above,
$M^{-1}_{0}M_{1}$ has positive eigenvalues. Hence $M^{-1}_{0}M_{1}$ is
a positive element in $(\mathscr{M})$. Since $M^{-1}_{0}M_{1}M=K$ is
in the centre, it follows that the positive elements $M$ and
$M^{-1}_{0}M_{1}$ in $(\mathscr{M})$ commute. Hence $K=K^{\ast}$ and
further $K$ has all eigenvalues positive. Thus $T_{0}T^{-1}_{1}\in
(\mathscr{Z})$ and has all its eigenvalues positive is
$[T_{0}]=[T_{1}]$.

From Proposition \ref{chap1:prop15}, we know that if $A$ is a principal
matrix for a generic $R$ in $\mathfrak{H}$, then any other principal
matrix $A_{1}=AM$ where $M$ is a positive element of
$(\mathscr{M})$. We shall investigate the cases when $A_{1}A^{-1}$ is
always $rE$ where $r>0$ in $\mathbb{Q}$. This will be indeed true if
the only elements $M\in(\mathscr{M})$ for which $M^{\ast}=M$ and all
the eigenvalues are real and positive and are precisely of the form 
$rE$\pageoriginale where $r>0$ and $r\in\mathbb{Q}$, A necessary
condition for this is that $\mathscr{Z}=\mathbb{Q}$. But even if
$\mathscr{Z}=\mathbb{Q}$, we know that in the case when
$\mathscr{V}=\mathfrak{g}$ or $\mathscr{V}$ is a noncommutative cyclic
algebra, there exist positive elements in $(\mathscr{V})$ other than
the positive elements in $(\mathbb{Q})$. But in the case when
$\mathscr{V}=\mathfrak{H}$, $(\mathscr{Z}=\mathbb{Q})$ is of type (i)
or (iv) or $\mathscr{V}=\mathscr{P}$ with
$\mathfrak{H}=\mathscr{Z}=\mathbb{Q}$, the only positive elements in
$(\mathscr{M})$ are of the form $rE$ with $r>0$ in
$\mathbb{Q}$. Therefore, in these cases, any two principal matrices
for $\mathfrak{H}$ differ at most by a positive rational scalar
factor.

We now go back to our definition of a multiplier of a Riemann matrix
$\mathscr{P}$. We called an {\em integral} matrix $M$ a multiplier of
$\mathscr{P}$ if $\mathscr{P}M=K\mathscr{P}$ for a complex nonsingular
$K$ and later we relaxed the condition that $M$ be integral and
allowed $M$ to be rational and not necessarily non-singular. We
constructed in \S 6, Riemann matrices $\mathscr{P}$
with the given division algebra $(\mathscr{M})$ as exact algebra of
multipliers. The integral matrices $M$ in this representation
$(\mathscr{M})$ form an order $(\mathscr{U})$ in $(\mathscr{M})$ and
$\mathscr{P}$ admits all elements of $(\mathscr{U})$ as (integral)
multipliers. One could ask the more difficult question of constructing
Riemann matrices $\mathscr{P}$ with $(\mathscr{U})$ as the exact ring
of multipliers. Now, when we say ``an integral multiplier of
$\mathscr{P}$'', it is necessary to mention the specific
representation $(\mathscr{M})$. For, an integral matrix $M$ in
$(\mathscr{M})$, will not, in general, go into an integral matrix in a
$\mathbb{Q}$-equivalent representation $C^{-1}(\mathscr{M})C$. But it
is true that $(\mathscr{U})$ will go over into an order in
$C^{-1}(\mathscr{M})C$. If $C$ is a unimodular matrix and
$C^{-1}(\mathscr{M})C=(\mathscr{M})$, then $C^{-1}(\mathscr{U})C$ will
again be equal to $(\mathscr{U})$. In this connexion, it is then of
interest to study the mappings $R\to U^{-1}RU$ for $R\in\mathfrak{H}$
and unimodular $U$. This, as we shall presently see, leads us to the
general modular groups associated with\pageoriginale $(\mathscr{M})$.

\begin{proposition}\label{prop17}
Let $U$ be a unimodular matrix such that the mapping $R\to U^{-1}RU$
is a mapping of $\mathfrak{H}$ into itself where $\mathfrak{H}$ is a
space of $R$-matrices associated with $(\mathscr{M})$ as above. Then
the mapping $M\to U^{-1}MU$ is an automorphism of
$(\mathscr{M})$. Further, the mapping $R\to U^{-1}\break RU$ is onto
$\mathfrak{H}$. 
\end{proposition}

\begin{proof}
Let $R$ be generic in $\mathfrak{H}$. Then $U^{-1}RU\in\mathfrak{H}$
and by the very construction of $\mathfrak{H}$, $U^{-1}RU$ admits
elements of $(\mathscr{M})$ as commutators. In other words, the
elements of $U(\mathscr{M})U^{-1}$ commute with $R$. But $R$ is
generic and the elements of $U(\mathscr{M})U^{-1}$ are rational so
that $U(\mathscr{M})U^{-1}\subset (\mathscr{M})$. By considerations of
rank, we see that $U(\mathscr{M})U^{-1}=(\mathscr{M})$, actually. Let
$R\in\mathfrak{H}$; then we claim that $R=U^{-1}R_{1}U$ for some
$R_{1}\in\mathfrak{H}$. For, the algebra $U^{-1}(\mathscr{M})U$ leads
us to another space $\mathfrak{H}_{1}$ of $R$-matrices having $U'AU$
for a principal matrix and admitting
$(\mathscr{M})=U^{-1}(\mathscr{M})U$ as algebra of multipliers. But
for the generic elements $R$ of $\mathfrak{H}$, $U^{-1}RU$ is again
generic and belongs to $\mathfrak{H}\cap \mathfrak{H}_{1}$. Thus by
Proposition \ref{chap1:prop14}, $\mathfrak{H}=\mathfrak{H}_{1}$ and in other
words, the mapping $R\to U^{-1}RU$ is onto $\mathfrak{H}$. The
proposition is proved.
\end{proof}

From the working above, we see that, for a generic $R\in\mathfrak{H}$,
both $A$ and $U'AU$ are principal matrices. Hence by Proposition
\ref{chap1:prop15}, $U'AU=AM$ for a positive element in
$(\mathscr{M})$. Rewriting this, we have (since $U^{\ast}=A^{-1}U'A$)
\begin{equation*}
U^{\ast}U=M, \text{ \ for a positive element \ } M\in
(\mathscr{M}).\tag{120}\label{eq120} 
\end{equation*}
If $U$ is a unimodular matrix satisfying \eqref{eq120}, then it is easy
to verify that the mapping $R\to U^{-1}RU$ is a mapping of
$\mathfrak{H}$ onto itself and hence
$U^{-1}(\mathscr{M})U=(\mathscr{M})$. 

It\pageoriginale is easy to verify that the $2n$-rowed unimodular
matrices $U$ satisfying \eqref{eq120} for some positive element
$M\in(\mathscr{M})$ constitute a group $\Gamma_{0}$ which is the {\em
  most general} form of the {\em homogeneous modular group of degree}
$n$. The group $\Gamma_{0}$ contains a trivial normal subgroup
$\Delta$ consisting of all $U\in\Gamma_{0}$, for which $U^{-1}RU=R$
for every $R\in\mathfrak{H}$. For any $U\in\Gamma_{0}$, the mappings
$R\to U^{-1}RU$ and $R\to (MU)^{-1}RMU$ are the same, whatever be $M$
in $\Delta$. The group $\Gamma_{0}/\Delta$ is the most general form of
the {\em inhomogeneous modular group of degree $n$.}

It is trivial to see that for $U\in\Delta$, $UM\in(\mathscr{M})$ for
every $M\in(\mathscr{M})$. For, taking a generic $R\in\mathfrak{H}$,
$UMR=URM=RUM$, \ie $UM\in(\mathscr{M})$.

We shall now define two subgroups $\Gamma_{1}$, $\Gamma_{2}$ of
$\Gamma_{0}$ such that $\Gamma_{1}$ is of finite index in $\Gamma_{0}$
and $\Gamma_{2}$ is of finite index in $\Gamma_{1}$ and each one of
them containing $\Delta$.

Now, under the automorphism $M\to U^{-1}MU$ of $(\mathscr{M})$, the
centre $(\mathfrak{R})$ is taken onto itself \ie
$U^{-1}(\mathfrak{R})U=(\mathfrak{R})$. But the centre
$\mathfrak{R}$ being an algebraic number field of finite degree over
$\mathbb{Q}$, admits only finitely many automorphisms over
$\mathbb{Q}$ and we define $\Gamma_{1}$ to be the subgroup of
$U\in\Gamma_{0}$ which correspond to the identity automorphism of
$\mathfrak{R}$. In other words,
$$
\Gamma_{1}=\left\{U\in\Gamma_{0}\int U^{-1}KU=K,\text{ \ for every \ }
K\in (\mathfrak{R})\right\}
$$
It is easy to see that $\Gamma_{1}$ is of finite index in
$\Gamma_{0}$. Moreover $\Gamma_{1}\supset \Delta$; for, if
$K\in(\mathfrak{R})$ and $U\in\Delta$, then, by our remark above,
$U\in(\mathscr{M})$ and therefore $KU=UK$. We call the group
$\Gamma_{1}$, the {\em homogeneous modular group of degree $n$ in the
  wide sense} and the quotient\pageoriginale group
$\Gamma_{1}/\Delta$, the {\em inhomogeneous modular group of degree
  $n$ in the wide sense.}

If $U\in\Gamma_{1}$, we see than that the mapping $M\to U^{-1}MU$ of
$(\mathscr{M})$ is an automorphism of $(\mathscr{M})$ which is
identity on the centre $(\mathfrak{R})$. Thus, by Skolem's Theorem
\eqref{eq23}, here exists $M_{1}\in (\mathscr{M})$ such that for every
  $M\in(\mathscr{M})$, we have $U^{-1}MU=M^{-1}_{1}MM_{1}$ \ie
  $UM^{-1}_{1}M=MUM^{-1}_{1}$. In other words,
  $UM^{-1}_{1}=T_{1}\in(\mathscr{F})$, or 
\begin{equation*}
U=M_{1}T_{1}=T_{1}M_{1}\text{ \ with \ } T_{1}\in (\mathscr{F}),
M_{1}\in (\mathscr{M})\tag{121}\label{eq121}
\end{equation*}
The decomposition \eqref{eq121} of $U\in\Gamma_{1}$ is clearly not
unique. Now $U^{\ast}U=M_{0}$ for a positive element
$M_{0}\in(\mathscr{M})$ and this gives
$T^{\ast}_{1}M^{\ast}_{1}M_{1}T_{1}=M_{0}$ or
$T^{\ast}_{1}T_{1}=(M^{-1}_{1})^{\ast}M_{0}M^{-1}_{1}=M_{2}$ in
$(\mathscr{M})$. Since $M_{0}$ is a positive element in
$(\mathscr{M})$, so is $M_{2}$, by Proposition \ref{chap1:prop12}. But since
$M_{2}\in(\mathscr{M})\cap (\mathscr{F})=(\mathfrak{R})$ and since
$M_{2}=M^{\ast}_{2}$, it is immediate that $M_{2}$ represents a
totally positive number in $\mathscr{Z}$. Thus, for $T_{1}$ in
\eqref{eq121}, we have
\begin{equation*}
T^{\ast}_{1}T_{1}=K_{1}\text{ \  totally positive in \ }
(\mathscr{L})\tag{122}\label{eq122} 
\end{equation*}

Suppose for $U\in\Gamma_{1}$, we have two decompositions as in
\eqref{eq121}, say $U=T_{1}M_{1}=T_{2}M_{2}$. Then it is immediate that
$T_{1}=T_{2}K$ for some $K\in(\mathfrak{R})$. We now claim that in the
decomposition $U=T_{1}M_{1}$ as in \eqref{eq121}, we can, by replacing
$T_{1}$, $M_{1}$ respectively by $T_{1}K^{-1}$, $KM_{1}$ with suitable
$K\in(\mathfrak{R})$, ensure that $KM_{1}$ is integral and furthermore
that $T_{2}=T_{1}K^{-1}$ has the following property, namely, {\em
  there exists $d>0$ in $\mathbb{Z}$ (depending only on
  $(\mathscr{M})$ and not on $T_{2}$) for which $dT_{2}$ is integral.}
Thus in \eqref{eq121}, we can suppose already that $M_{1}$ is {\em
  integral} and\pageoriginale $T_{1}$ {\em if of ``bounded
  denominator''} (We shall briefly sketch a proof of this fact in a
special case. Let $\mathscr{V}$ be an indefinite quaternion algebra
over $\mathbb{Q}$ and $q=1$. Then $\mathscr{V}$ has a splitting field
$\mathfrak{Z}=\mathbb{Q}(\sqrt{a})$ with $a<0$ in $\mathbb{Z}$. For a
element $\delta=\xi+\eta j\in\mathscr{V}$ with $\xi$,
$\eta\in\mathfrak{Z}$ and $j^{2}=b(>0)$ in $\mathbb{Z}$, we
have the representation $(\mathscr{M})$ of $\mathscr{V}$ given by
$\delta\to M=K_{1}[D \; \ob{D}]K^{-1}_{1}$ where
$D=\left(\begin{smallmatrix} \xi & \eta\\ \mathfrak{b}\ob{\eta}
  &\ob{\xi}\end{smallmatrix}\right)$, $P_{1}=\left(\begin{smallmatrix}
  1 & 1 \\ \sqrt{a} & -\sqrt{a}\end{smallmatrix}\right)$,
$K_{1}=P_{1}\times E_{2}$. The commutator algebra $(\mathscr{F})$ of
$(\mathscr{M})$ is precisely the set of
$T=K_{1}\left(\begin{smallmatrix} \lambda E_{2} &
  \mu\mathscr{F}\\ \ob{\mu}\mathscr{F} & \ob{\lambda}E_{2}
\end{smallmatrix}\right)K^{-1}_{1}$  where $\lambda$,
$\mu\in\mathfrak{Z}$ and $\mathscr{F}=\left(\begin{smallmatrix} 0 &
  1\\ b &0\end{smallmatrix}\right)$. Let $T\in(\mathscr{F})$ be such
  that
\begin{equation*}
TM=U\tag{$\ast$}
\end{equation*}
with $M \varepsilon (\mathscr{M})$ and $U$, unimodular. In $(\ast)$,
we can suppose that $M$ is 
integral already, by replacing $T$, $M$ respectively by $m^{-1}T$,
$mM$ for a suitable $m\in\mathbb{Z}$. Let $\omega_{1}$, $\omega_{2}$
be a basis over $\mathbb{Z}$ for the integers in
$\mathfrak{Z}$. Denote the matrix $\left(\begin{smallmatrix}
  \omega_{1} & \ob{\omega}_{1}\\ \omega_{2} & \ob{\omega}_{2}
\end{smallmatrix}\right)$ by $P$ and $P_{1}P^{-1}$ by $P_{2}$. We can
find $\nu_{1}$, $\nu_{2}\in\mathbb{Z}$ such that $\nu_{1}P_{1}$,
$\nu_{1}P^{-1}_{1}$, $\nu_{2}P_{2}$, $\nu_{2}P^{-1}_{2}$ have elements
which are integers in $\mathfrak{Z}$. Since
$M=K_{1}\left(\begin{smallmatrix} D & \ub{0}\\ 0 & D
\end{smallmatrix}\right)K^{-1}_{1}$ and
$U=K_{1}\left(\begin{smallmatrix} \lambda E_{2} &
  \mu\mathscr{F}\\ \ob{\mu}\mathscr{F} & \ob{\lambda}E_{2}
\end{smallmatrix}\right)K^{-1}_{1}M$ are both integral, we see that
$\nu^{2}_{1}D$, $\nu^{2}_{1}\lambda D$,
$\nu^{2}_{1}\mu\mathscr{F}\ob{D}$, $\nu^{2}_{1}\ob{D}$ are all
integral. Let $\mathscr{G}_{1},\ldots,\mathscr{G}_{h_{0}}$ be {\em
  fixed} integral ideals (say, of minimum norm) in the $h_{0}$ ideal
classes of $\mathfrak{Z}$. Then there exists $\alpha\in \mathfrak{Z}$
and an ideal $\mathscr{G}_{\rho}(1\leq \rho \leq h_{0})$ such that
$\nu^{2}_{1}D=\alpha\left(\begin{smallmatrix} \xi_{1} &
  \eta_{1}\\ \mathfrak{b}\ob{\eta}_{1} & \ob{\xi}_{1}
\end{smallmatrix}\right)$ and $\xi_{1}$, $\eta_{1}$, $b\ob{\eta}_{1}$,
$\ob{\xi}_{1}$ have the greatest common\pageoriginale divisor
$\mathscr{G}_{\rho}$. Define
$T_{1}=(\nu_{1}\nu_{2})^{-2}K_{1}\left(\begin{smallmatrix}
  \lambda\alpha E_{2} &
  \mu\ob{\alpha}\mathscr{F}\\ \ob{\mu}\alpha\mathscr{F} &
  \ob{\lambda}\ob{\alpha}E_{2}\end{smallmatrix}\right)K^{-1}_{1}$ and
$M_{1}=(\nu_{1}\nu_{2})^{2}K_{1}\left(\begin{smallmatrix} \alpha^{-1}D
  & 0\\ 0 &
  \ob{\alpha}^{-1}\ob{D}\end{smallmatrix}\right)K^{-1}_{1}$. It is
clear that $M_{1}$ is in $(\mathscr{M})$ and is integral; further
$T_{1}M_{1}=U$. Moreover, if we define
$d=b\nu^{4}_{1}\nu^{2}_{2}\prod\limits^{h_{0}}_{k=1}N(\mathscr{G}_{k})$
(where $N(\mathscr{G}_{k})$ denotes the norm of $\mathscr{G}_{k}$ over
$\mathbb{Q}$), we see that $dT_{1}$ is integral).

Let us denote by $\Gamma_{2}$, the subgroup of $U\in\Gamma_{1}$ for
which there is a decomposition of the form \eqref{eq121} with unimodular
$T_{1}$ and $M_{1}$ in $(\mathscr{F})$ and $(\mathscr{M})$
respectively. We now prove

\begin{proposition}\label{prop18}
The group $\Gamma_{2}$ is of finite index in $\Gamma_{1}$.
\end{proposition}

\begin{proof}
Let $U_{1}$, $U_{2}$ be in $\Gamma_{1}$ and $U_{1}=T_{1}M_{1}$,
$U_{2}=T_{2}M_{2}$ be the decompositions of $U_{1}$, $U_{2}$ as in
\eqref{eq121}. Now, as we remarked, $M_{1}$, $M_{2}$ may be supposed to
be integral. We shall now prove that if $M_{1}\equiv M_{2}\pmod{d}$,
then $U^{-1}_{2}U_{1}\in\Gamma_{2}$. Since the number of residue
classes of $2n$-rowed integral square matrices modulo $d$, is finite,
it will follow that $\Gamma_{2}$ is of finite index in
$\Gamma_{1}$. So let 
\begin{equation*}
M_{1}\equiv M_{2}\pmod{d}\tag{123}\label{eq123}
\end{equation*}
It is clear that $dM^{-1}_{1}=dU^{-1}_{1}T_{1}$ and
$dM^{-1}_{2}=dU^{-1}_{2}T_{2}$ are integral. But from \eqref{eq123}, we
have $dE_{2n}\equiv dM_{2}M^{-1}_{1}\pmod{d}$ which means that
$M_{2}M^{-1}_{1}$ is integral. In a similar way, $M_{1}M^{-1}_{2}$ is
also integral so that $M_{2}=WM_{1}$ with unimodular $W$. But now
$U_{2}U^{-1}_{1}=T_{2}M_{2}M^{-1}_{1}T^{-1}_{1}=T_{2}T^{-1}_{1}W$ so
that $T_{2}T^{-1}_{1}$ is itself unimodular in $(\mathscr{F})$. Thus
$U_{2}U^{-1}_{1}$ is in $\Gamma_{2}$ which is what we sought to prove.

If $U\in\Delta$, then $U\in(\mathscr{M})$ and therefore $\Delta\subset
\Gamma_{2}$. 
\end{proof}

We\pageoriginale define now another group $\overdot{\Gamma}_{2}$
consisting of unimodular matrices $T_{1}\in(\mathscr{F})$ for which
$T^{\ast}_{1}T_{1}=K$ which represents a totally positive unit in
$\mathscr{L}$. It is clear that $\overdot{\Gamma}_{2}\subset
\Gamma_{2}$. Defining $\overdot{\Delta}_{2}$ as the subgroup of
$\overdot{\Gamma}_{2}$ consisting of unimodular $U\in(\mathfrak{R})$,
we see that $\overdot{\Delta}_{2}\subset \Delta$.

Any $U\in\Gamma_{2}$ is of the form $T_{1}M_{1}$ with unimodular
$T_{1}$ in $(\mathscr{F})$ satisfying \eqref{eq122} and unimodular
$M_{1}$ in $(\mathscr{M})$. Since $T_{1}$ and $T^{\ast}_{1}$ in this
decomposition commute, we get, by iteration,
\begin{equation*}
(T^{\ast}_{1})^{-1}T^{1}_{1} = (T^{1}_{1})^{\ast}T^{1}_{1} =
  K^{1}_{1}\tag{124}\label{eq124}   
\end{equation*}
for every positive integer $l$. Now although $T_{1}$ is unimodular,
$T^{\ast}_{1}$ is not necessarily integral so that $K_{1}$ is not
necessarily integral. But since $dT^{l}_{1}$ is integral and $A$ is
fixed, we see that $K^{l}_{1}$ is of bounded denominator for every
$l>0$, from \eqref{eq124}. This is impossible, unless $K_{1}$ represents
an integer in $(\mathscr{Z})$. By the same argument, we can show that
$K_{1}$ is integral so that $K_{1}$ is actually a (totally positive)
unit in $(\mathscr{Z})$. Thus for $U=T_{1}M_{1}\in\Gamma_{2}$ with
$T_{1}\in(\mathscr{F})$, we see first that
$T_{1}\in\overdot{\Gamma}_{2}$ and furthermore, for any
$T_{1}\in\overdot{\Gamma}_{2}$. 
\begin{equation*}
T^{\ast}_{1}T_{1}=K_{1}, \text{ \ a totally positive {\em unit} in \ }
(\mathscr{Z}) \tag{125}\label{eq125}
\end{equation*}

We construct a mapping $\psi$ of $\Gamma_{2}$ into
$\overdot{\Gamma}_{2}/\overdot{\Delta}_{2}$ by defining $\psi(U)=$ the
coset of $\overdot{\Gamma}_{2}$ modulo $\overdot{\Delta}_{2}$
containing $T_{1}$ where $T_{1}$ in $(\mathscr{F})$ occurs in the
decomposition $U=T_{1}M_{1}$. It is clear that $\psi$ is well-defined,
for if $U=T_{1}M_{1}=T_{2}M_{2}$, then $T^{-1}_{2}T_{1}\in
\overdot{\Delta}_{2}$ by using \eqref{eq125}. Further clearly $\psi$ is
a homomorphism of $\Gamma_{2}$ onto
$\overdot{\Gamma}_{2}/\overdot{\Delta}_{2}$, the kernal being exactly
$\Delta$. Thus we have proved that
$$
\Gamma_{2}/\Delta { \ \text{\em is isomorphic to} \ }
\overdot{\Gamma}_{2}/\overdot{\Delta}_{2} 
$$

The\pageoriginale group $\overdot{\Gamma}_{2}/\overdot{\Delta}_{2}$ is
referred to as the {\em inhomogeneous modular group of degree $n$.}

Finally, we define the group $\overdot{\Gamma}_{3}$ as the subgroup of
$T\in\overdot{\Gamma}_{2}$ for which
\begin{equation*}
T^{\ast}T=E\tag{126}\label{eq126}
\end{equation*}
and $\overdot{\Delta}_{3}$ as the subgroup of $K$ in
$\overdot{\Gamma}_{3}$ for which $K\in(\mathfrak{R})$. It is easy to
see that $\overdot{\Delta}_{3}$ is precisely the set of roots of unity
in $(\mathscr{F})$ which belong to the order $(\mathscr{U})$ in
$(\mathscr{M})$ and therefore, $\overdot{\Delta}_{3}$ is
finite. Although, in view of \eqref{eq117}, the definition \eqref{eq126}
of $\overdot{\Gamma}_{3}$ apparently depends on $\underset{q}{G}$, it
is trivial to verify that \eqref{eq126} depends only on
$\underset{q}{F}$.
\vskip -50pt


\begin{proposition}\label{chap1:prop19}
The group $\overdot{\Gamma}_{3}/\overdot{\Delta}_{3}$ is of finite
index in $\overdot{\Gamma}_{2}/\overdot{\Delta}_{2}$.
\end{proposition}


\begin{proof}
Let $\mathscr{E}$ be the group of all totally positive units in
$(\mathscr{Z})$, $\mathscr{E}_{1}=\mathscr{E}\cap (\mathscr{U})$ and
$\mathscr{E}_{2}$, the group of squares of elements in
$\mathscr{E}_{1}$. By Dirichlet's theorem on units in algebraic number
fields, there exist finitely many elements $L_{1},\ldots,L_{a}$ of
$\mathscr{E}$ such that any $K$ in $\mathscr{E}$ is of the form
$K = NL_{\nu}$ for some $N\in\mathscr{E}_{2}$ and some $L_{\nu}$. Let
now $T_{1}\in\overdot{\Gamma}_{2}$ satisfy \eqref{eq125} and let
$K_{1}=N^{2}_{1}L_{\nu}$ for some $N_{1}\in\mathscr{E}_{1}$ and some
$L_{\nu}$. Thus
\begin{equation*}
(N^{-1}_{1}T_{1})^{\ast}(N^{-1}_{1}T_{1})
=L_{\nu}\tag{127}\label{eq127}
\end{equation*}
On the other hand, let $A_{\nu}$ unimodular in $(\mathscr{F})$ be a
fixed matrix satisfying $A^{\ast}_{\nu}A_{\nu}=L_{\nu}$, for $1\leq
\nu\leq a$. Then clearly
$N^{-1}_{1}T_{1}A^{-1}_{\nu}\in\overdot{\Gamma}_{3}$ \ie
$T_{1}=N_{1}BA_{\nu}$ for $N_{1}\in(\mathscr{Z})$,
$B\in\overdot{\Gamma}_{3}$ and one of the finitely many matrices
$A_{1},\ldots,A_{a}$. It is immediate using \eqref{eq127} that
$\overdot{\Gamma}_{3}/\overdot{\Delta}_{3}$ is of finite index in
$\overdot{\Gamma}_{2}/\overdot{\Delta}_{2}$. 
\end{proof}

The\pageoriginale group $\overdot{\Gamma}_{3}$ is called the {\em
  homogeneous modular group of degree $n$ in the restricted sense.}
The quotient $\overdot{\Gamma}_{3}/\overdot{\Delta}_{3}$ is the {\em
  inhomogeneous modular group in the restricted sense.}

The groups $\overdot{\Gamma}_{2}/\overdot{\Delta}_{2}$ and
$\overdot{\Gamma}_{3}/\overdot{\Delta}_{3}$ occur in the literature
already in special cases.

In face, taking $\mathscr{V}=\mathfrak{R}$, a totally real field over
$\mathbb{Q}$, $q=2$ and with obvious restrictions on $(\mathscr{M})$
we see that they are nothing but the inhomogeneous Hilbert modular
group over $\mathfrak{R}$, in the wide sense and in the narrow sense
respectively.

It might be of interest to construct fundamental regions for these
groups in $\mathfrak{H}$ and study the automorphic functions on
$\mathfrak{H}$ relative to these groups. We refer the interested
reader to some recent work of K.G.\@ Ramanathan \eqref{eq15} in this
direction. 

We might conclude with an outline of a method of constructing a
fundamental region in the $\mathfrak{H}$-space, for one of the groups
above, say $\overdot{\Gamma}_{3}$. The group $\overdot{\Gamma}_{3}$
acts on $\mathfrak{H}$ as follows; namely, to
$T\in\overdot{\Gamma}_{3}$ corresponds the mapping $R\to T^{-1}RT$ of
$\mathfrak{H}$ onto itself. Let $A$ be a principal matrix for
$\mathfrak{H}$. We simplify our problem by considering the matrices
$AR=S=S'>0$ (for $R\in\mathfrak{H}$). In terms of $S$, the mapping
$R\to T^{-1}RT$ is just the mapping $S\to T'ST$. By Minkowski's
``reduction theory'' for unimodular matrices acting on the space of
$2n$-rowed real symmetric positive-definite matrices, we know that
corresponding to the given $S$, there exists a unimodular matrix $T$
such that $S_{1}=T'ST$ lies in the ``reduced'' Minkowski domain
$\mathscr{F}_{2n}$. But\pageoriginale $T$ may not belong to
$\overdot{\Gamma}_{3}$. On the other hand, we know that
$R^{2}=-E_{2n}$ \ie $A^{-1}SA^{-1}S=-E_{2n}$ \ie $A'S^{-1}A=S$ \ie
$(T'AT)'S^{-1}_{1}(T'AT)=S_{1}$. Again, since $S_{1}$ is ``reduced''
in the sense of Minkowski, we conclude by a theorem of Siegel (Satz 5,
p.200 \cite{20}) that $T'AT$ belongs to a finite set of matrices, say
$T'_{1}AT_{1}$, $T'_{2}AT_{2},\ldots,T'_{\mu}AT_{\mu}$. Now, for any
``reducing'' matrix $T$ obtained as above, we have $T'AT=T'_{k}AT_{k}$
for some 
$$T_{k}(1\leq k\leq \mu). \ie
(TT^{-1}_{k})'A(TT^{-1}_{k})=A.$$ 
In other words,
$(TT^{-1}_{k})^{\ast}(TT^{-1}_{k})=E$ \ie
$TT^{-1}_{k}\in\overdot{\Gamma}_{3}$. It may now be verified as usual 
that
$$\mathscr{F}=\bigcup\limits^{\mu}_{k=1}(A^{-1}{T'}^{-1}_{k}\mathscr{F}_{2n}T^{-1}_{k}\cap    
\mathfrak{H})$$ 
is a fundamental region for $\overdot{\Gamma}_{3}$ in
the $\mathfrak{H}$-space.

\begin{thebibliography}{99}\pageoriginale
\bibitem{1} A.A.\@ Albert~: Structure of Algebras, New York, 1939.

\bibitem{2} -----~: On the construction of Riemann matrices $I$, Ann.\@
of Math., pp.1-28, Vol.35 (1934).

\bibitem{3} -----~: A solution of the principal problem in the theory
  of Riemann matrices, Ann.\@ of Math., pp.\@ 500-515, Vol.35 (1934).

\bibitem{4} -----~: On the construction of Riemann matrices II, Ann.\@
  of Math.\@ pp.376-394, Vol.36 (1935).

\bibitem{5} -----~: Involutorial simple algebras and real Riemann
  matrices, Ann.\@ of Math., pp.886-964 Vol.\@ 36 (1935).

\bibitem{6} -----~: On involutorial algebras, Proc.\@ Nat.Acad.\@
  Sci.\@ U.S.A., pp.480-482, Vol.41, (1955).

\bibitem{7} R.\@ Brauer, H.\@ Hasse and E.\@ Noether~: Beweis
  eines Hauptsatzes in der Theorie der Algebren, Crelle's Journal,
  pp.399-404, vol.167 (1931).

\bibitem{8} M.\@ Deuring~: Algebren, Chelsea, 1948.

\bibitem{9} H.\@ Hasse~: Theory of cyclic algebras over an algebraic
  number field, Trans. A.M.S., pp.170-214, Vol.34 (1932).

\bibitem{10} G.\@ Humbert~: Sur les fonctions ab\'eliennes
  singuli\`eres, Oeuvres, pp.297-498, t.II, 1936.

\bibitem{11} S.\@ Lefschetz~:\pageoriginale On certain numerical invariants of
  algebraic varieties with applications to abelian varieties, Trans.\@
  A.M.S., pp.327-482, Vol.22 (1921).

\bibitem{12} H.\@ Poincar\'e~: Sur la r\'eduction des int\'egrales
  ab\'eliennes, Oeuvres, pp.333-351, t.III, 1934.

\bibitem{13} I.I.\@ Pyatetskii-Shapiro~: Singular Modular functions,
  Izv.\@ Akad.\@ Nauk SSSR, Ser.\@ mat.\@ pp.53-98, Vol.20 (1956)
  (also A.M.S.\@ Translations, Ser.\@ 2., pp.13-58, Vol.10 (1956)).

\bibitem{14} -----~: Theory of modular functions and related questions
  in the theory of discrete groups, Uspekhi Math.\@ Nauk, pp.99-136,
  Tom.\@ XV, No.1 (1960). (also Russian Math.\@ Surveys, pp.97-128,
  Vol.XV (1960)).

\bibitem{15} K.G.\@ Ramanathan~: Quadratic forms over
  involutorial division algebras II, Math.\@ Ann.\@ pp.293-332, Bd.\@
  143 (1961).

\bibitem{16} B.\@ Riemann~: Theorie der abelschen Funktionen,
  Gesamm.\@ Math.\@ Werke, pp.88-142, Dover, 1953.

\bibitem{17} C.Rosati~: Sulle matrici di Riemann, Rend.\@
  Circ.\@ Mat.\@ Palermo, pp.79-134, t.53 (1929).

\bibitem{18} G.\@ Scorza~: Intorno alla teoria generale delle
  matrici di Riemann, Rend.\@ Circ.\@ Mat.\@ Palermo, pp.263-380, t.41
  (1916). 

\bibitem{19} C.L.\@ Siegel~:\pageoriginale Darstellung total positiver
  Zahlen durch Quadrate, Math.\@ Zeit.\@ pp.246-275, Bd.11(1921).

\bibitem{20} -----~: Einheiten quadratischer Formen, Abh.\@ math.\@
  Sem.\@ Hans.\@ Univ., pp.209-239, Bd.\@ 13(1940).

\bibitem{21} -----~: Discontinuous groups, Ann.\@ of Math.,
  pp.674-689, Vol.44 (1943).

\bibitem{22} -----~: Die Modulgruppe in einer einfachen
  involutorischen Algebra, Festschrift Akad.\@ Wiss.\@ G\"ottingen,
  1951.

\bibitem{23} T.\@ Skalem~: Zur Theorie der assoziativen Zahlensysteme,
  Skr.\@ Norske Vid.\@ - Akad., Oslo, pp.21-22, 1927.

\bibitem{24} J.H.M.\@ Wedderburn~: On division algebras, Trans.\@
  A.M.S., pp.129-135, Vol.\@ 22(1921).

\bibitem{25} A.\@ Weil~: Algebras with involutions and the classical
  groups, Jour.\@ Ind.\@ M.S., pp.589-623, Vol.24 (1960).

\bibitem{26} -----~: Introduction \`a l'\'etude des vari\'et\'es
  k\"ahl\'eriennes, Hermann, 1958.

\bibitem{27} H.\@ Weyl~: On generalized Riemann matrices,
  Ann.\@ of Math.\@ pp.714-729, Vol.\@ 35 (1934).

\bibitem{28} -----~: Generalized Riemann matrices and factor sets,
  Ann.\@ of Math.\@ pp.709-745, Vol.37(1936). 
\end{thebibliography}
