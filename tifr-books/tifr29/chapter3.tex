\chapter{Modular forms of Real Dimension}\label{chap3}

\section{Modular forms and Partial Fraction Series}\label{chap3:sec1}\pageoriginale

As the study of `the theta-series associated with a quadratic form'
amply makes it clear, we need to consider modular forms of nonintegral
weight as well, if we wish to apply the theory of modular forms to
number-theoretic problems. Keeping in mind such an objective, we
consider, a little more generally, modular forms of arbitrary real
weight for a horocyclic group $\Gamma$. Unless otherwise stated, the
horocyclic groups under consideration will contain $-E$.

Before defining a modular form of real weight for a horocyclic group
$\Gamma$, we prove, for the sake of completeness, the transformation
formula for the theta-series, which shows that, in general, the
theta-series is not a modular form of integral weight.
$$
\text{Let } \qquad Q [x] = \sum^m_{k,\ell=1}q_{k\ell}
x_kx_{\ell}(q_{k\ell}=q_{\ell k}) 
$$
be a real positive definite quadratic form in $m$
variables. Corresponding to $Q$, we define the theta-series
$$
\vartheta(\tau, Q) = \sum_g e^{\pi i \tau Q[g]}
$$
where $g$ runs over all $m$-rowed columns with integral
coefficients. The series $\vartheta(\tau,Q)$ obviously converges
absolutely and uniformly in any compact set of $\mathscr{G}$ and
therefore represents a regular function of $\tau$ in $\mathscr{G}$. If
the \pageoriginale matrix $Q$ associated to the quadratic form $Q[x]$,
is integral i.e. $q_{k\ell}$ is an integer for $k,\ell=1,2,\ldots, m$,
and if, further, the determinant $|Q|=1$, then $\vartheta(\tau,Q)$
satisfies the transformation formula
$$
\vartheta\left(-\frac{1}{\tau}, Q\right) = (-i\tau)^{m/2} \vartheta(\tau,Q),
$$
where $(-i\tau)^{\alpha}=e^{\alpha \log(-i\tau)}$ abd $\log(-i\tau)$
is positive for $x=0$. In order to prove this, we consider the
function 
$$
f(x,Q) = \sum_g e^{-\pi Q[g+x]}
$$
with $x'=(x_1, x_2,\ldots, x_m)$ and $Q[x]$ a real positive definite
quadratic form. Since $f(x,Q)$ is a periodic function, it has a
Fourier expansion 
$$
f(x,Q) = \sum_g a(g)e^{2\pi i g'x}
$$
where the coefficients $a(g)$ are given by 
\begin{align*}
a(g) & = \int\limits^1_0 \ldots \int\limits^1_0 f(x,Q)e^{-2\pi i g'x}
dx \;\; (dx=dx_1dx_2\ldots dx_n)\\
& = \sum_n\int\limits^1_0 \ldots \int\limits^1_0 e^{-\pi Q[h+x]-2\pi i
g'x} dx\\
& = \int\limits^{\infty}_{-\infty} \ldots \int\limits^{\infty}
_{-\infty} e^{-\pi Q[x]-2\pi i g'x}dx\\
& = e^{-\pi Q^{-1}} [g] \int\limits^{\infty}_{-\infty} \ldots
\int\limits^{\infty}_{-\infty} e^{-\pi Q[x+iQ^{-1}g]}dx.
\end{align*}

If $R$ is a real matrix such that $Q=R'R$, let
$R'^{-1}g=RQ^{-1}g=(a_t)$. Then with the help of the substitution
$y=Rx$, we obtain that 
\begin{align*}
a(g) & = |Q|^{-\frac{1}{2}} e^{-\pi Q^{-1}[g]}
\int\limits^{\infty}_{-\infty} \ldots \int\limits^{\infty}_{-\infty}
e^{-\pi (y+iR'^{-1}g)(y+i R'^{-1}g)} dy\\
& = |Q|^{-\frac{1}{2}} e^{-\pi Q^{-1}[g]} \prod^{m}_{r=1}
\int\limits^{\infty}_{-\infty} e^{-\pi(y_r+ia_r)^2} dy_r\\
& = |Q|^{-\frac{1}{2}} e^{-\pi Q^{-1}[g]}
\left\{\int\limits^{\infty}_{-\infty} e^{-\pi y^2} dy \right\}^m\\
& = |Q|^{-\frac{1}{2}} e^{-\pi Q^{-1}}[g]
\end{align*}\pageoriginale

Substituting the values of $a(g)$ in the Fourier expansion of $f(x,Q)$
we get, for $x=0$,
$$
f(0,Q) = \sum_g e^{-\pi Q [g]} = \sum_g a(g) = |Q|^{-\frac{1}{2}}
\sum_g e^{-\pi Q^{-1}[g]}.
$$

If $t$ is a positive real number, then replacing $Q$ by $tQ$ in the
above relation, we see immediately that
$$
\sum_g e^{-\pi t Q[g]} = (t)^{-\frac{m}{2}} |Q|^{-\frac{1}{2}} \sum_g
e^{-\frac{\pi}{t} Q^{-1}[g]}. 
$$

Our assertion follows at once, on replacing $t$ by $-i\tau$
(i.e. essentially invoking the principle of analytic
continuation). Finally, we assume that $Q$ is integral and $|Q|=1$ so
that on the right hand side of the last relation we can replace $g$
by $Qg$. We state the result proved above in the following 


\setcounter{thm}{14}
\begin{thm}\label{chap3:thm15}
Let $Q$ be an integral symmetric positive matrix of $m$ rows and
determinant 1. Let $Q[x]$ be the quadratic form associated with $Q$. 
Then \pageoriginale the theta series
$$
\vartheta (\tau, Q) = \sum_g e^{\pi i \tau Q[g]}
$$
satisfies the transformation formulae:
$$
\vartheta\left(-\frac{1}{\tau}, Q\right) = (-i\tau)^{\frac{m}{2}}
\vartheta(\tau, Q), \vartheta(\tau+2,Q) = \vartheta(\tau, Q).
$$
\end{thm}

Theorem~\ref{chap3:thm15} shows that the function $\vartheta^8(\tau,Q)$ behaves like
a modular form of weight $4m$ with respect to the substitutions $T$
and $U^2$, which generate $\Gamma_{\vartheta}$, the theta
group. Indeed, $\vartheta^8(\tau,Q)$ is a modular form of weight $4m$
for $\Gamma_{\vartheta}$ and therefore 
$$
\vartheta^8(S<\tau>,Q) = (c\tau+d)^{4m} \vartheta^8(\tau, Q), \text{
  for } S = \begin{pmatrix}
a&b\\c&d
\end{pmatrix} \in \Gamma_{\vartheta}. 
$$
Thus
$$
\vartheta(S<\tau>, Q) = v(S) (c\tau+d)^{\frac{m}{2}}
\vartheta(\tau,Q), 
$$
where $v(S)$ is a certain $8$-th root of unity uniquely determined on
fixing a branch of the multiple-valued function
$(c\tau+d)^{\frac{m}{2}}$. This shows that in order to apply the
theory of modular forms to theta-series for odd $m$, we require the
notion of a modular form of semi-integral weight with multipliers. In
particular, when $Q[x]=x^2$,
$$
\vartheta(\tau) = \vartheta(\tau, Q) = \sum^{\infty}_{n=-\infty}
e^{\pi i \tau n^2}. 
$$

We shall call the multiplier system $v$ of this theta-series, the
\textit{theta multiplier system}.

Let \pageoriginale $r$ be a real number. We define
$$
(c\tau+d)^r = e^{r\log(c\tau+d)} \text{ for real } (c,d) \neq (0,0)
\text{ and } \tau \in \mathscr{G}
$$
with $\log z=\log |z|+i\arg z$, where $\log|z|$ is real and $-\pi
<\arg z\leq \pi$. Obviously
$$
\arg(c\tau+d) = 
\begin{cases}
\arg \left(\tau + \frac{d}{c}\right) + \frac{\pi}{2} (\sgn c-1) & \text{
  for } c \neq 0\\
\frac{\pi}{2} (1-\sgn d) & \text{ for } c=0.
\end{cases}
$$
Let $\underline{M}=(m_1, m_2)$ be a pair of real numbers distinct from
$(0,0)$ and let $S = \left(\begin{smallmatrix}
a&b\\c&d\end{smallmatrix} \right)$ be a real matrix with determinant
1. Then we have 
$$
(m_1S<\tau>+m_2) (c\tau+d) = (m'_1\tau+m'_2),
$$
with $(m'_1, m'_2)=\underline{M}S$. Therefore it follows that 
$$
\log (m_1 S <\tau>+m_2) =\log (m'_1\tau+m'_2) -\log(c\tau+d) + 2\pi i
w(\underline{M}, S)
$$ 
where $w(\underline{M},S)$ is an integer depending on $\underline{M}$
and $S$. We can now conclude that 
$$
w(\underline{M}, S) = \frac{1}{2\pi} \{\arg (m_1S<\tau>+m_2) -
\arg(m'_1\tau+ m'_2) + \arg (c\tau +d)\}.
$$
Obviously $|w(\underline{M},S)|\leq \dfrac{3}{2}$. But
$w(\underline{M}, S)$ is an integer, and therefore $|w(\underline{M},
S)|\leq 1$. Let $M=\left(\begin{smallmatrix}m_0
  &m_3\\m_1&m_2 \end{smallmatrix} \right)$ denote a real matrix of
determinant 1, with $\underline{M}$ as second row. Defining
$\underline{w(M,S)=w(\underline{M},S)}$, we shall now compute $w(M,S)$
explicitly for $M = \left(\begin{smallmatrix}
m_0 & m_3 \\ m_1 & m_2
\end{smallmatrix} \right)$, $S = \left(\begin{smallmatrix}
a&b\\c&d\end{smallmatrix} \right)$ and $MS=\left(\begin{smallmatrix}
\ast & \ast\\ m'_1 & m'_2\end{smallmatrix} \right)$ with
$|M|=|S|=1$. Here and in the sequel, $\underline{M}$ will always
denote the second row \pageoriginale of the matrix $M$.
\begin{enumerate}
\item $\underline{m_1 c m'_1\neq 0}$. By the definition of $w(M,S)$,
\begin{align*}
2\pi w (M,S) & = \arg(S<\tau>+\frac{m_2}{m_1})
-\arg(\tau+\frac{m'_2}{m'_1}) + \arg(\tau+\frac{d}{c})\\
& + \frac{\pi}{2}(\sgn m_1-1) -\frac{\pi}{2} (\sgn m'_1-1) +
\frac{\pi}{2} (\sgn c-1)
\end{align*}
Let $x=\re \tau$ be fixed and $y=\im \tau\to \infty$. Then
\begin{align*}
& S<\tau> + \frac{m_2}{m_1} \to \frac{a}{c}+ \frac{m_2}{m_1} =
  \frac{m'_1}{cm_1},\\
&\arg (S <\tau> + \frac{m_2}{m_1}) \to
  -\frac{\pi}{2} (\sgn m_1 cm'_1 -1),\\
&\arg(\tau+\frac{m'_2}{m'_1}) \to \frac{\pi}{2} \text{ and }
  \arg(\tau+\frac{d}{c}) \to \frac{\pi}{2}.
\end{align*}

This shows that
$$
4w(M,S) = -\sgn m_1 cm'_1 + \sgn m_1- \sgn m'_1 + \sgn c.
$$

\item $\underline{cm_1\neq 0}$, $\underline{m'_1=0}$. Obviously, $m_1=-cm'_2$
  and 
\begin{align*}
2\pi w (M,S) = \arg (S<\tau> + \frac{m_2}{m_1}) & +
\arg(\tau+\frac{d}{c}) + \frac{\pi}{2} (\sgn m'_2-1) \\
& + \frac{\pi}{2} (\sgn m_1-1) +\frac{\pi}{2}(\sgn c-1).
\end{align*}
Let us take $x=-\dfrac{d}{c}$. Then we see immediately that
$$
S<\tau> + \frac{m_2}{m_1} = \frac{i}{c^2y} + \frac{a}{c} +
\frac{m_2}{m_1} = \frac{i}{c^2y}, \quad \tau + \frac{d}{c} = iy. 
$$
and
\begin{align*}
4w(M,S) & = \sgn m_1-1 + \sgn m'_2 + \sgn c\\
& = -(1-\sgn c) (1-\sgn m_1).
\end{align*}

\item $\underline{cm'_1\neq0}$, $\underline{m_1=0}$. It is obvious that
  $m'_1=m_2c$ and 
\begin{align*}
2\pi w (M,S) & = -\arg(\tau + \frac{m'_2}{m'_1}) +
\arg(\tau+\frac{d}{c}) - \frac{\tau}{2}(\sgn m_2-1)\\
& \qquad -\frac{\pi}{2} (\sgn m'_1-1) + \frac{\pi}{2} (\sgn c-1)
\end{align*}\pageoriginale
Letting $y$ tend to $\infty$ for a fixed $x$, we obtain 
\begin{align*}
4w(M,S) & = 1 + \sgn c-\sgn m_2 - \sgn m'_1\\
& = (1+\sgn c)(1-\sgn m_2).
\end{align*}

\item $\underline{c=0}$, $\underline{m_1 m'_1\neq 0}$. We obtain
  immediately that $ad=1$, $S<\tau>=a^2\tau + ab$ and 
\begin{align*}
2\pi w (M,S) & = \arg(S<\tau> + \frac{m_2}{m_1}) -
\arg(\tau+\frac{m'_2}{m'_1}) + \frac{\pi}{2} (\sgn m_1-1)\\
& \qquad - \frac{\pi}{2} (\sgn m'_1-1) -\frac{\pi}{2} (\sgn d-1).\\
\text{or }\quad  4w(M,S)& =1+\sgn m_1-\sgn m'_1-\sgn d\\
&=(1-\sgn a)(1+\sgn m_1)
\end{align*}

\item $\underline{c=m = m'_1 = 0}$. We have $m'_2=m_2d$, $ad=1$ and
\begin{align*}
2\pi w (M,S) & = \frac{-\pi}{2} (\sgn m_2-1) + \frac{\pi}{2} (\sgn
m'_2-1) -\frac{\pi}{2} (\sgn d-1)\\
\text{or }\; 4w(M,S) & = 1 - \sgn d-\sgn m_2 + \sgn m'_2\\
&=(1-\sgn a)(1-\sgn m_2)
\end{align*}
We collect the results above in 
\end{enumerate}

\begin{thm}\label{chap3:thm16}
Let $M=\left(\begin{smallmatrix} \ast&\ast\\m_1 &
  m_2 \end{smallmatrix} \right)$, $S =
\left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix} \right)$ be two
real matrices with determinant $1$ and $(m'_1,m'_2)$ the second row of
the matrix $MS$. Then 
{\fontsize{10}{11}\selectfont
$$
w(M,S) = 
\begin{cases}
\frac{1}{4}\{\sgn c+ \sgn m_1 - \sgn m'_1-\sgn(m_1 c m'_1)\}, &
\text{ if } m_1 cm'_1 \neq 0\\
-\frac{1}{4} (1-\sgn c) (1-\sgn m_1), & \text{ if } cm_1 \neq 0,
m'_1=0\\
\frac{1}{4} (1+\sgn c)(1-\sgn m_2), & \text{ if } cm'_1 \neq 0,
m_1=0\\
\frac{1}{4} (1-\sgn a)(1+\sgn m_1), & \text{ if } m_1 m'_1 \neq 0,
c=0\\
\frac{1}{4} (1-\sgn a)(1-\sgn m_2), & \text{ if } c=m_1=m'_1=0
\end{cases}
$$}\relax

With \pageoriginale the help of the \textit{summand system} $w(M,S)$,
we form the 
\textit{factor system} $\sigma^{(r)}(M,S)$ for an arbitrary real
number $r$, by defining
$$
\sigma(M,S) = \sigma^{(r)} (M,S) = e^{2\pi i r w (M,S)}.
$$
\end{thm}

It is immediate from the definition that 
$$
(m_1 S<\tau>+m_2)^r = \sigma(M,S) \frac{(m'_1\tau+m'_2)^r}{(c\tau+d)^r}.
$$

If $S_1$ and $S_2$ are two real two-rowed matrices with determinant
1, then from the relation $S_1<S_2<\tau>>=S_1S_2<\tau>$, we have 
$$
\sigma(M,S_1S_2) \;\; \sigma(S_1, S_2) = \sigma(MS_1, S_2) \;\;
\sigma(M,S_1). 
$$ 
In particular, we get from theorem~\ref{chap3:thm16} that 
$$
\sigma (S,S^{-1}) = \sigma(S^{-1},S) \text{ and } \sigma(E,S) = \sigma
(S,E) =1.
$$

Since the value of $w(M,S)$ does not depend on the first row of $M$ or
the second column of $S$, we have 
$$
w(U^{\xi} M, S U^{\eta}) = w (M, S), \sigma(U^{\xi} M, S U^{\eta}) = \sigma(M,S),
$$
where $U^{\xi} = \left(\begin{smallmatrix}
  1&\xi\\0&1 \end{smallmatrix} \right)$ for any real number $\xi$.

Let $\Gamma$ be a horocyclic group and let $f(\tau)\not\equiv 0$ be a
function with the transformation property
$$
f(S<\tau>) = v (S) (c\tau+d)^r f (\tau) \text{ for } S =
\left(\begin{smallmatrix} a&b\\c &d \end{smallmatrix} \right)
\in \Gamma. 
$$
It follows immediately from the transformation property of $f(\tau)$
that 
\begin{equation*}
v(-E)(-1)^r = 1, \text{ i.e. } 
v(-E) = e^{-\pi i r}. \tag{1}\label{c3:eq1:1}
\end{equation*}

If \pageoriginale $S=S_1S_2$ with $S_j = \left(\begin{smallmatrix} a_j
  & b_j\\c_j & d_j \end{smallmatrix} \right)\in \Gamma
(j=1,2)$, then  
\begin{align*}
f(S<\tau>) = f(S_1S_2<\tau>) & = v(S_1) (c_1 S_2 <\tau>+d_1)^r
f(S_2<\tau>)\\
& = v(S_1)v(S_2) \sigma (S_1, S_2) (c\tau +d)^r f(\tau),
\end{align*}
where $\overline{S}=(c,d)$. Since $f(\tau)\not\equiv 0$, we have
\begin{equation*}
v(S_1S_2) = \sigma(S_1, S_2) 
v(S_1)v(S_2). \tag{2}\label{c3:eq1:2}
\end{equation*}
We shall call a system of numbers $v(S)$ defined for all $S\in
\Gamma$ a \textit{multiplier system} for the group $\Gamma$ and real
weight $r$, when $|v(s)|=1$ and $v(S)$ satisfies equations 
\eqref{c3:eq1:1} and \eqref{c3:eq1:2}. If $v_1(S)$ 
(respectively $v_2(S)$) is a multiplier system for
the group $\Gamma$ and weight $r_1$ (respectively $r_2$) then
$v_1v_2(S):=v_1(S)v_2(S)$ is a multiplier system for $\Gamma$ and
weight $r_1+r_2$, because
$$
\sigma^{(r_1)} (S_1, S_2) \sigma^{(r_2)}(S_1, S_2) =
\sigma^{(r_1+r_2)} (S_1, S_2). 
$$

Moreover, when $r$ is an even integer, $v(S)$ is an \textit{even
abelian character} of $\Gamma$ i.e. $S \to v(S)$ is a homomorphism
of $\Gamma$ into the multiplicative group of complex numbers of
absolute value 1 such that $v(-S)=v(S)$. This shows that if $v_1(S)$
and $v_2(S)$ are two multiplier system for $\Gamma$ and the same
weight $r$, then $v_0(S):=\dfrac{v_1}{v_2}(S)$ is a multiplier system
for $\Gamma$ and weight $0$; therefore $v_0(S)$ is an abelian
character of $\Gamma$. Hence we obtain all multiplier systems for
$\Gamma$ and weight $r$, from a fixed multiplier system $v_1$, in the
form $v_1v_0$, when $v_0$ runs over the set of even abelian characters
of $\Gamma$. But the group of even abelian characters of $\Gamma$ is
isomorphic to the group $\Gamma/K^{\ast}$, where $K^{\ast}$ denotes
the group generated by the commutator \pageoriginale subgroup of
$\Gamma$ and $-E$. Thus the number of distinct multiplier systems for
any real weight $r$ and group $\Gamma$ is equal to the order of the
group $\Gamma/K^{\ast}$ provided there exists \textit{one} multiplier
system for the weight $r$ and otherwise, it is 0. In particular, if
$\Gamma$ is the modular group, then the number of distinct multiplier
systems for $\Gamma$ and any real weight is 6, since
$(\Gamma:K^{\ast})=6$ as already proved in Chapter~\ref{chap2}, \S~\ref{chap2:sec1} 
and since at least one multiplier system for an arbitrary real weight will be
shown to exist (See proof of theorem~\ref{chap3:thm19} below).

Let us further assume that the above-mentioned function $f(\tau)$ is
regular in $\mathscr{G}$. We shall now examine the behaviour of
$f(\tau)$ at the fixed points of $\Gamma$. Let $\rho$ be a parabolic
cusp of $\Gamma$ and $A = \left(\begin{smallmatrix} a_0&a_3\\a_1 &
a_2 \end{smallmatrix}\right)$ a real matrix of determinant 1 such
that $A<\rho>=\infty$. Let $N$ denote the least positive real number
with the property that 
$$
H = A^{-1} U^N A \in \Gamma, H =
\left(\begin{smallmatrix} h_0 & h_3\\h_1 & h_2 \end{smallmatrix}
\right). 
$$
Obviously, the subgroup of $\Gamma$ which leaves $\rho$ fixed is
generated by $H$ and $-E$. We set 
$$
g(\tau) = (a_1 \tau+a_2)^r f(\tau).
$$
Using the transformation property of $f(\tau)$, we obtain that 
\begin{align*}
g(H<\tau>) & = (a_1H<\tau> + a_2)^r v(H) (h_1\tau+h_2)^r f(\tau)\\
& = \sigma(A,H) v(H) (a_1 \tau+a_2)^rf(\tau)\\
& = e^{2\pi i \kappa} g(\tau), 
\end{align*}
\begin{equation*}
\text{where} \qquad 
 e^{2\pi i \kappa} = \sigma(A,H)v(H), 0\leq \kappa <1.\tag{3}\label{c3:eq1:3}
\end{equation*}
If \pageoriginale we replace $\tau$ by $A^{-1}<\tau>$, then 
$$
g(A^{-1}U^N <\tau>) = e^{2\pi i \kappa} g(A^{-1} <\tau>)
$$
and therefore the function
$$
h(\tau) =g(A^{-1}<\tau>) e^{-2\pi i \kappa \tau/N}
$$
is a periodic function of $\tau$, of period $N$. Hence
\begin{align*}
h(\tau) & = P(e^{2\pi i \tau/N})\\
{\rm or } (a_1\tau+a_2)^r f(\tau) & = e^{2\pi i \kappa
  A<\tau>/N} P(e^{2\pi i A<\tau>/N}), 
\end{align*}
where $P(z)$ is a convergent Laurent series in $z$. If we assume that
$P(z)$ does not contain negative powers of $z$, then 
\begin{equation*}
(a_1\tau + a_2)^r f(\tau) = \sum_{n+\kappa \geq 0} c_{n+\kappa}
  e^{2\pi i (n+\kappa) A<\tau>/N}. \tag{4}\label{c3:eq1:4}
\end{equation*}

Let $\tau_0$ be an elliptic fixed point of $\Gamma$. Since the
transformation 
$$
z=\frac{\tau -\tau_0}{\tau-\overline{\tau}_0} = A<\tau>, \quad A
= \begin{pmatrix}
1 & -\tau_0\\
1 & -\overline{\tau}_0
\end{pmatrix}
$$
maps the complex conjugate fixed points $\tau_0$ and
$\overline{\tau_0}$ to the points $0$ and $\infty$ respectively, the
group $A\Gamma A^{-1}$ has the elliptic fixed point pair 0 and
$\infty$. Therefore for some real number $\vartheta$, the matrix
$\left(\begin{smallmatrix} e^{i\zeta} & 0\\ 0 &
  e^{-i\vartheta}\end{smallmatrix} \right)$ belongs to $A\Gamma
A^{-1}$ and the set of all real numbers $\vartheta$, such that
$\left(\begin{smallmatrix} e^{i\vartheta}& 0\\ 0 &
  e^{-i\vartheta}\end{smallmatrix} \right)$ belongs to $A\Gamma
A^{-1}$, is a discrete module containing $\pi$. If $\vartheta_0$ is
the least positive number in this discrete module, then $\pi =
\vartheta_0\ell$ for \pageoriginale some integer $\ell >0$. Let us set 
$$
L =\begin{pmatrix}
a&b\\c&d
\end{pmatrix} = A^{-1}\begin{pmatrix}
e^{\pi i /\ell} & 0\\
0 & e^{-\pi i /\ell}
\end{pmatrix} A.
$$

Then the order of $L$ is $2\ell$ i.e. $L^{\ell}=-E$ and the group of
transformations of $\Gamma$ which leave $\tau_0$ fixed is generated by
$L$. From the definition of $L$, we obtain that $a-\tau_0c=e^{\pi
i/\ell}$. For
$$
AL = \begin{pmatrix}
a-\tau_0 c & b-\tau_0d\\
a-\overline{\tau}_0 c & b - \overline{\tau}_0 d
\end{pmatrix} = \begin{pmatrix}
e^{\pi i /\ell} & 0\\
0 & e^{-\pi i /\ell}
\end{pmatrix} A = \begin{pmatrix}
e^{\pi i/\ell} & -e^{\pi i/\ell_{\tau_0}}\\
e^{-\pi i /\ell} & -e^{-\pi i/\ell_{\overline{\tau}_0}}. 
\end{pmatrix}
$$
But $a+d=e^{\pi i /\ell} + e^{-\pi i /\ell}$ and therefore 
$$
(c\tau_0+d) = (a+d) -(a-\tau_0c) = e^{-\pi i /\ell}.
$$

If $\varphi(\tau)=(\tau -\overline{\tau}_0)^r$, then
\begin{align*}
\varphi(L<\tau>) & = (L<\tau>-L<\overline{\tau}_0>)^r =
\left(\frac{\tau-\overline{\tau}_0}{(c\tau+d)(c\overline{\tau}_0
  +d)}\right)^r \\
& = \gamma_r (L) (c\tau +d)^{-r} \varphi(\tau),
\end{align*}
with a certain constant $\gamma_r(L)$ depending on $L$. Putting
$\tau=\tau_0$ in the above relation, we see immediately that 
$$
\gamma_r(L) = (c\tau_0+d)^r = e^{-\pi i r/\ell}
$$
Writing 
$$
g(\tau) = (\tau -\overline{\tau}_0)^r f(\tau)
$$
and using the transformation property of $\varphi(\tau)$ and
$f(\tau)$, we obtain
$$
g(L<\tau>) = \varphi(L<\tau>) f(L<\tau>) = e^{-\pi i r/\ell} v(L)
g(\tau). 
$$\pageoriginale
But $L^{\ell}=-E$ and $g(\tau)\not\equiv 0$; therefore applying the
mapping $\tau\to L<\tau>$ successively $\ell$ times, we get 
\begin{align*}
(v(L))^{\ell} & = e^{\pi i r}\\
\text{or } v(L) & = e^{\pi i r/\ell} e^{2\pi i a_0/\ell}
(0\leq a_0 < \ell).
\end{align*}

This implies that 
\begin{align*}
&g(L<\tau>)  = e^{2\pi i a_0/\ell}\\
&\text{i.e. } g(A^{-1} \begin{pmatrix} e^{\pi i /\ell} &
  0\\
0 & e^{-\pi i /\ell}\end{pmatrix}<z>) =e^{2\pi i a_0/\ell}
g(A^{-1}<z>)\\ 
&\qquad\qquad (\text{with } z=A<\tau>).
\end{align*}

Thus the function 
$$
h(z) = z^{-a_0} g(A^{-1}<z>),
$$
which is invariant under the transformation $z\to e^{2\pi i /\ell}$,
has a power-series expansion in terms of the local coordinate
$t=((\tau-\tau_0)/(\tau-\overline{\tau}_0))^{\ell}=z^{\ell}$ at the
point $\tau_0$. We may now conclude that 
\begin{align*}
g(A^{-1}<z>) & = \sum_{\ell n + a_0 \geq 0} c_{n+a_0/\ell}
\;\; t^{n+a_0/\ell}\\
\text{i.e} (\tau-\overline{\tau}_0)^r f(\tau) & = \sum_{n+a_0/\ell
  \geq 0} c_{n+a_0/\ell} \;\; t^{n+a_0/\ell} \; \text{ with } \; 
t=((\tau-\tau_0)/(\tau-\overline{\tau}_0))^{\ell}.\tag{5}\label{c3:eq1:5}
\end{align*}

\begin{defi*}
A function $f(\tau)$ is said to be an \textit{automorphic form of
  weight} $r$ ($r$ a real number) for a horocyclic group $\Gamma$ and
the multiplier system $v(S)$ if 
\begin{itemize}
\item[1)] $f(\tau)$ \pageoriginale is regular in $\mathscr{G}$, 

\item[2)] $f(S<\tau>)=v(S)(c\tau+d)^{-r}f(\tau)$, for $S =
  \left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix} \right)$ in
  $\Gamma$, and 

\item[3)] at every parabolic cusp $A^{-1} <\infty>$ of $\Gamma$,
  $f(\tau)$ has a Fourier expansion given by \eqref{c3:eq1:4} i.e.
\end{itemize}
$$
(a_1\tau+a_2)^rf(\tau) = \sum_{n+\kappa \geq 0} c_{n+\kappa} e^{2\pi
  i(n+\kappa) A<\tau>/N},
$$
where $A=\left(\begin{smallmatrix} a_0&a_3\\a_1&a_2 \end{smallmatrix}
\right)$ is a real matrix of determinant 1. In particular, if, for the
horocyclic group $\Gamma$, we take a subgroup $\Gamma_0$ of finite
index in the modular group then, in condition 3), the matrix $A$
already belongs to the modular group, since every parabolic cusp of
the subgroup $\Gamma_0$ can be obtained in this way. We have shown
above that if $\tau_0$ is an elliptic fixed point of $\Gamma$, then
$(\tau-\overline{\tau}_0)^rf(\tau)$ has a power series expansion given
by \eqref{c3:eq1:5} at the point $\tau_0$. 
\end{defi*}

In the following, we shall \textit{confine ourselves to the subgroups
  of finite index in the modular group}. As in chapter~\ref{chap2}, 
\S~\ref{chap2:sec1}, $\Gamma$ will denote the modular group and the subgroups $\Gamma_0$
under consideration will be assumed to contain $-E$. The set of all
automorphic forms of weight $r$ for the group $\Gamma_0$ and the
multiplier system $v(S)$ forms a vector space over the complex number
field. We shall denote this vector space by $[\Gamma_0, r, v]$.

We shall now show that the power series expansions of $f(\tau)$ at
equivalent points are of the same type, so that the degree of
$f(\tau)$ at any point on the Riemann surface associated with
$\Gamma_0$ is well-defined.
\begin{itemize}
\item[{\textbf{Case 1.}}] Let \pageoriginale $\rho=A^{-1}<\infty>$,
  $\rho^{\ast}=B^{-1}<\infty>=L^{-1}A^{-1}<\infty>$, $L\in
  \Gamma_0$, be two equivalent parabolic cusps of $\Gamma_0$. Then
  $B^{-1}=\pm L^{-1}A^{-1}U^{-k}$ for some integer $k$. Since $-E$
  belongs to $\Gamma$ $\Gamma_0$, we can assume that $B=U^kAL$. Let
  $A= \left(\begin{smallmatrix} \ast & \ast\\a_1 &
    a_2 \end{smallmatrix}\right)$, $B=
  \left(\begin{smallmatrix} \ast & \ast\\b_1 & b_2 \end{smallmatrix}
  \right)$ and $L=\left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix}
  \right)$. Replacing $\tau$ by $L<\tau>$ in~\eqref{c3:eq1:4}, we obtain that 
{\fontsize{10}{12}\selectfont
\begin{align*}
&(a_1 L<\tau>+a_2)^r f(L<\tau>)  = \sum_{n+\kappa \geq 0} c_{n+\kappa}
  \;\; e^{2\pi i (n+\kappa)AL <\tau>/N}\\
&\text{or } (a_1L<\tau>+a_2)^r v(L) (c\tau+d)^r f(\tau)  =
\sum_{n+\kappa \geq 0} c_{n+\kappa} \;\; e^{2\pi i
  (n+\kappa)U^{-\kappa}B<\tau>/N}\\ 
&\text{or } \sigma(A,L) v(L)(b_1\tau+b_2)^r f(\tau)  =
\sum_{n+\kappa\geq 0} (c_{n+\kappa} \;\; e^{-2\pi (n+\kappa)k/N}) \; e^{2\pi
  i(n+\kappa)B<\tau>/N}\\
&\text{or }(b_1\tau+b_2)^rf(\tau)  = \sum_{n+\kappa \geq 0}
c'_{n+\kappa} \;\; e^{2\pi i(n+\kappa)B<\tau>/N}\\
&\text{with } c'_{n+\kappa}  =\frac{c_{n+\kappa}e^{-2\pi
    i(n+\kappa)k/N}}{\sigma(A,L)v(L)}.
\end{align*}}\relax

\item[{\textbf{Case 2.}}] Let $\tau_0$ and $\tau^{\ast}_0$ be two
  equivalent points of $\mathscr{G}$. Then
  $\tau^{\ast}_0=S^{-1}<\tau_0>$ for some
  $S=\left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix} \right)$ in
  $\Gamma_0$. If we replace $\tau$ by $S<\tau>$ in~\eqref{c3:eq1:5}, we obtain that
\begin{align*}
(S<\tau>-\overline{\tau}_0)^r f(S<\tau>) & = v(S)
  (c\tau+d)^r \left(\frac{\tau-\overline{\tau}^{\ast}_0}
  {(c\tau+d)(c\overline{\tau}^{\ast}_0+d)}\right)^r f(\tau)   \\
& = \sum_{\ell n+a_0\geq 0} c_{n+a_0/\ell}
  \left(\frac{\tau-\tau^{\ast}_0}{\tau - \overline{\tau}^{\ast}_0}
  \cdot \frac{c\overline{\tau}^{\ast}_0
    +d}{c\tau^{\ast}_0+d}\right)^{\ell n+a_0}. 
\end{align*}
Thus \pageoriginale
\begin{align*}
(\tau-\overline{\tau}^{\ast}_0)^rf(\tau) & = \sum_{n+a_0/\ell\geq0}
  c^{\ast}_{n+a_0/\ell}
  \left(\frac{\tau-\tau^{\ast}_0}{\tau-\overline{\tau}^{\ast}_0}\right)^{\ell
  n+a_0} \\
\text{with } c^{\ast}_{n+a_0/\ell} & = (c\overline{\tau}^{\ast}_0+d)^r
c_{n+a_0/\ell}
\left(\frac{c\overline{\tau}^{\ast}_0+d}{c\tau^{\ast}_0+d}\right)^{\ell
n+a_0} \gamma(S,\tau_0)
\end{align*}
where $\gamma(S,\tau_0)$ is a complex number of absolute value
1. Hence our assertion is completely proved.
\end{itemize}

We define the degree $v_{\mathfrak{g}}(f)$ of $f(\tau)$ at the point
$\mathfrak{g}$ of $\mathscr{R}_0$, the Riemann surface associated to
$\Gamma_0$, to be the least index $n+\kappa$ (respectively
$n+a_0/\ell$) such that $c_{n+\kappa}\neq 0$ (respectively
$c_{n+a_0/\ell \neq 0}$) according as $\mathfrak{g}$ is an equivalence
class of parabolic cusps or $\Gamma_0$ or not. Obviously, the degree
of $f(\tau)$ at any point $\mathfrak{g}_0$ of $\mathscr{R}_0$ is the
multiplicity of the zero of $f(\tau)$ at $\tau_0\in
\mathfrak{g}_0$ measured in terms of the local coordinate. Thus the
total degree of $f(\tau)$ i.e. the sum of the degrees of $f(\tau)$ at
all points of $\mathscr{R}_0$ is equal to the `number of zeros' of
$f(\tau)$. It can be proved easily that the number $\kappa$ defined by
\eqref{c3:eq1:3} does not depend upon the choice of the cusp $A^{-1}<\infty>$ in
its equivalence class. We shall call the multiplier system $v$ at a
given cusp $A^{-1}<\infty>$ \textit{unramified} (respectively
\textit{ramified}) according as the number $\kappa$ defined above is
zero (or not). 

Let $f(\tau)$ be an element of $[\Gamma_0,r,v]$. Then, at a parabolic
cusp $\rho=A^{-1}<\infty>$ with
$A=\left(\begin{smallmatrix} a_0 & a_3\\a_1 &
  a_2 \end{smallmatrix}\right)\in \Gamma$, the form $f(\tau)$
has a Fourier expansion:
\begin{equation*}
  \left.
    \begin{aligned}
      & f(\tau)\\
      & (\tau -\rho)^r f(\tau)
    \end{aligned}
  \right\}=
  \sum_{n+\kappa \geq 0} b_{n+\kappa} \;\; e^{2\pi i(n+\kappa)A<\tau>/N} 
\begin{array}{l}
\text{ for } \rho = \infty \;\; A = E\\
\text{ for } \rho\neq \infty,
\end{array}
\end{equation*}
because, \pageoriginale if $\rho\neq \infty$, then
$(\tau-\rho)^r=(\tau+a_2/a_1)^r$ is a constant multiple of
$(a_1\tau+a_2)^r$. We define the number
$$
C(\rho) =\begin{cases}
0, & \text{ if $v$ is ramified at $\rho$}.\\
b_0, & \text{ if $v$ is unramified at $\rho$}.
\end{cases}
$$

The complex number $C(\rho)$ defined above does not depend upon the
choice of $A$, because if $B$ is another element in $\Gamma$ such that
$B^{-1}<\infty>=\rho$ , then $A=\pm U^k B$ and this does not affect
the coefficient $b_0$. We associate with $f(\tau)$ the \textit{partial
fraction series}
$$
G(\tau) = C(\infty) + \sum_{\rho\neq \infty} C(\rho) (\tau-\rho)^{-r}. 
$$
We shall prove that the series $G(\tau)$ converges absolutely and
uniformly in every domain $|x|\leq c$, $y\geq \in  >0$ for
$r>2$ and belongs to $[\Gamma_0, r, v]$. In order to find the
contributions of the various cusps to the partial fraction series
$G(\tau)$, we have to consider only those cusps at which the
multiplier system is unramified. In that case, the contribution of the
cusp $\rho = A^{-1}<\infty>$ is the first term of the series 
$$
f(\tau) = (a_1\tau+a_2)^{-r} \sum^{\infty}_{n=0} c_n e^{2\pi i n
  A<\tau>/N}, \text{ with } c_n =c_n(A).
$$

Let $L= \left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix}\right)$
be an element of $\Gamma_0$ and let
$M=AL=\left(\begin{smallmatrix}
  m_0&m_3\\m_1&m_2 \end{smallmatrix}\right)$. Then replacing $\tau$
by $L<\tau>$ in the above series for $f(\tau)$, we obtain that 
\begin{align*}
f(\tau) & =\frac{1}{(a_1L<\tau>+a_2)^rv(L)(c\tau+d)^r}
\sum^{\infty}_{n=0} c_n e^{2\pi i n A L<\tau>/N}\\
& = \frac{1}{\sigma(A,L)v(L)(m_1\tau+m_2)^r} \sum_{n=0} c_n e^{2\pi i
  n M<\tau>/N}
\end{align*}

This \pageoriginale shows that the contribution of the cusp
$L^{-1}A^{-1}<\infty> = L^{-1}<\rho>$ to the series $G(\tau)$ is 
$$
\frac{c_0(A)}{\sigma(A,L)v(L)(m_1\tau+m_2)^r}.
$$
Let $\rho_k=A^{-1}_k<\infty>$, $k=1,2, \ldots, \sigma_0$ be a complete
system of inequivalent parabolic cusps of $\Gamma_0$ at which the
multiplier system is unramified. Then 
\begin{align*}
G(\tau) & = \sum^{\sigma_0}_{k=1} c_0(A_k) G(\tau, A_k)\\
\text{with } G(\tau, A_k) & = \sum_{L^{-1} <\rho_k>}
\frac{1}{\sigma(A_k,L)v(L)(m_1+m_2)^r}
\end{align*}
where $(m_1, m_2)$ is the second row of the matrix $A_kL$ and the sum
runs over those elements $L$ of $\Gamma_0$ which give rise to distinct
cusps in the equivalence class of $\rho_k$. Obviously
$L^{-1}_1<\rho_k>=L^{-1}_2 <\rho_k>$ for $L_i$ in $\Gamma_0(i=1,2)$ if
and only if $L_2 L^{-1}_1$ belongs to the group $Z_k$ generated by
$-E$ and $A^{-1}_k U^{N_k} A_k$, where $N_k$ is the least positive
real number such that $A^{-1}_kU^{N_k}A_k$ belongs to $\Gamma_0$. This
shows that in the summation of the so-called `Eisenstein series'
$G(\tau, A_k), L$ runs over a complete representative system of the
right cosets of $\Gamma_0$ modulo $Z_k$. Thus $G(\tau)$ is a finite
linear combination of Eisenstein series, which converge absolutely and
uniformly in every domain $|x|\leq c$, $y\geq \in >0$ for
$r>2$ and therefore $G(\tau)$ is regular in $\mathscr{G}$. In order to
prove this statement, we have to use the same argument as for
$G_k(\tau)$ in chapter~\ref{chap2}, \S~\ref{chap2:sec4}. Further, if
$S=\left(\begin{smallmatrix} a&b\\c&d \end{smallmatrix}\right)$
belongs to $\Gamma_0$, then 
\begin{align*}
&(c\tau+d)^{-r} G(S<\tau>,A_k)\\
 &\qquad\qquad = \sum_{L^{-1}<\rho>}
  \frac{1}{\sigma(A_k,L)}
  \frac{1}{(c\tau+d)^rv(L)(m_1S<\tau>+m_2)^r}\\
&\qquad\qquad = V(S) \sum_{L^{-1}<\rho>} \frac{1}{\sigma(A_k,L)
    v(S)v(L)\sigma(A_kL,S)(m'_1\tau+m'_2)^r}         
\end{align*}\pageoriginale 
where $(m'_1,m'_2)$ is the second row of the matrix $A_k LS$. But 
$$
\sigma(A_k,L) \sigma (A_k L,S) v(S) v(L) = \sigma (A_k, LS)v(LS)
$$
and $LS$ for a fixed $S$ in $\Gamma_0$ where $L$ runs over a complete
representative system of right cosets of $\Gamma_0$ modulo $Z_k$;
therefore
$$
(c\tau+d)^{-r} G(S<\tau>,A_k) = v(S) G(\tau,A_k),
$$
showing that 
$$
G(S<\tau>) = v(S) (c\tau+d)^r G(\tau) \text{ for } S \in
\Gamma_0. 
$$
In order to prove that $G(\tau)$ belongs to $[\Gamma_0, r, v]$, it
remains to show that $G(\tau)$ does not have negative exponents in its
Fourier expansion at various cusps of $\Gamma_0$. This follows
immediately from 

\begin{lem}\label{chap3:lem4}
Let $\rho_1, \rho_2, \rho_3, \ldots$ be a sequence of distinct real
numbers and $c_0, c_1, c_2, \ldots \ldots$ a sequence of complex
numbers. If the series 
$$
g(\tau) =c_0 + \sum^{\infty}_{n=1} c_n (\tau-\rho_n)^{-r}
$$
converges absolutely at a point of $\mathscr{G}$, then 
\begin{enumerate}
\renewcommand{\labelenumi}{\theenumi)}
\item the series $g(\tau)$ converges absolutely and uniformly in every
  domain $|x|\leq c$, $y\geq \in > 0$ and therefore $g(\tau)$
  is regular in $\mathscr{G}$, and 

\item
\begin{tabbing}
$\lim\limits_{y\to\infty} (A^{-1}<\tau>-\rho)^r g(A^{-1}<\tau>)$ \= =
  \= $ \begin{cases}
c_n & \text{ for } \rho = \rho_n\\
0 & \text{ for } \rho \neq \infty, \rho_1, \rho_2, \ldots
\end{cases}$\\
$\lim\limits_{y\to \infty} g(\tau) = \lim\limits_{y\to \infty}
g(A^{-1}<\tau>)$ \> = \> $c_0$ for $\rho = \infty,$
\end{tabbing}\pageoriginale
uniformly in a given domain $|x|\leq c$. Here $A$ is a real matrix of
determinant 1 such that $\rho=A^{-1}<\infty>$.
\end{enumerate}
\end{lem}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item Let $g(\tau)$ be absolutely convergent at the point
  $\tau_0=x_0+iy_0$ of $\mathscr{G}$. Let $\tau=x+iy$ be in
  $\mathscr{G}$ such that $|x|\leq c$, $y \geq \in >0$. Then
  for $\tau'=x'+iy'=\dfrac{\tau-x_0}{y_0}$, we have $|x'|\leq c'$,
  $y'\geq \in' >0$ for certain $c'$ and $\in'>0$
  depending on $c$ and $\in$. By lemma~\ref{chap2:lem3} 
(chapter~\ref{chap2}, \S~\ref{chap2:sec4}),
  it follows that 
\begin{align*}
& |\tau' + \frac{x_0-\rho_n}{y_0}| \geq \delta |
  i+\frac{x_0-\rho_n}{y_0}| = \frac{\delta}{y_0} |\tau_0-\rho_n|
  \Longrightarrow\\
& |\tau-\rho_n| \geq \delta |\tau-\rho_n|
\end{align*}
for some $\delta =\delta(\in, c)>0$. From here follows
immediately assertion 1) of the lemma.

\item If $\rho=\infty$, then $A^{-1} =
  \left(\begin{smallmatrix} \lambda & a\\0 &
    \lambda^{-1} \end{smallmatrix}\right)$ and $g(A^{-1}<\tau>)$ is a
  series of the same type as $g(\tau)$. Therefore, it suffices to
  consider the case $A=E$. But 
$$
\lim\limits_{y\to \infty} g(\tau) = c_0, \text{ uniformly in }|x|\leq c
$$
follows directly from $\lim\limits_{y\to \infty}(\tau-\rho_n)^{-r}=0$
for all $n$ and the uniform convergence of the series $g(\tau)$.

Let \pageoriginale us now assume that $\rho\neq \infty$. The case
$\rho=\rho_n$ for some $n$ is reduced to the case $\rho \neq \infty$,
$\rho_1, \rho_2, \ldots$, if we replace the series $g(\tau)$ by the
series $g(\tau)-c_n(\tau-\rho_n)^{-r}$, which satisfies the
requirements of lemma~\ref{chap3:lem4}. 
Let $A= \left(\begin{smallmatrix} a_0 & a_3\\a_1 &
  a_2 \end{smallmatrix}\right)$ and $\rho=A^{-1}<\infty> =
-\dfrac{a_2}{a_1}\neq \rho_1, \rho_2 ,\ldots$ After some computation,
we get 
\begin{align*}
A^{-1}<\tau> - \rho & = -1 /(a^2_1(\tau-A<\tau>)),\\
\frac{A^{-1}<\tau>-\rho}{A^{-1}<\tau>-\rho_n} & =
\frac{1}{a^2_1(\rho_n-\rho)(\tau-A<\tau_n>)} 
\end{align*}
Therefore
\begin{align*}
&(A^{-1}<\tau>-\rho)^r g(A^{-1}<\tau>)\\
&\qquad\qquad=a^{-2r}_1
\left\{\frac{c_0\in_0}{(\tau-A<\infty>)^r} +
 \sum^{\infty}_{n=1}
 \frac{c_n(\rho_n-\rho)^{-r}\in_n}{(\tau-A<\rho_n>)^r}
 \right\} 
\end{align*}
with certain $\in_n$ of absolute value 1 for $n\geq 0$. This
is just a series of the type described in lemma 4 but now without
constant term. Thus, as we have proved already
$$
\lim\limits_{y \to \infty} (A^{-1}<\tau>-\rho)^r g(A^{-1}<\tau>) = 0
\text{ uniformly in } |x| \leq c.
$$

This completes the proof of lemma~\ref{chap3:lem4}.

Hence our assertion that the series $G(\tau)$ belongs to $[\Gamma_0,
  r,v]$ follows immediately from lemma~\ref{chap3:lem4}. Moreover, it is obvious from
the above discussion that $f(\tau)-G(\tau)$ is a cusp form i.e. in the
Fourier expansion of $f(\tau)-G(\tau)$ at various cusps of $\Gamma_0$,
the constant term is equal to zero. Thus we have proved the following
\end{enumerate}
\end{proof}

\begin{thm}\label{chap3:thm17}
For every automorphic form $f(\tau)$ belonging to $[\Gamma_0, r,v]$,
$r>2$, there \pageoriginale exists an automorphic form $G(\tau)$ of
the same type in the form of a partial fraction series
$$
G(\tau) = C(\infty) + \sum_{\rho\neq \infty} C(\rho)(\tau-\rho)^{-r}
$$
where $\rho$ runs over all distinct parabolic cusps of $\Gamma_0$
different from $\infty$, such that $f(\tau)-G(\tau)$ is a cusp form.
\end{thm}

With the help of the general theory, we completely characterise some
linear spaces of automorphic forms for the theta-group in the
following 

\begin{thm}\label{chap3:thm18}
Let $v_{\vartheta}$ be the multiplier system for the theta series
$\vartheta(\tau) = \sum^{\infty}_{n=-\infty}e^{\pi i \tau n^2}$ and
$\Gamma_{\vartheta}$, the theta group generated by $\tau$ and
$U^2$. Then the space $[\Gamma, k/2, v^k]$ is generated by
$\vartheta^k(\tau)$ for $k=1,2,3,4,5,6,7$.
\end{thm}

\begin{proof}
We have proved, in chapter~\ref{chap2} \S~\ref{chap2:sec1}, that $\Gamma$ has two inequivalent
parabolic cusps say $\infty$ and 1. Obviously, the multiplier system
$v_{\vartheta}$ is unramified at $\infty$. We shall prove now that
$v_{\vartheta}$ is ramified at the cusp 1. Let $A=TU^{-1}=
\left(\begin{smallmatrix} 0&1\\-1&1 \end{smallmatrix}\right)$, so that
$\infty= A<1>$. Since the width of the cusp sector at the cusp 1 is 1,
the transformation $H=A^{-1}UA$ belongs to $\Gamma$. The
transformation $H=
\left(\begin{smallmatrix} 0&1\\-1&2 \end{smallmatrix}\right)$
generates a cyclic subgroup of $\Gamma_{\vartheta}/\{\pm E\}$ which
has 1 as a fixed point. By theorems~\ref{chap3:thm15} and 
\ref{chap3:thm16}, we obtain that with
respect to the cusp 1
\begin{align*}
e^{2\pi i \kappa} & = \sigma(A,H) v_{\vartheta} (H)\\
& = \sigma \left((-1,1), \begin{pmatrix}
0&1\\-1&2
\end{pmatrix}\right) \sigma ((-1,0), U^{-2}) v_{\vartheta}
(T)v_{\vartheta} (U^{-2})\\
& = v_{\vartheta} (T) = e^{\pi i/4}
\end{align*}
But \pageoriginale by assumption $0\leq \kappa <1$, therefore
$\kappa=1/8$ and if $f(\tau)$ belongs to $[\Gamma_{\vartheta}, k/2,
  v^k_{\vartheta}]$ for $1\leq k <8$, then $f(\tau)$ has an
unavoidable zero at $\tau=1$ of multiplicity $k/8$. Since
$v^8_{\vartheta}(S)=1$ for every $S\in \Gamma_{\vartheta}$,
$f^{24}(\tau)$ belongs to $[\Gamma_{\vartheta}, 12k, 1]$. But we have
already proved that $\Delta^k(\tau)$ belongs to $[\Gamma_{\vartheta},
  12k, 1]$; therefore, the function $f^{24}(\tau)/\Delta^k(\tau)$ is
invariant under the transformations of $\Gamma_{\vartheta}$. This
implies that $f^{24}(\tau)$ and $\Delta^k(\tau)$ have the same number
of zeros on the Riemann surface $\mathscr{R}$ associated to
$\Gamma$. Since $\Delta(\tau)$ has 3 zeros on the Riemann surface,
namely a double zero at $\infty$ and a simple zero at 1, the number of
zeros of $f(\tau)$ is $k/8$. In particular, the number of zeros of
$\vartheta(\tau)$ is $1/8$. But $\vartheta(\tau)$ has a zero of
multiplicity $1/8$ at 1; it follows that on $\mathscr{R}$, it is the
only zero of $\vartheta(\tau)$ and $\vartheta(\tau)\neq 0$ for $\tau$
in $\mathscr{G}$. Thus, for $1\leq k < 8$,
$\vartheta^{-k}(\tau)f(\tau)$ is invariant under the transformations
of $\Gamma_{\vartheta}$ and has no singularities. Consequently,
$\vartheta^{-k}(\tau)f(\tau)$ is constant, which proves our theorem
completely.

It is an immediate consequence of the above theorem that the partial
fraction series of $\vartheta^k(\tau)$ for $5\leq k<8$ is a constant
multiple of $\vartheta^k(\tau)$. But that constant has to be equal to
1, because the constant terms in the Fourier series of the
above-mentioned forms are equal to 1; therefore, from the Fourier
series of $G(\tau)$, we obtain the number of representations of an
integer as sum of $k$ squares for $5\leq k <8$. For $1 \leq k <5$, the
analogous results could be obtained by using the method of Hecke
i.e. by considering the series of the type $G_2(\tau,s)$, which we
introduced in chapter~\ref{chap2}, \S~\ref{chap2:sec4} for deducing an infinite product
expression for $\Delta(\tau)$. 

We \pageoriginale shall now determine all the multiplier systems for
the modular group.
\end{proof}

\begin{thm}\label{chap3:thm19}
For the modular group $\Gamma$ and for every weight $r$, there exist
exactly 6 multiplier systems.
\end{thm}

\begin{proof}
For the proof of the theorem, it is sufficient to prove that for every
weight $r$, there exists a multiplier system as we have already stated
above.

Since $\Delta(\tau)\neq 0$ for $\tau$ belonging to $\mathscr{G}$, we
can define $g(\tau)=\log \Delta(\tau)$ uniquely in $\mathscr{G}$, so
that 
$$
g(\tau) = 2\pi i \tau+24 \sum^{\infty}_{n=1} \log (1-e^{2\pi i
  n\tau}).
$$

We set $\Delta^r(\tau)=e^{rg(\tau)}$. From $\Delta(S<\tau>) =
(c\tau+d)^{12}\Delta(\tau)$, follows that 
$$
\Delta^{r/12} (S<\tau>) = v_0 (S)(c\tau+d)^r\Delta^{r/12}(\tau).
$$
for $S = \left(\begin{smallmatrix}
  a&b\\c&d \end{smallmatrix}\right)\in \Gamma$ with a certain
multiplier system $v_0(S)$ for the group $\Gamma$ and the weight
$r$. Hence the theorem is established.

In order to calculate the multiplier systems explicitly, we proceed as
follows. Let $\tau_0$ denote any one of the elliptic fixed points
$e^{2\pi i/3}, i$ of $\Gamma$. Then
$V=\left(\begin{smallmatrix} 1&1\\-1&0 \end{smallmatrix}\right)$
(respectively $T=\left(\begin{smallmatrix}
  0&1\\-1&0 \end{smallmatrix}\right)$) generates the subgroup of
$\Gamma$ leaving $e^{2\pi i /3}$ (respectively $i$) fixed. Let $v(S)$
be a multiplier system for $\Gamma$ and the weight $r$. Since 
$$
V^3 = T^2=-E, w(T,T) = w(V^2,V) =-1 \text{ and } w(V,V)=0,
$$
we have
\begin{align*}
e^{-\pi ir}& = \sigma(V^2,V) \;\; \sigma(V,V)(v(V))^3 =
\sigma(T,T)(v(T))^2\\
& = e^{-2 \pi i r} (v(V))^3 = e^{-2\pi ir} (v(T))^2,
\end{align*}\pageoriginale
showing that 
\begin{align*}
v(V) & = e^{\frac{\pi ir}{3}+\frac{2\pi i}{3}a}, (a=0,1,2)\\
v(T) & = e^{\frac{\pi i r}{2}+\frac{2\pi i}{2}b}, (b=0,1). 
\end{align*}
But the multiplier system $v$ is uniquely determined by $v(V)$ and
$v(T)$; therefore the six sets of pairs $(a,b)$ completely determine
the multiplier system.

Since $UV=T$, we have obviously
\begin{align*}
v(T) = \sigma(U,V)v(U)v(V) & = v(U)v(V)\\
& = e^{2\pi i\kappa} v(V), 0\leq \kappa < 1,
\end{align*}
because $v(U)=e^{2\pi i \kappa}$, $0\leq \kappa <1$. 

Therefore the following congruence holds:
\begin{equation*}
\frac{r}{12} \equiv \kappa + 
\frac{a}{3} + \frac{b}{2} (\mod 1). \tag{6}\label{c3:eq1:6}
\end{equation*}
It is obvious that for a given $r$, the integers $a,b$ are uniquely
determined by the number $\kappa$. Moreover
$$
\kappa \equiv \frac{r-h}{12} (\mod 1) \text{ for } h = 4a +6b.
$$

Hence we obtain the following table for the values of $h,a$ and $b$,
\begin{center}
\renewcommand{\arraystretch}{1.2}
\tabcolsep=12pt
\begin{tabular}{l|llllll}
h & 0 & 4 & 6 & 8 & 10 & 14\\\hline
a & 0 & 1 & 0 & 2 & 1 & 2\\\hline
b & 0 & 0 & 1 & 0 & 1 & 1\\
\end{tabular}
\end{center}
which \pageoriginale shows that the multiplier system is uniquely
determined by $h$. For instance, for the multiplier system $v_0(S)$
given in the proof of theorem~\ref{chap3:thm19} it can be proved easily that 
$$
v_0(U)=e^{\frac{\pi i r}{6}} \Longrightarrow \kappa \equiv r/12 (\mod
1) \Longrightarrow h = 0 \Longrightarrow a = 0, b=0,
$$
showing that $v_0$ is the multiplier system for $\Gamma$ and weight
$r$ determined by $h=0$. We deduce immediately that the theta
multiplier system cannot be extended to a multiplier system for the
group $\Gamma$, because if it were true, then 
$$
v_{\vartheta} (U^2) = 1 \Longrightarrow v_{\vartheta} (U) = \pm 1
\Longrightarrow \kappa = 0 \text{ or } \frac{1}{2}
$$
and this contradicts the congruence equation~\eqref{c3:eq1:6} when $r=\dfrac{1}{2}$.

Before concluding this section, we give an application of the theory
of modular forms to the theory of quadratic forms. We shall prove here
that an even integral matrix $Q>0$ with $|Q|=1$ exists only if
$m\equiv 0(\mod 8)$, where $m$ is the number of rows and columns of
$Q$. By theorem~\ref{chap3:thm15},
$$
\vartheta(\tau, Q) = \sum_g e^{\pi i \tau Q[g]}
$$
belongs to $[\Gamma, m/2, v]$ for some multiplier system $v$. Since
$Q[x]$ is an even integer for every integral vector $x$, it follows
immediately that $v(U)=1$ implying that $\kappa=0$. Similarly, we
obtain that $v(T)=e^{\pi i m/4}$ and therefore $b=0$. The result now
follows from the congruence relation~\ref{c3:eq1:6}. Conversely, if $m\equiv
0(\mod 8)$, then there exist $m$-rowed even integral matrices, 
as \pageoriginale is well known from the theory of quadratic forms. A
nice construction of such forms has been given by E. Witt with the
help of the theory of lattices in Abh. Math. Sem. Hamburg 14 (1941).
\end{proof}

\section{Poincare Series and Eisenstein Series}\label{chap3:sec2}

In this section, we shall denote, as before, the modular group by
$\Gamma$ and a subgroup of finite index in $\Gamma$ by
$\Gamma_0$. Moreover, unless otherwise stated, $\Gamma_0$ will be
assumed to contain $-E$. We shall construct some special modular forms
called `Eisenstein series' and `Poincar\'e series' which not only
belong to the space $[\Gamma_0, r, v]$ but also generate the same. The
proof of this statement will be completed in the next section. First
of all, we show that $[\Gamma_0, r, v]$ is a vector space of finite
dimension over the complex number field. Let $f(\tau)$ and $g(\tau)$
be two modular forms belonging to $[\Gamma_0, r, v]$ such that
$f(\tau)\not\equiv 0$, $g(\tau)\not\equiv 0$; then $f(\tau)/g(\tau)$
is an automorphic function for the group $\Gamma_0$, which is either a
constant or has the order zero. In any case, $f(\tau)$ and $g(\tau)$
have the same number of (always inequivalent) zeros. Let $\nu$ be the
number of zeros of $f(\tau)$ and $N_0$ the least integer greater than
$\nu$. Let
$$
f(\tau) = \sum_{n+\kappa \geq 0} c_{n+\kappa} e^{2\pi
  i(n+\kappa)\tau/N}, (0\leq \kappa <1)
$$
be the Fourier expansion of $f(\tau)$ at the cusp $\infty$. Then
$f(\tau)$ is determined uniquely by the coefficients $c_{n+\kappa}$
for $n \leq N_0$. Indeed, if $c_{n+\kappa}=0$ for $n\leq N_0$, then
$f(\tau)$ must vanish identically, since the total number $\nu$ of
zeros of $f(\tau)\not\equiv 0$ is less than $N_0$. This shows that a
modular form $f(\tau)$ belonging to $[\Gamma_0, r, v]$ is uniquely
determined by $N_0+1$ coefficients in its Fourier series at $\infty$
and therefore the dimension of $[\Gamma_0, r, v]$ is atmost $N_0+1$. 

We \pageoriginale shall now explain the construction of Poincar\'e
series for $\Gamma_0$. Let $f(\tau)$ be a modular form belonging 
to the space $[\Gamma_0,r,v]$. Then at the cusp $A^{-1}<\infty>$ with
$A=\left(\begin{smallmatrix} a_0 &
  a_3\\a_1&a_2 \end{smallmatrix}\right)\in \Gamma$, $f(\tau)$
has the Fourier expansion
$$
(a_1\tau+a_2)^r f(\tau) = \sum_{n+\kappa\geq 0} c_{n+\kappa} e^{2\pi i
(n+\kappa)A<\tau>/N} 
$$
where $N$ is the least natural number so that $H=A^{-1}U^NA$ belongs
to $\Gamma_0$ and $\kappa$ is a real number determined by
$$
\sigma(A,H)v(H) = e^{2\pi i \kappa}, 0 \leq \kappa < 1.
$$
If we replace $\tau$ by $L<\tau>$ with
$L=\left(\begin{smallmatrix}
a&b\\c&d \end{smallmatrix}\right)\in \Gamma_0$, then by
using the transformation formula for $f(\tau)$, we obtain 
$$
f(\tau) = \sum_{n+\kappa \geq 0} c_{n+\kappa} \frac{e^{2\pi i
    (n+\kappa)M<\tau>/N}}{\sigma(A,L)v(L)(m_1\tau+m_2)^r} 
$$
where $M=AL=\left(\begin{smallmatrix} m_0&m_3\\m_1
  &m_2 \end{smallmatrix}\right)$. We now make use of the
\textit{principle of cross summation} i.e. in the above expansion for
$f(\tau)$, we put $c_{n+\kappa}=0$ for all $n$ except for one fixed
$n$ for which we put $c_{n+\kappa}=1$ and then formally $\sum$ over a
complete system $\gamma(A,\Gamma_0)$ of transformations $M=AL$ with
$L\in \Gamma_0$ such that the second rows $(m_1, m_2)$ of
various matrices $M=AL$ are distinct. In other words, we form the
function 
\begin{equation*}
G_r(\tau, v, A, \Gamma_0, n+\kappa) = \sum_{M\in
  \gamma(A,\Gamma_0)} \frac{e^{2\pi
    i(n+\kappa)M<\tau>/N}}{\sigma(A,L)v(L)
(m_1\tau+m_2)^r}. \tag{1}\label{c3:eq2:1}
\end{equation*}

This series is a so-called \textit{Poincar\'e series for the group}
$\Gamma_0$. We shall now \pageoriginale show that the Poincare series
$G_r(\tau, v, A, \tau_0, n+\kappa)$ does not depend upon any special
choice of the system $\gamma(A,\Gamma_0)$ and for $r>2$, it belongs
to the space $[\Gamma_0, r,v]$. Let $L$ and $L^{\ast}$ be two
transformations of $\Gamma_0$ such that $M=AL$ and
$M^{\ast}=AL^{\ast}$ have the same second row. Then $M^{\ast}=U^kM$
for some integer $k$, therefore 
\begin{align*}
L^{\ast} L^{-1} = A^{-1} U^k A \in \Gamma_0 & \Longrightarrow
N \text{ divides } k\\
& \Longrightarrow L^{\ast} L^{-1} = H^{\ell} \text{ if } k = N\ell
\text{ and } H=A^{-1} U^N A.
\end{align*}

Consequently, we have 
$$
e^{2\pi i (n+\kappa)M^{\ast} <\tau>/N} =e^{2\pi i \kappa \ell} e^{2\pi
i(n+\kappa)M<\tau>/N}.
$$
Further
\begin{align*}
p_{\ell} & = \sigma(A, H^{\ell})v(H^{\ell}) = \sigma(A,H^{\ell})
\sigma(H,H^{\ell-1}) v(H)v(H^{\ell-1})\\
& = \sigma(A,H^{\ell-1}) \;\; \sigma(A,H) v(H) v(H^{\ell-1}) = e^{2\pi i
  \kappa} p_{\ell-1} = e^{2\pi i \ell \kappa}.
\end{align*}
But
\begin{align*}
\sigma(A,L^{\ast}) v(L^{\ast}) & = \sigma(A,H^{\ell}L)v(H^{\ell}L)\\
& = \frac{\sigma(AH^{\ell})
  \sigma(A,H^{\ell})\sigma(H^{\ell},L)}{\sigma(H^{\ell}, L)}
v(H^{\ell}) v(L)\\
& = \sigma (A,L) p_{\ell}v(L) =e^{2\pi i \ell \kappa} \sigma(A,L) v(L);
\end{align*}
therefore, if we take $M^{\ast}$ instead of $M$ in
$\gamma(A,\Gamma_0)$, the contribution to the series $G_r(\tau, v, A,
\Gamma_0, n+\kappa)$ remains unchanged, which proves that the series
does not depend upon any special choice of the system
$\gamma(A,\Gamma_0)$. The series
$\sum\limits_{(m_1,m_2)\neq(0,0)}|m_1\tau+m_2|^{-r}$, which converges
uniformly in every domain \pageoriginale $|x|\leq c$, $y\geq
\in >0$ for $r>2$, is a majorant for the series $G_r(\tau, v,
A, \Gamma_0, n+\kappa)$. It follows in the same manner as for $v=1$,
$n+\kappa=0$, that the series $G_r(\tau, v, A, \Gamma_0, n + \kappa)$
converges absolutely and uniformly in every domain $|x|\leq c$,
$y\geq \in >0$ for $r>2$ and therefore represents a regular
function in $\mathscr{G}$. In order to examine the behaviour of the
series $G_r(\tau, v,A,\Gamma_0 , n+\kappa)$ under the transformations
of the modular group,we define, for any $S\in \Gamma$, the 
transform $v^S$ of the multiplier system $v$ by 
$$
v^S(L^{\ast}) = v(L) \frac{\sigma(L,S)}{\sigma(S,L^{\ast})} \text{
  with } L^{\ast}=S^{-1}LS.
$$
If $L^{\ast}_i =S^{-1}L_i S$ for $L_i \in \Gamma_0 (i=1,2)$,
then 
\begin{align*}
&v^S(L^{\ast}_1 L^{\ast}_2)  = \sigma(L^{\ast}_1, L^{\ast}_2)
v^S(L^{\ast}_1)v^S(L^{\ast}_2) \\
&\Longleftrightarrow v(L_1L_2) \frac{\sigma(L_1L_2,
  S)}{\sigma(S,L^{\ast}_1 L^{\ast}_2)}  = \sigma(L^{\ast}_1,
L^{\ast}_2) v(L_1) \frac{\sigma(L_1,S)}{\sigma(S,L^{\ast}_1)} v(L_2)
\frac{\sigma(L_2,S)}{\sigma(S,L^{\ast}_2)} \\
&\Longleftrightarrow
\frac{\sigma(L_1,L_2)\sigma(L_1L_2,S)}{\sigma(S,L^{\ast}_1
  L^{\ast}_2)} = \frac{\sigma(L^{\ast}_1, L^{\ast}_2) \sigma(L_1,S)
\sigma(L_2,S)}{\sigma(S,L^{\ast}_1)\sigma(S,L^{\ast}_2)}\\
&\Longleftrightarrow \sigma(L_1, L_2 S) \sigma(S,L^{\ast}_1)
\sigma(S,L^{\ast}_2) = \sigma(L_1,S) \sigma(S,L^{\ast}_1)
\sigma(L_1S,L^{\ast}_2)\\
&\Longleftrightarrow \sigma(L_1, L_2 S)\sigma(S, L^{\ast}_1)
\sigma(S,L^{\ast}_2) = \sigma(L_1, S) \sigma(S,L^{\ast}_1)
\sigma(L_1 S, L^{\ast}_2)\\
&\Longleftrightarrow \sigma(L_1, L_2 S) \sigma(S, L^{\ast}_2) 
=\sigma(S,L^{\ast}_2) \sigma(L_1,L_2S).
\end{align*}
Thus $v^S$ is a multiplier system for the group $S^{-1}\Gamma_0 S$ and
weight $r$. In particular, when $S$ belongs to $\Gamma_0$,
$$
V^S(S^{-1}LS) = \frac{\sigma(S^{-1},LS)\sigma(L,S)}{\sigma(S^{-1},S)}
v(L) = \frac{\sigma(L,S)}{\sigma(S,L^{\ast})}v(L),
$$\pageoriginale
showing that $v^S=v$. It can be verified easily that 
\begin{align*}
\gamma(A,\Gamma_0) S & = \Gamma'(A,\Gamma_0) \text{ for } S
\in \Gamma_0, \\
\gamma(A,\Gamma_0)S & = \gamma''(AS,S^{-1}\Gamma_0S) \text{ for } S
\in \Gamma,
\end{align*}
where $\gamma'$ and $\gamma''$ are systems of the same type as
$\gamma$. Let $S=\left(\begin{smallmatrix}
  a&b\\c&d \end{smallmatrix}\right)$ be an arbitrary element of
$\Gamma$ and $(m^{\ast}_1, m^{\ast}_2)$ the second row of the matrix
$MS=ALS$. Since 
\begin{align*}
\sigma(A,L) \;\; \sigma(AL,S)v(L) & =
\frac{\sigma(A,L)\sigma(AL,S)\sigma(S,L^{\ast})}{\sigma(L,S)}
v^S(L^{\ast})\\
& = \sigma(A,LS) \sigma(S,L^{\ast})v^S(L^{\ast})\\
& = \sigma(A,S) \sigma(AS,L^{\ast})v^S(L^{\ast}),
\end{align*}
we have 
\begin{align*}
&\frac{G_r(S<\tau>, v,A,\Gamma_0,n+\kappa)}{(c\tau+d)^r}\\
&\qquad=
\frac{1}{\sigma(A,S)} \sum_{M^{\ast}\in \gamma''
  (AS,S^{-1}\Gamma_0 S)} \frac{e^{2\pi
    i(n+\kappa)M^{\ast}<\tau>/N}}{\sigma(AS,L^{\ast})v^S
  (L^{\ast})(m^{\ast}_1\tau + m^{\ast}_2)^r} 
\end{align*}
where $M^{\ast}=MS=ALS$. This shows that
$$
\frac{G_r(S<\tau>, v, A, \Gamma_0, N+\kappa)}{(c\tau+d)^r} =
\frac{1}{\sigma(A,S)} G_r(\tau, v^S, AS, S^{-1}\Gamma_0S, n+\kappa).
$$
In particular, when $S$ belongs to $\Gamma_0$, we have 
{\fontsize{10}{12}\selectfont
\begin{align*}
\frac{G_r(S<\tau>,v,A,\Gamma_0, n+\kappa)}{(c\tau+d)^r} &
=v(S)\sum_{M^{\ast} \in \gamma'(A,\Gamma_0)} \frac{e^{2\pi
    i(n+\kappa)M^{\ast}
    <\tau>/N}}{\sigma(A,LS)v(LS)(m^{\ast}_1\tau+m^{\ast}_2)^r} \;\; i.e.\\
\frac{G_r(S<\tau>, v, A, \Gamma_0, n+\kappa)}{(c\tau+d)^r} & =v(S) G_r
(\tau, v, A, \Gamma_0, n+\kappa), 
\end{align*}}\relax
since \pageoriginale
\begin{align*}
\sigma(A,L)\sigma(AL,S)v(L) & =
\frac{\sigma(A,L)\sigma(AL,S)}{\sigma(L,S)v(S)} v(LS)\\
& =\frac{\sigma(A,LS)v(LS)}{v(S)}.
\end{align*}

The above discussion shows that for $r>2$, $G_r(\tau, v, A, \Gamma_0,
n+\kappa)$ belongs to $[\Gamma_0, r, v]$ provided the Fourier
expansion of $G_r(\tau, v, A, \Gamma_0, n + \kappa)$ at the cusp
$\infty$ contains no terms with a negative exponent. But the latter is
obvious from the fact that the majorant
$\sum\limits_{(m_1,m_2)\neq(0,0)}|m_1\tau+m_2|^{-r}$ of $G_r(\tau,v,
A,\Gamma_0, n+\kappa)$ is bounded uniformly in $x$ when $y$ tends to
infinity. Hence we have proved 

\begin{thm}\label{chap3:thm20}
The Poincar\'e series
$$
G_r(\tau, v, A, \Gamma_0, n+\kappa) = \sum_{M\in
  \gamma(A,\Gamma_0)} \frac{e^{2\pi i
    (n+\kappa)M<\tau>/N}}{\sigma(A,L)v(L)(m_1\tau+m_2)^r} 
$$
represents a modular form of weight $r$ for the group $\Gamma_0$ and
multiplier system $v$, provided $r>2$. Moreover, it is a cusp form,
for $n+\kappa>0$.
\end{thm}

If the multiplier system $v$ is unramified at the cusp
$A^{-1}<\infty>$ i.e. $\kappa=0$, then for $n=0$
$$
G_r(\tau, v, A, \Gamma_0, 0) = \sum_{M\in \gamma(A,\Gamma_0)}
\frac{1}{\sigma(A,L)v(L)(m_1\tau+m_2)^r}. 
$$
Since we can assume that with $AL$, also $-AL$ belongs to
$\gamma(A,\Gamma_0)$ and the contribution of both these
transformations to $G_r(\tau, v, A, \Gamma_0, 0)$ is the same, we see
immediately that
$$
G_r(\tau, v, A, \Gamma_0,0) = 2G(\tau, A),
$$
where \pageoriginale $G(\tau, A)$ is the series introduced in the
previous section. We refer to the series $G_r(\tau, v, A, \Gamma_0,0)$
as the Eisenstein series for the group $\Gamma_0$.

In what follows, we shall adhere to the following notation:
$$
(f|S)(\tau):= f(S<\tau>)(c\tau+d)^{-r}
$$
for real $S= \left(\begin{smallmatrix}
  a&b\\c&d \end{smallmatrix}\right)$ with $|S|>0$ and any $f$ defined
on $\mathscr{G}$. It is obvious that 
$$
(f|S_1S_2)(\tau) = \sigma(S_1, S_2) ((f|S_1)|S_2)(\tau)
$$
for two real matrices $S_1$ and $S_2$. If $f(\tau)$ belongs to
$[\Gamma_0, r, v]$, then 
$$
(f|S)(\tau) =  v(S)f(\tau) \text{ for } S \in \Gamma_0. 
$$
If $S_1$ and $S_2$ are two transformations belonging to the modular
group, then 
$$
(v^{S_1})^{S_2} = v^{S_1S_2};
$$
for, if $L$ belongs to $\Gamma_0$; then 
{\fontsize{9}{11}\selectfont
\begin{align*}
(v^{S_1})^{S_2}(S^{-1}_2 S^{-1}_1LS_1S_2) & = v^{s_1}(S^{-1}_1LS_1)
  \frac{\sigma(S^{-1}_1 LS_1,S_2)}{\sigma(S_2, S^{-1}_2
    S^{-1}_1LS_1S_2)}\\
& =
  v(L)\frac{\sigma(L,S_1)\sigma(S^{-1}_1LS_1,S_2)}{\sigma(S_1,S^{-1}_1LS_1)
  \sigma(S_2,S^{-1}_2S^{-1}_1 LS_1S_2)}\\
& = v(L)\frac{\sigma(L,S_1) \sigma(S^{-1}_1, L
    S_1S_2)\sigma(LS_1,S_2) \sigma(S^{-1}_1, LS_1)}{\sigma(S^{-1}_1,
    LS_1) \sigma(S_1,S^{-1}_1)\sigma(S^{-1}_1S_1S_2,S^{-1}_2S^{-1}_1 L
    S_1 S_2)} \\
& = v(L) \frac{\sigma(S^{-1}_1, L S_1 S_2) \sigma(S_1, S_2)
    \sigma(L,S_1 S_2) \sigma(S^{-1}_1, S_1 S_2)}{\sigma (S_1,
    S^{-1}_1) \sigma(S^{-1}_1, L S_1 S_2) \sigma(S_1S_2, S^{-1}_2
    S^{-1}_1 L S_1S_2)}\\
& =v(L) \frac{\sigma(L,S_1S_2)}{\sigma(S_1 S_2, S^{-1}_2 S^{-1}_1 L
    S_1 S_2)} = v^{S_1S_2} (S^{-1}_2 S^{-1}_1 L S_1 S_2).
\end{align*}}\relax

With \pageoriginale the help of this composition rule for multiplier
system, it is easy to see that the mapping 
$$
f(\tau) \to g(\tau) = (f|A) (\tau) \;\; (A\in \Gamma)
$$
is a bijection linear transformation from the vector space $[\Gamma_0,
r,v]$ to the vector space $[A^{-1} \Gamma_0 A, r, v^A]$. If $f(\tau)$
belongs to $[\Gamma_0, r,v]$, then $(f|A)(\tau)$ belongs to
$[A^{-1}\Gamma_0 A, r, v^{A}]$; indeed, for $L\in \Gamma_0$, 
\begin{align*}
((f|A)|A^{-1}LA) (\tau) & = \sigma(A^{-1}, L A)(((f|A)|A^{-1})|LA)
  (\tau)\\
& = \frac{\sigma(A^{-1},LA)}{\sigma(A,A^{-1})} (f|LA)(\tau)\\
& = \frac{\sigma(A^{-1},LA)\sigma(L,A)}{\sigma(A,A^{-1})}
  ((f|L)|A)(\tau)\\
& = \frac{\sigma(A^{-1},LA)\sigma(L,A)}{\sigma(A,A^{-1})}
  v(L)(f|A)(\tau)\\
& = v^A (A^{-1}LA)(f|A)(\tau).
\end{align*}
It is, moreover, trivial to see that $(f|A)(\tau)$ is regular in
$\mathscr{G}$ and in its Fourier expansion at the various parabolic
cusps of $A^{-1}\Gamma_0A$, no term with a negative exponent
occurs. Conversely, if $g(\tau)$ belongs to\break $[A^{-1}\Gamma_0
  A,r,v^A]$, then as above $(g|A^{-1})(\tau)$ belongs to $[\Gamma_0,
  r,(v^A)^{A^{-1}}]$. but $(v^A)^{A^{-1}}=v^AA^{-1}=v$; therefore
$(g|A^{-1})(\tau)$ belongs to $[\Gamma_0, r,v]$, proving our assertion
above.

The following theorem gives explicitly the dimension of the space
$[\Gamma, r, v]$.

\begin{thm}\label{chap3:thm21}
Let $v$ be a multiplier system for the modular group and weight
\pageoriginale $r$ such that 
$$
v(U) =e^{2\pi ik}, v(V) =e^{\frac{\pi ir}{3}+\frac{2\pi ia}{2}}, v(T)
= e^{\frac{\pi i r}{2}+\frac{2\pi i b}{2}},
$$
where $0\leq \kappa < 1$, $0\leq a < 3$, $0 \leq b < 2$ and $a,b$ are
integers. Then
\begin{equation*}
\text{dimension } [\Gamma,r,v] = 
\begin{cases}
\frac{r}{12} - \kappa -\frac{a}{3} -\frac{b}{2} + 1 &, \text{ for }
r-12\kappa \geq 0\\
\qquad 0 &, \text{ for } r - 12\kappa < 0
\end{cases}
\end{equation*}
\end{thm}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item If $k$ is a negative even integer, then the dimension of the
  space $[\Gamma, k,1]$ is zero. Assume that $k$ is a negative even
  integer and the modular form $f(\tau)$ belongs to $[\Gamma,
    k,1]$. Then the function $f^2(\tau)G^{-k/2}_4(\tau)$, which is
  invariant under the transformations of $\Gamma$ and has no poles,
  must be a constant. But we have already seen that $G_4(\rho)=0$ for
  $\rho=e^{2\pi i/3}$; therefore $f^2(\tau)G^{-k/2}_4(\tau)=0$,
  implying that $f(\tau)\equiv 0$. Hence the dimension of the space
  $[\Gamma, k, 1]$ is zero for negative $k$.

\item Let $r^{\ast}$ be the weight and $v^{\ast}$ the multiplier
  system of the modular form $\Delta^{\kappa}(\tau)$ for the group
  $\Gamma$. Let $\kappa^{\ast}$, $a^{\ast}$ and $b^{\ast}$ be the
  numbers determined by $v^{\ast}$ in the sense of 
theorem~\ref{chap3:thm21}. Obviously $\kappa^{\ast}=\kappa$ and $r^{\ast}=12\kappa$;
  therefore $\kappa^{\ast}=\kappa=\dfrac{r^{\ast}}{12}$. But we have
  proved in the previous section that $\dfrac{r^{\ast}}{12}\equiv
  \kappa^{\ast}+ \dfrac{a^{\ast}}{3}+\dfrac{b^{\ast}}{2}(\mod 1)$;
  therefore, it follows that $a^{\ast}=b^{\ast}=0$. Consequently, we
  obtain
\begin{align*}
\frac{r^{\ast}}{6}+\frac{a^{\ast}}{3} = \frac{r^{\ast}}{6} = 2\kappa
\equiv \frac{r}{6} + \frac{a}{3} (\mod 1)\\
\frac{r^{\ast}}{4} + \frac{b^{\ast}}{2} =\frac{r^{\ast}}{4} = 3\kappa
\equiv \frac{r}{4} + \frac{b}{2} (\mod 1),
\end{align*}
\end{enumerate}
showing that $v$ and $v^{\ast}$ take the same values for the
transformations $U,V$ and \pageoriginale $T$. Since $r\equiv
12\kappa=r^{\ast}(\mod 2),v$ and $v^{\ast}$ are identical. Let
$f(\tau)$ be a modular form belonging to the space $[\Gamma, r,
  v]$. Then $f(\tau)$ has an unavoidable zero of multiplicity at least
equal to $\kappa$ at the cusp $\infty$ and the above discussion shows
that $g(\tau)=f(\tau)\Delta^{-\kappa}(\tau)$ belongs to $[\Gamma,
  r-12\kappa, 1]$. Conversely, if $g(\tau)$ belongs to $[\Gamma,
  r-12\kappa, 1]$, then $g(\tau)\Delta^{\kappa}(\tau)$ belongs to
$[\Gamma, r-12\kappa, 1]$, then $g(\tau)\Delta^{\kappa}(\tau)$ belongs
to $[\Gamma, r, v]$. Thus it is proved that the dimensions of
$[\Gamma,r,v]$ and $[\Gamma,r-12\kappa,1]$ are the same. Therefore, if
$$
\frac{r-12\kappa}{12} = g+ \frac{a}{3} + \frac{b}{2}, 
$$
where $g,a$ and $b$ are integers such that $0\leq a < 3$, $0\leq b
<2$, then by theorem~\ref{chap2:thm14} it follows that, for $r-12\kappa \geq 0$ (due
to $r-12\kappa$ being an even integer),
$$
\text{dimension } [\Gamma,r-12\kappa,1] =g +1 =\frac{r-12\kappa}{12}
-\frac{a}{3} -\frac{b}{2} + 1. 
$$
For $r-12\kappa<0$, the dimension of $[\Gamma, r-12\kappa,1]$ is zero
by part (i) of our proof. Hence the theorem is established.

As in the case of modular forms of integral weight for the modular
group, the number of zeros of a modular form $f(\tau)$ belonging to
$[\Gamma, r, v]$ is $r/12$. Indeed, the number of zeros of $f(\tau)$
is equal to the sum of the number of zeros of $\Delta^{\kappa}(\tau)$
and $g(\tau)=f(\tau)\Delta^{-\kappa}(\tau)$. Now we have seen in the
course of the proof of theorem~\ref{chap3:thm21} that $g(\tau)$ is a modular form the
integral weight $r-12\kappa$ and so the number of zeros of $g(\tau)$
is a modular form of integral weight $r-12\kappa$ and so the number of
zeros of $g(\tau)$ is $\dfrac{r-12\kappa}{12}$. Therefore the number
of zeros of $f(\tau)$ is $\kappa +
\dfrac{r-12\kappa}{12}=\dfrac{r}{12}$. 
\end{proof}

\section{Metrisation and Completeness Theorem}\label{chap3:sec3}%%% 3

The general transformation formula
$$
(G_r(\tau,v,A,\Gamma_0,n+\kappa)|A)(\tau) = \frac{1}{\sigma(A,S)}
G_r(\tau, v^S, A S, S^{-1} \Gamma_0 S, n+ \kappa)
$$\pageoriginale 
proved for the Poincar\'e series defined by~\eqref{c3:eq2:1} of 
\S~\ref{chap2:sec2} shows that\break
$G_r(\tau, v, A, \Gamma_0, b+\kappa)$ is a cusp form for $n+\kappa>0$;
in case $n+\kappa=0$ i.e. $n=0=\kappa$, $G_r(\tau, v, A, \Gamma_0,0)$
does not represent a cusp form, since the constant term in its Fourier
expansion at the cusp $A^{-1}<\infty>$ is different from zero. In
order to prove that the Poincar\'e series and Eisentein series
together generate the space $[\Gamma_0, r, v]$, it is sufficient in
view of theorem~\ref{chap3:thm17} to prove the same for the space $[\Gamma, r,v_0]$
of cusp forms contained in $[\Gamma_0, r, v]$. By using
\textit{Petersson's Metrisation Principle}, we shall prove presently
that the series $G_r(\tau, v,A,\Gamma_0,n+\kappa)$ generate the space
$[\Gamma_0, r, v]_0$ for any (fixed and) given $A$. 

Let $D$ be a hyperbolic triangle in $\mathscr{G}$ with proper or
improper vertices and $f(\tau)$, $g(\tau)$ two functions, which are
continuous in $D$ and are such that the function
$f(\tau)\overline{g(\tau)}y^r(\tau =x+iy)$ is bounded in $D$. Then the
integral 
$$
\chi(D,f,g) = \iint\limits_D  f(\tau) \overline{g(\tau)} y^{r-2} dx dy
$$
exists. If $S=\left(\begin{smallmatrix}
a&b\\c&d \end{smallmatrix}\right)$ is any real matrix of determinant
1, then 
$$
\chi(D_{S^{-1}},f|S, g|S) = \chi (D,f,g),
$$
where $D_{S^{-1}}$ is the image of $D$ by $S^{-1}$; indeed, if we
replace $\tau$ by $S<\tau>$ in the integrand in $\chi(D,f,g)$ and use
the invariance of $y^{-2}dx dy$, we have 
\begin{align*}
&f(S<\tau>) \overline{g(S<\tau>)} \{y/(c\tau+d)(c\overline{\tau}+d)\}^r
y^{-2} dx dy\\
&\qquad\qquad= (f|S) (\tau)\overline{(g|S)(\tau)} y^{r-2} dx dy, 
\end{align*}\pageoriginale 
which proves the assertion. In particular, if $f(\tau)$ and $g(\tau)$
belong to $[\Gamma_0, r, v]$, then the integral $\chi(D,f,g)$ exists
provided that either $f(\tau)$ of $g(\tau)$ vanishes at an improper
vertex of $D$. Moreover, if $S$ belongs to $\Gamma_0$, then obviously
$$
\chi(D_{S^{-1}},f,g)=\chi(D_{S^{-1}},f|S, g|S) = \chi (D,f,g).
$$
Let $\mathfrak{F}_0$ be a fundamental domain for $\Gamma_0$, which
consists of a finite number of hyperbolic triangles. Further, let at
least one of $f(\tau)$ and $g(\tau)$ in $[\Gamma_0,r,v]$, be a cusp
form. Then the integral 
$$
(f,g)_{\Gamma_0} = \frac{1}{\mathfrak{I}(\mathfrak{F_0})} 
\iint\limits_{\mathfrak{F}_0} f(\tau) \overline{g(\tau)} y^{r-2} dx dy
$$
exists, where $\mathfrak{I}(\mathfrak{F}_0)$ is the hyperbolic area of
$\Gamma_0$. From the transformation property of $\chi(D,f,g)$ above,
it follows that the integral $(f,g)_{\Gamma_0}$ does not depend upon
the choice of $\mathfrak{F}_0$. As a matter of fact,
$(f,g)_{\Gamma_0}$ is independent of $\Gamma_0$ as well, in the sense
that if $f$ and $g$ are two modular forms of weight $r$ for two
subgroups $\Gamma_0$ and $\Gamma_1$ of finite index in $\Gamma$, then
$(f,g)_{\Gamma_0}=(f,g)_{\Gamma_1}$. We consider first the case when
one of the groups $\Gamma_1$ or $\Gamma_2$ contains the other. Let us
assume that $\Gamma_1$ is contained in $\Gamma_0$ and
$(\Gamma_0:\Gamma_1)=\mu$. Let $\Gamma_0 = \bigcup^{\mu}_{i=1}\Gamma_1
A_i$ for some $A_i$ belonging to $\Gamma_0, i=1,2,\ldots,\mu$ be a
coset decomposition of $\Gamma_0$ modulo $\Gamma_1$. Then
$\mathfrak{F}_1 = \bigcup^{\mu}_{i=1}(\mathfrak{F}_0)_{A_i}$
\pageoriginale is a fundamental domain for $\Gamma_1$. Since
$\mathfrak{I}(\mathfrak{F}_1)=\mu\mathfrak{I}(\mathfrak{F}_0)$ and
$\bigcup(\mathfrak{F}_0)_{A_i}$ for $i=1,2,\ldots,\mu$ is a
fundamental domain for $\Gamma_0$, it follows immediately that 
\begin{align*}
  (f,g)_{\Gamma_0} & = \frac{1}{\mu} \sum\limits^\mu_{i=1}
  \frac{1}{\mathfrak{I}((\mathfrak{F_0})_{A_i})}
  \iint\limits_{(\mathfrak{F}_0)_{A_i}} f(\tau) \overline{g(\tau)}
  y^{r-2} dx \; dy\\
& = \frac{1}{\mu \mathfrak{I}(\mathfrak{F}_0)}
   \iint\limits_{\bigcup\limits_i(\mathfrak{F}_0)_{A_i}} f(\tau)
  \overline{g(\tau)} y^{r-2} dx \; dy\\
& = \frac{1}{\mathfrak{I}(\mathfrak{F}_1)}
   \iint\limits_{\mathfrak{F}_1} f(\tau) \overline{g(\tau)}
  y^{r-2} dx \; dy =(f,g)_{\Gamma_1}.
\end{align*}
In the general case, consider the group $\Gamma^{\ast}$ generated by
$\Gamma_0$ and $\Gamma_1$. It is obvious that both $\Gamma_1$ and
$\Gamma_0$ are subgroups of finite index in $\Gamma^{\ast}$ and
$f(\tau)$ and $g(\tau)$ are modular forms of weight $r$ for the group
$\Gamma^{\ast}$ and the same multiplier system. Hence, by the
particular case considered above, we get $(f,g)_{\Gamma_0} =
(f,g)_{\Gamma^{\ast}} = (f,g)_{\Gamma_1}$. 

In the following, so long as no confusion is possible, we shall simply
write $(f,g)$ for $(f,g)_{\Gamma_0}$. We call $(f,g)$ \textit{the
  scalar product} of $f$ and $g$. The following properties of the
scalar product $(f,g)$ are immediate from the definition:
\begin{itemize}
\item[1)] $(f,g)$ is linear in $f$.

\item[2)] $\overline{(f,g)}=(g,f)$.

\item[3)] $(f,f) \geq 0, (f,f)=0\Longrightarrow f=0$.

\item[4)] $(f|A,g|A) =(f,g)$ for $A$ in $\Gamma$ .
\end{itemize}

Properties \pageoriginale 1), 2) and 3) show that the scalar product
$(f,g)$ defines a positive-definite unitary metric on the space
$[\Gamma_0, r, v]_0$.

For the explicit calculation of the scalar product of a cusp form and
a Poincar\'e series, we need to prove 

\begin{lem}\label{chap3:lem5}
If $f(\tau)$ is a cusp form in $[\Gamma_0, r, v]$, then
$|f(\tau)|y^{r/2}$ (with $\tau=x+iy$) is bounded in $\mathscr{G}$.
\end{lem}

\begin{proof}
If in $|f(\tau)|y^{r/2}$, we replace $\tau$ by $A<\tau>(A\in
\Gamma)$, then $|f(\tau)|y^{r/2}$ is transformed to
$|(f|A)(\tau)|y^{r/2}$. In particular, when $A$ belongs to $\Gamma_0$,
$|(f|A)(\tau)|=|v(A)f(\tau)|=|f(\tau)|$ and therefore
$|f(\tau)|y^{r/2}$ is left invariant by $\Gamma_0$. Thus, in order to
complete the proof of the lemma, it is sufficient to prove that
$|f(\tau)|y^{r/2}$ is bounded in a fundamental domain $\mathfrak{F}_0$
or $\Gamma_0$. Let $\rho_1, \rho_2, \ldots, \rho_{\sigma}$ be a
complete system of inequivalent cusps of $\Gamma_0$ and let
$\infty=A_j<\rho_j>$, $A_j\in \Gamma$  for $j=1,2,\ldots,
\sigma$. Let $\mathfrak{p}_j(j=1,2,\ldots,\sigma)$ be cusp sectors at
the cusps $\rho_j$ of $\Gamma_0$. Then $\mathfrak{F}_0 =
\bigcup\limits^{\sigma}_{j=1}\mathfrak{p}_j$ is a fundamental domain for
$\Gamma_0$ and for the proof of the lemma, it is sufficient to prove
that $|f(\tau)|y^{r/2}$ is bounded in each $\mathfrak{p}_j$. It is
obvious that $|f(\tau)|y^{r/2}$ is bounded in $\mathfrak{p}_j$ if and
only if $|(f|A^{-1}_j)(\tau)|y^{r/2}$ is bounded in
$A_j<\mathfrak{p}_j>$. Since $(f|A{-1}_j)(\tau)$ has the Fourier
expansion 
$$
\sum_{n+\kappa_j>0} c_n+\kappa_j e^{2\pi i(n+\kappa_j)\tau/ N_j}.
$$
it tends to zero exponentially as $y\to \infty$, uniformly in $x$. It
follows that $|(f|A^{-1}_j)(\tau)|y^{r/2}$ tends to zero as $y\to
\infty$ and therefore is bounded in $A_j<\mathfrak{p}_j>$. Hence the
lemma is proved. 
\end{proof}

For \pageoriginale the sake of brevity, we set
$\gamma=\gamma(E,\Gamma_0)$ and 
$$
e(\tau) =e^{2\pi i(n+\kappa)\tau/N}.
$$
By the definition of Poincar\'e series, we have 
$$
G(\tau):=G_r(\tau,v,E,\Gamma_0,n+\kappa) = \sum_{\in \Gamma}
\overline{v(L)} (e|L)(\tau).
$$
For the scalar product $(f,G)$ of a cusp form $f(\tau)$ in
$[\Gamma_0,r,v]$ with $G(\tau)$, formal calculation yields
\begin{align*}
(f,G) & = \frac{1}{\mathfrak{I}(\mathfrak{F}_0)}
 \iint\limits_{\mathfrak{F}_0} f(\tau) \sum_{L\in \gamma}
v(L) \overline{(e|L)(\tau)} y^{r-2} dx dy\\
& = \frac{1}{\mathfrak{I}(\mathfrak{F}_0)} \sum_{L\in \gamma}
 \iint\limits_{\mathfrak{F}} (f|L)(\tau) \overline{(e|L)(\tau)}
 y^{r-2} dx dy.\\
& = \frac{1}{\mathfrak{I}(\mathfrak{F}_0)} \sum_{L\in \gamma}
  \iint\limits_{(\mathfrak{F}_0)_L} f(\tau) \overline{e(\tau)}
  y^{r-2} dx dy\\
& = \frac{2}{\mathfrak{I}(\mathfrak{F}_0)}
  \iint\limits_{\mathscr{L}} f(\tau) \overline{e(\tau)} y^{r-2} dx
  dy, \tag{1}\label{c3:eq3:1}
\end{align*}
where
$\mathscr{L}=\bigcup\limits_{L\in\gamma}(\mathfrak{F}_0)_L$. The
factor 2 appears on the right hand side of~\eqref{c3:eq3:1}, because we can assume
that both $L$ and $-L$ belong to the set and their contributions to
the sum are the same. The interchange of summation and integration
would be justified in the above formal computation, if we prove that
the last integral converges absolutely. Let $Z$ denote the subgroup of
$\Gamma_0$ generated by $U^N$, where $N$ is the least natural number
so determined that $U^N$ belongs to $\Gamma_0$. Let
$\Gamma_0=\bigcup_i ZL_i$, $L_i\in \Gamma_0$, be a coset
decomposition of $\Gamma_0$ modulo $Z$. Then the set of all the
matrices $L_i$ is a possible choice for the set $\gamma$ and therefore
$\mathscr{L}$ is a fundamental \pageoriginale domain for $Z$. We
decompose the set $\mathscr{L}$ into a countable number of sets
$\mathscr{L}_k$ such that the images of $\mathscr{L}_k$ by elements
$Z$ belongs to the domain $\mathscr{G}=\{\tau|\tau=x+iy, 0\leq x < N,
y\geq 0\}$, which is a fundamental domain for $Z$. Since the integrand
$f(\tau) \overline{e(\tau)}y^{r-2}dxdy$ is invariant under the
transformation $\tau\to\tau+N$, we get immediately that 
\begin{equation*}
\frac{2}{\mathfrak{I}(\mathfrak{F}_0)} \int\int\limits_{\mathscr{L}}
f(\tau) \overline{e(\tau)} y^{r-2} dx dy = \frac{2}{\mathfrak{I}
  (\mathfrak{F}_0)} \int\limits^{\infty}_0 \int\limits^N_0 f(\tau)
\overline{e(\tau)} y^{r-2} dx dy \tag{2}\label{c3:eq3:2}
\end{equation*}
Lemma 6 can be used to see that the integral on the right hand side of
\eqref{c3:eq3:2} converges absolutely for $r>2$. Hence the formal computation for
obtaining \eqref{c3:eq3:1} is justified and we have indeed 
$$
(f,G) = \frac{2}{\mathfrak{I}(\mathfrak{F}_0)} \int\limits^{\infty}_0
\int\limits^N_0 f(\tau) e^{-2\pi i(n+\kappa)\bar{\tau}/N} y^{r-2} dx dy.
$$
Using the Fourier expansion of $f$ at the cusp $\infty$ given by 
$$
f(\tau) = \sum_{k+\kappa>0} c_{k+\kappa} (f)e^{2\pi i(k+\kappa)\tau/N}
$$
with 
$$
c_{k+\kappa}(f) = \frac{1}{N} \int\limits^{N}_0 f(\tau)e^{2\pi i
  (k+\kappa)\tau/N} dx,
$$
it follows immediately that $(f,G)=0$ in case $n+\kappa=0$ and
otherwise,
\begin{align*}
(f,G) & = \frac{2}{\mathfrak{I}(\mathfrak{F}_0)}
  \int\limits^{\infty}_0\{\int\limits^{N}_0 f(\tau) e^{2\pi
    i(n+\kappa)\tau/N}dx\} e^{2\pi i(n+\kappa)(\tau-\bar{\tau})/N}
  y^{r-2} dy\\
& = \frac{2N}{\mathfrak{I}(\mathfrak{F}_0)} c_{n+\kappa} (f)
  \int\limits_0 e^{-4\pi(n+\kappa)y/N }y^{r-2} dy\\
& = \frac{2N}{\mathfrak{I}(\mathfrak{F_0})}(4\pi(n+\kappa)/N)^{1-r}
  \Gamma(r-1) c_{n+\kappa} (f) \text{ for } n+\kappa>0
\end{align*}
The \pageoriginale general case can be reduced to this particular case
$(A=E)$, if we note that 
\begin{align*}
& (f,G_r(\quad,v,A,\Gamma_0,n+\kappa))= \\
& = (f|A^{-1},G_r(\quad,v,A,\Gamma_0,n+\kappa)|A^{-1})\\
& = \frac{1}{\sigma(A,A^{-1})}
  (f|A^{-1},G_r(\quad,v^{A^{-1}},E,A,\Gamma_0 A^{-1},n+\kappa)).
\end{align*}
We are thus led to the \textit{fundamental formula of the metrisation
  principle} as stated in 

\begin{thm}\label{chap3:thm22}
Let $f(\tau)$ be a cusp form belonging to $[\Gamma_0, r, v]$ for
$r>2$, and let 
$$
(f|A^{-1})(\tau) = \sum_{n+\kappa>0} c_{n+\kappa} (f,A) e^{2\pi
  i(n+\kappa)\tau/N} 
$$
be the Fourier expansion of $f(\tau)$ at the parabolic cusp
$A^{-1}<\infty>$ of $\Gamma_0$ with $A$ in $\Gamma$. Then we have 
\begin{align*}
&(f,G_r(\quad,v,A,\Gamma_0, n+\kappa))\\
&\qquad = \begin{cases}
\frac{2N\Gamma(r-1)}{\sigma(A,A^{-1})\mathfrak{I}(\mathfrak{F}_0)} & 
\left(\frac{4\pi(n+\kappa)}{N}\right)^{1-r} c_{n+\kappa} (f,A), \text{
for } n+\kappa >0\\
0 & \text{ for } n+\kappa=0
  \end{cases}
\end{align*}

We call two modular forms $f(\tau)$ and $g(\tau)$ \textit{orthogonal},
if $(f,g)$ exists and is equal to 0. The above theorem enables us to
characterise the Poincar\'e series completely. In this connection, we
prove 
\end{thm}

\begin{thm}\label{chap3:thm23}
For $r>2$, the Poincar\'e series $G_r(\tau,v,A,\Gamma_0,n+\kappa)$ is
uniquely determined upto a constant factor, by the following properties:
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item $\underline{n+\kappa >0}$. \pageoriginale The series is a cusp
  form, which is orthogonal to all the forms $f(\tau)$ of the space
  $[\Gamma_0,r,v]$ for which the $(n+\kappa)-th$ Fourier coefficient
  $c_{n+\kappa}(f,A)$ at the cusp $A^{-1}<\infty>$ of $\Gamma_0$ vanishes.

\item $\underline{n+\kappa=0}$. The series is orthogonal to all the
  forms of the space $[\Gamma_0,r,v]_0$. Moreover, the constant term
  in the Fourier series of $G_r(\tau, v, A, \Gamma_0, 0)$ at a
  parabolic cusp of $\Gamma_0$ is different from zero or equal to zero
  according as the cusp under consideration is equivalent to
  $A^{-1}<\infty>$ or not.
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item $\underline{n+\kappa}>0$. The fact that the series
  $G_r(\tau,v,A,\Gamma_0,n+\kappa)$ has the property (i) is an
  immediate consequence of theorem~\ref{chap3:thm22}. If $c_{n+\kappa}(G_r,A)=0$ then
  applying theorem~\ref{chap3:thm22} to $f(\tau)=G_r(\tau, v, A,\Gamma_0,\break n+\kappa)$,
  this series vanishes identically and therefore $c_{n+\kappa}(f,A)=0$
  for every form $f(\tau)$ in $[\Gamma_0,r,v]$. Hence, if $g(\tau)$ is
  a cusp form which has the property (i) of theorem~\ref{chap3:thm23}, there exists
  clearly a constant $c$ such that
$$
c_{n+\kappa}(g,A) = cc_{n+\kappa} (G_r,A) \Longrightarrow
c_{n+\kappa}(g-cG_r,A) =0.
$$
This shows that the modular form $g-cG_r$ is orthogonal to $g$ as well
as to $G_r$ and therefore $g-cG_r$ is orthogonal to itself. Hence
$g-cG_r=0$ i.e. $g=cG_r$.

\item $\underline{n+\kappa}=0$. If $G_r(\tau, v,A,\Gamma_0,0)$ has in
  its Fourier expansion at the cusp $B^{-1}<\infty>(B\in
  \Gamma)$ a constant term different from zero, then
{\fontsize{10}{12}\selectfont
$$
(G_r(\quad,v,A,\Gamma_0,0)|B^{-1})(\tau) = \frac{1}{\sigma(A,B^{-1})}
  G_r(\tau, v^{B^{-1}}, A B^{-1}, B \Gamma_0 B^{-1},0)
$$}\relax
does not vanish at the parabolic cusp $\infty$. But this is possible
if and only if $AB^{-1}B\Gamma_0 B^{-1}=A\Gamma_0B^{-1}$ contains a
matrix $M$ whose second row is $(0,\pm 1)$ i.e. \pageoriginale for
some integral $t$, $A\Gamma_0 B^{-1}$ contains $\pm U^t$ or
equivalently $\pm A^{-1}U^t = LB^{-1}$ for some $L$ in $\Gamma_0$ and
an integer $t$. The last condition means precisely that the cusps
$A^{-1}<\infty>$ and $B^{-1}<\infty>$ are equivalent under
$\Gamma_0$. It is now clear that the Eisenstein series
$G_r=G_r(\tau,v,A,\Gamma_0,0)$ satisfies the second assertion in (ii)
of theorem~\ref{chap3:thm23}; by theorem~\ref{chap3:thm22}, $G_r$ is orthogonal to
$[\Gamma_0,r,v]_0$ i.e. $G_r$ has the property (ii) of theorem~\ref{chap3:thm23}. Let
$g(\tau)$ be any modular form in $[\Gamma, r,v]$ which has the same
property. We know that, for some constant $c$, $g-cG_r$ is a cusp
form. But then $g-cG_r$ is orthogonal not only to $G_r$ but also to
$g$. Thus $g-cG_r$ is also orthogonal to itself leading to $g=cG_r$,
proving theorem~\ref{chap3:thm23} completely.
\end{enumerate}

An other important consequence of theorem~\ref{chap3:thm22} is
\end{proof}

\begin{thm}[Completeness Theorem] \label{chap3:thm24}
The system of Eisenstein series for the group $\Gamma_0$ can be
completed into a basis of the space $[\Gamma_0, r, v], r>2$, by
adding a finite number of Poincar\'e series $G_r(\tau,v,A,\Gamma_0,
n+\kappa)$ for any (fixed and) given $A$. 
\end{thm}

\begin{proof}
By theorem~\ref{chap3:thm17}, it is sufficient to prove that the Poinca\'e series
series $G_r(\tau,v,A,\Gamma_0,n+\kappa)$ for some fixed generate
$[\Gamma_0,r,v]_0$. Let $\mathfrak{T}$ be the subspace of
$[\Gamma_0,r,v]_0$ generated by the Poincar\'e series
$G_r(\tau,v,A,\Gamma_0, n+\kappa)(n+\kappa>0)$. Let $t$ be the
dimension of $\mathfrak{T}$ and $\{f_1,f_2,\ldots, f_t\}$ an
orthonormal basis of $\mathfrak{T}$ i.e.
$$
(f_i,f_k) = \delta_{ik} \text{ for } i,k=1,2,\ldots, t. 
$$
For an arbitrary element $f(\tau)$ of $[\Gamma_0,r,v]_0$, let
$$(f,f_i)=a_i \mbox{ for } i=1,2,\ldots,t.$$ 
It is obvious that the cusp form
$f-\sum\limits^{t}_{i=1}a_if_i$ is orthogonal to \pageoriginale
$f_1,f_2,\ldots, f_t$ and therefore to every cusp form
$G_r(\tau,v,A,\Gamma_0, n+\kappa) \;\; (n+\kappa>0)$. But this implies, by
theorem~\ref{chap3:thm22}, that $f-\sum\limits^t_{i=1}a_if_i=0$ i.e. $f$ belongs to
$\mathfrak{T}$. Hence $\mathfrak{T}=[\Gamma_0,r,v]_0$, which proves
theorem~\ref{chap3:thm25}.
\end{proof}

By using the theory of the `Weierstrass' points', Petersson has shown
how to choose the values $n_1,n_2,\ldots,n_t$ so that the series
$G_r(\tau, v,A,\Gamma_0,n_i\break+\kappa) i=1,2,\ldots, t$ form a basis of
the space $[\Gamma_0,r,v]_0$.

In the case of the modular group $\Gamma$, theorem~\ref{chap3:thm21} enables us to
make a very precise statement. We observed already that $f\to
f\Delta^{-\kappa}$  defines a bijective linear mappain of
$[\Gamma,r,v]$ onto $[\Gamma, r-12\kappa, 1]$, showing that the
dimension $\mu$ of $[\Gamma, r,v]$ is positive if and only if
$r-12\kappa \geq 0$ and $r-12\kappa \neq 2$ (since $r-12\kappa$ always
is even). The assumption $r>2$ guarantees the convergence of the
Poincar\'e series. By theorem~\ref{chap3:thm21}, we get 
$$
\frac{r}{12} = \mu + \kappa + \frac{a}{3} + \frac{b}{2} -1\text{ for }
r - 12 \kappa \geqq 0
$$
and as we have proved already, this number coincides with the number
of zeros of a modular form $f$ in $[\Gamma,r, v]$ not vanishing
identically.

\begin{thm}\label{chap3:thm25}
If $[\Gamma, r, v]$ has positive dimension $\mu$ and $r>2$, the series
$G_r(\tau, v, n+\kappa):=G_r(\tau,v,E,\Gamma, n+\kappa)$ for
$n=0,1,2,\ldots, \mu-1$ form a basis for $[\Gamma, r,v]$.
\end{thm}

\begin{proof}
Every form $f(\tau)\in [\Gamma,r,v]$ has a Fourier expansion
of the type
$$
f(\tau)=\sum^{\infty}_{n=0} c_{n+\kappa} e^{2\pi i (n+\kappa)\tau}.
$$
The necessary and sufficient condition that $f(\tau)$ vanishes
identically is that \pageoriginale $c_{n+\kappa}=0$ for $n=0,1,\ldots
, \mu-1$. For, if the first $\mu$ coefficients $c_{n+\kappa}$ vanish,
then $f(\tau)$ has unavoidable zeros of order
$\kappa,\dfrac{a}{3},\dfrac{b}{2}$ and an ordinary zero of order
$\mu$, which cannot happen unless $f(\tau)=0$. Let 
$$
\varphi_m(\tau) = \sum^{\infty}_{n=0} c^{(m)}_{n+\kappa} e^{2\pi i
  (n+\kappa)\tau} (m=0,1,2,\ldots,\mu-1)
$$
form a basis of $[\Gamma,r,v]$. Then the matrix 
$$
C = (c^{(m)}_{n+\kappa}) m,n =0,1,2,\ldots,\mu-1
$$
is non-singular. If the matrix $C$ were singular, then there exist
complex numbers $x_0, x_1, \ldots, x_{\mu-1}$ not all zero such that 
$$
\sum^{\mu-1}_{m=0} x_m c^{(m)}_{n+\kappa} = 0, \quad n = 0,1,2,\ldots,
\mu-1. 
$$
This implies that 
$$
\sum^{\mu-1}_{m=0} x_m \varphi_m(\tau) = \sum^{\infty}_{n=0}
\{\sum^{\mu-1}_{m=0} x_m c^{(m)}_{n+\kappa}\}e^{2\pi i(n+\kappa)\tau}=0
$$
showing that $\varphi_0(\tau)$, $\phi_1(\tau),\ldots,
\varphi_{\mu-1}(\tau)$ are not linearly independent,\break which is a
contradiction. Thus $C$ is non-singular and therefore $C^{-1}$
transforms $\{\varphi_0(\tau), \varphi_1(\tau),\ldots,
\varphi_{\mu-1}(\tau)\}$ into a new basis for which the analogue of the
matrix $C$ is the unit matrix. Thus we can assume, without loss of
generality, that already $\varphi_0(\tau)$, $\varphi_1(\tau)$,
$\varphi_2(\tau),\ldots, \varphi_{\mu-1}(\tau)$ form such a basis
i.e. 
$$
\varphi_m(\tau) =e^{2\pi i(m+\kappa)\tau} + \sum^{\infty}_{n=\mu} 
c^{(m)}_{n+\kappa} e^{2\pi i(m+\kappa)\tau} \;\; (m=0,1,\ldots, \mu-1).
$$
We now prove the linear independence of
$G_r(\tau,v,n+\kappa)(n=0,1,2,\ldots,\break \mu-1)$. Let 
$$
\sum^{\mu-1}_{n=0} x_n G_r (\tau,v,n+\kappa)=0.
$$
Then, \pageoriginale by theorem~\ref{chap3:thm22}, for $m+\kappa >0$, $m<\mu$, it
follows that
$$
0=(\varphi_m(\tau), \sum^{\mu-1}_{n=0} x_n G_r(\tau,v,n+\kappa)) =
\frac{6}{\pi} \Gamma(r-1) (4\pi(n+\kappa))^{1-r} x_m.
$$
As a result, for $\kappa>0$, $x_m=0$ for $m=0,1,\ldots, \mu-1$, and
if $\kappa=0$, then $x_m=0$ for $m=1,2,\ldots,\mu-1$. But, in the
latter case, $x_0G_r(\tau,v,0)=0$ implies that $x_0=0$, since
$G_r(\tau,v,0)\not\equiv 0$. Hence
$G_r(\tau,v,n+\kappa) \;\; (n=0,1,2,\ldots, \mu-1)$ in any case are linearly
independent and theorem~\ref{chap3:thm25} is proved.

We have already shown that the space $[\Gamma, 12, 1]$ is of dimension
1 and is generated by the modular form $\Delta(\tau)$, which has a
Fourier expansion 
$$
\Delta(\tau) =\sum^{\infty}_{n=1} \tau(n) e^{2\pi in\tau}
$$
at the parabolic cusp $\infty$ of $\Gamma$, where $\tau(n)$ is
Ramanujan's function. By using theorem~\ref{chap3:thm23}, we shall now show that the
Poincar\'e series $G_{12}(\tau,1,n)$ vanishes identically if and only
if $\tau(n)=0$. If $\tau(n)=0$, then, by theorem~\ref{chap3:thm23}, the series
$G_{12}(\tau, 1, n)$ is orthogonal to $\Delta(\tau)$ and therefore to
all cusp form including itself, which implies that
$G_{12}(\tau,1,n)=0$. Conversely, if $G_{12}(\tau,1,n)=0$, then
obviously $\tau(n)=0$. It has been proved in 1959 by D.H. Lehmer that
$\tau(n)\neq 0$ for $n \leq 113740230287998$.
\end{proof}

\section[The Fourier Coefficients of...]{The Fourier Coefficients of Integral\hfill\break Modular forms}\label{chap3:sec4}
Let $\Gamma_0$ be a subgroup of finite index in the modular group
$\Gamma$ and let $v$ be a multiplier system for the group $\Gamma_0$
and the weight $r>0$. We shall find some estimates for the Fourier
coefficients $c_{n+\kappa}$ in the Fourier \pageoriginale expansion at
the cusp $\infty$ of a modular form $f(\tau)\in [\Gamma_0, r,
  v]$, namely
\begin{equation*}
f(\tau) = \sum_{n+\kappa \geq 0} c_{n+\kappa} e^{2\pi
  i(n+\kappa)\tau/N}, \tag{1}\label{c3:eq4:1}
\end{equation*}
where $N>0$ is the least integer such that $U^N$ belongs to $\Gamma_0$
and $v(U^{N}) =e^{2\pi i \kappa}$.

Let $s_{\ell}=A^{-1}_{\ell}<\infty>(\ell = 1,2,\ldots, \sigma)$ with
$A_{\ell} \in \Gamma$ be a complete system of pairwise
inequivalent parabolic cusps of $\Gamma_0$. In particular, let
$s_1=\infty$ and $A_1=E$. Let $P_{\ell}$ be the subgroup of $\Gamma_0$
consisting of those transformations of $\Gamma_0$ which leave
$s_{\ell}$ fixed i.e.
$$
P_{\ell} = \{L|L<s_{\ell}> = s_{\ell}, L \in \Gamma_0\}.
$$
It is nothing but the group generated by $-E$ and $H_{\ell} =
A^{-1}_{\ell}U^{N_{\ell}} A_{\ell}$ where $N_{\ell}$ is the least
natural number such that $A^{-1}_{\ell} U^{N_{\ell}} A_{\ell}$ belongs
to $\Gamma_0$. Let 
$$
\Gamma_0 = \bigcup_k P_{\ell} L_{k\ell}
$$
be a coset decomposition of $\Gamma_0$ modulo $P_{\ell}$. We set
$$
M_{k\ell} = A_{\ell} L_{k\ell} = \begin{pmatrix}
a_{k\ell} & b_{k\ell}\\
c_{k\ell} & d_{k\ell}
\end{pmatrix}.
$$
Since we can replace $L_{k\ell}$ by $-L_{k\ell}$, we can assume without
loss of generality that either $c_{k\ell}>0$ or $c_{k\ell}=0$ and
$d_{k\ell}=1$. The mapping $M_{k\ell}\to M^{-1}_{k\ell} <\infty>$ is
one-to-one; because, if $M^{-1}_{k\ell}<\infty>=M^{-1}_{pq}<\infty>$,
then $s_{\ell}=A^{-1}_{\ell}<\infty>$ and $s_q=A^{-1}_q<\infty>$ are
equivalent with respect to $\Gamma_0$, which is possible only if
$q=\ell$. It follows now that
$L_{p\ell}L^{-1}_{k\ell}<s_{\ell}>=s_{\ell}$ i.e. $L_{p\ell}
L^{-1}_{k\ell}$ belongs to $P_{\ell}$ and \pageoriginale therefore
$p=k$. Every rational number $s$ can be represented in the form
$M^{-1}_{k\ell}<\infty>$. Since $s$ is a parabolic cusp of
$\Gamma_0,s$ is equivalent to $s_{\ell}=A^{-1}_{\ell}<\infty>$ for
some suitable $\ell$ i.e. $s=L^{-1}A^{-1}_{\ell}<\infty>$ for $L$
belonging to $\Gamma_0$. Let $L=K_{\ell}L_{k\ell}$, where $K_{\ell}$
belongs to $P_{\ell}$. Then 
$$
s=L^{-1}_{k\ell} K^{-1}_{\ell} A^{-1}_{\ell} <\infty> = L^{-1}_{k\ell}
A^{-1}_{\ell} <\infty> = M^{-1}_{k\ell} <\infty>.
$$
We form the set
$$
\mathfrak{F}^{\ast} = \bigcup^{\infty}_{n=-\infty} U^n <\mathfrak{K}>,
$$
where $\mathfrak{F}$ is the fundamental domain of $\Gamma$ given by 
$$
\mathfrak{F} =\{\tau ||\tau| \geq 1, |2x| \leq 1\}.
$$
It can be seen easily that 
$$
\mathscr{G} = \bigcup^{\sigma}_{\ell-1} \bigcup_k M^{-1}_{k\ell}
<\mathfrak{F}^{\ast}>, 
$$
because $M^{-1}_{k\ell}<\mathfrak{F}^{\ast}>$ consists exactly of
those images of $\mathfrak{F}$ under $\Gamma$, which have
$M^{-1}_{k\ell}<\infty>$ as an improper vertex. Since
$\mathfrak{F}^{\ast}\subset \{\tau |y\geq \dfrac{\surd3}{2}\}$, it
follows that $M^{-1}_{k\ell}<\infty^{\ast}>$ is contained in the
circle
\begin{equation*}
(x+\frac{d_{k\ell}}{c_{k\ell}})^2 + (y-\frac{1}{\sqrt{3}c^2_{k\ell}})
  \leq \frac{1}{3c^4_{k\ell}}, \tag{2}\label{c3:eq4:2}
\end{equation*}
when $c_{k\ell}>0$. We shall now find out a set of necessary
conditions, which have to be satisfied, if the domain
$M^{-1}_{k\ell}<\mathfrak{F}^{\ast}>$ is to intersect a given line
$$
\mathfrak{n} = \{\tau|0\leq x \leq N, y=\eta\},
$$
with \pageoriginale $0< \eta< \dfrac{\sqrt{3}}{2}$ in at least two
points. If $c_{k\ell}=0$, then
$M^{-1}_{k\ell}<\mathfrak{F}^{\ast}>=\mathfrak{F}^{\ast}$ and
therefore $M^{-1}_{k\ell}<\mathfrak{F}^{\ast}>$ does not intersect
$\mathfrak{n}$. Therefore $c_{k\ell}$ is necessarily greater than
zero. Since $M^{-1}_{k\ell}<\mathfrak{F}^{\ast}>$ is contained in the
circle defined in \eqref{c3:eq4:1}, the line $y=\eta$ must intersect this circle in
two points, if it is to have at least two points in common with the
set $M^{-1}_{k\ell}<\mathfrak{F}^{\ast}>$. This means that the
coordinates of the points of intersection given by 
$$
x = -\frac{d_{k\ell}}{c_{k\ell}} \pm
\sqrt{\frac{2n}{\sqrt{3}c^2_{k\ell}}-\eta^2}, y =\eta
$$
must satisfy the conditions 
\begin{align*}
0< c_{k\ell}& < \sqrt{\frac{2}{\sqrt{3}\eta}}, \tag{3}\label{c3:eq4:3}\\
x_1 = x_1(M_{k\ell}) & = -\frac{d_{k\ell}}{c_{k\ell}} -
\sqrt{\frac{2n}{\sqrt{3}c^2_{k\ell}}-\eta^2}< N,\tag{4}\label{c3:eq4:4}\\
x_2 = x_2 (M_{k\ell}) & = -\frac{d_{k\ell}}{c_{k\ell}} +
\sqrt{\frac{2\eta}{\sqrt{3}c^2_{k\ell}}-\eta^2}>0. \tag{5}\label{c3:eq4:5}
\end{align*}
If the condition \eqref{c3:eq4:3} were not satisfied, then the line $y=\eta$ will
intersect the circle \eqref{c3:eq4:1} in at most one point. Moreover if $x_1\geq N$
or $x_2\leq 0$, then, from $x_1< x_2$, the line $y=\eta$ and the
circle will have again at most one point in common. From \eqref{c3:eq4:4} and 
\eqref{c3:eq4:5}, we have
$$
-\sqrt{\frac{2\eta}{\sqrt{3}c^2_{k\ell}}-\eta^2} < -
\frac{d_{k\ell}}{c_{k\ell}} < N +
\sqrt{\frac{2\eta}{\sqrt{3}c^2_{k\ell}}-\eta^2} 
$$
and therefore 
\begin{equation*}
-1<-d_{k\ell}/c_{k\ell} < N + 1. \tag{6}\label{c3:eq4:6}
\end{equation*}
Moreover, \pageoriginale it is immediate from~\eqref{c3:eq4:3} that the interval
$x_1\leq x \leq x_2$ is contained in the interval
$|x+d_{k\ell}/c_{k\ell}|\leq hc^{-1}_{k\ell}\sqrt{\eta}$ with
$h=\sqrt{\frac{2}{\sqrt{3}}}$. Consequently
$$
\mathfrak{n}_{k\ell} = M^{-1}_{k\ell} <\mathfrak{F}^{\ast}> \cap
  \mathfrak{n} \subset\{\tau | y = \eta,
  |x+\frac{d_{k\ell}}{c_{k\ell}}|< h c^{-1}_{k\ell} \sqrt{\eta}\}.
$$
It is obvious that $\mathfrak{n}_{k\ell}$ consists of at most a finite
number of connected segments and 
$$
\mathfrak{n} = \bigcup_{k,\ell} \mathfrak{n}_{k\ell}, 
$$
where $\mathfrak{n}_{k\ell}$ contains at least two points only if 
$$
0 < c_{k\ell} < \frac{h}{\sqrt{\eta}}, -1 < -
\frac{d_{k\ell}}{c_{k\ell}} < N +1.
$$
Let 
$$
(f|A^{-1}_{\ell}) (\tau) = \sum_{n+\kappa_{\ell} \geq 0}
c^{(\ell)}_{n+\kappa_{\ell}} e^{2\pi i(n+\kappa_{\ell})\tau/N_{\ell}}
$$
be the Fourier expansion of $f(\tau)$ at the cusp
$A^{-1}_{\ell}<\infty>$ of $\Gamma_0(\ell=1,2,\ldots, \sigma)$. If
$\ell=1$, this series is identical with the series (1). We set 
$$
\tau = M^{-1}_{k\ell}<\tau^{\ast}> = L^{-1}_{k\ell} A^{-1}_{\ell}
<\tau^{\ast}>, L_{k\ell} =\begin{pmatrix}
\alpha & \beta\\
\gamma & \delta\end{pmatrix} \text{ and } A_{\ell} =\begin{pmatrix}
a&b\\c&d
\end{pmatrix}.
$$
Then
\begin{align*}
|f(\tau)| & = |f(L^{-1}_{k\ell} A^{-1}_{\ell}<\tau^{\ast}>)| =
|-\gamma A^{-1}_{\ell} <\tau^{\ast}>+\alpha|^r |f(A^{-1}_{\ell}
|<\tau^{\ast}>)|\\
& = |(-\gamma A^{-1}_{\ell} <\tau^{\ast}>+\alpha)^r
|(-c\tau^{\ast}+a)^r (f|A^{-1}_{\ell})(\tau^{\ast})|\\
& = |(-c_{k\ell} \tau^{\ast} + a_{k\ell})^r
|(f|A^{-1}_{\ell})(\tau^{\ast})|, 
\end{align*}
which \pageoriginale implies that
\begin{equation*}
|f(\tau)| = |c_{k\ell} \tau+d_{k\ell}|^{-r}
|(f|A^{-1}_{\ell}(\tau^{\ast}))|. \tag{7}\label{c3:eq4:7}
\end{equation*}
Since $|(f|A^{-1}_{\ell})(\tau^{\ast})|$ is bounded for
$\tau^{\ast}\in \mathfrak{F}^{\ast}$, we can assume after any
necessary normalization that 
\begin{equation*}
|(f|A^{-1}_{\ell})(\tau^{\ast})| \leq 1 \text{ for } \tau^{\ast}
\in \mathfrak{F}^{\ast}, \ell = 
1, 2, \ldots, \sigma, \tag{8}\label{c3:eq4:8}
\end{equation*}
The Fourier coefficient $c_{n+\kappa}$ of $f(\tau)$ is given by
$$
c_{n+\kappa} = \frac{1}{N} \int\limits^{N}_0 f(\tau) e^{-2\pi i
  (n+\kappa)\tau/N}dx. 
$$
Choosing the path of integration along the line $y=\eta$, we obtain
that 
\begin{align*}
N|c_{n+\kappa}| & \leq e^{2\pi (n+\kappa)\eta/N}
\int\limits_{\mathfrak{n}} |f(\tau)| dx\\
& \leq e^{2\pi (n+\kappa)\eta/N} \sum_{k,\ell}
\int\limits_{\mathfrak{n}_{k\ell}} |f(\tau)| dx. \tag{9}\label{c3:eq4:9}
\end{align*}
But, by the definition of $\mathfrak{n}_{k\ell}$, we see that if
$\tau$ belongs to $\mathfrak{n}_{k\ell}$, then
$\tau^{\ast}=M_{k\ell}<\tau>$ belongs to $\mathfrak{F}^{\ast}$ and
therefore, by \eqref{c3:eq4:7} and \eqref{c3:eq4:8},
$$
|f(\tau)| \leq |c_{k\ell} \tau + d_{k\ell} |^{-r} \text{ for } \tau
\in \mathfrak{n}_{k\ell}. 
$$
Consequently
\begin{align*}
|\int\limits_{\mathfrak{n}_{k\ell}} f(\tau)dx| & \leq
\int\limits_{|x+\frac{d_{k\ell}}{c_{k\ell}}|\leq h c^{-1}_{k\ell}
  \sqrt{n}} \{(c_{k\ell}x + d_{k\ell})^2 + c^2_{k\ell}
\eta^2\}^{-\frac{r}{2}} dx\\
& \leq 2c^{-r}_{k\ell} \int\limits^{hc^{-1}_{k\ell}\sqrt{\eta}}_0
(u^2+\eta^2)^{-\frac{r}{2}} du\\
& \leq 2c^{-r}_{k\ell} \{\int\limits^{\eta} \eta^{-r} du +
\int^{hc^{-1}_{k\ell}\sqrt{\eta}}_{\eta} u^{-r} du\},
\end{align*}
because \pageoriginale $0< \eta < h c^{-1}_{k\ell} \sqrt{\eta}$ by
\eqref{c3:eq4:3}. It follows now that 
\begin{equation*}
\int\limits_{n_{k\ell}} |f(\tau)| dx = 
\begin{cases}
0 (c^{-r}_{k\ell} \eta^{1-r}, c^{-1}_{k\ell} \eta^{\frac{1-r}{2}})
\text{ for } r > 0, r \neq 1\\[5pt]
0 (c^{-1}_{k\ell} \log (\frac{he}{c_{k\ell} \sqrt{\eta}})) \text{ for
} r = 1 
\end{cases} \tag{10}\label{c3:eq4:10}
\end{equation*}

Here and in the following, $0(\omega)$ means in general that
$|\dfrac{0(\omega)}{\omega}|$  is bounded by a constant depending only
on $\Gamma_0$ and $r$. Summing up the right hand side of \eqref{c3:eq4:10} over all
pairs of integers $(c_{k\ell}, d_{k\ell})$ (not necessarily coprime)
which satisfy the inequalities \eqref{c3:eq4:3} and \eqref{c3:eq4:6}, we get
\begin{equation*}
\sum\limits_{k,\ell}
\int\limits_{\mathfrak{n}_{k\ell}} |f(\tau)|dx = 
\begin{cases}
\eta^{1-r} o(\sum_{k,\ell} c^{-r}_{k\ell}) + \eta^{\frac{1-r}{2}} 0
(\sum_{k,\ell} c^{-1}_{k\ell}), \text{ for } r>0, r\neq 1\\[5pt]
0(\sum_{k,\ell} c^{-1}_{k\ell}) \log (\frac{eh}{c_{k\ell}
  \sqrt{\eta}}), \text{ for } r = 1, 
\end{cases}\tag{11}\label{c3:eq4:11}
\end{equation*}
since the right hand side certainly includes the pairs $(c_{kl},
d_{kl})$ occurring on the left hand side. In what follows, all
summations over $k,\ell$ are carried out in this sense. It can be
proved easily that 
\begin{gather*}
\sum_{k,\ell} c^{-\rho}_{k\ell} \leq (N+2) \sum^{[h\eta^{1/2}]}_{c=1}
c^{1-\rho}\\
= \begin{cases}
o(1) & \text{ for } \rho > 2,\\
o(\log\frac{1}{\eta}) & \text{ for } \rho =2,\\
o (\eta^{\rho/2-1}) & \text{ for } 0< \rho < 2.
\end{cases}
\end{gather*}
and
$$
\sum_{k,\ell} c^{-1}_{k\ell} \log \{\frac{eh}{c_{k\ell}\sqrt{\eta}}\} 
\leq (N+2) \sum^{[h\eta^{-1/2}]}_{c=1} \log
\{\frac{eh}{c\sqrt{\eta}}\} = o (\eta^{-1/2});
$$
by Stirling's \pageoriginale formula, namely
$$
n! = \alpha_n n^{n+\frac{1}{2}} e^{-n}, \lim\limits_{n\to \infty}
\alpha_n = \sqrt{2\pi}, 
$$
with $h\eta^{-1/2}=n+\vartheta$, where $0\leq \vartheta<l$ and $n$ is
an integer, we have indeed 
\begin{align*}
\sum^{[h\eta^{-1/2}]}_{c=1} \log\{\frac{eh}{c\sqrt{\eta}}\} & = n + n
\log (n + \vartheta) - \log \alpha_n - (n+ \frac{1}{2}) \log n + n \\
& = 2n + n \log (1+ \frac{\vartheta}{n}) - \log \alpha_n -\frac{1}{2}
\log n \\
& < 2n + \vartheta  - \log \alpha_n -\frac{1}{2} \log n = o(n) =
o(\eta^{-\frac{1}{2}}). 
\end{align*}
Therefore, equation \eqref{c3:eq4:11} gives 
\begin{equation*}
\sum_{k,\ell} \int\limits_{\mathfrak{n}_{k\ell}} |(f(\tau)| dx = 
\begin{cases}
o(\eta^{1-r}) & \text{ for } r > 2,\\
o(\frac{1}{\eta} \log \frac{1}{\eta}) & \text{ for } r = 2, \\
o(\eta^{-r/2}) & \text{ for } 0 < r < 2.
\end{cases}\tag{12}\label{c3:eq4:12}
\end{equation*}
With $n + \kappa =\dfrac{1}{\eta}$, \eqref{c3:eq4:9} and 
\eqref{c3:eq4:12} lead us then to

\begin{thm}\label{chap3:thm26}
Let $\Gamma_0$ be a subgroup of finite index in the modular group and
$v$ a multiplier system for the group $\Gamma_0$ and weight $r>0$. Let 
$$
f(\tau) = \sum_{n+\kappa} c_{n+\kappa} e^{2\pi i (n+\kappa)\tau/N}
$$
be the Fourier expansion of a modular form belonging to the space
$[\Gamma_0, r, v]$. 
Then we have 
$$
c_{n+\kappa} = 
\begin{cases}
o((n+\kappa){r-1}) & \text{ for } r > 2, \\
o((n+\kappa)\log(n+\kappa)) & \text{ for } r=2, \text{ as } n + \kappa
\to \infty, \\
o ((n+\kappa)^{r/2}) & \text{ for } 0 < r< 2, 
\end{cases}
$$
\end{thm}

For \pageoriginale the Fourier coefficients of a cusp form $f(\tau)$
belonging to $[\Gamma_0,r,v]$ with $r\geq 2$, we have sharper
estimates given by the following 

\begin{thm}\label{chap3:thm27}
Let
$$
f(\tau) = \sum_{n+\kappa >0} c_{n+\kappa} e^{2\pi i (n+\kappa)\tau/N}
$$
be a cusp form in $[\Gamma_0, r, v]$, where $\Gamma_0$ is a subgroup
of the modular group with finite index and $v$ is a multiplier system
for the group $\Gamma_0$ and weight $r$. Then 
$$
c_{n+\kappa} = o ((n+\kappa)^{\frac{r}{2}}) \text{ as }  n + \kappa
\to \infty
$$
for all $r\geq 2$.
\end{thm}

\begin{proof}
Since $f(\tau)$ is a cusp form, by \S~\ref{chap3:sec3}, 
lemma~\ref{chap3:lem5}, the function
$|f(\tau)|y^{\frac{r}{2}}$ is bounded in $\mathscr{G}$ i.e. 
$$
|f(\tau)| \leq C y^{-\frac{r}{2}} \text{ for some constant } C.
$$
Therefore for the Fourier coefficient $c_{n+\kappa}$ we have 
\begin{align*}
|c_{n+\kappa}| & = \frac{1}{N}| \int\limits^N_0 f(\tau) e^{-2\pi
  i(n+\kappa)\tau/N} dx|\\
& \leq C y^{-\frac{r}{2}} e^{2\pi(n+\kappa)y/N}
\end{align*}
The estimate for $c_{\eta+\kappa}$ stated in theorem~\ref{chap3:thm27} follows
immediately on taking $y=1/(n+\kappa)$.
\end{proof}

We have seen in chapter~\ref{chap1} \S~\ref{chap1:sec5}, 
that the $n-th$ Fourier coefficient of
the Eisenstein series $G_k(\tau)$, for any even integer $k\geq 4$,
coincides with \pageoriginale $d_{k-1}(n)$ but for a constant factor
independent of $n$. But 
$$
n^{k-1} <d_{k-1}(n) < \zeta (k-1) n^{k-1}, (k>2)
$$
since 
$$
d_{k-1}(n) = \sum_{d|n} (\frac{n}{d})^{k-1} < n^{k-1}
\sum^{\infty}_{d=1} \frac{1}{d^{k-1}} = \zeta (k-1) n^{k-1}.
$$
Therefore, for $k>2$, the Fourier coefficients of the Eisenstein
series increase more rapidly than the Fourier coefficients of cusp
forms. Hence the estimates given in theorem~\ref{chap3:thm26} 
for $r>2$ can not be
sharpened, in general. However, sharper estimates for the Fourier
coefficients of cusp forms belonging to the congruence subgroups of
the modular group have been obtained by using some estimates of the
so-called \textit{Kloosterman sums} given by A. Weil. For this, we
refer to a paper of K.B. Gundlach \cite{c3:key1}. 

\begin{thebibliography}{99}
\bibitem{c3:key1} K.B. Gundlach: \"Uber die Darstellung der ganzen
  Spitzenformen zu den Idealstufen der Hilbertschen Modulgruppe und
  die Absch\"atzung ihrer Fourierkoeffizienten, Acta Math., 92 (1954),
  309-345. 

\bibitem{c3:key2} D.H. Lehmer: The vanishing of Ramanujan's function
  $\tau(n)$, Duke Math. J., 14 (1947), 429-433.

\bibitem{c3:key3} J. Lehner: The Fourier coefficients of automorphic
  forms belonging to a class of horocyclic groups, Michigan Math. J.,
  4 (1957), 265-279.

\bibitem{c3:key4} J. Lehner: The \pageoriginale Fourier coefficients of
  automorphic forms on horocyclic groups II, Michigan Math. J., 6
  (1959),  173-193. 

\bibitem{c3:key5} J. Lehner: Magnitude of the Fourier coefficients of
  automorphic forms of negative dimension, Bull. Amer. Math. Soc., 67
  (1961), 603-606. 

\bibitem{c3:key6} H. Petersson: Theorie der automorphen Formen beliebiger
  reeler Dimension und ihre Darstellung durch eine neue Art
  Poincar\'escher Reihen, Math. Ann., 103 (1930), 369-436.

\bibitem{c3:key7} H. Petersson: Die linearen Relationen zwischen den
  ganzen Poincar\'eschen Reihen von reeller Dimension zur Modulgruppe,
  Abh. Math. Sem. Hamburg, 12 (1938), 515-472.

\bibitem{c3:key8} H. Petersson: \"Uber die Entwicklungskoeffizienten der
  ganzen Modulformenund ihre Bedeutung f\"ur die Zahlentheorie,
  Abh. Math. Sem. Hamburg, 8 (1931), 215-242.

\bibitem{c3:key9} H. Petersson: \"Uber die systematische Bedeutung der
  Eisensteinschen Reihen, Abh. Math. Sem. Hamburg,. 16 (1949), 104-126.

\bibitem{c3:key10} H. Petersson: \"Uber eine Metrisierung der ganzen
  Modulformen, Jahresber. D.M.V., 49 (1939), 49-75.

\bibitem{c3:key11} H. Petersson: \"Uber eine Metrisierung der dutomorphen
  Formen und die Theorie der Poincar\'eschen Reihen, Math. Ann., 117
  (1940), 453-537.

\bibitem{c3:key12} H. Petersson: \"Uber Weierstrasspunkte und die
  expliziten Darstellungen der automorphen Formen von reeller
  Dimension, Math. Zeit., 52 (1950), 32-59. 

\bibitem{c3:key13} H. Petersson: \"Uber \pageoriginale Betragmittelwerte und die
  Fourier-Koeffizienten der ganzen automorphen Formen, Arch. Math., 9
  (1958), 176-182.

\bibitem{c3:key14} H. Petersson: \"Uber die Entwicklungskoeffizienten der
  automorphen Formen, Acta Math., 58 (1932), 169-215.

\bibitem{c3:key15} R.A. Rankin: Contributions to the theory of
  Ramanujan's function $\tau(n)$ and similar arithmetical functions,
\begin{itemize}
\item[I.] The zeros of the function
  $\sum\limits^{\infty}_{n=1}\dfrac{\tau(n)}{n^s}$ on the line $\re s =
  \dfrac{13}{2}$,

\item[II.] The order of the Fourier coefficients of integral modular
  forms, 

\item[III.] A note on the sum function of the Fourier coefficients of
  integral modular forms,
\end{itemize}

Proc. Camb. Phil. Soc., 35 (1939), 351-356, 357-372 and 36 (1940),
150-151. 
\end{thebibliography}
