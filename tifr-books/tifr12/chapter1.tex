\chapter{Measure Theory}\label{chap1}

\section{Sets and operations on sets}\label{chap1:sec1}

We\pageoriginale consider a space $\mathfrak{X}$ of elements (or point) $x$ and systems
of this subsets $X, Y, \ldots$ The basic relation between sets and the
operations  on them are defined as follows: 
\begin{enumerate}
  \renewcommand{\theenumi}{\alph{enumi}}
  \renewcommand{\labelenumi}{(\theenumi)}
\item \textit{Inclusion:} We write $X\subset Y$ (or $Y \supset
  X$) if every point of $X$ is contained in $Y$. Plainly, if  $0$  is empty
  set, $0$ $\subset X \subset \mathfrak{X}$ for every subset
  $X$. Moreover, $X \subset X$  and $X \subset Y$, $Y \subset Z$ imply
  $X \subset Z$. $X=Y$ if $X \subset Y$ and $Y \subset X$.

\item \textit{Complements:} The complements $X'$  of  $X$ is the
  set of point of $\mathfrak{X}$ which do not belong  to $X$. Then
  plainly $(X')'=X$ and $X' = Y$ if $Y'= X$. In particular ,
  $O'= \mathfrak{X}$, $\mathfrak{X}'=0$. Moreover, if $X \subset Y$,
  then $Y'\subset X'$.  

\item \textit{Union:} The union of any system of sets is the set
  of points $x$ which belong to at least one of them. The system need not
  be finite or even countable. The union of two sets  $X$ and  $Y$  is
  written $X\cup Y$, and obviously $X \cup Y  = Y \cup X $. The
  union of a finite or countable sequence of sets $X_1$, $X_2,\dots$ can
  be written $\displaystyle\bigcup^{\infty}_{n=1} X _n$. 


\item \textit{Intersection:} The intersection of a system of sets
  of points which belong to every set of the system. For two sets it
  is  written $X\cap Y$ (or $X.Y$) and for a sequence $\{X_n\}$,
  $\displaystyle\bigcap^{\infty}_{n=1} X_n$. Two sets are disjoint if
  their intersection is $0$, a system of sets  is disjoint if every pair
  of sets of the system is. For disjoint system we write\pageoriginale $X+Y$ for
  $X\cup Y$ and $\sum X_n$ for $\cup X_n$, this notation implying
  that the sets are disjoint.

\item \textit{Difference:} The difference $X. Y'$ or $X-Y$
  between two $X$ and $Y$  is the sets of point of $X$ which do not belong
  to $Y$. We shall use the notation $X-Y$ for the difference only if
  $Y\subset X$. 
\end{enumerate}

It is clear that the operations of taking unions and intersection are
both commutative and associative. Also they are related t to the
operation of taking complements by  
$$
X.X'=0,X+X'=\mathfrak{X},(X \cup Y)'=X',Y',(X.Y)'=X'\cup
  Y'.
$$

More generally
$$
(\cup X)'=\cap X',(\cap X)'=\cup X'.
$$

The four operations defined above can be reduced to two in several
different ways. For examples they can all be expressed  in terms  of
unions  and complements. In fact there is complete duality in the
sense that any true proposition about sets remains true if we
interchange  
\begin{align*}
  0 & \quad {\rm and}  \quad \mathfrak{X}\\
  \cup &  \quad{\rm and}  \quad \cap\\
  \cap &  \quad{\rm and}  \quad \cup\\
  \subset &  \quad{\rm and}  \quad \supset
\end{align*}
and leave = and $'$ unchanged all through.

A countable union can be written as a sum by the formula
$$
\bigcup^{\infty}_{n=1}
  X_n=X_1+X'_1. X_2+X'_1.X'_2.X_3 +\cdots
$$ 

\section{Sequence of sets}\label{chap1:sec2} %%%2

A\pageoriginale sequence of sets $X_1$, $X_2,\dots $ is {\em increasing} if
$$
X_1\subset X_2 \subset X_3 \subset \ldots
$$
{\em decreasing} If
$$
X_1\supset X_2\supset X_3\supset\ldots 
$$

The {\em upper limit}, lim  sup $X_n$ of a sequence $\{X_n\}$ of sets is the
set of points which belong to $X_n$ for infinitely many $n$. 
The {\em lower limit},
$\lim \inf X_n$ is the set of points which belong to $X_n$ for all but a
finite number of $n$. It follows that $\lim \inf X_n\subset \lim$  $\sup X_n$
and if $\lim \sup X_n= \lim \inf X_n=X$, $X$ is called the {\em limit} of the
sequence, which then {\em coverage} to $X$.  

It is easy to show that
$$
\lim\inf X_n =\bigcup^{\infty}_{n=1}\bigcap^{\infty}_{m=n} X_m
$$
and that 
$$
\lim\sup X_n = \bigcap^{\infty}_{n=1}\bigcup^{\infty}_{m=n} X_m.
$$

Then if  $X_n \downarrow$,
\begin{align*}
  & \bigcap^{\infty}_{m=n} X_{m}= \bigcap^{\infty}_{m=1} X_m. \lim
  \inf X_n  = \bigcap^{\infty}_{m=1} X_m,\\ 
  &\bigcup^{\infty}_{m=n} X_m = X_n,  \lim\sup X_n =\bigcap^{\infty}_{n=1} X_n,\\
  &\lim X_n = \bigcap^{\infty}_{n=1} X_n,
\end{align*}
and similarly if $X_n \uparrow$,
$$
\lim X_n=\bigcup^{\infty}_{n=1} X_n.
$$

\section{Additive system of sets}\label{chap1:sec3}

A\pageoriginale system of sets which contains $\mathfrak{X}$ and is closed under a
finite number of complement and union operations is called a ({\em finitely})
{\em additive system or a field}. It follows from the duality principle that
it is then closed under a finite number of intersection operations. 

If an additive system is closed under a countable number of union and
complement operations (and therefore under countable under inter sections),
it is called a {\em completely additive system}, a {\em Borel system} or a
{\em $\sigma$-field}. 

It follows that any intersection (not necessarily countable) of
additive or Borel system is a system of the same type. Moreover, the
intersection of {\em all} additive (of Borel) systems containing a
family of sets is a uniquely defined minimal additive (or Borel)
system containing the given
family. The existence of {\em at least one} Borel system containing a given
family is trivial, since the system of {\em all} subsets of $\mathfrak{X}$ is a
Borel system. 

A construction of the actual minimal Borel system containing a given
family of sets has been given by Hausdorff (Mengenlehre,1927, p.85). 

\begin{theorem}\label{chap1:sec3:thm1}
  Any given family of subsets of a space
  $\mathfrak{X}$ is contained in a unique minimal additive system $S_0$
  and in a unique minimal Borel system $S$. 
\end{theorem}

\noindent
{\bf Example of a finitely additive system:} The family
  of rectangles $a_i \le x_i < b_i(i=1,2,...,n)$ in $R_n$ is not
  additive, but has a minimal\pageoriginale additive $S_0$ consisting of all ``element
  ary figures'' and their complements. An elementary figure is the
  union of a finite number of such rectangles. 

The intersections of sets of an additive (or Borel) system with a
fixed set(of the system) from an additive (or Borel) subsystem of the
original one. 

\section{Set Functions}\label{chap1:sec4}

Functions con be defined on a system of sets to take values in any
given space. If the space is an abelian group with the group operation
called addition, one can define the additivity of the set function. 

Thus, if $\mu$ is defined on an additive system of sets, $\mu$ is {\em
  additive} if
$$
\mu\left(\sum X_n\right)=\sum \mu(X_n)
$$ 
for any {\em finite} system of (disjoint) sets $X_n$.

In general we shall be concerned only with functions which take real
values. We use the convention that the value $-\infty$ is  excluded but
that $\mu $ may take the value $+ \infty$. It is obvious that
$\mu(0)=0$ if $\mu(X)$ is additive and finite for at least one $X$. 

For a simple example of an additive set function we may take $\mu(X)$
to be the {\em volume} of $X$ when $X$ is an elementary figures in $R_n$. 

If the additive property extends to countable system of sets, the
function is called {\em completely} additive, and again we suppose that $\mu
(X)\neq - \infty$. Complete additive of $\mu$ can defined even if the\pageoriginale
field of $X$ is only {\em finitely} additive, provided that $X_n$ and $\sum
X_n$ belong to it. 

\medskip
\noindent
{\bf Example of a completely additive function:} $\mu(X)$ = number of
elements (finite of infinite) in $X$ for all subsets $X$ of $\mathfrak{X}$

\medskip
\noindent
{\bf Examples of additive, but not completely additive functions:}
\begin{enumerate}
\item $\mathfrak{X}$ is an infinite set,
\begin{align*}
\mu(X)&= 0 \text{ if } X \text{ is a finite subset of } \mathfrak{X}\\
&=\infty \text{ if } X \text{ is an infinite subset of } \mathfrak{X}
\end{align*}

Let $X$ be a countable set of elements $(x_1,x_2,\ldots)$ of $\mathfrak{X}$.

Then
$$
\mu(x_n)=0, \sum \mu(x_n)=0, \mu(X)=\infty.
$$

\item $\mathfrak{X}$ is the interval $0\le x < 1$ and $\mu(X)$ is the
  sum of the lengths of finite sums of open or closed intervals {\em with
  closure in $\mathfrak{X}$}. 
  These sets together with $\mathfrak{X}$ from an additive system on
  which $\mu$ is additive but not completely additive if
  $\mu(\mathfrak{X})=2$. 

  A non-negative, completely additive function $\mu$ defined on a Borel
  system $S$ of subsets of a set $\mathfrak{X}$ is called a {\em measure}. It is
  bounded (or finite) if $\mu(\mathfrak{X}) < \infty$. it is called a
  probability measure if $\mu(\mathfrak{X})=1$. The sets of the system
  $S$ are called {\em measurable sets}. 
\end{enumerate}

\section{Continuity of set functions}\label{chap1:sec5}

\begin{defi*}
A set function $\mu$ is said to be
  continuous, from below if $\mu(X_n)\rightarrow \mu (X)$ whenever
  $X_n \uparrow X$. It is continuous from above if
  $\mu(X_n)\rightarrow \mu(X)$ whenever $X_n \downarrow X$ and $\mu
  (X_{n_o}) < \infty$ for some $n_0$. 
\end{defi*}

It\pageoriginale is continuous if it is continuous from above {\em
  and} below. Continuity at 0 means continuity from above at 0.  

(For general ideas about limits of 
set functions when $\{X_n\}$ is not monotonic, see Hahn and
Rosenthal, Set functions, Ch. I).  

The relationship between additivity and
complete additivity can be expressed in terms of continuity as follows.

\begin{theorem}\label{chap1:sec5:thm2} %%% 2
\begin{itemize}
\item[\rm(a)] A completely additive function is continuous.
  
\item[\rm(b)] Conversely, an additive function is completely additive if it is
  either continuous from below or finite and continuous at 0. 
  (The system of sets on which $\mu$ is defined need only be finitely additive).
\end{itemize}
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \renewcommand{\theenumi}{\alph{enumi}}
    \renewcommand{\labelenumi}{(\theenumi)}
  \item If $X_n \uparrow X$, we write
    \begin{align*}
      X &=X_1 + (X_2 - X_1) + (X_3 - X_2) +\cdots,\\
      \mu(X) &=-\mu(X_1) + \mu(X_2 -X_1) +\cdots\\
      &=\mu(X_1) + \lim_{N \rightarrow \infty} \sum^{N}_{n=2} \mu(X_n - X_{n-1})\\
      &=\lim_{N \rightarrow \infty} \mu(X_N).
    \end{align*}
    
    On the other hand, if $X_n \downarrow X$  and $\mu(X_{n_0}) <
    \infty,$ we write 
    \begin{gather*}
      X_{n_0} = X + \sum^{\infty}_{n={n_0}} (X_n - X_{n+1})\\
      \mu (X_{n_0}) = \mu (X) + \sum^{\infty}_{n={n_0}} \mu
      (X_n - X_{n+1}), \text { and } \mu (X) =  lim  \mu (X_n) 
    \end{gather*}
    as above  since $\mu (X_{n_0})<\infty$. 
  \item First,\pageoriginale if $\mu$ is additive and continuous from below, and 
    $$ 
    Y= Y_1 + Y_2 + Y_3 +\cdots
    $$
    we write 
    \begin{align*}
      Y &= \lim_{N \rightarrow \infty} \sum^{N}_{n=1} Y_n,\\
      \mu (Y) &= \lim_{N \rightarrow \infty}\mu\left(\sum^{N}_{n=1}
      Y_n\right), \text{ since }\sum^{N}_{n=1} Y_n \uparrow Y\\
      &=\lim_{N \rightarrow \infty} \sum^{N}_{n=1} \mu (Y_n)
    \end{align*}
    by finite additivity, and therefore $\mu(Y) =
    \sum\limits^{\infty}_{n=1} \mu(Y_n)$. 
    
    On the other hand, if $\mu$ is finite and continuous at 
    0, and $X = \sum\limits^{\infty}_{n=1} X_n $, we write  
    \begin{align*}
      \mu (X) &= \mu \left(\sum^{N}_{n=1} X_n\right) + \mu
      \left(\sum^{\infty}_{n=N+1} X_n\right)\\ 
      & = \sum^{N}_{n=1} \mu(X_n) + \mu \left(\sum^{\infty}_{n=N+1}
      X_n\right), \text { by finite additivity,} 
    \end{align*}
  \end{enumerate}
    since $\sum\limits^{\infty}_{N+1} X_n \downarrow 0$ and has finite $\mu$.
\end{proof}

\begin{theorem}[Hahn-Jordan]\label{chap1:sec5:thm3}%%% 3 
  Suppose that $\mu$
    is completely additive in a Borel system $S$ of subsets of a space
    $\mathfrak{X}$. Then we can write 
    $\mathfrak{X}= \mathfrak{X}^{+}+ \mathfrak{X}^{-}$ (where
    $\mathfrak{X}^+$, $\mathfrak{X}^-$ belong to $S$ and one may be empty)
    in such a way that 
  \begin{enumerate}
  \item  $0\leq \mu (X) \leq \mu (\mathfrak{X}^+) = M \leq \infty$ for $X
    \subset \mathfrak{X}^+$, 

    $- \infty < m = \mu (\mathfrak{X}^-) \leq \mu (X) \leq 0$ for $X \subset
    \mathfrak{X}^-$  

    while $m \leq \mu (X) \leq M$ for all $X$.
  \end{enumerate}
\end{theorem}

\begin{corollary}\label{chap1:sec5:coro1}
The upper and lower bounds $M$, $m$ of
  $\mu(X)$ in $S$ are attained for the sets $\mathfrak{X}^+$,
  $\mathfrak{X}^-$ respectively and $m > - \infty$. 

Moreover,\pageoriginale $M <\infty$ if $\mu(X)$ is finite for all $X$. In
particular, a finite {\em measure} is bounded. 
\end{corollary}

\begin{corollary}\label{chap1:sec5:coro2}
  If we write 
  $$
  \mu^{+}(X) = \mu(X\cdot\mathfrak{X}^{+}), \mu^-(X) = \mu(X\cdot
  \mathfrak{X}^-)
  $$
  we have
  \begin{align*}
    \mu(X) &= \mu^{+}(X) + \mu^-(X),  \mu^+(X) \geq 0, \mu^-(X) \leq 0 \\
    \mu^+(X) &= \sup_{Y\subset X} \mu(Y), \mu^-(X) = \inf_{Y\subset X} \mu
    (Y).
  \end{align*}
  
  If we write  $\overline{\mu}(X) = \mu^+(X) -\overline{\mu}(X)$, we
  have also
  $$
  |\mu  (Y)|\leq \overline{\mu}(X) \text{ for all } Y\subset X. 
  $$
\end{corollary}

It follows from the theorem and corollaries that an additive function
can always be expressed as the difference of two measures, of which
one is bounded 
(negative part here). From this point on, it is sufficient to consider
only measures. 

\setcounter{proofofthm}{2}
\begin{proofofthm}\label{chap1:sec5:pot3}%pot 3
  [Hahn and Rosenthal, with
    modifications] 
  We suppose that $m < 0$ for otherwise there is
  nothing to prove. Let $A_n$ be defined 
  so that $\mu (A_n) \rightarrow m$ and let $A
  =\bigcup\limits^{\infty}_{n=1} A_n$. For every $n$, we
  write 
  $$ 
  A=A_k + (A - A_k), A = \bigcap^{n}_{k=1} [A_k
    + (A - A_k)]
  $$ 
\end{proofofthm}

This can be expanded as the union of $2^n$ sets of the form
$\bigcap\limits^{n}_{k=1} A^*_k$, $A^*_k = A_k$ or $A - A_k$,
and we write 
$B_n$ for the sum of those for which $\mu < 0$. (If there is no such
set, $B_n = 0$). Then, since $A_n$ consists of disjoint sets which
either belong to $B_n$ or have $\mu \geq 0$, we\pageoriginale get
$$
\mu(A_n)\ge(B_n)
$$

Since the part of $B_{n+1}$ which does not belong to $B_n$ consists of
a finite number of disjoint sets of the form
$\bigcap\limits^{n+1}_{k=1} A^*_k$ for each of which $\mu <
0$,  
$$
\mu (B_n\cup B_{n+1}) = \mu(B_n) + \mu(B_{n+1} B'_n)\le\mu(B_n)
$$
and similarly
$$
\mu(B_n) \geq \mu(B_n \cup B_{n+1}\cup \ldots \cup B_{n'})
$$
for any $n' > n$. By continuity from below, we can let $n'
\rightarrow \infty$, 
$$
\mu(A_n) \geq \mu (B_n) \geq \mu \left(\bigcup^{\infty}_{k=n} B_k\right)
$$

Let $\mathfrak{X}^-= \lim_{n\to\infty}  \bigcup\limits^{\infty}_{k=n} B_k$. Then  
$$
\mu(x^-) \leq \lim_{n\to\infty} \mu (A_n)=m,
$$
and since $\mu(x^-) \geq m$ by definition of $m$, $\mu(x^-)=m$.

Now, if $X$ is any subset of $\mathfrak{X}^-$ and $\mu (X) > 0$, we have
$$
m=\mu(\mathfrak{X}^-)= \mu (X) + \mu(\mathfrak{X}^- - X) > \mu (\mathfrak{X}^- - X)
$$ 
which contradicts the fact that $m$ is $\inf\limits_{Y\subset\mathfrak{X}}\mu(Y)$. 

This proves (1) and the rest follows easily.

It is easy to prove that corollary 2 holds also for a completely
additive function on a {\em finitely} additive system of sets, but sup
$\mu(X)$, $\inf \mu(X)$ are then not necessarily attained. 

\section[Extensions and contractions of...]{Extensions and contractions of additive\\ functions}\label{chap1:sec6}

We\pageoriginale get a contraction of an additive (or completely additive) function
defined on a system by considering only its values on an function
defined on a system by considering only its values on an additive
subsystem.  More important, we get an  {\em extension} by embedding the
system of sets in a larger system and defining a set function on the
new system so that it takes the same values as before on the old system. 

The basic problem in measure theory is to prove the existence of a
measure with respect to which certain assigned sets are measurable and
have assigned measures.  The classical problem of defining a measure
on the real line with respect to which every interval is measurable
with measure equal to its length was solved by Borel and Lebesgue.  We
prove Kolmogoroff's theorem (due to Caratheodory in the case of $R_n$)
about conditions under which an additive function on a finitely
additive system $S_0$ can be extended to a measure in a Borel system
containing $S_0$. 

\setcounter{theorem}{3}
\begin{theorem}\label{chap1:sec6:thm4}
\begin{enumerate}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item If $\mu(I)$ is non-negative and additive on an additive system
  $S_0$ and if $I_n$ are disjoint sets of $S_0$ with 
  $I = \sum\limits^{\infty}_{n=1}I_n$ also in $S_0$, then
  $$
  \sum^{\infty}_{n=1} \mu(I_n) \leq \mu (I).
  $$
  
\item In order that  $\mu(I)$ should be completely additive, it is
  sufficient that 
  $$
  \mu(I)\leq \sum^{\infty}_{n=1} \mu (I_n).
  $$
  
\item Moreover,\pageoriginale if $(I)$ is completely additive, this last inequality holds
  whether $I_n$ are disjoint or not, provided that  
  $I\subset \bigcup\limits^{\infty}_{n=1} I_n$.
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item For any N,
$$
\sum^{N}_{n=1} I_n, I - \sum^{N}_{n=1} I_n 
$$
belong to $S_0$ and do not overlap.  Since their sum is $I$, we get
$$
\mu(I) = \mu\left(\sum^{N}_{n=1} I_n\right) + \mu\left(I
-\sum^{N}_{n=1} I_n\right)
$$
$$
\geq \mu \left(\sum^{N}_{n=1}  I_n\right) = \sum^{N}_{n=1} \mu(I_n)
$$
by finite additivity.  Part (a) follows if we let $N \rightarrow
\infty$ and (b) is a trivial consequence of the definition. 

For (c), we write
$$
\bigcup^{\infty}_{n=1} I_n=I_1 + I_2\cdot I'_1 + I_3
\cdot I'_1\cdot I'_2 + \cdots
$$
and then
$$
\mu(I)\leq \mu [\cup^{\infty}_{n=1} I_n] =  \mu(I_1)
+ \mu (I_2\cdot I'_1) + \cdots
$$
$$
\leq \mu(I_1) + \mu(I_2) + \cdots
$$
\end{enumerate}
\end{proof}

\section{Outer Measure}\label{chap1:sec7}

We define the out or measure of a set $X$ with respect to a 
completely\pageoriginale additive non-negative $\mu(I)$ defined on a
additive system 
$S_0$ to be $\inf \sum \mu(I_n)$ for all sequences $\{I_n\}$ of sets of
$S_0$ which cover $X$ (that is, $X \subset
\bigcup\limits^{\infty}_{n=1}$). 

Since any $I$ of $S_0$ covers itself, its outer measure does not exceed
$\mu(I)$. On the other hand it follows from
Theorem \ref{chap1:sec6:thm4}(c) that   
$$ 
\mu (I) \leq \sum^{\infty}_{n=1} \mu (I_n) 
$$
for {\em every} sequence $(I_n)$ covering $I$, and the inequality remains true
if the right hand side is replaced by its lower bound, which is the
outer measure of $I$. It follows that the outer measure of a set $I$ of
$S_0$ is $\mu(I)$, and there is therefore no contradiction if we use
the same symbol $\mu (X)$ for the outer measure of every set $X$,
whether in $S_0$ or not. 

\begin{theorem}\label{chap1:sec7:thm5}% 5
  If $X\subset
  \bigcup\limits^{\infty}_{n=1} X_n$, then 
  $$
  \mu (X) \leq \sum^{\infty}_{n=1} \mu (X_n)
  $$
\end{theorem}

\begin{proof}
  Let $\epsilon > 0$,
  $\sum\limits^{\infty}_{n=1} \epsilon_n \leq \epsilon$. Then we
  can choose $I_{n\nu}$ from $S_0$ so that  
  $$
  X_n \subset \bigcup^{\infty}_{\nu=1} I_{n\nu},
  \sum^{\infty}_{\nu=1} \mu (I_{n\nu}) \leq \mu
  (X_n) + \epsilon_n,
  $$ 
  and then, since
  $$
  X \subset \bigcup^{\infty}_{n=1}
  X_n \subset \bigcup^{\infty}_{n,\nu=1}
  I_{n\nu}, 
  $$
  \begin{align*}
    \mu (X)\le \sum_{n=1}^{\infty} \sum_{\nu=1}^{\infty} \mu
    (I_{n\nu}) & \leq \sum_{n=1}^{\infty} (\mu (X_n)+\epsilon_n)\\ 
    &\leq \sum_{n=1}^{\infty}  \mu (X_n) + \epsilon, 
  \end{align*}
  and\pageoriginale we can let $\epsilon \rightarrow 0$.
\end{proof}

\medskip
\noindent\textit{\textbf{Definition of Measurable Sets.}}

We say that $X$ is {\em measurable} with respect to the function $\mu$
if
$$ 
\mu (PX)+ \mu (P-PX) = \mu (P) 
$$
for every $P$ with $\mu (P)< \infty$.

\begin{theorem}\label{chap1:sec7:thm6}%6
  Every set I of $S_o$ is measurable. 
\end{theorem}

\begin{proof}
If $P$ is any set with $\mu (P)<\infty$, and
  $\epsilon > 0$,  we can define $I_n$ in \break $S_0$ so that 
$$ 
P \subset  \bigcup_{n=1}^{\infty} I_n ,
\sum_{n=1}^{\infty} \mu(I_n)\leq \mu (P) + \epsilon  
$$

Then
$$ 
PI  \subset{\displaystyle\mathop{\bigcup}_{n=1}^{\infty}} I\cdot I_n , p-PI \subset
\bigcup_{n=1}^{\infty} (I_n - II_n)  
$$
and since $I I_n $ and $I_n-II_n$ both belong to $S_0$,
$$ 
\mu (PI) \leq  \sum_{n=1}^{\infty} \mu (II_n), \mu
(P-PI) \leq  \sum_{n=1}^{\infty}\mu (I_n-II_n)   
$$
and
\begin{align*}
\mu (PI)+ \mu (P-PI) &\leq  \sum_{n=1}^{\infty}  (\mu (II_n)+\mu (I_n-II_n))\\
 &\quad =  \sum_{n=1}^{\infty} \mu (I_n)\leq \mu (P) + \epsilon 
 \end{align*}
 by\pageoriginale additivity in $S_0$. Since $\epsilon$ is arbitrary,
 $$ 
\mu (PI)+ \mu (P-PI)\le\mu(P)
$$
 as required.
 
 We can now prove the fundamental theorem.
\end{proof}

\begin{theorem}[Kolmogoroff-Caratheodory]\label{chap1:sec7:thm7}% 7
  If $\mu$ is a non-negative and completely additive set function in
  an additive system $S_0$, a measure can be defined in a Borel system
  $S$ containing $S_0$ and taking the original value $\mu(I)$ for $I
  \in S_0$. 
\end{theorem}

\begin{proof}
  It is sufficient to show that the
  measurable sets defined above form a Borel system and that the
  outer measure $\mu$ is completely additive on it. 
 
 If $X$ is measurable, it follows from the definition of measurablility
 and the fact that 
\begin{align*}
  & PX' = P - PX, P - PX' =PX,\\
  & \mu (PX')+ \mu (P-PX) = \mu (PX)+ \mu (P-PX)
\end{align*}
that $X'$ is also measurable.
 
 Next suppose that $X_1$, $X_2$ are measurable. Then if $\mu (P)<\infty$,
 \begin{align*}
 \mu (P) &= \mu (PX_1)+ \mu (P-PX_1) \text{ since } X_1 \text{ is measurable}\\
 & = \mu (PX_1X_2) + \mu (PX_1-PX_1X_2) + \mu (PX_2-PX_1X_2)\\ 
 & \qquad \qquad + \mu(P-P(X_1 \cup X_2)) ~\text{since $X_2$ is measurable}
 \end{align*}

Then, since
$$ 
(PX_1-PX_1X_2)+ (PX_2-PX_1X_2)+ (P-P(X_1 \cup X_2))=P-PX_1X_2,
$$
it\pageoriginale follows from Theorem \ref{chap1:sec7:thm5} that
$$ 
\mu (P) \geq \mu (PX_1 X_2) + \mu (P- PX_1 X_2)
$$
and so $X_1 X_2$ is measurable.

It follows at once now that the sum and difference of two measurable
sets are measurable and if we take $ P = X_1 + X_2$ in the formula
defining measurablility of  $ X_1 $, it follows that 
$$ 
\mu (X_1 + X_2)= \mu (X_1) + \mu (X_2)
$$

When $X_1$ and $X_2$ are measurable and $X_1 X_2 = 0$. This shows that
the measurable sets form an additive system $S$ in which $\mu(X)$ is
additive. After Theorems \ref{chap1:sec6:thm4}(b)
and \ref{chap1:sec7:thm5}, $\mu (X)$ is also completely 
additive in $S$. To complete the proof, therefore, it is sufficient to
prove that $X = \bigcup\limits_{n =1}^{\infty} X_n$ is measurable if
the $X_n$ are measurable and it is sufficient to prove this in the
case of disjoint $X_n$. 

If $\mu (P) < \infty$,
$$
\mu (P) = \mu \left(P \sum_{n=1}^{n}X_n\right) + \mu \left(P - P
\sum^{N}_{n=1} X_n\right)
$$
since $\sum\limits^{N}_{n=1} X_n$ is measurable,
$$
\geq \mu \left(P \sum^{N}_{n=1} X_n\right) + \mu (P- PX)
= \sum^{N}_{n=1} \mu (PX_n) + \mu (P - PX)
$$
by definition of measurablility applied $N-1$ times, the $X_n$ being disjoint.

Since\pageoriginale this holds for all $N$,
\begin{align*}
\mu (P) & \geq \sum^{\infty}_{n=1} \mu (PX_n) + \mu (P- PX)\\
        &\geq \mu (PX) + \mu (P- PX),
\end{align*}
by Theorem \ref{chap1:sec7:thm5}, and therefore X is measurable.
\end{proof}

\begin{defi*}
A measure is said to be {\em complete} if every subset of a measurable
set of zero measure is also measurable (and therefore has measure
zero). 
\end{defi*}

\begin{theorem}\label{chap1:sec7:thm8}% 8
The measure defined by Theorem \ref{chap1:sec7:thm7} is complete.
\end{theorem}

\begin{proof}
If $X$ is a subset of a measurable set of measure $0$, then $\mu (X)= 0$,
$\mu (PX) = 0$, and  
\begin{align*}
 \mu(P) &\le \mu (PX) + \mu (P - PX) = \mu (P - PX) \le \mu (P),\\
&\mu (P) = \mu (P - PX) = \mu (P - PX) + \mu (PX),
\end{align*}
and so $X$ is measurable.

The measure defined in Theorem \ref{chap1:sec7:thm7} is not generally the minimal measure
generated by $\mu$, and the minimal measure is generally not
complete. However, any measure can be completed by adding to the
system of measurable sets $(X)$  the sets $X \cup N$ where $N$ is a subset
of a set of measure zero and defining $\mu (X \cup N) = \mu (X)$. This
is consistent with the original definition and gives us a measure
since countable unions of sets $X \cup N$ are sets of the same form,
$(X \cup N)'=X'\cap N'= X'\cap(Y' \cup N\cdot Y')$ (where $N \subset Y$,
$Y$ being measurable and of $0$ measure) $= X_1 \cup N_1$ is of the same
form and $\mu$ is clearly completely additive on this extended
system.

The\pageoriginale essential property of a measure is complete additivity or the
equivalent continuity  conditions of Theorem
\ref{chap1:sec5:thm2}(a). Thus, if $X_n \downarrow X$ or 
$X_n \uparrow X$, then $\mu (X_n) \rightarrow \mu (X)$, if $X_n
\downarrow 0$, $\mu (X_n) \rightarrow 0$ and if
$X=\sum\limits^{\infty}_{1} X_n$, $\mu (X)= \sum\limits^{\infty}_{1}
\mu (X_n)$.  In particular, the union of a sequence of sets of measure
zero also has measure zero. 
\end{proof}

\section{Classical Lebesgue and Stieltjes measures}\label{chap1:sec8}

The fundamental problem in measure theory is, as we have remarked
already, to prove the existence of a measure taking assigned values on
a given system of sets. The classical problem solved by Lebesgue is
that of defining a measure on sets of points on a line in such a way
that every interval is measurable and has measure equal to its
length. We consider this, and generalizations of it, in the light of
the preceding abstract theory. 

It is no more complicated to consider measures in Euclidean space
$R_K$ than in $R_1$. A set of points defined by inequalities of the
form 
$$ 
a_i \leq x_i < b_i (i = 1, 2, \ldots, k)
$$
will be called a {\em rectangle} and the union of a finite number of
rectangles, which we have called an {\em elementary figure}, will be
called simply a {\em figure}. It is easy to see that the system of
figures and complements of figures forms a finitely additive system in
$R_k$. The  {\em volume} of the rectangle defined above is defined to
be $\prod\limits^{k}_{i=1}(b_i -a_i)$. A figure can be decomposed
into disjoint rectangles in many different ways, but it is easy to
verify that the sum of the volumes of its  
components\pageoriginale remains the same, however, the decomposition is carried
out. It is sufficient to show that this is true when one rectangle is
decomposed to be $+ \infty$, it is easy to show by the same argument
that the volume function $\mu(I)$ is finitely additive on the system
$S_0$ of figures and their complements. 

\begin{theorem}\label{chap1:sec8:thm9}% 9
The function $\mu (I)$ (defined above) is {\em completely} additive in $S_0$.
\end{theorem}

\begin{proof}
  As in Theorem \ref{chap1:sec5:thm2}, it is sufficient to show that
  if $\{I_n\}$ is a 
  decreasing sequence of figures and $I_n \rightarrow 0$, then $\mu
  (I_n) \rightarrow 0$. If $\mu (I_n)$ does not $\rightarrow 0$, we can
  define $\delta > 0$ so that $\mu (I_n) \geq \delta$ for all $n$ and we
  can define a decreasing sequence of figures $H_n$ such that {\em
    closure} $\overline{H}_n$ of $H_n$ lies in $ I_n$, while  
  $$
  \mu (I_n - H_n) < \frac{\delta}{2}
  $$
  
  It follows that  $\mu (H_n)= \mu (I_n) - \mu (I_n - H_n)> \frac{\delta}{2}$
  so that $H_n$, and therefore $\overline{H}_n$, contains at least one
  point. But the intersection of a decreasing sequence of non-empty
  closed sets $(\overline{H}_n)$ is non-empty, and therefore the $H_n$
  and hence the $I_n$ have a common point, which is impossible since
  $I_n \downarrow 0$. 
\end{proof}

The measure now defined by Theorem \ref{chap1:sec7:thm7} is Lebesgue Measure.

\section{Borel sets and Borel measure}\label{chap1:sec9}

The sets of the {\em minimal} Borel system which contains all figures
are called Borel sets and the measure which is defined by Theorem
\ref{chap1:sec8:thm9}
and \ref{chap1:sec7:thm7} is called Borel measure when it is
restricted to these sets. The 
following results follow immediately. 

\begin{theorem}\label{chap1:sec9:thm10} % 10
  A\pageoriginale sequence of points in $R_K$ is Borel measurable and
  has measure 0. 
\end{theorem}

\begin{theorem}\label{chap1:sec9:thm11}% 11
  Open and closed sets in $R_K$ are Borel sets. 
\end{theorem}

(An open set is the sum of a sequence of rectangles, and a closed set
is the complement of an open set). 

\begin{theorem}\label{chap1:sec9:thm12}% 12
If $X$ is any (Lebesgue) measurable set, and $\epsilon>0$, we can find
an open set $G$ and a closed set $F$ such that
$$
F \subset X \subset G , \mu (G-P)< \in
$$

Moreover, we can find Borel sets $A$, $B$ so that
$$
A \subset X \subset B , \mu (B-A)=0.
$$

Conversely, any set $X$ for which either of these is true is measurable.
\end{theorem}

\begin{proof}
First suppose that $X$ is bounded, so that we can find a sequence of
rectangles $I_n$ so that 
$$
X \subset \bigcup^{\infty}_{n=1} I_n, \sum^{\infty}_{n=1} \mu (I_n)<
\mu (X) + \epsilon/4. 
$$

Each rectangle $I_n$ can be enclosed in an open rectangle (that is, a
point set defined by inequalities of the from $a_i < x_i < b_i,
i=1,2,\ldots,k$, its measure is defined to be
$\prod\limits^{k}_{i=1}(b_i - a_i)Q_n$ of measure not
greater than  $\mu (I_n) + \dfrac{\epsilon}{2^n +2}$. 
\end{proof}

Then
$$
X \subset Q = \bigcup^{\infty}_{n=1} Q_n, \mu (Q)
\leq \sum^{\infty}_{n=1} \mu (Q_n) \leq
\sum^{\infty}_{n=1} \mu (I_n) + \epsilon
\sum^{\infty}_{n=1} \frac{1}{2^n + 2} \leq \mu (X)
+ \frac{\epsilon}{2}
$$ 

Then $Q$ is open and $\mu (Q -X)\leq  \epsilon/2$.

Now\pageoriginale any set $X$ is the sum of a sequence of \textit{bounded} sets $X_n$ (which
are measurable if $X$ is), and we can apply this each $X_n$ with
$6/2^{n+1}$ instead of $\in$. Then  
$$ 
X = \sum^{\infty}_{n=1} X_n, X_n \subset Q_n , \sum^{\infty}_{n=1} Q_n =G,
$$ 
where $G$ is open and  
\begin{gather*}
  G- X \subset \bigcup^{\infty}_{n=1} (Q_n - X_n),
  \mu (G -X) \leq \sum^{\infty}_{n=1} \mu (Q_n -
  X_n)\leq \sum^{\infty}_{n=1} \frac{\in}{2^n + 1}=\frac{\in}{2}
\end{gather*}

The closed set $F$ is found by repeating the argument on $X$ and complementing.

Finally, if we set $\in_n \downarrow 0$ and $G_n , F_n$ are open and
closed respectively, 
$$ 
F_n \subset X \subset G_n ,\; \mu (G_n - F_n) < \in_n 
$$
and we put
$$ 
A= \bigcup^{\infty}_{n=1} F_n,\; B= \bigcup^{\infty}_{n=1} G_n, 
$$
we see that 
$$ 
A \subset X \subset B, \mu (B -A) \leq \mu (G_n - F_n) \leq \in_n
\text{for all n},
$$
and so
$$ 
\mu (B- A) =0,
$$
while $A$, $B$ are obviously Borel sets.

Conversely, if $\mu (P) < \infty$ and
$$ 
F \subset X \subset G,
$$
We\pageoriginale have, since a closed set is measurable, 

$
\begin{aligned}
\mu (P) & = \mu (PF) + \mu (P-PF)\\
        & \geq \mu (PX) - \mu (P(X-F)) + \mu (P-PX)\\
        & \geq \mu (PX) +\mu (P - PX) - \mu (X - F)\\
        & \geq \mu (PX) + \mu (P-PX) - \mu (G - F)\\
        & \geq \mu (PX) + \mu (P -PX) -\in\\
\end{aligned}
$ 

true for every $\epsilon > 0$ and therefore
$$
\mu (P) \geq \mu (PX) + \mu (P - PX)
$$
so that X is measurable.

In the second case, $X$ is the sum of $A$ and a subset of $B$ contained in a
Borel set of measure zero and is therefore Lebesgue measurable by the
completeness of Lebesgue measure. 

It is possible to defined measures on the Borel sets in $R_k$ in
which the measure of a rectangle is not equal to its volume. All that
is necessary is that they should be completely additive on
figures. Measures of this  kind are usually called positive \textit{Stiltjes
measures} in $R_k$ and Theorems \ref{chap1:sec9:thm11} and
\ref{chap1:sec9:thm12} remain valid for them but  
\textit{Theorem \ref{chap1:sec9:thm10} 
does not}. For example, a single point may have positive Stieltjes measure.

A particularly important case is $k=1$, when a Stieltjes measure can be
defined on the real line by any monotonic increasing function
$\Psi(X)$. The figures I are finite sums of intervals  $a_i \leq x <
b_i $ and $\mu (I)$ is defined by 
$$ 
\mu (I) = \underset{i}{\sum} \{\Psi (b_i -0) - \Psi (a_i - 0)\}.
$$

The\pageoriginale proof of Theorem 9 in this case is still valid. We observe that
since $\underset{\beta \rightarrow b-0} {lim} $ 
 $\Psi (\beta)= \Psi (b-0),$ it is possible to choose $\beta $ so
that  $\beta <b $ and $\Psi (\beta-0) - \Psi (a-0)>\frac 1 2, [\Psi
  (b-0)-\Psi (a-0)]$. 

The set function $ \mu$ can be defined in this way even if $\Psi (x)$
is not monotonic. If $\mu$ is bounded, we say that $\psi(x)$ is of
\textit{bounded variation}. 
 In this case, the argument of Theorem \ref{chap1:sec8:thm9} can still be used to prove
 that $\mu $ is completely additive on figures. After the remark  on
 corollary \ref{chap1:sec5:coro2} of  
Theorem \ref{chap1:sec5:thm3}, we see that it can be expressed as the difference of two
completely additive, non-negative functions $\mu^+,-\mu^-$ defined on  
figures. These can be extended to a Borel system of sets $X$, and the
set function $\mu=\mu^++\mu^-$ gives a set function associated with
$\Psi (x)$. We can also write $\Psi (x)=\Psi^+(x)+\Psi^-(x)$ where
$\Psi^+$(x) increases, $\Psi $ (x) decreases and both are bounded if
$\Psi (x)$ has bounded variation.
 
A non-decreasing function $\Psi (x)$ for which $\Psi (-\infty)=0, \Psi
(\infty)=1$ 
 is called a \textit{distribution function}, and is of basic importance in
 probability.
 
\section {Measurable functions}\label{chap1:sec10} %%% 10
 
 A function $f(x)$ defined in $ \mathfrak{X} $ and taking real values
 is called \textit{measurable with respect to a  
 measure $\mu$} if $\varepsilonup[f(x)\geq k](\varepsilonup[P(x)]$
 is the set of points $x$ in $\mathfrak{X}$ for which $P(x)$ is true)  is
 measurable with respect to $\mu$ for every real $k$.
 
\begin{theorem}\label{chap1:sec10:thm13}  %%% 13
The memorability condition
\end{theorem}

\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item  $\varepsilonup[f(x)\geq k]$ is measurable for all real $k$ is equivalent
to\pageoriginale each one of 
 \item $\varepsilonup[f(x)> k]$ is measurable for all real $k$,
  \item $\varepsilonup[f(x)\leq k]$ is measurable for all real $k$,     
  \item $\varepsilonup[f(x)< k]$ is measurable for all real $k$,
 \end{enumerate}

 \begin{proof}
 Since
 $$
\varepsilonup[f(x)\geq k] =\bigcap^{\infty}_{n=1}
\varepsilonup\left[f(x)>k-\frac{1}{n}\right],
$$
(ii) implies (i). Also 

$$
\varepsilonup[f(x)\geq k] =\bigcup^{\infty}_{n=1} 
\varepsilonup \left[f(x)\geq k+\frac{1}{n}\right],
$$
and so (i)implies (ii). This proves the theorem since (i) is equivalent
  with (iv) and (ii) with (iii) because the corresponding sets are complements.
 \end{proof}

\begin{theorem}\label{chap1:sec10:thm14} 
  The function which is constant in $\mathfrak{X}$ is  measurable. If
  $f$ and $g$ are measurable, so are $f \pm g$ and $f \cdot g$.
\end{theorem}
  
\begin{proof}
     The first is obvious. To prove the second , suppose $f$, $g$ are
     measurable. Then 
     \begin{align*}
       \varepsilonup[f(x) +g(x)> k] &= \varepsilonup[f(x) > k-g(x)]\\
       &=\cup  \varepsilonup[f(x) >r> k-g(x)]\\
       &= \bigcup^{r}_{r} \varepsilonup[f(x) >r] \cap
       \varepsilonup[ g(x)>k-r]
     \end{align*}  
     the union being over all rationals $r$. This is a countable union of
     measurable sets so that $f+g$ is measurable. Similarly $f-g$ is
     measurable. Finally 
     $$
     \varepsilonup[f(x))^2>k]=\varepsilonup[f(x)>\sqrt{k}]+
     \varepsilonup[f(x) < -\sqrt{k}]~\text{for}~k\geq0 
     $$
     so\pageoriginale that $f^2$ is measurable. Since
     $$
     f(x)g(x)= \frac{1}{4}(f(x) +g(x))^2 -\frac{1}{4} (f(x)-g(x))^2
     $$
     $f\cdot g$ is measurable.
\end{proof}

\begin{theorem}\label{chap1:sec10:thm15} 
  If $f_n$ measurable for $n=1,2,\ldots$ then so are $\lim \sup f_n$,
  $\lim$ $\inf$  $f_n$.
\end{theorem}

\begin{proof}
  $\epsilon [\lim \sup  f_n(x) < k]$
  \begin{align*}
    &=\epsilon[f_n(x)< k ~\text{for all sufficiently large $n$}]\\
    &=\bigcup^{\infty}_{N=1} \bigcap^{\infty}_{n=N} \epsilon[f_n(x)< k] 
  \end{align*}
  is measurable for all real $k$. Similarly  $\lim \inf f_n$ is measurable.
\end{proof}

In $R_n$, a function for which $\epsilon[f(x)\geq k]$ is Borel
measurable for all $k$ is called a \textit{Borel measurable function} or a \textit{Baire
function}.
 
\begin{theorem}\label{chap1:sec10:thm16} 
In $R_n$, a continuous function is Borel measurable.
\end{theorem}
\begin{proof} 
The set $\epsilon[f(x)\geq k]$ is closed.
\end{proof}

\begin{theorem}\label{chap1:sec10:thm17} 
 A Baire function of a measurable function is measurable.
\end{theorem}

\begin{proof}
  The Baire functions form the smallest class which contains continuous
  functions and is closed under limit operations. 
  Since the class of measurable functions is closed under limit
  operations, it is sufficient to prove that a continuous function of a
  measurable, 
  function is measurable. Then if $\varphi (u)$ is continuous and $f(x)$
  measurable, $\epsilon [\varphi(f(x))> k]$ is the set of $x$ for which
  $f(x)$ lies in an open 
  Set,\pageoriginale namely the open set of points for which $\varphi(u)>k.$ Since an
  open set is a countable union of open intervals, this set is
  measurable, thus proving the theorem. 
\end{proof}

\begin{theorem}[Egoroff]\label{chap1:sec10:thm18} 
  If $\mu(X)< \infty$ and $f_n(x)\rightarrow f(x) \neq \pm
  \infty$   p.p in $X$, and if $\delta>0$, then we can find a subset
  $X_\circ$ of $X$ such that $\mu(X-X_\circ) <\delta$ and $f_n(x)
  \rightarrow f(x)$ uniformly in $X_\circ$. 
\end{theorem}

We write p.p for ``almost everywhere'', that is, everywhere expect for
a set of measure zero. 
\begin{proof}
  We may plainly neglect the set of zero measure in which
  $f_n(X)$ dose not converge to a finite limit. Let 
  $$
  X_{N,\nu} =\epsilon[\mid f(x)-f_n(x)\mid< 1/\nu ~\text{for all}~
    \geq N].
  $$
  Then, for fixed $\nu$,
  $$
  X_{N,\nu} \uparrow X ~\text{as}~ N \rightarrow \infty
  $$
  
  For each $\nu$ we choose $N_\nu$ os that $X_\nu =X_{N_\nu, \nu}$ satisfies 
  $$
  \displaylines{\hfill
  \mu(X-X_\nu) < \delta/2^\nu, \hfill \cr
  \text{and let}\hfill  
  X_\circ = \bigcap^{\infty}_{\nu=1} X_\nu\hfill }
  $$
  
  Then
  $$ 
  \displaylines{\hfill 
  \mu(X-X_\circ)\leq \sum^{\infty}_{\nu=1} \mu(X-X_\nu)< \delta\hfill \cr
  \text{and }\hfill
  \mid f(x)-f_n(x)\mid< 1/\nu \text{ for } n  \geq N_\nu\hfill }
  $$
  if $x$ is in $x_\nu$ and therefore if $X$ is in $X_\circ$. This proves
  the theorem. 
\end{proof}

\section{The Lebesgue integral}\label{chap1:sec11} 

Suppose\pageoriginale that $f(x) \geq 0$, $f$ is measurable in $X$, and let 
$$
  \displaylines{\hfill 
    0=y_\circ < y_1 < y_2 \dots <y_\nu \rightarrow \infty\hfill \cr
    \text{and}\hfill 
    E_\nu= \epsilon\left[y_\nu \geq f(x)< y_{\nu+1}\right], \nu
    =0,1,2, \ldots \hfill }
    $$
so that $E$ is measurable and $x=\sum^{\infty}_{\nu=0} E_\nu$.

We call the set of the $y_\nu , \{ y_\nu \}$ subdivision.

Let
$$
S=S\{y\} =\sum^{\infty}_{\nu=1} y_\nu \mu(E_\nu).
$$

Then we define $\sup S$ for all subdivisions $\{ y_\nu \}$ to be the
\textit{Lebesgue Integral} of $f(x)$ over $X$, and write it $ \int f(X)d \mu $. We
say that $f(x)$ is \textit{integrable} or \textit{summable} if its integral is finite. 
It is obvious that changes in the values of $f$ in a null set (set of
measure 0) have no effect on the integral. 

\begin{theorem}\label{chap1:sec11:thm19} 
  Let $\{y_\nu^k \}, k = 1,2, \dots$ , be a sequence  of subdivisions
  whose maximum intervals 
  $$
  \delta_k = \sup  (y_{\nu+1}^{k} - y_\nu^{k}) \rightarrow 0
  ~\text { as }~ k  \rightarrow \infty
  $$ 
  Then, if $S_k$ is the sum corresponding to $\{y_\nu^k \}$,
  $$
  \lim_{k \rightarrow \infty} S_k = {\int\limits_{\mathfrak{X}}}f(x) d \mu =
  F(\mathfrak{X}).
  $$
\end{theorem}

\begin{coro*} 
  Since $S_k$ is the integral of the function taking
  constant values $y_\nu^k$ in the sets $E_\nu^k$, it follows, by
  leaving out suitable remainders
  $\sum^{\infty}_{\nu=v+1} y_\nu^k \mu(E_\nu^k)$,
  that $F(\mathfrak{X})$ is the limit of the integrals of simple functions, a
  \textit{simple function} being a function taking\pageoriginale  constant values  on each of
  a finite  number of measurable sets whose union is  $\mathfrak{X}$. 
\end{coro*}

\begin{proof}
  If $A <F (\mathfrak{X})$, we can choose  a subdivision
  $\{y'_{\nu}\}$   so that if  $E_{\nu} $  are the corresponding sets,
  $S'$ the corresponding sum,
  \begin{equation*}
    S'\geq \sum^{V}_{\nu=1}  y'_{\nu}    \mu (E'_{\nu})
  \end{equation*}
  for a finite $V$. One of the $\mu  (E'_{\nu}) $  can be infinite only
  if $ F(x) = \infty $ and then there is nothing  to prove.  
  Otherwise,  $ \mu (E'_{\nu}) < \infty $  and we let $\{ y_{\nu} \}$
  be  a subdivision  with  $\delta = \sup  (y{_{\nu +1}}-y_{\nu}) $ 
  and denote by $S''$ the sum defined for $\{ y_{\nu} \} $  and by $S$ the
  sum defined for the  subdivision  consisting of points  
  $ y_{\nu}  $  and $ y'_{\nu} $. Since  $S'$  is not decreased  by
  insertion of extra points of sub-division, 
  $$
  \displaylines{\hfill
   S''  \geq S' \geq \sum^{V}_{\nu=1}  y'_{\nu}  \mu  (E'_{\nu})  >
   A,   \hfill \cr
   \text{while}\hfill
    S'' -  S \leq   \delta \sum^{V}_{1}  \mu (E'_{\nu})\hfill }
  $$
  and, by making $\delta$ small enough we get $S>A$. Since   $S \leq  F
  (\mathfrak{X}) $ and  $ A < F (\mathfrak{X})$ is arbitrary, this
  proves the theorem. 
\end{proof}

The definition can be extended  to integrals over subsets  $X$  of by defining
$$
F (X)   = \int\limits_{X} f (x) d \mu =
\int_{\mathfrak{X}}  f_{x} (X) d_\mu  
$$
where $f_{X} (x) = f(x)$ for $x$ in $x$ and  $ f_{X}  (x) =0 $  for $x$
in  $ \mathfrak{X} -X $. We may therefore  always  assume  
(when it is  convenient)  that integrals are over the whole space
$\mathfrak{X}$. 

The\pageoriginale condition  $ f(X) \ge  0 $  can easily be removed.

We define
\begin{align*}
  f^{+} (x) & = f(x)  ~\text{when}~  f(x) \geq  0, f^{+} (x) = 0
  ~\text{ when }~ f(x) \leq  0,\\ 
  f^{-} (x)& = f(x)  ~\text{when}~ f (x) \leq   0, f^{-} (x) = 0
  ~\text{when}~ f(x) \geq  0.
\end{align*}

Then $f(x) =  f^{+}(x) + f^{-} (x), \mid f(x) \mid  =  f^{+}(x) -(x)$.  

We define
\begin{equation*}
\int\limits_X  f(x) d \mu = \int\limits_{X} f^{+} (x)d \mu   -
\int\limits_{X} (-f^{-} (x)) d \mu 
\end{equation*}
when both the integrals  on the right are finite, so that $f(x)$ is
integrable if and only if $\mid f(x) \mid $ is integrable. 

In general, we use the integral sign only when the integrand is
integrable  in this absolute sense. The only exception to this  
rule is that we may sometimes write
$\int\limits_{X} f(x)d \mu    =  \infty$  when $f(x) \geq - r(x)$  and
$r(x)$ is integrable.  

\begin{theorem}\label{chap1:sec11:thm20} 
  If $f(x)$ is integrable on  $\mathfrak{X}, $  then 
\begin{equation*}
   F (X) =  \int\limits_{X}  f(x) d \mu 
\end{equation*}            
 is defined for every measurable subset  $X$  of $\mathfrak{X}$  and is
 completely additive  on these sets. 
\end{theorem}

\begin{coro*}  
  If $f(x) \geq  0$,  then $F(Y) \leq   F (X)  $   if  $Y \subset X$ 
\end{coro*}

\begin{proof}
  It is sufficient to prove  the  theorem in the case   $ f(X) \geq 0 $.  
  Let  $ X= \displaystyle \sum^{\infty}_{n=1} X_n$ where are $X_n$ are
  measurable and disjoint. Then, if $\{y_\nu\}$ is a subdivision, $E_\nu =
  \sum\limits^{\infty}_{n=1} E_{\nu} X_{n}, \mu
  (E_{\nu})=  \sum\limits^{\infty}_{n=1} \mu \left(E_{\nu} X_{n}\right)$ 
  and 
  \begin{align*}
    S = \sum^{\infty}_{\nu=1} y_{\nu}  \mu (E_{\nu}) & =
    \sum^{\infty}_{\nu=1}  y_{\nu}   \sum^{\infty}_{n=1} \mu (E_{\nu}
    X_{n}) \\ 
    & = \sum^{\infty}_{n=1}   \sum^{\infty}_{\nu=1}   y_{\nu}  \mu
    \left(E_{\nu} X_{n}\right) \\
    & = \sum^{\infty}_{n=1}  S_{n} \\
  \end{align*}  
  where\pageoriginale  $ S_{n}$ is the sum for  $f(x)$ over $ X_{n} $.  Since  $S$ and
$S_{n} $ (which are $\ge 0 $)  tend to  $F(X)$  and $ F(X_{N})$  
respectively as the maximum interval of subdivision tends  to 0, we get
$$ 
F(X) = \int\limits_{X}  f(x)d \mu = \sum^{\infty}_{n=1} F(X_{n}).
$$ 
\end{proof}

\begin{theorem}\label{chap1:sec11:thm21} 
  If a is a constant,
  $$ 
  \int\limits_{X} a f(x) d \mu =a  \int\limits_{X} f(x) d \mu 
  $$
\end{theorem}

\begin{proof}
  We may again suppose that $ f(x) \ge  0 $ and that  $a
  > $ 0. If we use the subdivision  $ \{ y_{\nu}\} $ 
  for  $f(x)$ and $ \{ ay_{\nu} \} $ for af $(x)$, the sets  $E_\nu$  are the
  same in each case, and the proof is trivial.  
\end{proof}

\begin{theorem}\label{chap1:sec11:thm22} 
If   A $ \leq   f(x) \leq  B ~\text{in}~ X$, then 
 $$
 A \mu (X) \leq  F(X) \leq   B  \mu  (X).  
 $$
\end{theorem}

\begin{theorem}\label{chap1:sec11:thm23} 
If $f(x) \ge   g(x)$ in $X$, then 
$$
  \int\limits_{X} f(x) d \mu \geq  \int\limits_{X} g(x) d \mu 
$$
\end{theorem}

\begin{coro*} 
  If $ \mid  f(x) \mid \leq g(x)$ and $g(x)$ is
  integrable, then so is $f(x)$. 
\end{coro*}

\begin{theorem}\label{chap1:sec11:thm24} 
If  $f(x) \geq 0 $ and  $\int \limits_{X} f(x) d \mu  = 0$, then
$ f(x) =0$ p.p. in $X$. 
\end{theorem}

\begin{proof}
  If\pageoriginale this were not so, then 
  $$
  \in[ f(x)  >  0 ]= \sum_{n=0}^\infty \in \left[\frac{1}{n+1} \leq f(x) <
    \frac{1}{n} \right]
  $$
  has positive measure, and hence, so has at least one subset
  $E_n =\in \left[\frac{1}{n+1} \leq f(x) < \frac{1}{n} \right]$ Then
  $$			
  \int \limits_{x} f(x)d \geq \int \limits_{E_n} f(x)d \mu \geq
  \frac{\mu(E_n)}{n+1} >0  
  $$
  which is impossible.
\end{proof}

\setcounter{corollary}{0}
\begin{corollary}\label{chap1:sec11:coro1} 
  If $\int\limits_K  f(x)d \mu = 0 $ for all $X\subset
  \mathfrak{X}, f(x)$ not necessarily of the same sign, then $f(x) = 0$
  p.p. 
  
  we have merely to apply Theorem \ref{chap1:sec11:thm24} to $X_1 =
  \in[ f(x)  \geq 0 ]$ and to $X_2 = \in [ f(x)  <  0]$. 
\end{corollary}

\begin{corollary}\label{chap1:sec11:coro2} 
  If $\int\limits_{X} f(x)d \mu = \int\limits_{x} g(x)d\mu$ for all
  $X\subset \mathfrak{X}$ , then $f(x) = g(x)$ p.p. If $f(x) = g(x)$ p.p. we
  say that f and g are equivalent. 
\end{corollary}

\section{Absolute Continuity}\label{chap1:sec12} 
				
A completely additive set function $F(x)$ defined on a Borel system is
said to be \textit{absolutely continuous} with respect to a measure
$\mu$ on the same
system if $F(X)\rightarrow o$ uniformly in $X$ as
$\mu(X)\rightarrow 0$. In other words, if $\in > 0$, we can find
$\delta > 0$ so that $|F(X)| < \in$ for all sets $X$ which satisfy
$\mu(X)< \delta$. In particular, if $F(X)$ is defined in $R$ by a point
function $F(x)$ of bounded variation, then it is absolutely continuous,
if given $\epsilon > o $ we can find $\delta > 0 $ so that  
$$
\sum_{i=1}^n\Big| F(b_i)- F(a_i) \Big| \leq \in
\text{if}  \sum_{i-1}^n (b_i-a_1) < \delta
$$\pageoriginale
					
Moreover, it is clear from the proof of Theorem \ref{chap1:sec5:thm3} that a set function
$F(X)$ is absolutely continuous if and only if its components
$F^{+}(X)$, $F^{-}(X)$ are both absolutely continuous. An absolutely
continuous point function $F(x)$ can be expressed as the difference of
two absolutely continuous non-decreasing functions as we see by
applying the method used on page 22 to decompose a function of bounded
variation into two monotonic functions. We observe that the concept of
absolute continuity does not involve any topological assumptions on
$X$. 

\begin{theorem}\label{chap1:sec12:thm25} 
  If $f(x)$ is integrable on $X$, then 
  $$
  F(X) = \int \limits_{x} f(x)d \mu
  $$
  is absolutely continuous.
\end{theorem}

\begin{proof}
  We may suppose that $f(x)\geq 0$. If $\in > 0$, we choose a
  subdivision \{ y$_\nu$ \}so that 
  $$
  \sum^{\infty}_{\nu=1} y_\nu \mu (E_\nu) > F(\mathfrak{X}) - \in/4
  $$ 
  and then choose $V$ so that  					
  $$
  \sum^{V}_{\nu = 1} y_{\nu} \mu (E_\nu) > F(\mathfrak{X}) - \in/2
  $$
  
  Then, if $A > y_{v+1}$ and $E_A = \varepsilon[ f(x) \geq A]$
   $$ 
  \displaylines{\text{we have}\hfill   
  E_\nu \subset \mathfrak{X} - E_A  {\text{for}}\nu \leq V.\hfill\cr
  \text{Now}\hfill 
  F(\mathfrak{X} - E_A) \geq \sum_{\nu = 1}^{V} \int_{E_V} f(x) d \mu
  \geq \sum_{\nu =1 }^{V} y_\nu \mu (E_\nu)\hfill \cr
  \hfill  > F(\mathfrak{X}) - \in/2\hfill \cr
  \text{and therefore,}\hfill 
  F(E_A) < \in/2.\hspace{2cm}\hfill }
  $$\pageoriginale

If $X$ is any measurable set,
\begin{multline*} 
\qquad F(x) = F(XE_A) + F(X-E_A)\\
 < \frac{\in}{2} + A \mu (X) (\text{since} f(x) \leq A ~\text{in}~ \mathfrak{X}
 - E_A)\qquad 
\end{multline*}
provided that $\mu (X) \leq \in /2A = \delta$
\end{proof}
					
\begin{theorem}\label{chap1:sec12:thm26} 
  If $f(x)$ is integrable on $X$ and $X_n \uparrow X$, then 
  $$ 
  F(X_n)\rightarrow F(X).
  $$
\end{theorem}

\begin{proof}
  If $\mu(X)< \infty $ this follows from Theorem \ref{chap1:sec12:thm25} and the continuity
  of $\mu $ in the sense of Theorem \ref{chap1:sec5:thm2}. If $\mu (X) = \infty, \in > o$ we
  can choose a subdivision \{ $y_\nu$ \} and corresponding subsets
  $E_\nu$ of $X$ so that 
  $$ 
  \sum_{\nu = 1}^{\infty} y_\nu \mu (E_\nu) > F(X) - \in
  $$
  (assuming that $f(x)$ $\geq 0$, as we may) 
  
  But 
  $$ 
  F(X_n) =  \sum_{\nu = 1}^{\infty} F(X_n E_\nu)
  $$ 
  and $F(X_n E_\nu) \rightarrow F(E_\nu)$ as $n \rightarrow \infty$ for
  every $\nu $, since $\mu (E_\nu) < \infty$. Since all the terms
  $y_\nu  F(X_n E_\nu)$ are positive, it follows\pageoriginale that
  $$
  \lim\limits_{n\to \infty}F(X_n)= \sum^{\infty}_{\nu =1} \text{F}(E_{\nu})\geq 
  \sum^{\infty}_{\nu =1} y_{\nu}\mu (E_{\nu}) > \text{F(X)} - \in   
  $$
  
  Since $F(X_n)\leq F(X)$, the theorem follows.
\end{proof}

\begin{theorem}\label{chap1:sec12:thm27} 
  If $f(x)$ is integrable on $X$ and $\in > 0$, we can find a subset
  $X_1$ of $X$ so that $\mu (X_1) < \infty$, $\int_{X-X_{1}}\mid
  \text{f(x)}\mid d \mu < \in$ and $f(x)$ is bounded in $X_1$. 
\end{theorem}

\begin{proof}
  The theorem follows at once from Theorems \ref{chap1:sec12:thm25}
  and \ref{chap1:sec12:thm26}   since we can take
  $ X_1 \subset \in [f(x) \geq y_1]$ and this set has finite measure
  since f(x) is integrable. 
\end{proof}

\begin{theorem}\label{chap1:sec12:thm28} 
  If $f(x)$ and $g(x)$ are integrable on $\mathfrak{X}$, so is $f(x) + g(x)$ and
  $$
  \int\limits_{\mathfrak{X} }[f(x)+g(x)]\text {d}\mu  = 
  \int\limits_{\mathfrak{X}}f(x)d \mu + 
  \int\limits_{\mathfrak{X}}g(x)\text {d}\mu.  
  $$
\end{theorem}

\begin{proof}
  Since $\mid f(x) + g(x)\mid\leq\mid f(x)\mid + \mid g(x) \mid \leq 2
~\text{sup}(\mid f(x)\mid,\mid g(x)\mid) $ 

we have 
\begin{multline*}
\int\limits_{\mathfrak{X}}\mid f(x) + g(x)\mid d\mu 
\leq 2 \int\limits_{\mathfrak{X}} \sup(\mid f(x)\mid , \mid g(x)\mid)
d \mu\\
= 2\left[~\int\limits_{\mid f \mid \geq \mid g \mid}\mid f(x) \mid 
  d\mu +\int \limits_{\mid f \mid < \mid g \mid}\mid g(x) \mid 
  d\mu \leq 2 \right.\\
  \left.\int \limits_{\mathfrak{X}} \mid f(x)\mid d \mu +
  2 \int\limits_{\mathfrak{X}}\mid g(x)\mid d \mu\right]
\end{multline*}
so that $f(x) + g(x)$ is integrable. After Theorem \ref{chap1:sec12:thm27}, there is no loss
of generality in supposing that $\mu (\mathfrak{X})< \infty$. Moreover,
by subdividing $\mathfrak{X}$ into the sets (not more than 8) in which
$f(x)$, $g(x),f(x)+g(x)$ have constant signs, the theorem can be reduced to
the case in which  
$f(x)\geq 0$, $g(x) \geq 0$ and so $f(x) + g(x) \geq 0$ in $\mathfrak{X}$.
\end{proof}

The\pageoriginale conclusion is obvious if $f(x)$ is a constant $c \geq 0$, for we can
then take as subdivisions, $\{ y_{\nu}\}$ for $g(x)$ and $\{
y_{\nu}+c\}$ for $g(x) + c$. In the general case, if 
\begin{equation*}
  \begin{aligned}
    E_{\nu} &= \varepsilon[y_{\nu}\leq g(x)<y_{\nu+1}]\\
    \int\limits_{\mathfrak{X}}[f(x) + g(x)] d \mu & = \sum^{\infty}_{\nu=0}
    \int\limits_{E_{\nu}}[f(x)+g(x)]d \mu , \text{by
      Theorem \ref{chap1:sec11:thm20}}\\   
    \geq \sum^{\infty}_{v=0} & \quad \int\limits_{E_{\nu}}f(x) d \mu +
    \sum^{\infty}_{\nu = 1} y_{\nu}\mu(E_{\nu}\\ 
    &=\int\limits_{\mathfrak{X}} f(x) d \mu + S,
  \end{aligned}
\end{equation*} 
and since $\int \limits_{\mathfrak{X}} g(x) d \mu$ is sup s for
all subdivisions $\{ y_{\nu}\}$, we get 
$$
\int\limits_{\mathfrak{X}}[f(x) + g(x)]d  \mu \geq \int
\limits_{\mathfrak{X}}f(x)d \mu + \int \limits_{\mathfrak{X}}
g(x) d \mu 
$$ 

On the other hand, if $\in >0$, and we consider subdivisions for which
$$
y_1 \leq \in , y_{\nu+1} \leq (1+\in)y_{\nu}~ \text{for}~\nu \geq ,1
$$
we get

\begin{equation*}
  \begin{aligned}
    \int\limits_{\mathfrak{X}}[f(x)+g(x)] d \mu &\leq 
    \sum^{\infty}_{\nu=0..} \int\limits_{E_{\nu}} f(x) d\mu +
    \sum^{\infty}_{\nu=0} y_{\nu+1}\mu(E_{\nu})\\ 
    &\leq \int\limits_{\mathfrak{X}} f(x) d \mu + (1+\in)S+y_1 \mu (E_o)\\
    &\leq \int\limits_{\mathfrak{X}} f(x) d\mu + (1+\in)
    \int\limits_{\mathfrak{X}} g(x) d \mu + \in\mu (\mathfrak{X})\\ 
  \end{aligned}
\end{equation*}
and the conclusion follows if we let $\in \rightarrow 0$.

Combining this result with Theorem \ref{chap1:sec11:thm21}, we get

\begin{theorem}\label{chap1:sec12:thm29} 
  The integrable functions on $\mathfrak{X}$ form a linear space over $R$
  on which $\int\limits_{\mathfrak{X}} f(x)d\mu$ is a linear functional. 
\end{theorem}

This\pageoriginale space is denoted by $L(\mathfrak{X})$, and $f(x) \varepsilon
L(\mathfrak{X}$) means that $f(x)$ is (absolutely) integrable on
$\mathfrak{X}$. 

\section{Convergence theorems}\label{chap1:sec13} 

\begin{theorem}[Fatou's Lemma]\label{chap1:sec13:thm30}  %%% 30 
  If $\gamma $(x) is integrable on $\mathfrak{X}$, and
  $f_n(x)$, $n=1,...$are measurable functions, then 
  \begin{gather*}
    \lim \sup \int\limits_{\mathfrak{X}} f_n(x) d\mu \leq
    \int\limits_{\mathfrak{X}}(\lim \sup f_n(x))d \mu ~\text{if}~
    f_n(x)\leq \gamma (x),\\ 
    \lim \inf \int\limits_{\mathfrak{X}} f_n(x)d \mu \geq
    \int\limits_{\mathfrak{X}}(\lim \inf f_n(x))d \mu ~\text{if}~
    f_n(x)\geq -\gamma (x),
  \end{gather*}
\end{theorem}
As immediate corollaries we have


\begin{theorem}[Lebesgue's theorem on dominated
    convergence]\label{chap1:sec13:thm31}   %%% 31
  If $\gamma (x)$ is integrable on $\mathfrak{X}, ~\mid f_n(x)\mid \leq
  \gamma (x)$ and
  $$
  \displaylines{\hfill 
    f_n(x)\rightarrow f(x) ~\text{p.p. in}~ \mathfrak{X}\hfill \cr
    \text{then}\hfill 
    \int\limits_{\mathfrak{X}} f_n(x)d \mu \rightarrow
    \int\limits_{\mathfrak{X}} f(x)d \mu\hfill }
  $$ 
  
  In particular, the conclusion holds if $\mu (\mathfrak{X})< \infty$
  and the $f_n(x)$ are uniformly bounded. 
\end{theorem}

\begin{theorem}[Monotone convergence theorem]\label{chap1:sec13:thm32} %%% 32
  If $\gamma  $(x) is integrable on $\mathfrak{X}$, $f_n(x)\geq - \gamma
  (x)$ and $f_n(x)$ is an increasing sequence for each $x$, with limit
  $f(x)$ then 
  $$
  \lim\limits_{n \rightarrow \infty} \int\limits_{\mathfrak{X}}
  f_n(x)d \mu = \int\limits_{\mathfrak{X}} f(x)d \mu
  $$
  in the sense that  if either side is finite, then so is the other and
  the two values are the same, and if one side is $+ \infty$, so is the
  other. 
\end{theorem}

\noindent\textit{\underline{Proof of Fatou's lemma}}\pageoriginale

The two cases in the theorem are similar. It is sufficient to prove
the second, and since $f_n(x)+\gamma(x)\geq 0$,  there is no loss of
generality in supposing that $\gamma(x)=0$, $f_n(x)\geq 0$,

Let $f(x)=\lim \inf f_n(x)$ and suppose that $\int _{\mathfrak{X}}
f(x)d\mu <{\infty}$. Then after Theorem \ref{chap1:sec12:thm27}, given $\in >0$ we can
define $X_1$ so that $\mu(X_1)<{\infty}$ and  
$\in >\int_ {{\mathfrak{X}}{-X_1}} f(x) d\mu$ while $f(x)$ is bounded in $X_1$.

A straight-forward modification of Egoroff's theorem to gether with theorem 
\ref{chap1:sec12:thm25} shows that we can find a set $X_2\subset X_1$ so that 
$$
\displaylines{\hfill 
  \int\limits_{X_{1}-X_{2}} f(X) d\mu<\in \hfill \cr
  \text{while }\hfill
  f_n(x)\geq f(x)-\in/\mu(X_1)\hfill }
$$
for all $x$ in $X_2$ and $n \geq $ N. Then
\begin{gather*}
  \int\limits _{\mathfrak{X}} f_n(x)d \mu \geq \int _{X_2}f_n(x)d \mu \geq
  \int_{X_2} f(x) d \mu-\in\\
  \geq \int\limits_{\mathfrak{X}}xf(x)d\mu-3\in ~\text{for}~n\geq N 
\end{gather*}
and our conclusion follows. 

If
$$
\int\limits_{\mathfrak{X}}f(x)d\mu=\infty
$$
it follows from the definition of the integral that $A > 0$, we can
define $\varphi(x) \in L(\mathfrak{X})$ so that  
$$
\int\limits_{\mathfrak{X}}\varphi(x) d\mu \geq A,\quad 0\leq  \varphi (x)
\leq  f(x)
$$

The\pageoriginale argument used above now shows that 
$$
\int\limits_{\mathfrak{X}}f_n(x) d\mu\geq \int\limits_{\mathfrak{X}}\varphi (x) d
\mu-3 \in \geq A-3 \in
$$ 
for sufficiently large $n$, and hence
$$ 
\lim\inf~\int\limits_{\mathfrak{X}}f_n(x)d\mu=\infty.
$$

Restatement of Theorems \ref{chap1:sec13:thm31} and
\ref{chap1:sec13:thm32} in terms of series, rather than 
sequences given us  
\begin{theorem}\label{chap1:sec13:thm33} 
   (Integration of series)  If $u_n(x)$ is measurable for each $n$,  $u(x)=
   \displaystyle  \sum^{\infty}_{n=1} u_n(x)$, then
   $$
   \int\limits_{\mathfrak{X}}u(x) d\mu= \sum^{\infty}_{n=1}
   \int\limits_{\mathfrak{X}}u_n(x) d\mu
   $$
   provided that 
    $
   \displaystyle
   \mid \sum^{n}_{\nu=1} u_\nu(x)\mid \leq  \gamma(x)
   $ ~for all~ $N$  and $x$, $\gamma(x) \in L(\mathfrak{X})$.
   
    The equation is true if $u_n(x)\geq 0,$  in the sense that if
    either side is finite, then so is the other and equality holds,
    while if either side is $\infty$ so is the other. 
\end{theorem}

\begin{theorem}\label{chap1:sec13:thm34} 
  (Differentiation under the integral sign)
  
  If $f(x, y)$ is integrable in $a<x<b$ in a neighbourhood of
  $y=y_\circ$ and if $\dfrac{\partial f}{\partial y_\circ}$ exists in
  $a<x<b$, then  
   $$
  \frac{d}{dy_\circ}\int\limits^b_a f(x,y)dx=\int\limits^b_a
  \frac{\partial f}{\partial  y_\circ}dx
 $$  
 provided that  
  $$
 \bigg|\frac{f\left(x,y_\circ+h\right)-f\left(x,y_\circ\right)}{h}\bigg|\leq
 \gamma(x)\varepsilon L (a,b)
  $$
 for\pageoriginale all sufficiently small $h$.
\end{theorem}

 This theorem follows from the analogue of
 Theorem \ref{chap1:sec13:thm31} with $n$ replaced 
 by a continuous variable $h$. The proof is similar.  
 
 \section{The Riemann Integral}\label{chap1:sec14} 

 If we proceed to define an integral as we have done, but restrict the
 set function to one defined only on a \textit{finitely} additive system of
 sets (we call this set function ``measure'' even now), we  
 get a theory, which in the case of functions of a real variable, is
 equivalent to that of Riemann. It is then obvious that
 an-$R$-integrable function is also $L$-integrable and that the two
 integrals have the same value. 
 
 
 The  more direct definition of the $R$-integral is that $f(x)$ is
 $R$-inte\-grable in $a\leq  x\leq  b$ if it is bounded and if we can
 define two sequences $\{ \varphi_n(x)\},\{\psi_n(x)\}$ of step
 functions so that $\varphi_n(x)\uparrow,\psi_n(x)\downarrow,$ for
 each $x$, 
 
 $\varphi_n(x)\leq  f(x)\leq
 \psi_n(x),\int\limits^b_a(\psi_n(x)-\varphi_n(x)) dx \rightarrow 0
 ~\text{as}~ n  \rightarrow \infty $ since lim $\varphi_n(x)=lim \Psi_n(x)=f(x)$
 p.p., it is clear that $f(x)$ is $L$-integrable and that its $L$-integral
 satisfies  
 $$
 \int\limits^b_a f(x) dx=\lim\limits_{n\rightarrow
   \infty}\int\limits^b_a\varphi_n(x)dx=\lim\limits_ {n\rightarrow 
   \infty}\int\limits^b_a\psi_n(x)dx,
  $$ 
 and the common value of these is the R-integral. The following is
 the main theorem. 

\begin{theorem}\label{chap1:sec14:thm35} 
 A\pageoriginale bounded function in $(a,b)$ is $R$-integrable if and
 only if it is continuous p.p 
\end{theorem}

\begin{lemma*}
If $f(x)$ is R-integrable and  $ \in > 0$, we can define
 $ \delta >0$ and a measurable set $E_\circ$ in $(a,b)$ so that
 \begin{gather*} 
   \mu (E_0)> b-a-\in, \\
   |f( x+ h)-f( x)|\leq \in  ~\text{for}~ x \in E_0, x+h \in
   (a,b),|\rm h| < \delta.
 \end{gather*}
\end{lemma*}

\noindent \textbf{\underline {Proof of Lemma:}} We can define
continuous functions $ \varphi(x),\psi (x)$  
in a $\leq x \leq b$ so that
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item $\varphi (x)\leq  f(x)\leq  \psi (x)$, $a\leq  x \leq  b$
\item $\int\limits^b_a (\psi (x)-\varphi (x)) dx \leq  \in^2/2 $
\end{enumerate}

If $E_0$ is the set in $(a,b)$ in which $\psi (x)- \varphi (x) < \in/2 $
it is plain that $ \mu (E_0) > b-a-\in.$ For otherwise, the integral 
in(ii) would exceed $\in^2/2$. By\text { uniform continuity of}
$\varphi (x), \psi (x),$ 
we can define $\delta = \delta (\in) > 0$ so that
$$ 
\psi (x + h)-\psi (x) 1 \leq  \epsilon/2, |\varphi
(x+h)-\varphi(x)|\leq  \epsilon/2 
$$
for $x$, $x+h$ in $(a,b)$, $|h | \leq  \delta$.

Then, if $x$ is in $E_0$, $x+h$ is in $(a,b)$ and $|h |\leq  \delta$ 
\begin{align*}
  f(x+ h) -f(x) \leq  \psi (x+h) - \varphi(x)& = \psi (x)- \varphi (x)+
  \psi (x+h)-\psi(x)\\ 
  \leq  \epsilon/2 + \epsilon/2=\epsilon 
\end{align*}
and similarly $f(x+h)- f (x)\geq  - \epsilon $, as we require.

\noindent \textbf{\underline {Proof of Theorem 35}}\pageoriginale

If $f(x)$ is $R$-integrable, let

$$
\in > 0, \in_n >0, \sum^\infty_{n=1}\in_n < \in ,
$$
and define measurable sets $E_n$ in $(a,b)$ by the lemma so that 
\begin{multline*}
\mu (E_n)>b-a-\in_n,|f(x+ h)-f(x)|<\in_n ~\text{fo}r ~ x \varepsilon
E_n,\\ 
| h| \leq  \delta_n,\delta_n=\delta_n (\in_n)>0.
\end{multline*}

Let $E^\ast=\bigcap\limits^\infty_{n=1} E_n$, so that
$$
\mu (E^\ast) \geq  b-a-\sum \in _n >  b-a-\epsilon 
$$

Since $f(x)$ is continuous at every point of $E^*$ and $\in $ is
arbitrarily small, $f(x)$ is continuous p.p. 

Conversely, suppose that $f(x)$ is continuous p.p. Then if $\epsilon >
0$ we can define $E_0$ so that 
\begin{gather*}
 \mu (E_0)> b-a-\epsilon ~\text{and}~ \delta >0 \text{so that} \\
 |f(x+h)-f(x)|< \in ~\text{for} ~ x \epsilon E_0, | h|< \delta
\end{gather*}

If now we divide $(a,b)$ into intervals of length at most $\delta$
those which contain a point of $E_0$ contribute not more than $2\in (b-a)$ to 
the difference between the upper and lower Riemann sums $S$, $s$ for
$f(x)$, while the intervals which do not contain points of $E_0$ have
total length $\epsilon$ at most and contribute not more than $2\in M$ where
$M=\sup |f(x)|$. Hence 
$$ 
S-s \leq  2 \in (b-a)+ 2 \in M 
$$
which\pageoriginale can be made arbitrarily small.
 
\section{Stieltjes Integrals}\label{chap1:sec15}

In the development of the Lebesgue integral, we have assumed
that the measure $\mu$ is non-negative. It is easy to extend the theory 
to the case in which $\mu$ is the difference between two measures
$\mu ^+$ and $\mu^-$ in accordance with Theorem
\ref{chap1:sec5:thm3}.In this case, we define 
$$ 
\int_{\sqrt{x}} f(x) d \mu = \int_{\sqrt{x}} f (x)d \mu^+ -
\int_{\sqrt{x}} f(x) d (- \mu^-),
$$ 
when both integrals on the right are finite, and since $\mu^+$  and $\mu^-$
are measure, all our theorems apply to the integrals separately and
therefore to their sum with the exception of Theorems
\ref{chap1:sec11:thm22}, \ref{chap1:sec11:thm23},
\ref{chap1:sec11:thm24}, \ref{chap1:sec13:thm30},  
\ref{chap1:sec13:thm32} in which the sign of $\mu$ obviously plays a part. 
The basic inequality which takes the place of Theorem
\ref{chap1:sec11:thm22} is  

\begin{theorem}\label{chap1:sec15:thm36}
  If $\mu = \mu^+ + \mu^-$ in accordance with Theorem \ref{chap1:sec5:thm3},
  and $\mu^-=\mu^+ -\mu^-$then
  $$
  |\int_{\sqrt{x}}f(x)d\mu |\leq  \int_{\sqrt{x}}|f(x)| d \mu^-.
  $$
  [The integral on the right is often written
    $\int_{\sqrt{x}}|f(x)|~~|d \mu |. $] 
\end{theorem}

\begin{proof}
  \begin{align*}
    \bigg|\int\limits_{\sqrt{x}}f(x)d\mu\bigg| & = \bigg| \int\limits_{x} f(x)d
    \mu^+ -_{{~}_{X}}  f(x)d (-\mu ^-)\bigg|\\ 
    & \leq  \bigg| \int\limits_{\sqrt{x}} f(x)d
    \mu^+\bigg|+\bigg|\int\limits_{\sqrt{x}} f(x) d(-\mu^-)\bigg| \\ 
    & \leq  \int\limits_{\sqrt{x}}|f (x)d \mu ^+ +
    \int\limits_{\sqrt{x}}|f(x)| d(-\mu^-)\\ 
    & = \int\limits_{\sqrt{x}} |f(x)|d \mu^- = \int\limits_{\sqrt{x}}
    | f(x)|~|d \mu |. 
  \end{align*}

  We\pageoriginale shall nearly always suppose that $\mu$ is a measure with 
  $\mu \ge 0$ but it will be obvious when theorems do not depend on the
  sign of $\mu$ and these can be extended immediately to the general case.
  When we deal with \textit{inequalities}, it is generally essential to restrict
  $\mu$ to the positive case (or replace it by $\overline \mu$).
\end{proof}

Integrals with $\mu$ taking positive and negative values are
usually called \textit{Stieltjes integrals}. If they are integrals of functions 
$f(x)$ of a real variable. $x$ with respect to $\mu$ defined by a function 
$ \psi (x)$ of bounded variation, we write
$$ 
\int\limits_X  f(x) d \psi (x) ~\text{for}~ \int\limits_X f(x) d \mu, 
$$
and if $X$ is an interval $(a,b)$ with $\psi(x)$ continuous at $a$ and at $b$,
we write it as 
$$ 
\int\limits^b_a ~ f(x)d ~ \psi (x). 
$$

In particular, if $\psi (x) = x$, we get the classical Lebesgue integral,
which can always be written in this from.

If $\psi(x)$ is not continuous at $a$ or at $b$, the integral will
generally depend on whether the interval of integration is open or
closed at each end, and we have to specify the integral in one of the
four forms.
$$ 
\int\limits^{b \pm 0}_{a \pm o} ~f(x)d \psi~~ (x) 
$$ 

Finally, if $f(x) =  F_1(x)~ + ~if_2(x), (f_1(x), f_2(x)
~~\text{real})$ is a  complex valued function, it is integrable if $
f_1 ~~ \rm and ~~ f_2 $ are both integrable\pageoriginale if we define
$$ 
\int\limits_{X}  f(x)d \mu =  \int\limits_{X}  f_1(x) d \mu ~ +~ i
          \int\limits_{X} ~ f_2(x) d \mu. 
$$
The inequality
$$
\bigg|\int\limits_{X} f(x)d \mu \bigg| \le \int\limits_{X}\mid
f(x)\mid ~\mid d \mu \mid 
$$
(Theorem \ref{chap1:sec15:thm36}) still holds.

\section{\texorpdfstring{$L$}{L}-Spaces}\label{chap1:sec16}

$A$ set $L$ of elements $f$, $g$, $\ldots$ is a \textit{linear} space over the 
field $R$ of real numbers (and similarly over any field) if 
\begin{enumerate}[(1)]
\item $L$ is an abelian group with operation denoted by $+$.
\item $\propto f$ is defined and belongs to $L$ for any $\alpha$ of
  $R$ and $f$ of $L$. 
\item $(\alpha + \beta)f ~ = ~ \alpha f + \beta f$
\item $\alpha (f + g) ~ =~ \alpha f + \alpha g$
\item $\alpha (\beta f)~=~ (\alpha \beta) f$
\item $1. f ~ = ~ f.$
\end{enumerate} 
A linear space is a \textit{topological linear space if}
\begin{enumerate}[(1)]
\item $L$ is a topological group under addition,
\item scalar multiplication by $\alpha$ in $R$ is continuous in this topology.
  $L$ is a \textit{metric linear space} if its topology is defined by a metric.
\end{enumerate}

It is a Banach space if 
\begin{enumerate}
\renewcommand{\labelenumi}{(\theenumi)}
\item $L$ is a metric linear space in which metric is defined by $d(f,g) =
  \Arrowvert f - g \Arrowvert$ where the \textit{norm} $ \Arrowvert f
  \Arrowvert$ is defined as a real number for all $f$ of $L$ and has the
  properties 
  
  $\Arrowvert  f \Arrowvert = 0$\pageoriginale if and only if $f=0$, $\Arrowvert \rm
  f \Arrowvert \ge 0$ always  
  $$
  \Arrowvert \alpha f \Arrowvert =  \mid \alpha \mid~ \Arrowvert
  f \Arrowvert, ~\Arrowvert f + g \Arrowvert \le \Arrowvert
  f \Arrowvert + \Arrowvert g \Arrowvert
  $$ 
  and 
\item $L$ is \textit{complete}. That is, if a sequence $f_n$ has the property
  that $\Arrowvert f_n - f_m \Arrowvert \rightarrow 0 $ as $m$, $n
  \rightarrow  \infty,$ then there is a limit $f$ in $L$ 
  for which $\Arrowvert f_n- f \Arrowvert \rightarrow 0$. A Banach space
  $L$ is called a \textit{Hilbert space}  
  if and inner product $(f,g)$ is defined for every $f$, $g$ of $L$ as a 
  complex number and
  \begin{enumerate}[(1)]
  \item $(f, g)$ is a linear functional in $f$ and in $g$
  \item $(f,g) =  \overline {(g,f)}$
  \item $(f,f) = \Arrowvert f \Arrowvert^2$
  \end{enumerate}
  
  Two point $f$, $g$ are \textit{orthogonal} if $(f,g) = 0$, It is
  obvious that the 
  integrable functions $f(x)$ in $\mathfrak{X}$  
  form a linear space $L (\mathfrak{X})$ on which
  $\int\limits_{\mathfrak{X}} f(x)d \mu$ is a linear
  functional. If $p \ge 1$ the  
  space of measurable functions $f(x)$ on $\mathfrak{x}$ for which $\mid
  \rm f(x) \mid^p$ is integrable is denoted by $L_p(\mathfrak{X})$ 
  and we have the fallowing basic theorems.
\end{enumerate}

\begin{theorem}\label{chap1:sec16:thm37}
  (Holder's inequality; Schwartz' inequality if p=2)
  
  If p $\ge 1, \frac {1}{p} + \frac {1}{p'} = 1, f(x) \in L_p
  (\mathfrak{X})$ then 
  $$ 
  \bigg| \int\limits_{\mathfrak{X}} f(x) g(x) d \mu \bigg|\le ~
  \left(\int\limits_{\mathfrak{X}} f(x) \mid^p d \mu\right)^{1/p}
  \left(\int\limits_{\mathfrak{X}} \mid g(x) \mid^{p'} d \mu\right)^{1/{p'}} 
  $$ 
  
  If $p = 1$,  $\left(\int\limits_{\mathfrak{X}}\mid g(x) \mid^{p'} d
  \mu\right){}^{1/{p'}}$ is interpreted as the \textit{essential upper
    bound} of $g(x)$  
  that is, the smallest number $\Lambda$ for which $\mid g(x) \mid
  \le \Lambda$ p.p 
\end{theorem}

\begin{theorem}\label{chap1:sec16:thm38}
  If\pageoriginale $q \ge p \ge 1 $ and $\mu (\chi)< \infty$, then 
  $$
  L_q(\mathfrak{X}) \subset L_p(\mathfrak{X}).
  $$
  If $\mu (\mathfrak{X})=\infty $, there is no inclusion relation
  between $L_p$, $L_q $.
  For the proof we merely apply Holder's theorem with $f(x)$ ,$g(x)$, $p$
  replaced by $|f(x)|^p$, 1, $\dfrac{q}{p}$ respectively.
\end{theorem}

\begin{theorem}[Minkowski's Inequality]\label{chap1:sec16:thm39}
~\\
  If $p \ge 1$ and $||f||=(\int_x |f(x)|^p d\mu)^{1/p}$ , then
  $$
  ||f+g||\le||f||+||g||
  $$
  
  For the proofs see Hardy, Littlewood and Polya: Inequalities.
\end{theorem}

\begin{theorem}\label{chap1:sec16:thm40}
  If $p\ge 1, L_p(\chi)$ is complete. (the case $p=2$ is the
  Riesz-Fischer theorem). 
\end{theorem}
\begin{proof}
  We support that $p< \infty$ and that
  $$
  ||f_n ~~- f_m||\rightarrow 0 ~~as ~~m,n \rightarrow \infty,
  $$
  (in the notation introduced in Theorem \ref{chap1:sec16:thm39}) and define $A_k>0 \in_k
  \downarrow 0$ so that $\sum A_k <\infty$ and $\sum (\in_k/A_k)^p<
  \infty$. 
  
  We can choose a sequence $\{n_k \}$ so that $n_{k+1} >n_k$ and\\
  $$
  ||f_{n_k}~~-~f_m||\le \in_k ~\text{for}~ m \geq n_k
  $$
and in particular
$$
||f_{n_{k+1}}~~-f_{n_k}|| \le \epsilon_k.
$$

Let $E_k$ be the set in which $|f_{n_{k+1}}~~(x)-f_{n_k}(x)|\le A_k$. 
\end{proof}

Then\pageoriginale
\begin{gather*}
  \in^p_k \geq
  \int\limits_{\mathfrak{X}}|f_{n_{k+1}}~(x)-f_{n_k}(x)|~^p~d \mu \geq
  \int\limits_{\mathfrak{X}-E_k}|f_{n_{k+1}}~(x)-f_{n_k}(x)|~^p~d
  \mu \\ 
  \geq A^p_k \mu(\mathfrak{X}-E_k),
\end{gather*}
so that $\mu\left[\bigcup\limits^{\infty}_{K} (\mathfrak{X}-
  \in_k)\right]\rightarrow 0$ as $K 
\rightarrow \infty$ since $\sum(\in_k/A_k)^p<\infty$.

Since $f_{n_k}(x)$ tends to a limit at every point of each set
$\bigcap\limits^{\infty}_{K}~E_k$ (because $\sum A_k< \infty$) , it
  follows that $f_{n_k}(x)$ tends to a limit $f(x)$ p.p.

Also, it follows from Fatou's lemma that, since $||f_{n_k}||$ is
bounded, $f(x)\in L_p(\mathfrak{X})$ and that 
$$
||f{n_k}~-f||\geq \in_k,||f{n_k}~-f||\rightarrow 0~~as~~k \rightarrow
\infty
$$

Since $||f{n_k}~ - f_m||\rightarrow 0~ as~k,m \rightarrow \infty$ it
follows from Minkowski's inequality that $|f_m~-f||\rightarrow0~~as~~m
\rightarrow \infty$.

If $p=\infty$, the proof is rather simpler.

From these theorems we deduce

\begin{theorem}\label{chap1:sec16:thm41}
  If $p \ge 1$, $L_p(\mathfrak{X})$ is a Banach space with 
  $$ 
  ||f||=\left(\int\limits_{\mathfrak{X}}|f(x)|^p ~~d \mu\right)^{1/p}
  $$
  $L_2$ is a Hilbert space with
  $$
  (f,g)=\int\limits_{\mathfrak{X}}f(x)~~ \overline{g(x)}~d\mu.
  $$
\end{theorem}

The spaces $L_p$ generally have certain separability properties
related to the topological properties (if any) of $\mathfrak{X}$.

A\pageoriginale function with real values defined on an additive system $S_0$,
taking  constant values on each of a finite number of sets of $S_0$
id called a \textit{step function}.

\begin{theorem}\label{chap1:sec16:thm42} %%% 42
 The set of step functions (and even
  the sets step function taking relation values) is dense in $L_p$
  for $1\le p< \infty$. If the Borel system of measurable sets in
  $\mathfrak{X}$ is generated by a \textit{countable,} finitely additive system
  $S_0$ ,then the set of steps functions with rational values is
  countable and $L_p$ is separable. 
\end{theorem}
The proof follows easily from the definition of the integral.

\begin{theorem}\label{chap1:sec16:thm43}
  If every step function can be approximated in
  $L_p(\mathfrak{X})$ by continuous functions, the
  continuous functions, are dense in
  $L_p(\mathfrak{X})$, (assuming of course that $(\mathfrak{X})$ is a
  topological space). 
  
  In particular, continuous functions in $R_n$ are dense in
  $L_p(R_n)$. Since the measure in $R_n$ can be generated by a
  completely additive function on the finitely additive countable
  system of finite unions of rectangles $a_i \le x_i < b_i$, and  
  with $a_i$, $b_i$ rational their complements, $L_p(R_n)$ is separable. 
  
  We have proves in Theorem \ref{chap1:sec12:thm25} that an integral over an arbitrary set $X$
  is an absolutely continuous function of
  $X$. The following theorem provides a converse. 
\end{theorem}

\begin{theorem}\label{chap1:sec16:thm44}
  (Lebesgue for $R_n$; Radon-Nikodym in the general case)

  If $H(x)$ is completely additive and finite in $\mathfrak{X}$ and if
  $\mathfrak{X}$ has finite measure or is the limit of a sequence of
  subset of finite measure, then\pageoriginale
  $$ 
  \displaylines{\hfill 
  H(X)=F(X)+Q(X)\hfill \cr
  \text{where}\hfill 
  F(X)=\int\limits_X f(x)d \mu ,f(x) \in L (\mathfrak{X})\hfill }
  $$ 
  and $Q$ is a (\textit{singular}) function with the property that there is a 
  set $\mathfrak{X}_s$ of measure zero for which 
  $$
  0\leq  Q(X)=Q(X.\mathfrak{X}_s)
  $$
  for all measurable $X$. Moreover,$F(X),Q(X)$ are unique and $f(x)$ is
  unique up to a set of measure zero.
  
  In particular, if $H(X)$ is absolutely continuous, $Q(X)=0$
  and
  $$ 
  H(X)=F(X)=\int\limits_X f(x)d \mu,f(x) \in L(\mathfrak{X}) 
  $$
  In this case,$f(x)$ is called the \textit{Radon derivative} of $H(X)=F(X)$
\end{theorem}

\begin{proof}
  We assume that $\mu (\mathfrak{X})< \infty$. The extension is 
  straightforward.
  
  By Theorem \ref{chap1:sec5:thm3}, we can suppose that $H(X)\geq 0$. let $\Theta$ be
  the class of measurable function $\theta (x)$ with the property that
  $\theta(x)\geq 0$, 
  $$ 
  \int_x \theta(x)d \mu \leq  H(X)
  $$ 
  for all measurable $X$.
  Then we can find a sequence $\{\theta_n(x)\}$ in $\Theta$ for which
  $$
  \int_ {\mathfrak{X}} \theta_n(x)d \mu \longrightarrow \underset{\theta
    \epsilon \Theta}{sup} \int_x \theta(x)d\mu \leq   H(\mathfrak{X})<
  \infty
  $$ 
\end{proof}

If\pageoriginale we define $\theta'_n(x)= \underset{k \leq  n}{sup} \theta_k(x)$ and
observe that 
$$
X= \bigcup^{n}_{k=1} X.\epsilon [\theta'_n(x)=\theta_k(x)]
$$
we see that $\theta'_{n}(x)$ belongs to $\Theta$. Since $\theta
'_{n}(x)$ increases with $n$ for  
each $x$, it has a limit $f(x)\geq 0,$ which also belongs to $\Theta$ ,and we 
can write
$$
F(X)=\int_X f(x)d\mu \leq  H(X), ~Q(X)=H(X)-F(X)\geq 0
$$
while

(1) \quad $\int_{\mathfrak{X}} f(x)d \mu = \sup_{\theta \epsilon \Theta}
\int_{\mathfrak{X}} \theta(x)d \mu < \infty$. 

Now let
$$ 
Q_n(X)=Q(X)-\frac{\mu(X)}{n}
$$
and let $\mathfrak{X}^+_n$, $\mathfrak{X}^-_n$ be the sets defined by
Theorem \ref{chap1:sec5:thm3} for which 
$$ 
Q_n(X)\geq 0 ~\text{if}~ X \subset \mathfrak{X}^+_n, ~Q_n(X)\leq  0
~\text{if}~  X \subset \mathfrak{X}^+_n
$$

Then,
$$ 
H(X)\geq F(X)+ \frac{\mu(X)}{n}=\int_X (f(x)+\frac{1}{n})\text {d}\mu
~\text{if}~ X \subset\mathfrak{X}^+_n
$$
and if
\begin{align*}
f(x) & =f(x)+\frac{1}{n} ~\text{for}~ x \in\mathfrak{X}^+_n\\
f(x) & =f(x) ~\text{for}~ x \in \mathfrak{X}^+_n,
\end{align*}
it follows that $ f(x)$ be longs to $\Theta$, and this contradicts(1) unless
$\mu(\mathfrak{X}^+_n)=0$. Hence $\mu(\mathfrak{X}^+_n)=0$ and
$Q(X)=0$ if $X$ is disjoint from 
$$
\mathfrak{X}_s=\bigcup^{\infty}_{n=1}\mathfrak{X}^+_n
$$
which has measure zero.

To\pageoriginale prove uniqueness, suppose that the decomposition can be
made in two ways so that 
$$ 
H(X)=F_1(X)+Q_1(X)=F_2(X)+Q_2(X),
$$
where $F_1(X),F_2(X)$ are integrals and $ Q_1(X)$, $Q_2(X)$ vanish on all sets 
disjoint from two sets of measure zero, whose union $\mathfrak{X}_s$ also has 
measure zero. Then
\begin{multline*}
  F_1(X)=F_1(X-X \mathfrak{X}_S),F_2(X)=F_2(X-X\mathfrak{X}_S),\\
  F_1(X)-F_2(X)=Q_2(X-X\mathfrak{X}_s)-Q_1(X-X\mathfrak{X}_s)=0.
\end{multline*}

\begin{theorem}\label{chap1:sec16:thm45}
  If $\phi(X)$ is absolutely continuous in $\mathfrak{X}$ and 
  has Radon derivative $\varphi(X)$, with respect to a measure
  $\mu$in$\mathfrak{X}$,then 
  $$
  \int_{\mathfrak{X}}f(x)d \phi=\int_{\mathfrak{X}} f(x) \varphi (x)
  d\mu
  $$
  if either side exists.
\end{theorem}
\begin{proof}
  We may suppose that $f(x)\geq 0,\varphi(x)\geq 0, \phi(X)\geq 0,$
  Suppose that
  $$
  \int_{\mathfrak{X}}f(x)d \phi < \infty
  $$
  
  Then, it follows from Theorem \ref{chap1:sec12:thm27} that we may suppose that $
  \phi(\mathfrak{X})<\infty$.
  If $\mathcal{E}>0$, we consider subdivisions $\{y_\nu\}$ for which
  $$
  y_1\leq  \epsilon, y_{\nu+1}\leq (1+\epsilon)y_{\nu} (\nu\geq 1)
  $$
  so that
  \begin{align*}
    s=\sum^{\infty}_{\nu=1} y_{\nu}\phi\left(E_\nu\right)&\leq 
    \int_{\mathfrak{x}} f(x)d \phi\\
    &\leq \sum_{v=0}^{\infty} y_{\nu +1} \Phi(E)\\ 
    &\leq(1+ \in)s + \in \Phi(\mathfrak{X})
  \end{align*}

  But\pageoriginale 
  $$ 
  \int_{\mathfrak{X}} f(x) \phi (x)d\mu = \sum^{\infty}_{\nu=o}
  \int_{E_{\nu}} f (x) \phi(x)d \mu 
  $$ 
  by Theorem \ref{chap1:sec11:thm20}, and  
  $$ 
  y_\nu \Phi (E_\nu) \leq \int_{E_\nu} f(x) \varphi (x)d\mu \leq
  y_{\nu+1} \Phi (E_\nu ) 
  $$
  by Theorem \ref{chap1:sec11:thm22}, and therefore we have also  
  $$ 
  s \leq \int_{\mathfrak{X}} f(x) \phi (x) d \mu \leq  (1+\in) s+\in
  \Phi (x). 
  $$

  The conclusion follows on letting $\in \rightarrow 0 $ 
\end{proof}

Moreover the first part of this inequality holds even if
$\int_{\mathfrak{X}} f(x) d \mu =\infty$, but in this case, $s$ is not
bounded and since the inequality holds for all s,
$$ 
\int_{\mathfrak{X}} f(x) \varphi (x) d \mu = \infty. 
$$

\section{Mappings of measures}\label{chap1:sec17}

Suppose that we have two spaces $\mathfrak{X}$, $\mathfrak{X}^*$ and a
mapping $X \rightarrow X^*$ of $\mathfrak{X}$ into $
\mathfrak{X}^*$. If S is a Borel system of measurable sets X with a
measure $\mu$ in $\mathfrak{X}$, the mapping induces a 
Borel system $ S^* $ of `measurable' sets $X^*$ in $\mathfrak{X}^*$,
these being defined as those sets $X^*$ for which the inverse images $X$
in $ \mathfrak{X} $ are measurable, the measure $\mu^*$ induced by
$\mu$ 
on $ s* $ being defined by $\mu^*(x^*) = \mu (x) $ where $X$ is the
inverse image of $X^*$. 

If the mapping is (1-1), the two spaces have the same properties of
measure and we call the mapping a measure isomorphism. 

\begin{theorem}[Change of variable]\label{chap1:sec17:thm46}
  If\pageoriginale the measure $\mu$, $\mu^*$ in $\mathfrak{X}$ and
  $\mathfrak{X}^*$ 
  are isomorphic under the (1-1) mapping $ X \rightarrow X^*$ of  
  $\mathfrak{X}$  onto $\mathfrak{X}^*$ and if $f^*(x^*)=f(x)$ then 
  $$ 
  \int_{\mathfrak{X}} f(x) d \mu =
  \int_{\mathfrak{X}^*} f^* (x^*) d \mu^*
  $$
\end{theorem}

The proof is immediate if we note that the sets $E_\nu$ and $ E^*{_\nu}$
defined in $\mathfrak{X}$ and $\mathfrak{X}^*$ respectively by any
subdivision correspond under the mapping $ x \rightarrow x^* $ 
and have the same measure $ \mu~ (E_\nu) = \mu^*(E^*{_\nu})$. 

As an immediate corollary of this theorem we have 
\begin{theorem}\label{chap1:sec17:thm47}
  If $ \alpha (t)$ increases for $A  \leq t \leq b$ and $\alpha (A) = a$,
  $\alpha(B) = b $ and $G(x)$ is of  
  bounded variation in $a \leq x \leq b$, then
  $$
  \int^b_{a} f(x) dG(x) = \int^B_{A} f(\alpha(t)) dG (\alpha(t)).
  $$  
  
  In particular 
  $$ 
  \int^b_{a}  f(x) dx= \int^B_{A} f(\alpha(t)) d \alpha(t)
  $$
  and, if $\alpha(t)$ is absolutely continuous 
  $$ 
  \int^b_{a} f(x) dx = \int^B_{A} f(\alpha(t)) \alpha' (t) dt. 
  $$
\end{theorem}

\section{Differentiation}\label{chap1:sec18}

It has been shown in Theorem \ref{chap1:sec16:thm44} that any completely additive and
absolutely continuous finite set function can be expressed as the  
the integral of an integrable function defined uniquely upto a set of
measure zero called its Radon derivative. This derivative does not depend  
upon any topological properties of the space $\mathfrak{X}$. On the
other hand\pageoriginale the derivative of a function of a real variable is defined,  
classically, as a limit in the topology of R. An obvious problem is to
determine the relationship between Radon derivatives and those  
defined by other means. We consider here only the case $\mathfrak{X}=
R$  where the theory is familiar (but not easy). We need some  
preliminary results about derivatives of a function $F(x)$ in the
classical sense. 

\begin{defi*}
  The upper and lower, right and left
  derivatives of $F(x)$ at $x$ are defined respectively, by  
  \begin{align*}
    D ^+ F &= \lim\limits_{h \rightarrow + 0}\sup ~\frac{F(x+h)-F(x)}{h}\\
    D_+ F & = \lim\limits_{h \rightarrow + 0}\inf ~\frac{F(x+h)-F(x)}{h}\\
    D^- F & = \lim\limits_{h \rightarrow -0} \sup ~\frac{F(x+h)-F(x)}{h}\\
    D_- F & = \lim\limits_{h \rightarrow - 0} \inf ~\frac{F(x+h)-F(x)}{h}
  \end{align*}
\end{defi*}

Plainly $D_+ F \leq D^+F$, $D_-F \leq D^-F$. If $D_+F = D^+F$ or $D_-F = D^-F$ 
we say that $F(x)$ is differentiable on the or on the left,
respectively, and the common values are called the right or left 
derivatives, $F'_+ , F'_-$. If all four derivatives are equal, we say
that $F(x)$  is differentiable with derivative $F'(x)$ equal 
to the common value of these derivatives.

\begin{theorem}\label{chap1:sec18:thm48}
  The set of points at which $F'_+$ and $F'_- $ both are exist but different
  is countable. 
\end{theorem}

\begin{proof}
  It\pageoriginale is enough to prove that the set $E$ of points $x$ in 
  which $F'\_ (x) < F'_+ (x) $is countable. Let $r_1 , r_2 \dots $ be
  the sequence  of all rational numbers arranged in some definite
  order. If $x \in E$ let $k=k(x)$ be the smallest integer for which 
  $$ 
  F'\_ (x) < r_k < F'_+ (x)
  $$
  Now let $m$, $n$ be the smallest integers for which
  \begin{align*}
    r_m < x, &\frac{F(\zeta) - F(x)}{\zeta - x} < r_k \text { for } r_m <
    \zeta < x , \\
    r_n > x, &\frac{F(\zeta) - F(x)}{\zeta - x} > r_k \text { for } x <
    \zeta < r_n 
  \end{align*}
\end{proof}

Every $x$ defines the triple $(k, m, n)$ uniquely, and two numbers
$x_1 < x_2$ cannot have the same triple $(k,m,n)$ associated with
them. For if they did, we should have 
$$ 
r_m < x_1 < x_2 < r_n 
$$
and therefore 
$$
\displaylines{\hfill
  \qquad \quad\frac{F(x_1) - F(x_2)}{x_1 - x_2} < r_k \quad \text{from the
    inequality} \hfill\cr
  \text{while}\hfill  
  \frac{F(x_1) - F(x_2)}{x_1 - x_2} < r_k \quad \text{ from the second}\hfill}
$$
and these are contradictory. Since the number of triples $(k,m,n)$ is
 countable, so is $E$. 

\begin{theorem}[Vitali's covering theorem]\label{chap1:sec18:thm49}
  Suppose that every point of a bounded set $E$ of real numbers
  (not\pageoriginale 
  necessarily measurable) is contained in an arbitrarily small closed
  interval with positive length and belonging to a given family
  $V$. Suppose that $G$ is an open set containing $E$ and that $\in > 0$. 
\end{theorem}

Then we can select a \textit{finite} number $N$ of mutually dis joint intervals
$I_n$ of $V$ so that each $I_n$ lies in $G$ and  

$$
\sum^{N}_{n=1} \mu (I_n) - \in \leq \mu (E) \leq \mu
(E \sum^{N}_{n=1} I_n) + \in.
$$
($\mu$ standing of course, for \textit{outer} measure).
\begin{proof}
  If $\in > 0$, it is obviously enough, after Theorem \ref{chap1:sec9:thm12}, to prove the
  theorem in the case $\mu (G) \leq \mu (E) + \in$. We may also suppose
  that all the intervals of $V$ lie in $G$. 
\end{proof}

We define a sequence of intervals $I_1 , I_2 \dots$ inductively as
follows. $I_1$ is an arbitrary of $V$ containing points of $E$. If $I_1
, I_2 \dots , I_n$ have been defined, let $1_n$ be the upper bound of
lengths of all the intervals of $V$ which contain points of $E$ and which
are disjoint from $I_1 + I_2 + \dots + I_n$. Then, since the $I_k$
are closed, $1_n > 0$ unless $ I_1 + I_2 + \dots + I_n \supset E$. Now
define $I_{n+1}$ so that it is an interval of the type specified above
and so that $\lambda _{n+1} = \mu (I_{n+1})> \frac{1}{2} 1_n$. 

Then $I_{n+1}$ is disjoint from $I_1 + \dots + I_n$ and
$$ 
S = \sum^{\infty}_{n=1} I_n \subset G. 
$$

Suppose now that $A = E - SE$, $\mu (A) > 0$. Let $J_n$ be the
interval with that same centre as $I_n$ and 5 times the length of
$I_n$. We can then choose 

$N$\pageoriginale so that
$$
\sum^{\infty}_{n=N+1} \mu (J_n) = 5
\sum^{\infty}_{n=N+1} \mu (I_n) < \mu (A),
$$ 
since $\sum\limits^{\infty}_{n=1} \mu (I_n) \leq \mu (G) \leq \mu (E) + \in <
\infty \text { and } \mu (A) > 0.$ 
It follows that
$$ 
\mu \left(A -A \bigcup^{\infty}_{n=N+1} J_n\right) > 0 
$$
and that $A-A$  $\bigcup\limits^{\infty}_{n=N+1} J_n$ contains at least one
point $\xi$. Moreover, since $\xi$ does not belong to the \textit{closed} set
$\sum\limits^{N}_{n=1} I_n$, we can choose from $V$ an
interval $I$ containing $\xi$ and such that $I$. $I_n =0$ for $n=1,2
\dots , N$. On the other hand, $I. I_n$ cannot be empty for all $n\geq
N+1$ for, if it were, we should have 
$$ 
0< \mu (I) \geq 1_n < 2 \lambda_{n+1}
$$
for all $n \leq N +1$ and this is impossible since $\lambda _n
\rightarrow 0$ (for $\sum\limits^{\infty}_{1} \lambda_n=
\sum\limits^{\infty}_{1} \mu (I_n) \leq \mu (G) < \infty)$.  We can therefore
define $n_\circ \geq N+1$ to be the 
smallest integer for which $I.I_{n_\circ} \neq 0$. But  
$$ 
I.I_n =0 for n \leq n_\circ -1
$$
and it follows from the definition of $1_n$ that
$$ 
0 < \lambda \le 1_{n_\circ -1} < 2 \lambda_{n_\circ}
$$
Hence $I$, and therefore $\xi$, is contained in $J_{n_\circ}$ since
$J_{n_\circ}$ has five times the length of $I_{n_\circ}$ and
$I.I_{n_\circ} \neq 0$. 

This\pageoriginale is impossible since $\xi$ belongs to $A - A
\bigcup\limits^{\infty}_{n= N+1} J_n$  and $n_\circ \ge N + 1$.
 
 Hence we must have
 $$ 
\mu (A) = 0
$$ 
and $\mu (ES) =\mu (E), \sum\limits^{\infty}_{n=1}\mu (I_n)
\le \mu (G) \le \mu (E) + \in.$ 
  
 We can therefore choose $N$ as large that
$$
\sum^{N}_{n=1}\mu (I_n) - \in \le \mu (E) \le \mu (E
\sum^{N}_{n=1} I_n) + \in
$$
 
 \begin{theorem}\label{chap1:sec18:thm50}
   A function $F(x)$ of bounded variation is differentiable p.p. 
 \end{theorem}

\begin{proof}
It is sufficient to prove
 the theorem when $F(x)$ is increasing. We prove that $D^+F= D_+F$
 p.p. The proof that $ D ^- F= D_- F$ p.p. is similar, and the
 conclusion then follows from Theorem \ref{chap1:sec18:thm48}. 

The set
$$
\in [D^+F > D_+F]= \bigcup_{r_1, r_2} \in [ D^+F > r_1 >r_2 >
  D_+F]
$$

Where the union is over the countable pairs of rational $r_1, r_2.$

Hence, if we suppose that $ \mu (\in [D^+ F> D_+ F]) > 0$ we can find
rationals $r_1,r_2$ such that  
$$
D^+ F > r_1 > r_2 > D_+ F
$$
in a set $E$ of positive outer measure. Then every point $x$ of $E$ is the
left hand end point of an interval $(x, \eta)$ such that 
$$ 
F(\eta) -F(x) \le (\eta -x) r_2
$$
and\pageoriginale we may suppose that $\eta -x$ is arbitrarily small. It follows
from Vitali's theorem that we can define a set $K$ consisting of a
finite number of such intervals so that  
$$ 
\mu (E. K) > \mu (K) -\in
$$
 
While the increment $F(K)$ of $F(x)$ over the intervals satisfies
$$
F(K) \le r_2 \mu (K).
$$

But every point $x$ of $EK$, with the exception of the finite set of right
hand end points of $K$, is the left hand end point of an arbitrarily
small interval $(x, \xiup)$ for which 
$$ 
F(\xiup) -F(x) \ge (\xiup - x) r_1.
$$

If we now apply Vitali's theorem to the intersection of $E$  and the set
of interior pints of $K$ (Which has the same measure as $EK$), we can
construct a finite set of intervals $K'$ so that  
$$
K' \subset K, \mu (K') \ge \mu (E K) - \in \ge \mu(K) - 2 \in ,
$$
while the increment $F(K')$ of $F(x)$ over the intervals $K'$ satisfies 
$$
F(K')\ge r_1 \mu (K').
$$

Since $F(K') \le F(K)$, we get
$$
r_2 \mu (K) \ge r_1 \mu (K') \ge r_1 (\mu (K) - 2 \in),
$$
which gives a contradiction if $\epsilon$ is small enough. Hence we must have
$\mu (E) =0 $ and the theorem is proved. 
\end{proof}

\begin{theorem}\label{chap1:sec18:thm51}
  If $F(x)$ increases and is bounded in $a \le x \le b$ and if $F'(x)$ is
  its derivative, then $F'(x)$ is non-negative p.p,
  integrable\pageoriginale in $(a, b)$  and satisfies 
  $$
  \int^b_a F'(x) dx \le F(b) - F(a)
  $$
\end{theorem}

\begin{proof}
Since $\frac{F (x + h) - F(x)}{h} \ge 0$ for $ h \neq o$ it follows that
$$
F'(x)=\lim\limits_{h \to 0} \frac{F(x + h) - F(x)}{h} \ge 0 ~\text{p.p.}
$$

It follows now from Fatou's lemma that if $\delta > 0,$

\begin{align*}
  \int^{b-\delta}_a F'(x) dx & \le   \lim\limits_{h \to 0}\inf
  \int^{b-\delta}_a \frac{F(x + h) -F(x)}{h} dx\\ 
  & = \lim\limits_{h \to 0}\inf\bigg\{\frac{1}{h}\int^{b+h-\delta}_{a+h}
  F(x) dx - \frac{1}{h} \int ^b_a F (x) dx \bigg\} \\ 
  & = \lim\limits_{h \to
    0}\inf\bigg\{\frac{1}{h}\int^{b+h-\delta}_{b-\delta} F(x) dx
  -\frac{1}{h} \int^{a+ h}_a F(x)dx \bigg\}\\ 
  & \le \lim\limits_{h \to 0}[F (b+ h -\delta)- F(a)]\\
  & \le F(b)-F(a)
\end{align*}
since $F(x)$ is increasing.
\end{proof}

\begin{theorem}\label{chap1:sec18:thm52}
  IF $f(x)$ is integrable in $(a, b)$ and 
  $$
F(x)= \int^x_a f (t) dt = 0 ~\text{for}~ a \le x \le b
$$
then $f(x)=0$ p.p.
\end{theorem}
 
(This\pageoriginale is a refinement of corollary
\ref{chap1:sec11:coro1} of Theorem \ref{chap1:sec11:thm24}, since the 
condition here is lighter than the condition that the \textit{set} function
$F(X)=\int_x f(t)dt$ should vanish for for \textit{all measurable} $X$.) 

\begin{proof}
  Our hypothesis implies that $F(X)=0$ for all open or closed intervals
  $X$ and therefore, since $F(X)$ is completely additive $F(X)=0$ for all open
  sets $X$ every open set being the sum of a countable number of disjoint
  open  intervals. But every measurable set is $t$ he sum of a set of zero
  measure and the limit of a decreasing sequence of open sets by Theorem
  \ref{chap1:sec9:thm12}, and therefore $F(X) =0$ for every measurable
  set $X$. The conclusion 
  then follows from corollary \ref{chap1:sec11:coro1} to Theorem
  \ref{chap1:sec11:thm24}.  
\end{proof}

\begin{theorem}[Fundamental theorem of the calculus]\label{chap1:sec18:thm53}
\begin{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\rm (\theenumi)}
\item If $F(x)$ is\break an absolutely continuous point function and $f(x)$
  is the Radon derivative of its associated set function $F(X)$ (which is also
  absolutely continuous ; see page 30) then $F(x)$ is differentiable p.p
  and  
  $$ 
  F^{'}(x)=f(x) ~\text{p.p}
  $$ 
\item If $f(x) \varepsilon  L(a,b)$ then $F(x)= \int ^{x}_{a}f(t)dt$
  is absolutely continuous and $F^{'}(x)=f(x)$ p.p 
\item If $F(x)$ is absolutely continuous in a $\leq  x\leq b$, then
  $F^{'}$ is integrable and  
  $$
  F(x)=\int^x_a F'(t) dt + F(a)
  $$
\end{enumerate}
\end{theorem}

\begin{proof}
 (i) We may suppose that $F(x)$ increases and
  that $F(x)\geq o$. If $A > 0$, let 
$f_A(x)= \min [A,f(x)], F_A(x)=\int^x_af_A(t)dt$,
where\pageoriginale $f(x)$ is the Radon derivative of $F(X)$ and $F(x)=\int^x_a
f(t)dt$. 
\end{proof}

Then since $f_a(x)$ is bounded it follows from Fatou's lemma that 
\begin{align*}
  \int^x_aF^{'}_A(t)d &t=\int^x_a\lim\limits_{h \rightarrow
    0}\frac{F_A(t+h)-F_A(t)}{h}dt\\  
  &\ge \lim\limits_{h \rightarrow 0}\sup\int^x_a\frac{F_A(t+h)-F_A(t)}{h}dt\\ 
  &= \lim\limits_{h \rightarrow 0}\sup\bigg\{\frac{1}{h}\int^{x+h}_x
  F_A(t)dt-\frac{1}{h}\int^{a+h}_aF_A(t)dt\bigg\}\\ 
  &=F_A(x)-F_A(a)=F_A(x)
\end{align*}
since $F_A(t)$ is continuous. Since $f(t)\ge f_A(t)$ it follows that
$F^{'}(t)\ge F'_A(t)$ and therefore 
$$
\int^x_a F^{'}(t)dt\ge\int^x_aF_A^{'}(t) dt \ge F_A(x)
$$

This holds for all $A >0$ and since $F_A(x)\rightarrow F(x)as A
\rightarrow\infty$ by 

Theorem \ref{chap1:sec13:thm31}, we deduce that
$$
\int^x_a F^{'}(t)dt\ge F(x) =\int^x_a f(t)dt.
$$

Combining this with Theorem \ref{chap1:sec18:thm50} we get
$$
\int^x_a (F^{'}(t)-f(t)) dt =0
$$
for all $x$, and the conclusion follows from Theorem \ref{chap1:sec18:thm51}

Parts(ii) and (iii) follow easily form(i). If we did not wish to use
the Radon derivative, we could prove (ii) and (iii) with 
the\pageoriginale help of the deduction from Vitali's theorem that if $F(x)$ is
absolutely continuous and $F^{'}(x)=0$ p.p then $F(x)$
is constant 

\begin{theorem}[Integration by parts]\label{chap1:sec18:thm54}  
  If $F(x),G(x)$ are of bounded variation in an open or closed interval
  $J$ and  
  $$
  F(x)= \frac{1}{2} [ F(x-0)+F(x+0)], G(x)= \frac{1}{2} [G(x-0)+G(x+0)],
  $$
  then
  $$
  \int_j  F(x)dG (x)=\int_J [F(x) G(x)]-\int_j G(x)dF(x).
  $$
  
  In particular if $F(x)$, $G(x)$ are absolutely continuous then, 
  $$
  \int^b_a  F(x)G''(x)dx=\int^b_a[F(x)G(x)]-\int^b_a F^{'}(x)G(x)dx
  $$
\end{theorem}

\begin{proof}
  We may suppose that F(x), G(x) increase on the interval and are non -
  negative  and define 
  $$
  \Delta(I) =\int_I F(x)d G(x)+\int_I G(x)d F(x)- \int_I[F(x)G{x}]
  $$
  for intervals $I\subset J$. Then $\Delta (I)$ is completely additive
  and we shall prove that $\Delta (I)=0$ for all $I$ 
\end{proof}

Suppose first that $I$ consist of a single point $a$. Then
\begin{align*}
  \Delta
  (I)& = F(a)[G(a+0)-G(a-0)]+G(a) [F(a+0)-F(a-0)]\\
  & \hspace{4cm} -F(a+0)G(a+0)+F(a-0)G(a-0)\\
  &=0
\end{align*}
since $2F(a)=F(a+0)+F(a-0)$, $2G(a)=G(a+0)+G(a-0)$.

Next if $I$ is an open interval $a<x<b$,
\begin{align*}
  \Delta(I)& \leq F(b-0)[G(b-0)-G(a+0)]G(b-0) [F(b-0)-F(a+0)]\\
  & \hspace {4cm} -F(b-0) G(b-0)+ F(a+0) G(a+0)\\
  &=(F(b-0)-F(a+0)) (G(b-0)-G(a+0)),\\
  & = F(I)G(I)
\end{align*}\pageoriginale
where $F(I)$, $G(I)$ are the interval functions defined by F(x),G(x),
and similarly 
$$
\Delta (I) \ge -F(I)G(I) \text{so that} \mid \Delta
(I)\mid \ge F(I)G(I)
$$ 

Now, any interval is the sum of an open interval and one or two end
points and it follows from the additivity of $\Delta(I)$, that  
$$
\mid \Delta  (I)\mid \le F(I)G(I).
$$
for all intervals. Let $\in >0$. Then apart from a finite number of
points at which $F(x+0)-F(x-0) > \in$, and on which $\Delta =0$, we
can divide $I$ into a finite number of disjoint intervals $I_n$ on each
of which $F(I_n) \le \in.$ Then 
\begin{align*}
  \mid \Delta  (I)\mid=\mid \Delta \left(\sum I_n\right)\mid\, = &\mid
  \sum \Delta (I_n)\mid 
  \le \sum F(I_n)G(I_n)\\ 
  \le &\in \sum G(I_n)=\in G(I).
\end{align*}

The conclusion follows on letting $\in \rightarrow 0$.

\begin{theorem}[Second Mean Value Theorem]\label{chap1:sec18:thm55}
\begin{enumerate}[\rm (i)]
\item If $f(x)\in L(a,b)$\break and $\varphi (x)$ is monotonic,
  $$
  \int^b_a f(x) \phi(x)dx=\varphi(a+0) \int^\xi_a f(x)dx + \phi(b-0)
  \int^b_\xi f(x) dx 
  $$ \pageoriginale
  for some $\xi$ in a $\le \xi \le b$.
\item If $\varphi(x) \ge 0$ and $\phi (x)$ decreases in $a\leq x \le b$,
  $$
  \int^b_a f(x) \varphi (x)dx=\varphi(a+0) \int^\xi_a f(x) dx
  $$
  for some $\xi$, $a \le \xi \le b$.
\end{enumerate}
\end{theorem}

\begin{proof}
Suppose that $\phi (x)$ decreases in (i), so that, if we put
$F(x)=\int^x_a f(t)dt$, we have 
\begin{align*}
  \int^b_a f(x) \phi (x)dx & ={}^{b-0}_{a+0}[F(x) \phi
    (x)]-\int^{b-0}_{a+0} F(x)d (x)\\ 
  & =\phi(b-0) \int^b_a f(x)dx +[\phi (a+0)-\phi (b-0)]F(\xi)
\end{align*}
 by Theorem \ref{chap1:sec11:thm22} and the fact that $F(x)$ is continuous and attains every
 value between its bounds at some point $\xi \text{in a} \le \xi \le
 b$. This establishes (i) and we obtain (ii) by defining
 $\varphi(b+0)=0$ and writing  
 \begin{align*}
   \int^b_a f(x) \phi (x) dx&=\int^{b+0}_{a+0}[ F(x) \phi(x) ]-\int^{b+0}_{a+0}
   F(x)d \varphi (x)\\  
   &=\varphi(a+0) F(\xi)  ~\text{with}~ a \le \xi \le b.
\end{align*}

A refinement enables us to assert that a $< \xi < b$ in (i) and that
$a < \xi \le b$ in (ii). 
\end{proof}

\section{Product Measures and Multiple Integrals}\label{chap1:sec19}

Suppose that $\mathfrak{X}, \mathfrak{X}'$ are two spaces of points
$x$, $x'$. Then the space of pairs $(x,x')$ with $x$ in $\mathfrak{X}$, $x'$ in
$\mathfrak{X}$ is called the \textit{product space} of $\mathfrak{X}$ and
$\mathfrak{X}'$ and is written $\mathfrak{X} x \mathfrak{X}'$. 

\begin{theorem}\label{chap1:sec19:thm55}
Suppose that measures $\mu$, $\mu'$, are
  defined on Borel systems $S$, $S'$ of measurable sets $X$, $X'$ in two
  spaces $\mathfrak{X}$, $\mathfrak{X}$ respectively.\pageoriginale Then a measure $m$ can
  be defined in $\mathfrak{X} x \mathfrak{X}'$ in such a way that, if
  $X$, $X'$ are measurable in $\mathfrak{X}$, $\mathfrak{X}'$ respectively,
  then $X x X'$ is measurable in $\mathfrak{X} x \mathfrak{X}'$ and 
  $$
  m(X x X)=\mu(X). \mu'(X')
  $$

(The measure $m$ is called the \textit{product measure} of $\mu $ and
  $\mu '$). 
 The idea of product measures is basic in the theory of probability where
  it is vital to observe that the product measure is \textit{not the only}
  measure which can be defined in $\mathfrak{X} x \mathfrak{X}'$. 
\end{theorem}

\begin{proof}
  We define a \textit{rectangular} set in $ \mathfrak{X} x 
  \mathfrak{X}'$ to be 
  any set $X x X'$ with $X$ in $S$, $X'$ in $S$ and we define its measure
  $m(X x X')$
  to be $\mu(X)\cdot\mu '(X')$. (An expression of the form $0 \cdot
  \infty$ is taken 
  to stand for 0). We call the sum of a finite number of rectangular
  sets a \textit{figure} in $ \mathfrak{X} x \mathfrak{X}'$ and define its
  measure to be the sum of the measure of disjoint rectangular sets
  which go to form it. It is easy to verify that this definition is
  independent of the decomposition used and that the figures and their
  complements form a finitely additive system on which their measure is
  finitely additive. 

  After Kolmogoroff's theorem (Theorem \ref{chap1:sec7:thm7}), it s sufficient to show that
  $m$ is completely additive on figures. Suppose that 
  $$
  \sum\limits^\infty_{n=1} X_n x X'_n = X_0 x X'_0
  $$
  where the sets on the left are disjoint. If $x$ is any point of $X_0,$
  let $J'_n(x)$ be the set of points $x'$ of $X'_0$ for which $(x,x')$
  belongs to $X_n x X'_n$. Then $J'_n(x)$ is measurable in
  $\mathfrak{X}'$  for each $x$, 
  it\pageoriginale has measure $\mu'(X_{n'})$ when $x$ is in $X_n$ and 0
  otherwise. This measure $\mu^{'}(J_n^{'} (x))$ is plainly measurable
  as a function of $x$ and  
  $$
  \int_{X_o} \mu' (J'_n (x))d\mu = \mu (X_n) \mu^{'}(X^{'}_n)
  $$
  Moreover, $\sum\limits_{n=1}^{N} \qquad J'_n(X)$ is the set
  of points x of $X_o$ for which $(x,x')$ belongs to
  $\sum\limits_{n=1}^{N} X_n x X_n$. It is
  measurable and 
  $$ 
  \int_{X_o} \mu'\left(\sum\limits_{n=1}^{N} J_n (x)\right) d \mu =
  \sum\limits_{n=1}^{N} \mu (X_n) \mu^{'} (X^{'}_n). 
  $$
  
  But since $X_0 x X_0^{'} \sum\limits_{n=1}^{\infty} X_n x
  X_n^{'}$, it follows that  
  $$ 
  \displaylines{\hfill \qquad 
  \lim\limits_{N\to\infty} \sum\limits_{n=1}^{N}
  J'_n(x)=X'_0  ~\text{for every x of } X_0,\hfill \cr
  \text{and therefore}\hfill 
  \lim\limits_{N\to\infty} \mu' \left[\sum_{n=1}^{N}
    J_n(x) \right] = \mu (X'_0) ~\text{for every x of}~ X_0.\hfill }
  $$ 
  
  It follows from Theorem \ref{chap1:sec13:thm32} on monotone convergence that 
  $$ 
  \displaylines{\hfill 
    \mu (X_0) \mu' (X'_0)= \lim\limits_{N\to\infty} \int_{X_0}
    \mu'\left(\sum\limits_{n=1}^{N} J'_n(x)\right)d\mu =
    \sum\limits_{n=1}^{N} \mu (X_n) \mu' (X'_N) \hfill\cr
    \text{and so}\hfill 
    m(X_0 x X_0^{'}) = \sum\limits_{n=1}^{\infty} m (X_n x X_n^{'})\hfill }
  $$
  which completes the proof.
\end{proof}

\begin{theorem}\label{chap1:sec19:thm56}
Let\pageoriginale $ \mathfrak{X} ,
  \mathfrak{X}''$ be two measure spaces with measures $\mu ,
  \mu'$ respectively such that $\mathfrak{X}  (\mathfrak{X}')$ is
  the limit of a sequence $| \big \{ X_n\big\} (\big\{ X'_n\big\})$ of
  measurable sets of finite measure $\mu (X_n) (\mu' (X''_n))$.
  Let $Y$ be  a set in $\mathfrak{X} x \mathfrak{X}'$ measurable with
  respect to the product measure m defined by $\mu , \mu'$. Let
  $Y'(x)$ be the set of points $x'\in \mathfrak{X}'$ for
  which $(x, x')\in Y$. Then $Y'$ is measurable in
  $\mathfrak{X}'$ for almost all $x\in \mathfrak{X}$, its measure
  $\mu'(Y'(x))$ is a measurable function of x and 
  $$ 
  \int_{\mathfrak{X}} \mu' (Y'(x)) d \mu = m (Y). 
  $$
\end{theorem}

\begin{proof}
We note first that the theorem is
  trivially true if $Y$ is a rectangular set and follows immediately if
  $Y$ is the sum of a countable number of rectangular sets. Further, it
  is also true for the limit of a decreasing sequence of sets of this
  type. In the general case, we can suppose that 
  $$ 
  Y\subset Q, Q - Y \subset \Gamma 
  $$
  where $m(\Gamma)=0$ and $Q$, $\Gamma$ are limits of decreasing sequence
  of sums of rectangular sets. Then, if $Q^{'}(x)$, $\Gamma^{'} (x)$ are
  defined in the same way as $Y^{'}(x)$ we have 
  $$ 
  Y'(x) \subset Q'(x), Q'(x)-Y'\subset \Gamma'(x)
  $$
  where $\Gamma' (x)$, $Q'(x)$ are measurable for almost all $x$. But
  $$ 
  \int_{\mathfrak{X}} \mu'(\Gamma(x)) d \mu = m (\Gamma)=0 
  $$
  so that $\mu'(\Gamma^{'}(x))=0$ for almost all $x$ since $\mu^{'} \ge
  0$, and this is enough to show that $Y^{'}(x)$ is measurable for
  almost all $x$ and that 
  $$ 
  \mu'(Y'(x)) = \mu' (Q^{'}(x)) \text{p.p.} 
  $$
\end{proof}

Finally,\pageoriginale
$$ 
\int_{\mathfrak{X}} \mu^{'}(Y^{'}(x))d\mu =
\int_{\mathfrak{X}}\mu^{'}(Q(x)) d\mu = m(Q) = m (Y).
$$
\begin{theorem}[Fubini's theorem]\label{chap1:sec19:thm57} 
  Suppose $\mathfrak{X}, \mathfrak{X}^{'}$ satisfy
  the hypotheses of Theorem \ref{chap1:sec19:thm56}. If $f(x,x^{'})$
  is measurable with 
      respect to the product measure defined by $\mu , \mu^{'}$ it is
      measurable in $x$ for almost all $x^{'}$ and in $x^{'}$ for almost
      all $x$. The existence of any one of the integrals 
      $$ 
      \int_{\mathfrak{X}x  \mathfrak{X}'} |f(x,x')| dm ,
      \int_{\mathfrak{X}} d \mu \int_{\mathfrak{X}'} |f(x,x)| d\mu',
      \int_{\mathfrak{X}'}d \mu' \int_{\mathfrak{X}}|f(x,x)|d\mu 
      $$
      implies that of the other two and the existence and equality of
      the integrals 
      $$ 
      \int_{\mathfrak{X} x \mathfrak{X}'} f(x,x) dm ,
      \int_{\mathfrak{X}} d \int_{\mathfrak{X}'} f(x,x') d \mu',
      \int_{\mathfrak{X}} d\mu \int_{\mathfrak{X}'} f(x, x') d \mu 
      $$
\end{theorem}

\begin{proof}
  We may obviously suppose that $f(x,x^{'})\ge 0$.  Let $\big\{
  y_{\nu}\big\}$ be a subdivision with $E_{\nu} = \in [y_{\nu}\le
    f(x,x^{'})< y_{\nu+1}]$. The theorem holds for the function equal to
  $y_{\nu}$ in $E{\nu}$ for $\nu = 0, 1, 2, \dots , N$ ($N$ arbitrary) and
  zero elsewhere, by Theorem \ref{chap1:sec19:thm56}, and the general result follows easily
  from the definition of the integral. 
\end{proof}
