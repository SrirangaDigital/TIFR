\chapter{Probability}\label{chap2}

\section{Definitions}\label{chap2:sec1}

A\pageoriginale measure $\mu$ defined in a space $\mathfrak{X}$ of points $x$ is
called a \textit{probability 
measure} or a \textit{probability distribution} if $\mu (\mathfrak{X})
= 1$. The measurable sets $X$ are called \textit{events} and the
\textit{probability of the event} $X$
is real number $\mu(X)$. Two events $X_1,X_2$ are mutually exclusive
if $X_1\cdot X_2 =0$. 

The statement: \textit{$x$ is a random variable in $\mathfrak{X}$ with
  probability distribution} $\mu$ means 

\begin{enumerate}[(i)]
\item that a probability distribution $\mu$ exists $\mathfrak{X}$,
\item that the expression \textit{``the probability that $x$ belongs to
  $X$''}, where 
  $X$ is a given event, will be taken to mean $\mu (X)\cdot \mu(X)$ sometimes
  written $P(x \varepsilonup X)$. 
\end{enumerate}
 
The basic properties of the probabilities of events follow
immediately. They are that these probabilities are real numbers
between 0 and 1, inclusive, and that the probability that one of a
finite or countable set of mutually exclusive events $(X_i)$ should
take palace i.e. the probability of the event $\cup X_i$, is equal to the
sum of the probabilities of the events $X_i$. 

If a probability measure is defined in some space, it is clearly
possible to work with any isomorphic measure in another space. In
practice, this can often be taken to be $R_k$, in which case we speak
of a \textit{random real vector} in the place of a random variable. In
particular, if $k=1$ we speak of a random of a \textit{random real
  number or random 
real variable}.\pageoriginale The probability distribution is in this case defined by
a \textit{distribution function} $F(x)$ increasing from 0 to 1 in
-$\infty<x<\infty$. For example, a probability measure in any finite
or countable space is isomorphic, with a probability measure in $R$
defined by a step function having jumps $p_\nu \;{\rm \;at}\; \nu=0,
1, 2, \dots $, where 
$$
p_\nu \geq 0, \sum p_\nu = 1. 
$$

Such a distribution is called a \textit{discrete} probability distribution. If
$F(x)$ is absolutely continuous $F'(x)$ is called the
\textit{probability density function (or frequency function)}. 

\begin{example}\label{chap2:sec1:exp1}%%% 1
\textit{Tossing a coin} The space $\mathfrak{X}$ has only two points
$H$, $T$ with 
four subsets, with probabilities given by 
$$
\mu(0)=0, \mu(H) = \mu(T) = \frac{1}{2}, \mu(\mathfrak{X}) = \mu(H+T) =
1.
$$

If we make $H$, $T$ correspond respectively with the real numbers 0,1, we
get the random real variable with distribution function 
\begin{align*}
  F(x) &= 0 ({x}< 0)\\
  &= \frac{1}{2}(0 \leq {x} < 1)\\
  &= 1(1 \leq x)
\end{align*}

Any two real numbers a,b could be substituted for 0, 1.
\end{example}

\begin{example}\label{chap2:sec1:exp2}%%%2
Throwing a die- The space contains six
  points, each with probability $1/6$ (unloaded die). The natural
  correspondence with $R$ gives rise to $F(x)$ with equal jumps 1/6 at
  1, 2, 3, 4, 5, 6.  
\end{example}

\section{Function of a random variable}\label{chap2:sec2}

Suppose that $x$ is a random variable in $\mathfrak{X}$ and that $y=\alpha
(x)$ 
is\pageoriginale a function defined in $\mathfrak{X}$ and taking values in a
space $y$. Suppose that $y$ contains a Borel
system of measurable sets $Y$. Then $y$ is called a \textit{function} of the
variable $x$ if the set $\varepsilonup[\alpha(x)\in Y]$ is measurable in
$\mathfrak{X}$ for every measurable $Y$ and if we take the measure in
$\mathfrak{X}$ of this set as the probability measure of $Y$. Note that
the mapping $x \rightarrow y$ need not be one-one. 

(There is a slight ambiguity in notation as $x$ may denote either the
random variable in $\mathfrak{X}$ or a generic point of
$\mathfrak{X}$. In practice, there is no difficulty in deciding which
is meant.) 

\setcounter{example}{0}
\begin{example}\label{chap2:sec2:exp1}%%% 1
$x$ being a random real variable with distribution function $(d\cdot
  f) F(x)$   we compute the $d\cdot f\cdot G(y)$ of $y = x^2$ 
  $$
  P(y<0) = P(x^2< 0) = 0 ~\text{so that}~ G(y) =0 ~\text{for}~ y < 0.
  $$
\end{example}

If 
\begin{align*} 
  a \geq 0, P(y \leq a) & = P(x^2 \leq a) =P(0 \leq x \leq \sqrt{a})
  +P(-\sqrt{a}\leq x < 0),\\ 
  G(a+0) & =F(\sqrt{a}+0) -F(-\sqrt{a}-0).
\end{align*}

\begin{example}\label{chap2:sec2:exp2} %% 2
  If $F(x) =0$ for $x < 0$ and $G(y)$ is the
  distribution function of $y=1/x$, $x$ having d.f. $F(x)$ then 
  $$
  G(y) =0 for y < 0.
  $$
  
  If $a \geq 0$.
  $$
  G(a+0) =P(y \leq a) =P\left(\frac{1}{x} \leq a\right) =P(x \geq 1/a) =1 -F(1/a
  -0).
  $$
  
  Since $G(a)$ is a d.f., $G(a+0) \rightarrow 1$ as $a \rightarrow
  \infty$, so that $F$ must be continuous at 0. That is, $P(x=0) =0$. 
\end{example}

\section{Parameters of random variables}\label{chap2:sec3}

A \textit{parameter} of a random variable (or its distribution) is a number
associated with it. The most important parameters of real 
distributions\pageoriginale are the following.

\begin{enumerate}[(i)]
\item   The \textit{mean} or \textit{expectation}
  $\overline{\alpha{(x)}}$ of a real 
  valued function $\alpha (x)$ of a random real variable $x$ is defined
  by 
  $$
  \overline{\alpha(x)} = E(\alpha(x)) = \int_{\mathfrak{X}} \alpha(x)d \mu 
  $$
\item  The standard deviation or dispersion $\sigma$ of a random real
  number about its mean is defined by  
  \begin{align*}
    \sigma^{2} &= E(x -\overline{x})^2 = \int_{\mathfrak{X}}(x -
    \overline{x})^2 d\mu \\ 
    &=\int_{\mathfrak{X}}x^2 d \mu - 2x \int_{\mathfrak{X}} xd\mu +
    \overline{x}^{2} \int_{\mathfrak{X}} d\mu\\ 
    &=E(x^2) - 2\overline{x}^2 + \overline{x}^2 = E(x^2) - (E(x))^2
  \end{align*}
  $\sigma^2 $is called the \textit{variance} of $x$.
  
\item The \textit{range} is the interval $(r,R)$, where 
  $$
  r=\sup_{F(a)=0} a , R= \inf_{F(a) = 1} a
  $$
\item The \textit{mean error} is $\int_{\mathfrak{X}}|x - \overline{x}| d \mu =
  E(|x - \overline{x}|)$ 
\item A \textit{median} is a real number $A$ for which
  $$F
  (A - 0) + F(A + 0) \leq 1.
  $$
\item The \textit{mode} of an absolutely continuous distribution
  $F(x)$ is the\break  unique maximum, when it exists, of $F'(X)$. 
\end{enumerate}

\noindent \textit{Some special distributions in $R$.}

\begin{enumerate}[(i)]
\item {The \textit{Dinomial distribution}} 

  $0 < p < 1, q =1 -p, n$ is a positive integer. $x$ can take
  values $\nu  = 0, 1, 2, ..., n$ with probabilities 
  $$
  p_{\nu} = (^n_\nu) p^\nu q^{n-\nu}, \sum\limits^{n}_{0} p_\nu = 1.
  $$\pageoriginale
  
  Then
  \begin{align*}
    \overline{x} & = E (\nu) =  \sum\limits^{n}_{\nu=0} \nu p_\nu = \rm np,\\
    \sigma^2 & = E (\nu^2) - \overline{x}^2 = \sum\limits^{n}_{\nu=0}
    \nu^2 \rm p_\nu - n^2 p^2 = n p q
  \end{align*}
  $\mathcal{P}_\nu$  is the probability of $\nu$  successes out of n
  experiments in each of which the probability of success is $p$. 
\item \textit{The Poisson distribution} $x$ can take the values $\nu = 0, 1,
  2, \ldots$ with probabilities 
  $$
  p_\nu = e^{-c}  \frac{c^\nu}{\nu !} , 
  \sum\limits^{\infty}_{\nu=0} p_\nu = 1,
  $$
  where $c > 0$. Here 
  \begin{align*}
    \overline{x}&= e^{-c} \sum\limits^{\infty}_{\nu=0} \frac{\nu
      c^\nu}{\nu !}= c\\ 
    \sigma^2 & = e^{-c} \sum\limits^{\infty}_{\nu=0} \frac{\nu^2
      c^\nu}{\nu !} -c^2 = c  
  \end{align*}
  
  The binomial and Poisson distributions are discrete. The Poisson
  distribution is the limit of the binomial distribution as $n$
  $\rightarrow\infty'$ if we put 
  \begin{align*}
    p=c/n (p ~\text{then}~ \rightarrow 0).
  \end{align*}
\item \textit{The rectangular distribution}
  
  This is given by 
  \begin{align*}
    F(x) &= 0 ~\text{for}~ x \leq  a\\
    &= \frac{x-a}{b-a} ~~\text{for}~~ a \leq x \leq b\\
    &= 1 ~~\text{for}~~ b \leq x.
  \end{align*}
  
  It\pageoriginale is absolutely continuous and $F' (x) = 1/(b - a)$ for $a < x < b$
  and ~$= 0$ for $x < a, x >b$. Also 
  \begin{align*}
    \overline{x}= \frac{a+b}{2}, \sigma^2 = \frac{(b-a)^2}{12}
  \end{align*}
\item \textit{The normal (or Gaussian) distribution}
  
  This is an absolutely continuous distribution $F$ for which 
  $$
  F'(x) =\frac{1}{\sigma \sqrt{2\prod}} \hspace{0.3cm}   e^{-(x
    -\overline{x})^2 /2 \sigma^2}
  $$
  
  It is easy to verify that the mean and standard deviation are
  respectively $\overline{x} ~~\text{and} ~~\sigma$.
  
\item \textit{The singular distribution}:
 
  Here $x = 0$ has  probability 1,
  \begin {align*}
    F(x)  = D (x - a)  = & 1 ~\text{if}~ x \geq a \\
    & 0  \; \text {if}  \; x < a
  \end{align*}
\end{enumerate}

We now prove
\setcounter{theorem}{0} 
\begin{theorem}[Tehebycheff's inequality]\label{chap2:sec3:thm1} %% 1
  If $\propto (x)$ is a nonnegative function of a random variable $x$
  and $k > 0$ then 
  $$
  P(\alpha (x) \geq k) \leq \frac{E(\alpha(x))}{k}
  $$
\end{theorem}

\begin{proof}
  \begin{align*}
    E(\alpha (x)) & = \int_{\mathfrak{X}}\alpha (x)d \mu\\
    &= \int_{\alpha (x) \geq k}\alpha (x)d \mu + \int_{\alpha (x) < k}
    \alpha (x) d \mu\\ 
    & \ge \int_{\alpha(x)\ge k} \alpha(x)d\mu \ge k \int_{\alpha(x)\ge 
      k}d\mu=KF (\alpha(x)\ge k).  
  \end{align*}
\end{proof}\pageoriginale

\begin{coro*}
  If $k>0$ and $\overline{x}$, $\sigma$ are respectively the mean and the
  standard deviation of a random real $x$, then
  $$
  p(\alpha(x)\ge k \sigma^{-})\le 1/k^2.
  $$
\end{coro*}

We merely replace k by $k^2 \sigma^2,\alpha(x)$ by
$(x-\overline{x})^2$ in the Theorem \ref{chap2:sec3:thm1}. 

\section{Joint probabilities and independence}\label{chap2:sec4}

Suppose that $\mathfrak{X}_1$, $\mathfrak{X}_2$ are two spaces of points
$x_1,x_2$ and that a probability measure $\mu$ is define in a Borel
system of sets $X$ in their product space
$\mathfrak{X}_1 x\mathfrak{X}_2$. Then the set $x_1$ in
$\mathfrak{X}_1$ for which the set $\in [x_1 \in X_1,x_2 \in
 \mathfrak{X}_2]$ is measurable with respect to $\mu$ form a Borel
system in $\mathfrak{X}_1$.  if we define
$$
\mu_1(X_1)=\mu\left(\in\left[x_1\in X_1,x_2 \in \mathfrak{X}_2\right]\right)
$$
it follows that $\mu_1$ is a probability measure in $\mathfrak{X}_1$
and we define $\mu_2  ~in~ \mathfrak{X}_2$ in the same way. We call
$\mu_1(X_1)$ simply the probability that $x_1$ belongs to $X_1$ with
respect to the \textit{joint distribution defined by $\mu$}.

\begin{defi*}
If $\mu$ is the product measure of
  $\mu_1,\mu_2$ the random variables $x_1,x_2$ are said to be
  \textit{independent}. Otherwise, they are \textit{dependant}. 
\end{defi*}

When we wish to deal at the same time with several random variables, we
must know their \textit{joint probability distribution} and this, as we see that
once, is not necessarily the same as the product 
probability\pageoriginale as their separate distributions. This applies in
particular $\alpha (x_1,x_2\ldots)$ for the probability distribution of the
values of the function is determined by the joint distribution of
$(x_1,x_2,\ldots)$. In this way we can define the \textit{sum} $X_1+X_2$ and
\textit{product} $x_1\cdot x_2$ of random variables, each being treated as a
function $\alpha(x_1,x_2)$ over the product space $\mathfrak{X}_1 x
\mathfrak{X}_2$ with an assigned joint probability distribution. 

\begin{theorem}\label{chap2:sec4:thm2} %%% 2
  If $(x_1,x_2,...,x_n)$ is a random real vector then
  $$
  E(x_1+x_2+...+x_n)=E(x_1)+E(x_2)+...+E(x_n)
  $$
  whether $x_1,x_2,\ldots ,x_n$ are independent or not. $(E(x_i)$ is the mean
  of $x_i$ over the product space.) 
\end{theorem}

\noindent \textit{Proof.}
  Let $p$ be the joint probability distribution. Then
  \begin{align*}
    E\left(\sum x_i\right) &=\int_\Omega \left(\sum x_i\right) dp
    ~(\text{where}~ \Omega 
    =\mathfrak{X}_1 x\ldots x \mathfrak{X}_n)\\ 
    &\hspace{2cm}\sum \int_\Omega x_i ~~dp=~\sum E(x_i). \tag*{$\Box$}
  \end{align*}

\begin{theorem}\label{chap2:sec4:thm3} %%% 3
  If $x_1,x_2, \ldots ,x_n$ are \textit{independent} random real variables with
  standard deviations $\sigma_1,\sigma_2,\ldots ,\sigma_n,$ then the
  standard deviation $\sigma$ of their sum $x_i+x_2\ldots +x_n$is given by 
  $$
  \sigma^2= \sum^n_{i=1} \sigma_i~~^2
  $$
\end{theorem}

\begin{proof}
  It is sufficient to prove the theorem for $n=2$. Then
  \begin{align*}
    \sigma^2 & = E((x_1+x_2-\overline{x}_1-\overline{x}_2)^2)\\
    & =E \left((x_1
    -\overline{x}_1)^2 + (x_2 - \overline{x}_2)^2 +
    2(x_1-\overline{x}_1) (x_2 \overline{x}_2)\right) \\ 
    & =E((x_1-\overline{x}_1)^2)+E((x_2-\overline{x}_2)^2)
    2E((x_1-\overline{x}_2 )  (x_2-\overline{x}_2))\\   
    & \hspace{7cm}{\text{(by theorem \ref{chap2:sec4:thm2})}}\\ 
    & =\sigma_1^2+\sigma^2_2 +2 \int\limits_{\mathfrak{X}_1 x \mathfrak{X}_2}
    (x_1-\overline{x}_1)(x_2-\overline{x}_2) dp\\ 
    & =\sigma_1^2+\sigma_2^2+2 \int\limits_{\mathfrak{X}_1 (x
      \mathfrak{X}_2} (x_1-\overline{x}_1) (x_2-\overline{x}_2) d
    \mu_1 d \mu_2\\ 
    & =\sigma^2_1+\sigma^2_2+2 \int\limits_{\mathfrak{X}_1}
    (x_1-\overline{x}_1) d \mu_1 \int\limits_{\mathfrak{X}_2}
    (x_2-\overline{x}_2) d \mu_2\\ 
       & \hspace{7cm} \text{by Fubini's theorem,}\\
       &   =\sigma^2_1 +\sigma^2_2
  \end{align*}\pageoriginale
  this is an example of more general principle.
\end{proof}

\begin{theorem}\label{chap2:sec4:thm4} %%% 4
  If $\alpha(x_1,x_2)$ is function of two \textit{independent}
  variables $x_1,x_2$ then 
  $$
  \int_\Omega \alpha(x_1,x_2)dp=  \iint_\Omega \alpha(x_1,x_2)d
  \mu_1 ~d\mu_2, \Omega=\mathfrak{X}_1 x \mathfrak{X}_2.
  $$
\end{theorem}

  The proof is immediate from the definition of independence. In particulars,
\begin{theorem}\label{chap2:sec4:thm5}%%% 5
  If $x_1,x_2$ are independent, then
  $$
  E(x_1,x_2)=E(x_1)E(x_2)=\overline{x_1}.\overline{x_2}
  $$
  
  It is not generally sufficient to know the mean or other parameters of
  a function of random variables. The general problem is to find its
  complete distribution. This can be difficult, but the most important
  case is fairly easy.     
\end{theorem}

\begin{theorem}\label{chap2:sec4:thm6} %% 6
  If\pageoriginale $x_1, x_2$ are \textit{independent} random real numbers, with
  distribution functions $F_1, F_2$ then their sum has distribution
  function $F(x)$ defined by
  $$ 
  F(x) = F_1 \ast F_2 (x) = F_2 \ast F_1 (x) ={
  \underset{-\infty}{\int^\infty}}  F_1 (x - u) d F_2 (u)
  $$
  ($F(x)$ is called the \textit{convolution} of $F_1(x)$ and $F_2 (x)$).
\end{theorem}

\begin{proof}
  Let 
  \begin{align*}
    \alpha_x (x_1 , x_2)& =1 ~\text{when}~ x_1 + x_2 \leq x\\
    &= 0 ~\text{when}~ x_1 + x_2 > x 
  \end{align*}
  so that if we suppose that $F(x+ 0)= F (x)$ and put $\Omega= R {\rm
    x} R$, we have 
  \begin{align*}
    F(x) &= \iint\limits_{\Omega} \alpha_x (x_1 , x_2) d p\\
    &  = \iint\limits_{\Omega} \alpha_x (x_1 , x_2) dF_1 (x_1) dF_2 (x_2)\\
    &= \int\limits_{-\infty}^{\infty} d F_2 (x_2)
    \int\limits_{-\infty}^{\infty} \alpha_x (x_1 , x_2) dF_1
    (x_1)\\ 
    &= \int\limits_{-\infty}^{\infty} d F_2 (x_2) \int\limits_{x_1 + x_2
      \leq x } dF_1 (x_1)\\ 
    & = \int\limits_{-\infty}^{\infty} F_1 (x - x_2) d F_2 (x_2) =
    \int\limits_{-\infty}^{\infty} F_1 (x -u) d F_2 (u),
  \end{align*}
  and a similar argument shows that
  $$ 
  F(x)= {\underset{-\infty}{\int^\infty}} F_2 (x -u) dF_1 (u).
  $$
\end{proof}

It\pageoriginale is obvious that $F(x)$ increases, $F(-\infty) = 0$, $F(+\infty) =1$
so that $F(x)$ is a distribution function. Moreover, the process can be
repeated any finite number of times and we have 

\begin{theorem}\label{chap2:sec4:thm7}%%% 7
  If $x_1, \ldots , x_n $ are independent, with distribution functions
  $F_1 , \ldots , F_n,$ then the distribution function of $x_1 + \ldots +
  x_n $ is 
  $$F_{1^*} \ldots  \ast F_n $$
\end{theorem}

\begin{coro*}
  The convolution operator applied to two
  or more distribution functions (more generally, functions of bounded
  variation in (- co, co)) is commutative and associative.
\end{coro*}

\begin{theorem}\label{chap2:sec4:thm8} %%% 8
  If $F_1 (x), F_2 (x)$ are distribution functions, and $F_1 (x)$ is
  absolutely continuous with derivative $f_1(x)$ then F(x) is absolutely
  continuous and  
  $$ 
  f(x) = F' (x) = \int\limits_{-\infty}^{\infty} f_1 (x -u) d F_2 (u)
  p.p
  $$
  If both $F_1 (x)$ and $F_2(x)$ are absolutely continuous, then
  $$ 
  f(x) = \int_{- \infty}^{\infty} f_1 (x -u) f_2 (u) du ~\text{p.p}  
  $$
\end{theorem}

\begin{proof}
  We write
  \begin{align*}
    F(x) &= 
    \int\limits_{- \infty}^{\infty} F_1 (x- u) d F_2 (u) =
    \int\limits_{- \infty}^{\infty} d F_2 (u) 
    \int\limits_{- \infty}^{{x-u}} f_1 (t) dt\\
    \intertext{by the fundamental theorem of of calculus}
    &= \int\limits_{- \infty}^{\infty} d F_2 (u) \int\limits_{- \infty}^{x}
    f_1 (t - u) dt\\ 
    & = \int\limits_{- \infty}^{x} dt \int\limits_{- \infty}^{\infty} f_1
    (t - u) dF_2 (u)\\ 
    \intertext{and so} 
    f(x) &= F'(x) = \int\limits_{- \infty}^{\infty} f_1 (x - u) dF_2 (u) p.p 
  \end{align*}\pageoriginale
  again by the fundamental theorem of the calculus. The second part
  follows from Theorem \ref{chap1:sec16:thm45}, Chapter I, 
  
  We shall need the following general theorem on convolutions.
\end{proof}

\begin{theorem}\label{chap2:sec4:thm9} %%% 9
  Suppose that $F_1 (x)$, $F_2 (x)$ are distribution functions and that
  $\alpha (x)$ is bounded and is either continuous or is the limit of
  continuous functions. Then 
  $$ 
  \int\limits_{-\infty}^{\infty}\alpha (x + y) d F_2 (x) ~\text{is B
    - measurable as a }
  $$
  function of $y$ and   
  $$
  \int\limits_{- \infty}^{\infty} d F_1 (y) 
  \int\limits_{-\infty}^{\infty} \alpha (x + y) d F_2 (x) = 
  \int\limits_{-\infty}^{\infty} \alpha (x) d F(x)
  $$
  where
  $$ 
  F(x)= F_1 \ast F_2 (x)
  $$
\end{theorem}

\begin{proof}
  We may suppose that $\alpha (x) \geq 0$. If we consider first the case
  in which $\alpha (x) =1 for a \leq x \leq b$ and $\alpha (x) =0$
  elsewhere, 
{\fontsize{10pt}{12pt}\selectfont
  \begin{align*}
    \int\limits_{- \infty}^{\infty} \alpha (x+y) d F_2(x) & = F_2 (b - y -
    0) - F_2 (a - y - 0),\\ 
    \int\limits_{- \infty}^{\infty} dF_1 (y) \int\limits_{-
      \infty}^{\infty} \alpha (x+y) dF_2 (x) &=
    \int\limits_{-\infty}^{\infty} (F_2 (b-y-0)) -F_2(a-y-0)d F_1(y)\\  
    &= F(b-0)- F(a-0)\\
    &  = \rm{\int^\infty_{-\infty} \alpha(x) d F (x)},
  \end{align*}}\relax\pageoriginale
  and the theorem is true for function $\alpha  (x)$ of this type.
  
  Since an open set is the union of a countable number of intervals a $
  \leq x < b,$ the theorem is true also for functions $\alpha 
  (x)$ constant in each interval of an open set and 0 elsewhere. The
  extension to continuous function $\alpha (x)$ and their limits is immediate.
\end{proof}

\section{Characteristic Functions}\label{chap2:sec5}

A basic tool in modern probability theory is the notion of the
characteristic function $ \varphi (t)$ of a distribution function
$F(x)$.  

It is defined by 
$$ 
\varphi(t) = \int^\infty_{-\infty} e^{itx} dF(x)
$$

Since $\mid e^{itx}\mid =1$, the integral converges absolutely and
defines $ \varphi$ $(t)$ for all real t. 
 
\begin{theorem}\label{chap2:sec5:thm10} %%% 10
$\mid \varphi(t)\mid \leq 1$, $\varphi (0) =1$, $\varphi(-t) =
  \overline{\varphi (t)}$ and $\varphi (t)$ is uniformly continuous
  for all $t$. 
\end{theorem}

\begin{proof}
  \begin{align*}
    \mid \varphi(t)\mid &=\mid \int\limits^\infty_{-\infty} e^{\text{itx}} dF(x)\mid
    \leq  \int\limits^\infty_{-\infty} \mid e^{\text{itx}}\mid dF(x)\\  
    & = \int\limits^\infty_{-\infty} dF(x)=1.\\
    \varphi(0) & = \int\limits^\infty_{-\infty} dF(x)=1.\\
    \varphi(-t) & = \int\limits^\infty_{-\infty} e^{\text{-itx}} dF(x) =
    \int\limits^\infty_{-\infty} e^{\text{itx}} dF(x) = \overline{\varphi(t)} 
  \end{align*}\pageoriginale
  since $F(x)$ is real. 
\end{proof}

If  h $\neq$ 0,
\begin{align*} 
  \varphi(t+h) - \varphi(t) & = \int\limits^\infty_{-\infty}
  e^{\text{itx}} (e^{\text{ixh}}-1)
  dF(x),\\ 
  \mid \varphi(t+h) - \varphi(t)\mid & \leq \int\limits^\infty_{-\infty} \mid
  e^{\text{ixh}}-1 \mid dF(x)=o (1)~as~h \rightarrow 0 
\end{align*}
by Lebesgue's theorem, since $ \mid e^{\text{itx}} -1 \mid \leq 2$. 

\begin{theorem}\label{chap2:sec5:thm11}%%% 11
  If $ \varphi_1(t), \varphi_2(t)$ are the characteristic functions of
  $F_1 (x)$, $F_2 (x)$ respectively,  then $ \varphi_1(t)$. $\varphi_2(t)$
  is the characteristic function of $F_1 * F_2 (x)$. 
\end{theorem}

\begin{proof}
  \begin{align*}
    \varphi_1(t). \varphi_2(t) &= \int\limits_{- \infty}^{\infty}
    e^{ith} dF_1(y). \int\limits^\infty_{-\infty} e^{itx} dF_2(x)\\ 
    & = \int\limits^\infty_{-\infty} dF_1 (y)
    \int\limits^\infty_{-\infty}e^{it(x+y)} dF_2(x) \\ 
    & = \int\limits^\infty_{-\infty} dF_1 (y)
    \int\limits^\infty_{-\infty}e^{itx} dF_2(x-y) \\ 
    & = \int\limits^\infty_{-\infty}e^{itx} dF(x) \\
  \end{align*}
  where $F(x) = $ $F_1*F_2 (x)m$ by Theorem 9.
\end{proof}

As an immediate corollary of this and theorem \ref{chap2:sec4:thm7}, we have 

\begin{theorem}\label{chap2:sec5:thm12} %%% 12
  If $x_1,x_2,\ldots,x_n$ are independent random real variables with
  characteristic function $ \varphi_1(t),\varphi_2(t),\ldots,\varphi_n(t)$ 
  then\pageoriginale the characteristic function of $x_1+x_2+\cdots+x_n$ is
  $\varphi_1(t),\varphi_2(t),\cdots,\varphi_n(t)$. 
\end{theorem}

\begin{theorem}\label{chap2:sec5:thm13}
  Suppose that $F_1(x),F_2(x)$ are distribution functions with
  characteristic functions $\varphi_1(t),\varphi_2(t)$. 
  
  Then
  $$ 
  \int\limits^\infty_{-\infty}\varphi_1(t+u) dF_2(u) =
  \int\limits^\infty_{-\infty}e^{itx} \varphi_2(x)  dF_1(x)
  $$
\end{theorem}

\begin{proof}
  \begin{align*}
  \int\limits^\infty_{-\infty}e^{itx}  \varphi_2(x)  dF_1(x)  &=
  \int\limits^\infty_{-\infty}e^{itx} dF_1(x)
  \int\limits^\infty_{-\infty}e^{ixu}   
  dF_2(u)\\
  & = \int\limits^\infty_{-\infty} dF_1(x)
  \int\limits^\infty_{-\infty}e^{ix(t+u)} dF_2(u) \\ 
  & = {\int\limits^\infty_{-\infty} dF_2(u)
    \int\limits^\infty_{-\infty}e^{ix(t+u)} dF_1(x)} \\ 
  &= {\int\limits^\infty_{-\infty} \varphi_1(t+u) dF_2(u)}
  \end{align*}
\end{proof}

\begin{theorem}[Inversion Formula]\label{chap2:sec5:thm14} %%% 14
  If 
  $$ 
  \varphi(t)=\int\limits^\infty_{-\infty}e^{itx} dF(x),
  \int\limits^\infty_{-\infty}\mid dF(x)\mid < \infty
  $$ 
  then
  \begin{enumerate}[\rm (i)]
  \item $F(a+h)-F(a-h) = \lim\limits_{A \rightarrow
    \infty}\frac{1}{\pi}\int\limits^A_{-A} \frac{\sin~ht}{t}e^{-iat} \varphi(t)
    dt$ if $F(x)$ is continuous at a $\pm  h$.     
    
  \item $\int\limits^{a + H}_{a} F(x)dx - \int\limits^{a}_{a-H} F(x)dx =
    \frac{1}{\pi}~~ \int\limits^{\infty}_{-\infty} ~~ \frac {1-cos
      Ht}{t^2}e^{-iat} \varphi (t)dt$.  
  \end{enumerate}
\end{theorem}\pageoriginale

\begin{coro*}
  The expression of a function $\varphi (t)$ as an 
  absolutely convergent Fourier - Stieltjes integral in unique. In 
  particular, a distribution function is defined uniquely by its
  characteristic function. 
\end{coro*}

\begin{proof}
  \begin{align*}
    \frac {1}{\pi} \int\limits^A_{-A} \cdot \frac{\sin ~\text{ht}}{t} e^{-iat}
    ~\varphi (t) \text{dt}
    &= \frac {1}{\pi} \int\limits^A_{-A} \frac{\sin \text{ht}}{t} e^{-iat}
    \text {dt} \int\limits^\infty _{-\infty} e^{itx} \text {dF}(x) \\ 
    &= \frac {1}{\pi} ~\int\limits^\infty _{-\infty} \text {dF(x)}
    \int\limits^A_{-A}  \frac{\sin \text{ht}}{t}  e^{it(X-a)} \text{dt}\\  
    &= \frac {2}{\pi} ~\int^\infty_{-\infty} \text{dF(x)}
    \int\limits^A_o \frac{\sin \text{ht} \cos ((x-a)t)}{t} \text{dt} 
  \end{align*}
  But
  \begin{align*}
    &= \frac {2}{\pi} \int\limits^\infty_0  \frac {\sin \text{ht} \cos
      (x-a)t}{t} dt 
    = \frac {1}{\pi} \int\limits^\infty _0 \frac{\sin(x-a+h)t}{t} dt\\ 
    &\hspace{5cm}-\frac{1}{\pi}  \int\limits^A_0 \frac {\sin (x-a-h)t}{t}dt\\
    &= \frac {1}{\pi}  \int\limits^{A(x-a+h)}_{0} \frac{\sin t}{t} dt = \frac
    {1}{\pi} \int\limits^{A(x-a+h)}_{0} \frac{\sin t}{t} dt - \frac {1}{\pi}
    \int\limits^{A(x-a+h)}_{A(x-ah)} \frac{\sin t}{t}dt 
  \end{align*}
  and\pageoriginale 
  $$ 
  \frac {1}{\pi} \int^{T}_{0} \frac{\sin t}{t} 
        {dt} \rightarrow \frac{1}{2}, \frac {1}{\pi} \int_{-T}^{0} \frac{\sin
          t}{t} ~dt \rightarrow \frac{1}{2} ~\text{as}~ T \rightarrow 
        \infty.
  $$ 

  It follows that 
  $$ 
  \text {A} \lim\limits_{\rightarrow \infty}  \frac {1}{\pi}
  \int^{A(x-a+h)}_{A(x-a-h)} \frac{\sin t}{t} dt =
  \begin{cases} 
    0 & {\rm if}~ x > a + h \\ 
    1 & {\rm if}~ a-h<x<a+h \\ 
    0 & {\rm if}~ x<a-h 
  \end{cases}
  $$
  and since this integral is bounded, it follows from the Lebesgue
  convergence theorem that 
  \begin{align*} 
    A \lim\limits_{\rightarrow \infty} \frac {1}{\pi} &
    \int^{A}_{-A} \frac{\sin~ht}{t} e^{-iat} \varphi(t)dt\\ 
    &= \int^{a+h}_{a-h} dF(x) = F(a+h) - F(a-h) 
  \end{align*}
  provided that $F(x)$ is continuous at a $\pm h$.
\end{proof}

Since the integral on the left is bounded in $\mid h \mid \le H$,
we can apply Lebesgue's theorem to its integral with respect 
to $h$ over $\mid  h \mid \le H$, and (ii) follows. 


\section[Sequences and limits of...]{Sequences and limits of distribution and
    characteristic functions}\label{chap2:sec6} 

\begin{theorem}\label{chap2:sec6:thm15}  %%% 15
  If $\varphi (t)$, $\varphi_n (t)$ are the characteristic functions
  of distribution functions $F(x)$ and $F_n (x)$, and if
  $F_n (x)\rightarrow F(x)$ at  
  every point of continuity of $F(x)$, then $\varphi_n (t) \rightarrow
  \varphi (t)$ uniformly in  
  any finite interval.
\end{theorem}

\begin{proof}
  Let  $\in > 0$ and choose $X$, $N$ $(\in)$ so that $\pm X$ are 
  points\pageoriginale of continuity of $F(x)$ while
  $$
  \left(\int^{-X}_{- \infty} + \int^{\infty}_{X}\right) d  F(x) < \in/2,
  \left(\int^{-X}_{- \infty} + \int^{\infty}_{X} d F_n(X)\right) < \in/2  
  \text{ for }n \ge N. 
  $$
  
  This is possible since the first inequality is clearly satisfied for
  large $X$ and 
  $$
  \left(\int^{-X}_{- \infty} + \int^{\infty}_{X}\right) d F (X) = F_n (-X-0)
  + 1 - F_n (X + 0).
  $$ 
  
  Since $F_n$ is a distribution function and as $n \rightarrow \infty$ this
  $$
  \rightarrow F(-X) + 1 - F (X) = \left(\int^{-X}_{- \infty} +
  \int^{\infty}_X\right) dF (X) < \in/2 
  $$
  
  Since $F(x)$ is continuous at $\pm ~X$.
  
  Then 
  \begin{align*}
    \mid \varphi_n(t) - \varphi(t)\mid & \le \in + \mid
    \int^X_{-X}e^{itx} d(F_n(X)-F(X)) \mid\\
    = \in + &\bigg| \int^X_{-X} \left[e^{itx} (F_n(x)-F(x))\right]-
    \int^X_{-X} ite^{itx}(F_n(x)-F(x))dx \bigg| \\ 
    \le  \in  + &\mid F_n (X-0) - F(X-0) \mid + \mid F_n (-X+0) - F(-X+0) \mid\\
    + & \mid t \mid \int^X_{-X} \mid F_n (x)-F(x)\mid dx
    \le \in ~~+ 0 (1) ~\text {as}~ n \rightarrow \infty
  \end{align*}
  uniformly in any finite interval of values t, by Lebesgue's theorem. 
\end{proof}
 The converse theorem is much deeper.
 
\begin{theorem}\label{chap2:sec6:thm16} %%% 16 
  If\pageoriginale $\varphi_n (t)$ is the characteristic function of the distribution
  function $F_n (x)$ for $n=1,2, \dots$ and
  $\varphi_n(t)\rightarrow\varphi(t) $  
  for all $t$, where $\varphi (t)$ is continuous at 0, then $\varphi
  (t)$  is continuous at 0, then $\varphi$ is the  characteristic
  function of a distribution function $F(x)$ and 
  $$
  F_n(x) \rightarrow F(x)
  $$
  at every continuity point of $F(x)$.
\end{theorem}
We need the following

\begin{lemma}
  An infinite sequence of distribution functions $F_n (x)$ contains a
  subsequence $F_{n_k}(x)$ tending to a non-decreasing limit 
  function $F(x)$ at every continuity point of $F(x)$. Also
  $$ 
  0 \leq F(x) \leq 1.
  $$
  (but $F(x)$ is not necessarily a distribution function).
\end{lemma}

\begin{proof}
  Let  $\big\{r_m \big\}$ be the set of rational numbers arranged in a
  sequence. Then the numbers $F_n (r_1)$ are bounded and we can select  
  a sequence $n_{11}, n_{12},\dots $ so that $F_{n_{1 \nu}}(r_1)$
  tends to a limit as $\gamma \rightarrow \infty$ which we denote by
  $F(r_1)$.  The sequence $(n_{1 \nu})$ then 
  contains a subsequence $(n_{2 \nu})$ so that  $F_{n_{2 \nu} }(r_2)
  \rightarrow F (r_2)$ and we define by induction sequences $(n_{k\nu}),
  (n_{k+1,\nu})$ being a subsequence of $(n_{k \nu})$ so that
  $$
  F_{n_{k \nu}} (r_k)\rightarrow F(r_k)  \text{as} \nu \rightarrow
  \infty 
  $$
  
  If we then define $ n_k =n_{kk}$, it follows that
  $$ 
  F_{n_k} (r_m)\rightarrow F(r_m) ~\text {for all}~ m.
  $$
  Also,\pageoriginale $F(x)$ is non-decreasing on the rationals and it can be defined
  elsewhere to be right continuous and non-decreasing on the reals.  
  The conclusion follows since $F(x)$, $F_{n_k}  (x)$ are non-decreasing
  and every $x$ is a limit of rationals. 
\end{proof}


\noindent \textbf{\underline{\large Proof of the theorem}:} We use the
lemma to define a 
  bounded non-decreasing function $F(x)$ and a sequence $(n_k)$ so that 
$F_{n_k} (x) \rightarrow F(x)$ at every continuity point of $F(x)$.

If we put $a=0$ in Theorem 14 (ii), we have 
$$
\int^H_0 F_{n_k} (x) dx - \int^\circ_{-H} F_{n_k} (x)dx=\frac {1}
    {\pi} \int\limits^\infty_{-\infty} \frac {1-\cos Ht} {t^2} \varphi_{n_k}
    (t) \text{dt} 
$$
and if we let $k \rightarrow \infty$  and note tat  $\frac{1-\cos Ht}
{t^2}   \varepsilonup  L (- \infty, \infty) $ and that  $ F_{n_k}$ ,
$\varphi_{n_k}(t) \text {are bounded, we get} $  
\begin{align*}
  \frac {1} {H}  \int^H_\circ F(x)dx-\frac{1} {H} \int^{\circ}_{-H}
  F(x)dx & = \frac{1} {\pi H} \int\limits^\infty_{-\infty} \frac {1-\cos Ht} {t^2}
  \varphi (t) \text {dt}\\ 
  & = \frac{ 1} {\pi} \int\limits^\infty_{-\infty}  \frac {1-\cos t} {t^2}
  \varphi  (\frac{t}{H})  dt.
\end{align*}

Now, since $\varphi (t)$ is bounded in $(- \infty, \infty)$ and
continuous at 0, the expression on the right tends to $\varphi (0)=
\lim\limits_{k \rightarrow \infty} \varphi_{n_k} (0)=1$ as 

H $\rightarrow \infty $ . Since F(t) is non-decreasing, it easy to
show that the left hand side tends to $F (\infty)- F(-\infty)$ and
hence we have
$$
F(\infty)-F (-\infty)=1,
$$
and $F(x)$ is a distribution function. It now follows from Theorem 15
that $\varphi$ is the  characteristic function of $F(x)$.

Finally,\pageoriginale unless $F_n(x)\rightarrow F(x)$ through the entire sequence
we can define another subsequence $(n^*_k)$ so that $F^*_{n_k} (x)
\rightarrow F^* (x)$ and the same argument shows that $F^* (x)$ is a
distribution function and that 
$$
\varphi (t)= \int\limits^\infty_{-\infty} e^{itx}  dF^* (x)
$$

By the corollary to Theorem 13, $F(x)=F^* (x)$, and it follows
therefore that $F_n (x)\rightarrow F(x)$ at every continuity point of
$F(x)$. 

\section{Examples of characteristic functions}\label{chap2:sec7} 

\begin{theorem}\label{chap2:sec7:thm17}  %%% 17
\begin{enumerate}[\rm (i)]
\item The  binomial distribution $p_\nu=(^n_\nu) p^\nu p^{n-\nu},
  \nu=0, 1,\break 2, \dots$ has the distribution function
  $$
  F(x)=  \sum\limits_{\nu \leq x} p_\nu
  $$
  and the characteristic function 
  $$
  \varphi (t) = (q+pe^{it})^n
  $$
\item The Poisson distribution $p_\nu =e^{-c} \frac {c^{\nu}} {\nu !}
  , \nu =0 , 1, 2, \dots$ has distribution function
  $$ 
  F(x)= \sum\limits_{\nu \leq x} p_\nu 
  $$
  and characteristic function 
  $$ 
  \varphi (t)= e^{c(e^{it}-1)}
  $$
\item The rectangular distribution $F' (x)= \frac{1} {b-a}$ for $a<
  x< b$, 0 for $x < a $,  $x > b$, 0 for $x < a$,   $x > b$,  has the
  characteristic function  
  $$ 
  \varphi (t) =\frac{e^{itb}-e^{ita}} {(b-a) it} 
  $$
\item The\pageoriginale normal distribution $F'(x)=\frac{1}{\sigma \sqrt {2 \pi}}
  e^{-x^{2 / 2} \sigma^2}$ has characteristic function  
  $$
  \varphi(t)=e^{-t^2 \sigma^{2/2}}
  $$
\item The singular distribution
  \begin{align*}
    F(x) = D(x-a) & = 0, x < a\\
    & = 1, x \ge a
  \end{align*}
  has characteristic function 
   $$
   \varphi(t) = e^{ita}
   $$
   If $a = 0$ , $\varphi (t) = 1$. 
\end{enumerate}
\end{theorem}
These are all trivial except (iv) which involves a simple contour integration.

As a corollary we have the 

\begin{theorem}\label{chap2:sec7:thm18}  %%%% 18
\begin{enumerate}[\rm (i)]
\item The sum of independent variables with binomial distributions
  $(p, n_1), (p, n_2)$ is binomial with parameters $(p, n_1 + n_2)$.  
\item  The sum of independent variables with Poisson distributions
  $c_1, c_2$ is Poisson and has parameter $c_1 + c_2$. 

\item The sum of independent variables with normal
  distributions $(\overline{x_1}, \delta)\break (\overline{x_2}, \delta_2)$
  has normal distribution $(\overline{x_1}+\overline{x_2}, \sigma),
  \sigma^2 = \sigma_1^2 + \sigma_2^2$.  

  We have also the following trivial formal result.
\end{enumerate} 
\end{theorem}

\begin{theorem}\label{chap2:sec7:thm19}  %%%%% 19
  If $x$ is a random real number with characteristic function
  $\varphi(t)$, distribution function $F(x)$, and if $A$, $B$ are constants,
  then $Ax+B$ has distribution function $F(\frac{X-B}{A})$ if $A > O$ and
  $1-F(\frac{X-B}{A}+0)$ 
  if\pageoriginale $A < 0,$ and characteristic function $e^{itB} \varphi(At)$. In
  particular $-x$ has the characteristic function
  $\varphi(-t)=\overline{\varphi(t)}$.  
\end{theorem}

\begin{coro*}
If $\varphi(t)$ is a characteristic function, so is 
    $$ 
    \mid \varphi (t) \mid^2 =\varphi(t) \varphi(-t).
    $$

The converse of Theorem 18, (ii) and (iii) is deeper. We sate it
without proof. For the proof reference may be made to ``Probability
Theory'' by M. Loose, pages 213-14 and 272-274. 
\end{coro*}

\begin{theorem}\label{chap2:sec7:thm20}  %%% 20
  If the sum of two independent real variables is normal (Poisson), then
  so is each variable separately. 
\end{theorem}

\section{{Conditional probabilities}}\label{chap2:sec8} 

If $x$ is a random variable in $\mathfrak{X}$ and $C$ is a subset of
$\mathfrak{X}$ with positive measure, we define 
$$
P(X/C)= \mu (XC) / \mu (C)
$$
to be the \textit{conditional probability that $x$ lies in $X$, subject to the
condition that $x$ belongs to $C$}. It is clear that $P(X/C)$ is probability
measure over all measurable $X$. 

\begin{theorem}[Bayes' Theorem]\label{chap2:sec8:thm21}  %%% 21
  Suppose that $\mathfrak{X}= \sum\limits_{j=1}^J c_j, \mu (c_j)>0$. Then    
  $$
  P(c_J/X) = \frac{P(X/C_j) \mu (C_j)}{{\sum\limits^{J}_{j=1}
      P(X/C_i) \mu (C_i)}}
  $$
  
  The proof follows at once from the definition. In applications, the
  sets $C_j$ are regarded as \textit{hypotheses}, the numbers $\mu (c_j)$ being
  called the \textit{prior probabilities}. The numbers $P(C_j/X)$ are
  called their post  
  \textit{probabilities}\pageoriginale or \textit{likelihoods} under the
  \textit{observation} of the event $X$. 
\end{theorem}

\begin{example}
  Two boxes $A$, $B$ are offered at random with (prior)
  probabilities 1/3, 2/3. $A$ contains 8 white counters, 12 red counters,
  B contains 4 white  and 4 red counters. $A$ counter is taken at random
  from a box offered. If it turns out to be white, what is the
  likelihood that the box offered was $A$ ? 
\end{example}

If we denote the event of taking a  red (white) counter by $R(W)$ the
space $\mathfrak{X}$ under consideration has four points $(A, R)$, $(A,
W)$, $(B, R)$, $(B, W)$. The required likelihood is. 
$$ 
{P(A/W) }\; =  \frac{P(W/A) \;\mu\; (A)}{P (W/A) \; \mu\; (A)
  +P(W/B) \mu (B)}
$$ 
Here

$
\begin{aligned}
P(W/A) & = \text{probability of taking white counter from A}\\
& = 8/20 = 2/5\\
P(W/B) & = 4/8 = 1 / 2 
\end{aligned}$

\noindent Hence
$$ 
P(A/W) = \frac{\frac{2}{5}
  \frac{1}{3}}{\frac{2}{5}\frac{1}{3}+\frac{1}{2}\frac{2}{3}}= 2/7.
$$

Thus the likelihood that the box offered was A is 2 /7.

Conditional probabilities arise in a natural way if we think of
$\mathfrak{X}$ as a product space $\mathfrak{X}_1 x \mathfrak{X}_2$ in
which a measure $\mu$(not generally a product measure) is
defined. Then if we write $(P(X_2 /X_1)$ as the conditional
probability that  $x_2 \in X_2$ with respect to the condition $x_1 \in
X_1$, we have 
$$
\displaylines{\hfill
  P(X_2/X_1)=\mu(X_1 ~x~ X_2)/ \mu_1(X_1)\hfill\cr
  \text{where}\hfill
  \mu_1(X_1)=\mu(X_1 ~x~ \mathfrak{X}_2).\hfill }
$$\pageoriginale

The set $X_1$, may reduce to a single point $x_1$, and the 
definition remains valid provided that $\mu_1(x_1)> 0$.  But usually
$\mu_1(x_1)=0$, but the conditional probability with respect to a single
point is not difficult to define. It follows from the Randon-Nikodym
theorem that for fixed $X_2,\mu(X_1 x X_2)$ has a Radon derivative which
we can write $R(x_1, X_2)$ with the property that
$$
\mu(X_1 ~x~ X_2)=\int_{x_1} R(x_1, X_2)d\mu_1
$$
for all measurable $X_1$. For each $X_2, R(x_1,X_2)$ is defined for almost
all $x_1$ and plainly $R(x_1,\mathfrak{X}_2)=1$ p.p. But unfortunately,since
the number of measurable sets $X_2$ is not generally countable, the 
union of all the exceptional sets may not be a set of measure zero.
This means that we cannot assume that, for almost all $x_1,R(x_1,X_2)$
is a measure defined on all measurable sets $X_2$. If how ever, it is ,
we write it $P(X_1/x_1)$ and call it that conditional probability that
$x_2 \mathcal{E}X_2$ subject to the condition that $x_1$ has a specified value.

Suppose now that $(x_1,x_2)$ is a random variable in the plane
with probability density$f(x_1,x_2)(i.ef(x_1,x_2)\geq 0$ and 
$\iint f(x_1,x_2)dx_1 dx_2=1)$. Then we can define \textit{conditional
  probability densities} as follows:
$$
P(x_2/x_1)=\frac{f(x_1,x_2)}{f_1(x_1)} f_1(x_1)=
\int\limits^{\infty}_{-\infty} f(x_1,x_2)dx_2
$$\pageoriginale
provided that $f_1(x_1)>0$.

The \textit{conditional expectation} of $x_2$ for a fixed value of $x_1$ is 
$$
m(x_1)=\int\limits^{\infty}_{-\infty} x_2 P(x_2/x_1)dx_2 =
\frac{\int\limits^{\infty}_{-\infty} x_2
  f(x_1,x_2)dx_2}{\int\limits^{\infty}_{-\infty} f(x_1,x_2)dx_2}
$$ 
The conditional standard deviation of $x_2$ for the value $x_1$ is $\sigma(x_1)$
where
\begin{align*}
  \sigma^2(x_1)= &\int\limits^{\infty}_{-\infty}
  (x_2-m(x_1))^2 P(x_2/x_1)dx_2\\ 
  & \frac{\int\limits^{\infty}_{-\infty} (x_2-m(x_1))^2
    f(x_1, x_2)dx_2}{\int\limits^{\infty}_{-\infty}f(x_1,x_2)dx_2}
\end{align*}

The curve $x_2=m(x_1)$ is called the \textit{regression curve of $x_2$
  on $x_1$}. 
It has following minimal property it gives the least value of 
$$
E(x_2-g(x_1))^2)={\iint_{RXR}}(x_2-g(x_1))^2 f(x_1,x_1) dx_1 dx_2
$$
for all possible curves $x_2=g(x_1)$.  If the curves $x_2=g(x_1)$ are 
restricted to specified families, the function which minimizes $E$ in 
that family. For example, the linear regression is the line $x_2=Ax_1+B$
for\pageoriginale which $E((x_2-Ax_1-B)^2)$ is least, the $n^{th}$ degree polynomial
regression is the polynomial curve of degree $n$ for which the 
corresponding $E$ is least.

\section{Sequences of Random Variables}\label{chap2:sec9} 

We can define limit processes in connection with sequences
of random variables in several different ways. The simplest is the 
convergence of the distribution or characteristic functions, $F_n(x)$
or $\phi_n(t)$, of random real numbers $x_n$ to a limiting distribution
or characteristic function $F(x)$ or $\phi(t)$.  As in
Theorem \ref{chap2:sec6:thm15}, it is 
sufficient to have $\lim\limits_{n \longrightarrow \infty}
F_n(x)=F(x)$ at every continuity point  
of $F(x)$.  Note that this does not involve any idea of a limiting random
variable. If we wish to introduce this idea, we must remember that it 
is necessary, when making probability statements about two or more
random variables, to specify their joint probability distribution in 
their product space.

There are two important definitions based on this idea. We
say that a sequence of random variables $x_n$ \textit{converges in probability}
to a limit random variable $x$, and write
$$ 
\displaylines{\hfill 
x_n\longrightarrow x  ~\text{ in prob}.\hfill \cr
\text{if}\hfill 
\lim\limits_{n \longrightarrow \infty} P(|x_n-x|>\epsilon) =0\hfill }
$$
for every $\epsilon>0$, $P$ being the \textit{joint} probability in
the product space  
of $x_n$ and $x$. In particular, if $C$ is a constant, $x_n\longrightarrow
C$ in prob, if  
$$ 
\lim\limits_{n \rightarrow \infty}  E (\mid x_{n} - x \mid^{\alpha}) =0.
$$\pageoriginale

 The  most important  case is that in which  $\alpha =2$.   The
 following result is got almost immediately from the definition. 
  
 \begin{theorem}\label{chap2:sec9:thm22}  %%% 22
   If $ F_{n} (X) $ is the
     distribution function of $x_{n}$ the necessary  and  sufficient
     condition that $ x_{n} \rightarrow  0 $  in prob. is that  
   \begin{align*}
     F_{n} (x)  \rightarrow  D(x) 
  \end{align*}  
  where $D(x) =  0$, $x < 0; =1, x \ge 0$  is the singular
  distribution. The  necessary and sufficient condition that $
  x_{n}\rightarrow 0$ in mean of  order  $ \alpha$  is that 
  \begin{align*}
    \lim\limits_{n \rightarrow \infty} \int\limits^{\infty}_{-\infty} \mid
    x \mid^{\alpha}  d F_{n} (x) =0.  
  \end{align*}  
 \end{theorem}

\begin{theorem}\label{chap2:sec9:thm23}  %%%% 23
   \begin{enumerate}[\rm (i)]
   \item If $x_{n} \rightarrow x $ in prob., then  $F_{n} (x)
     \rightarrow  F(x)$. 
   \item If  $x_{n} \rightarrow x $ in  mean, then  $
     x_{n}\rightarrow  x$ in prob. and  $F_{n}(x) \rightarrow  F(x)$. As  
    corollaries we have
   \end{enumerate}
\end{theorem}

\begin{theorem}[Tchebycheff]\label{chap2:sec9:thm24}
  If $x_{n}$ has mean $\overline{x}_{n}$ and standard  deviation
  $\sigma_{n}$  then $x_{n}- \overline{x}_{n} \rightarrow  0$ in
  prob. if $\sigma_{n}   \rightarrow   0$.  
\end{theorem}

\begin{theorem}[Bernoulli: Weak law of large
    numbers]\label{chap2:sec9:thm25} %%% 25 
  If $\xi_{1},         \xi_{2},....$.  are independent random
  variables with means $ \overline{\xi}_{1}$, 
  $\overline{\xi}_{2},\ldots $ 
  and standard  deviations  $ \sigma_{1},  \sigma_{2},.. $ and if 
  $$
  \displaylines{\hfill
    x_{n} = \frac{1}{n} \sum \limits^{n}_{\nu=1}  \xi_{\nu}    m_{n} =
    \frac{1}{n}  \sum \limits^{n}_{\nu =1} \overline{\xi}_{\nu} \hfill
    \cr 
    \text{then}\hfill  
    X_{n} -m_{n} \rightarrow  0  ~\text{in prob. if}~ \sum \limits^{n}_{\nu=1}
    \sigma^{2}_{\nu}  = 0 (n^{2}) \hfill}
  $$
\end{theorem}
\pageoriginale

\begin{theorem}[Khintchine]\label{chap2:sec9:thm26} %%% 26
  If $ \xi_{\nu}$ are independent random variables with the same
  distribution  function and finite  mean  $m$, then 
  $$
  x_{n} = \frac{1}{n} \sum\limits^{n}_{\nu=1} \xi \rightarrow m ~\text{in prob.}
  $$
  (Note that  this cannot be  deduced from
  Theorem \ref{chap2:sec9:thm25} since we do not 
  assume  that $ \xi_{\nu}$ has finite  standard  deviation.) 
\end{theorem}

\begin{proof}
  Let $\phi  (t)$  be the characteristic  function  of $\xi_{\nu}$
  so that the characteristic function of $x_{n}$ is  $(\phi
  (t/n))^{n}$. If 
  $$
  \displaylines{\hfill
  \phi (t) = \int\limits^{\infty}_{-\infty} e^{itx}  d F(x),\hfill \cr
  \text{we have }\hfill
  \phi (t) -1 - m i t  = \int\limits^{\infty}_{-\infty} \left(e^{itx} -1-
  itx\right)  dF(x).\hfill} 
  $$
  
  Now $\bigg| \frac{e^{itx}-1- itx}{t} \bigg|$  is  majorised by a
  multiple of $\mid x \mid $ and $ \rightarrow 0 $ as $t \rightarrow 0 $
  for each $x$. 
  
  Hence, by Lebesgue's theorem,
  $$
  \phi (t)  - 1 - \text{mit}  = \sigma (t)  ~\text{as}~ t \rightarrow  0. 
  $$

  Thus 
  $$
  (\phi (t/n))^{n}  = \left[1 + \frac{\text{mit}}{n} +  0
    \left(\frac{1}{n}\right)\right]^{n} \rightarrow e^{\text{mit}} as
  n \rightarrow  \infty,   
  $$
  and since $e^{\text{mit}}$ is the  characteristic  function of $D(x-m)$  the
  conclusion follows easily. 
\end{proof}

 In\pageoriginale these definitions we need only the joint distribution of $x$ and
 each $x_{N}$  separately. In practice, of course, we may know  
 the joint distributions of some of the $x_{n}$  $s$ (they may be
 independent, for example), but this is not necessary. 
 
 
 On the other hand, when we come to consider  the notion of a random
 sequence, the appropriate probability space is the infinite  
 product space of all the separate variables. This is a deeper
 concept than those we have used till now and we shall treat  it later
 as a special case of the theory of random functions.
 
\section{The Central Limit Problem}\label{chap2:sec10}
 
  We  suppose  that 
   $$
    x_{n} =  \sum \limits_{\nu} x_{n\nu}
   $$
is a finite sum of independent  random  real variables $x_{n_\nu}$ and
that $F_{n_\nu} (x)$, $F_{n\nu} (x)$, $\phi_{n_\nu}(t)$, $\phi_{n{\nu}}
(t)$ are  the  associated distribution and characteristic
functions. The  general  central limit problem is to find 
 conditions under which $F_{n}(x)$  tends  to some limiting function
 $F(x)$  when each of the components  $x_{n_\nu}$  is small  
(in a sense  to be  defined later)  in  relation to $x_{n} $.  Without the
 latter condition, there is no general result of this kind. 
Theorems  \ref{chap2:sec9:thm25} and \ref{chap2:sec9:thm26} show that $F(x)$  may take  the special  form $D(x)$
and the next  two theorems show that the Poisson and normal 
forms  are  also admissible. The  general problem includes that  of
finding  the most  general class of such functions. The problem goes  
back to Bernoulli  and Poisson  and  was solved (in the case  of $R$)
by Khintchine and P. L\'evy. 

\begin{theorem}[Poisson]\label{chap2:sec10:thm27} %%%% 27
  The\pageoriginale binomial distribution $P(x = \nu) = (^n_\nu) p^{\nu} q^{n-\nu}$,
  $p= - \frac{c}{n}$, $q=1-p$, $c$ constant, tends to the Poisson distribution
  with mean $c$ as $n \rightarrow \infty$. 
\end{theorem}

\begin{proof}
  \begin{align*}
    \varphi_n (t) & = (q + pe^{it})^n\\
    & = \frac{{\it [} 1 + c(e^{it} -1) {\it ]^n}}{n}\\
    & \quad \rightarrow e^{c(e^{it} -1)}
  \end{align*}
  which, after Theorem \ref{chap2:sec6:thm16}, is sufficient.	
\end{proof}

\begin{theorem}[De Moivre] \label{chap2:sec10:thm28}%%%% 28
  If $\xi_1$ , $\xi_2 \cdots$ are independent random variables with the
  same distribution, having mean 0 and finite standard deviation
  $\sigma$, then the distribution of  
  $$ 
  x_n = \frac{\xi_1 + \xi_2 + _{\cdots} + \xi_n}{\sqrt{n}}
  $$ 
  tends to the normal distribution (0, $\sigma^2$).

  This is proved easily using the method of Theorem \ref{chap2:sec9:thm26}.
\end{theorem}

The general theory is based on a formula due to Khintchine and Levy,
which generalizes an earlier one for distributions of finite variance
due to Kolmogoroff.
	
We say that $\psi (t)$ is a $K-L$ function with representation $(a, G)$
if 
$$
\psi (t) = i a t + \int\limits^\infty_{-\infty}  \left[
  e^{itx}- 1 - \frac{itx}{1+x^2} \right] \frac{1+x^2}{x^2}dG(x)
$$
where\pageoriginale $a$ is a real number and $G(x)$ is bounded and non-decreasing in 
($- \infty$ , $\infty$). The value of the integrand at $x=0$ is defined to
be -$\frac{1}{2}t^2$ and it is then continuous and bounded in 
$-\infty < x < \infty$ for each fixed $t$.

\begin{theorem}\label{chap2:sec10:thm29} %%% 29
  A $K-L$ function $\psi (t)$ is bounded in every finite interval and
  defines $a$, $G$ uniquely. 
\end{theorem}

\begin{proof}
  The first part is obvious. 
  
  If we define
  $$
  \theta (t)  = \psi (t) -\frac{1}{2} \int_{-1}^1 \psi
  (t+u)du = \frac{1}{2} \int^1_{-1}(\psi(t)-\psi(t+u))du,
  $$
  we have
  \begin{align*}
    \theta(t)  & = \frac{1}{2} \int^1_{-1} du
    \int\limits_{-\infty}^\infty
    \left(e^{itx}(1-e^{iux})\frac{iux}{1+x^2}\right)\frac{1+x^2}{x^2}dG(x)\\ 
    & = \int\limits_{-\infty}^\infty e^{itx}\left( 1-\frac{\Sin
      x}{x} \right) \frac{1+x^2}{x^2} dG(x)=
    \int\limits_{-\infty}^\infty e^{itx} dT(x)
  \end{align*}
  where
  \begin{align*}
    T(x) & = \int\limits_{-\infty}^x  \left[ 1-\frac{\sin~ y}{y}
      \right] \frac{1+y^2}{y^2} dG(y) ,\\ 
    G(x) & = \int\limits_{-\infty}^x
    \frac{y^2}{(1-siny/y)(1+y^2)} dT(x) 
  \end{align*}
  since $\left[1-\frac{\sin y}{y}\right]\frac{1+y^2}{y^2}$ and its
  reciprocal are bounded. 
\end{proof}

	This proves the theorem since $G(x)$ is defined uniquely by $T(x)$
        which is defined uniquely $\psi (t)$ by Theorem 14 and this in
        turn is defined uniquely by $\psi(t)$.
	
	The\pageoriginale next theorem gives analogues of theorems
        \ref{chap2:sec6:thm15} and \ref{chap2:sec6:thm16}. We
        shall write $G_n \rightarrow G$ if $G_n(x)$ and $G(x)$ are bounded and
        increasing and $G_n (x)\break G(x)$ at every continuity point of $G(x)$ and
        at $\pm \infty$
	
\begin{theorem}\label{chap2:sec10:thm30} %%% 30
  If $\psi_n (t)$ has $K-L$ representation $(a_n, G_n)$ for each n
  and if $a_n \rightarrow a$, $G_n \rightarrow G$ where $G(x)$ is
  non-decreasing and bounded, then $\psi_n(t) \rightarrow
  \psi (t)$ uniformly in any finite interval. 
  
  Conversely, if $\psi_n(t) \rightarrow \psi(t)$ for all $t$ and
  $\psi (t)$ is continuous at 0, then $\psi (t)$ has a $K-L$ representation
  $(a, G)$ and $a_n \rightarrow a$, $G_n \rightarrow G$. 
\end{theorem}

\begin{proof}
  The first part is proved easily using the method of Theorem
  \ref{chap2:sec6:thm15}. For the second part, define $\theta_n (t)$, $T_n (t)$ as in
  the last theorem. Then, since $\theta_n (t)\rightarrow
  \theta(t)= \psi(t)-\frac{1}{2} \int_{-1}^1 \psi (t+u)du$,
  which is continuous at 0, it follows from Theorem 16 that
  there is a non-decreasing bounded function $T(x)$ such that
  $T_n \rightarrow T$ . Then $G_n \rightarrow G$ where $G(x)$ is
  defined as in Theorem \ref{chap2:sec10:thm29}, and is bounded, and $\psi (t)$
  plainly has $K-L$ representation $(a, G)$ where $a_n \rightarrow 
  a$. 
\end{proof}

\begin{defi*} We say that the random
      variables $x_{n1}$, $x_{n2},\ldots$ are \textit{uniformly asymptotically
      negligible (u.a.n.)} if, for every $\in > 0$,   
      $$
      \sup\limits_\nu \int\limits_{|x| \geq \in} d
      F_n(x)\rightarrow 0 ~\text{as}~ n \rightarrow  \infty.
      $$  	 
\end{defi*}

 	The\pageoriginale condition that the variables are u.a.n implies that
        the variables  
 are ``centered'' in the sense that their values are concentrated near 0. 
 In the general case of u.a.n variables by considering the new
 variables  $x_{n_\nu}-C_{n_\nu}$.   
 Thus, we need only consider the u.a.n case, since theorems for this
 case can be   
 extended to the u.a.n. case by trivial changes. We prove an
 elementary result about u.a.n. variables first. 
 
 \begin{theorem}\label{chap2:sec10:thm31} %%% 31
The conditions 
 
 \begin{enumerate}[\rm (i)]
 \item $x_{n_\nu }$ are u.a.n.
 \item $\sup\limits_\nu \int\limits^\infty_{- \infty}\frac{x^2}{1+x^2}  d
   F_{n_\nu}\left(x+a_{n_\nu}\right) \rightarrow 0 $ for every set of
   numbers  $\left(a^{-\infty}_{n_\nu}\right)$ for which  $\sup\limits_\nu
   \big| a_{n\nu} \big|  \rightarrow 0$ as $n \rightarrow \infty$ 
 are equivalent and each implies that 
 \item $\sup\limits_\nu \mid \varphi_{n\nu}(t) -1\mid \rightarrow 0$ as $n
  \rightarrow \infty $, uniformly in every finite  $t-$interval.
  \end{enumerate}
 \end{theorem}

\begin{proof}
   The equivalence of (i) and (ii) follows from the inequalities
   \begin{align*}
     \int\limits^{\infty}_{-\infty} \frac{x^2}{1+x^2}dF_{n\nu}(x+a_{n_\nu})& \le  
     (\epsilon + \mid a_{n_\nu}\mid^2) +  \int\limits_{\mid x \mid \ge
       \epsilon}dF_{n_\nu} (x)\\ 
     \int\limits_{\mid x \mid \ge \epsilon} dF_{n_\nu} (x) &\le
     \frac{1+\epsilon^2}{\epsilon^2}  \int\limits_{\mid x \mid \ge
       \epsilon} \frac{x^2}{1+x^2}   dF_{n_\nu} (x).
   \end{align*}
   
   For (iii) we use the inequality $ \mid 1-e^ {itx} \mid \le \mid xt
   \mid if \mid xt \mid \le 1 $  and deduce that  
   $$ 
   \mid \varphi_ {n \nu}(t) -1 \mid = \mid \int\limits_{ -\infty
   }^\infty(e^{itx} - 1) dF_{n_\nu}(x) \mid \le \epsilon \mid t \mid +
   2 \int\limits_{\mid x\mid \ge \epsilon } dF_{n_\nu } (x). 
   $$
 \end{proof}
 
\begin{theorem}[The Central Limit Theorem]\label{chap2:sec10:thm32} %%% 32
   Suppose\pageoriginale that $x_{n\nu}$ are independent
   u.a.n. variables and that $F_n \rightarrow F$. Then. 
   \begin{enumerate}[\rm (i)] 
   \item $\psi (t) =\log \varphi$ $(t)$ is a $K-L$ function 
   \item If $\psi (t)$ has representation $(a, G)$ and the real numbers 
     $a_n $ satisfy
   \end{enumerate}
   $$ 
   \int\limits_{-\infty }^\infty \frac{x}{1+ x^2} d F_{n_\nu}(x+a_{n_\nu})=0, 
   $$
   and are bounded uniformly in $\nu$ then 
   \begin{align*}
     G_n(x) & = \int\limits_{-\infty}^x
     \frac{y^2}{1+y^2}dF_{n_\nu}(y+a_{n_\nu})\rightarrow G(x),\\
     a_n &= \sum\limits_{\nu}a_{n_\nu }\rightarrow a. 
   \end{align*}
   Conversely, if the conditions (i) and (ii) hold, then $F_n \rightarrow F$. 
\end{theorem}
 
 \begin{proof}
  It follows from the definition of the $ a_{n\nu}$ that
  $$ 
  \displaylines{\hfill
  e^{-ita_{n_\nu}}\phi_{n_\nu} (t) = \int\limits_{-\infty}^\infty
  e^{itx}d F_n (x+a_{n\nu}) = 1+\gamma_{n_\nu}(t)\hfill\cr
  \text{where}\hfill  
  \gamma_{\sqrt{n_\nu}}(t) = \int\limits_{-\infty}^\infty  \left(e^{itx}-1-
  \frac{itx}{1+x^2}\right)
  dF_{\sqrt{n_\nu}}(x+a_{\sqrt{n_\nu}}),\hfill }
$$
and
\begin{equation*}
\alpha _{\sqrt{n_\nu}}(t)=- \mathfrak{R}
  (\gamma_{\sqrt{n_\nu}} (t)) \ge 0 \tag{1}\label{chap2:sec10:thm32:eq1}
\end{equation*}

  It follow easily from the u.a.n. condition and the definition of
  $a_{n \nu}$  that $a_{\sqrt{n_\nu}}\rightarrow 0$ uniformly in $\nu$ as
  $n\rightarrow \infty$ and from Theorem \ref{chap2:sec10:thm31}
  that\pageoriginale 
  $\gamma_{n_\nu}(t)\rightarrow 0$ uniformly in $\nu$ for $\mid t \mid
  \le H$ where $H > 0$ is fixed. Hence 
  $$
  \log \varphi_{n_\nu}(t)=it a_{n_\nu}+\gamma_{n_\nu}(t)+0 (\mid
  \gamma_{n_\nu}(t)\mid^2),
  $$
the 0 being uniform in $\nu$ and , by addition,
\begin{equation*}
  \log \varphi_n (t) = ~\text{it}~ a_n + \sum_\nu
  \gamma_{n_\nu}(t)+0  [\sum \mid
    \gamma_{n_\nu}(t)\mid^2],\tag{2}\label{chap2:sec10:thm32:eq2} 
\end{equation*}
uniformly in $\mid t \mid \le H$. 

Now let 
\begin{align*}
  A_{\sqrt{n_\nu}} &= \frac{1}{2 h}\int_{-H}^{H} \propto_{n_\nu}(t) dt\\
  & = \int\limits_{- \infty}^\infty \left[ 1-\frac{\sin Hx}{Hx}
    \right] d F_{n_\nu} (x+a_{n_\nu}) 
\end{align*}

Using the inequality 
$$
\mid e^{itx}-1- \frac{itx}{1+x^2}\mid \le C(H) \left[1-\frac{\sin
    Hx}{Hx}\right] 
$$
for $\mid t \mid \le H$, we have
$$
\mid \gamma_{n_\nu}(t)\mid \le C A_{n_\nu},
$$
and therefore , taking real  parts in (\ref{chap2:sec10:thm32:eq2})
and using the fact that  
$\sup\limits_\nu \mid\gamma_{n_\nu}(t)\mid \rightarrow 0$ uniformly in
$\mid t \mid \le H$,  
$$
\sum_\nu \alpha_{n_\nu}(t)\le - \log \mid \varphi_n(t)\mid +
\circ \left(\sum_\nu A_{n_\nu}\right)
$$ 

This again holds uniformly in $\mid t \mid \le H$, and after
integration we get 
$$ 
\sum_{\nu} \ge - \frac{1}{2H} \int^H_{-H} \log  
\mid\varphi_n (t)\mid {dt} + \circ  \left(\sum A_{n_\nu}\right)  
$$
from\pageoriginale which it follows that $\sum_\nu A_{n_\nu}=0(1)$ and that
\begin{equation*}
  \log \varphi_n (t)= i~t~  a_n + \sum_\nu \gamma_{n_\nu}(t)+~0(1),
  \tag{3}\label{chap2:sec10:thm32:eq3}  
\end{equation*}
uniformly  for $|t|\leq H $, and the before, since $H$ is at our
disposal, for each real $t$. 

The first part of the conclusion follows from
Theorem \ref{chap2:sec10:thm30}.

For the converse, our hypothesis implies that $G_n
(\infty)\rightarrow G(\infty)$ and if we use the inequality 
$$
\bigg|e^{itx}-1-\frac {i t x}{1+ x^2}\bigg|\leq C(t)\frac{x^2}{1+x^2},
$$
it follows from (\ref{chap2:sec10:thm32:eq1}) that 
$$
\sum\limits_\nu |\gamma_{n_\nu}(t)|~\leq C(t) G_n (\infty)=0(1) 
$$
uniformly in $\gamma$. But $\gamma_{\nu}~(t)\rightarrow 0$ uniformly in
$\gamma$ for any fixed t so that (\ref{chap2:sec10:thm32:eq2}) and
(\ref{chap2:sec10:thm32:eq3}) remain valid and 
\begin{gather*}
  \log \varphi_n(t)\rightarrow ita +
  \int\limits^{\infty}_{-\infty} (e^{itx}-1-\frac{itx}{1+x^2})
  \frac{1+x^2}{x^2}~d G(x) \\
  =\psi (t)\log~ \varphi(t,)
\end{gather*}
since $ G_n \rightarrow G$. Hence $\varphi_n(t)\rightarrow \varphi_(t)
$ as we require. 
 \end{proof}

\begin{description}
\item[\underline{Notes on Theorem 32.}]
\end{description}

\begin{enumerate}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\item The first part of the theorem shows that the admissible limit
  functions for sums of u.a.n variables are those for which $\log \varphi(t)$
  is a $K-L$ function.
  
\item The numbers $a_{n_\nu}$ defined in stating the theorem always 
  exist\pageoriginale when $n$ is large since $\int\limits^\infty_{-\infty}
  \frac{x}{1+x^2} d F_{n_\nu} (x+ \xi)$ is continuous in $\xi$ 
  and takes positive and negative values at $\xi =\mp 1$
  when n is large. They can be regarded  as extra correcting terms 
  required to complete the centralization of the variables. The u.a
  n. condition centralizes each of them separately, but this is not
  quite enough. 
  
\item The definition of $a_{n_\nu}$ is not the only possible one. It is 
  easy to see that the proof goes through with trivial changes provided
  that the $a_{n_\nu}$ are defined so that $a_{n_\nu} \rightarrow 0$ and 
  $$ 
  \int\limits^\infty_{- \infty} \frac{x}{1+x^2}d F_{n_\nu} (
  x+a_{n_\nu})= 0(\int\limits^\infty_{-\infty} \frac{x^2}{1+x^2}d
  F_{n_\nu} (x+a_{n_\nu}))
  $$
  uniformly in $\nu ~\text{as}~ n \rightarrow \infty$, and this is easy to
  verify if we define $a_{n_\nu}$ by 
  $$ 
  a_{n_\nu}= \int\limits^\tau _{-\tau} x \,d F_n(x)
  $$
  for some fixed $\tau > 0$.  This is the definition used by Gnedenko
  and L\'evy. 
   
  WE can deduce immediately the following special cases.
\end{enumerate}

\begin{theorem}[Law of large numbers]\label{chap2:sec10:thm33} %%%% 33
  In order that $F_n$ should tend to the singular distribution with
  $F(x)=D(x-a)$ it is necessary and sufficient that 
  $$
  \sum _\nu \int\limits^\infty_{-\infty}  \frac{y^2}{1+y^2}~d~
  F_{n_\nu}(y+a_{n_\nu}) \rightarrow 0, \sum _\nu a_{n_\nu}
  \rightarrow a 
  $$
  (Here $\psi(t)= ita$, $G(x)=0)$.
\end{theorem}

\begin{theorem}\label{chap2:sec10:thm34}%%%% 34
In\pageoriginale order that $F_n$ should tend to the Poisson distribution with
parameter c, it is necessary and sufficient that  
$$
\sum_\nu a_{n_\nu} \to \frac{1}{2} ~c ~\text{and}~
\sum_\nu \int\limits^x_{-\infty} \frac{y^2}{1+y^2}~d ~F_{n_\nu}(y+
a_n)\rightarrow \frac{c}{2} ~ D(x-1)
$$ 

(Here $\psi(t)=c(e^{it}-1)$,  $a =\frac{1}{2}c, ~G(x)=\frac
{c}{2}~D(x-1)$) 
\end{theorem}

\begin{theorem}\label{chap2:sec10:thm35} %%% 35
  In order that $F_n$ should tend to the normal 
  $(\alpha,\sigma^2)$ distribution, it is necessary and sufficient that 
  $$
  \sum_\nu a_{n_\nu}\rightarrow \alpha ~\text{ and }~
  \int\limits^x_{-\infty} \frac{y^2}{1+y^2} d F_{n \nu}(y+a_{n_\nu})
  ~ \rightarrow~\sigma^2 D(x).  
  $$ 
  
  (Here $\psi(t)=i~t \alpha-\frac{\sigma^2 t^2}{2}~, a=\alpha ~,
  G(x)=\sigma^2 D(x)$.  From this and the note (c) after
  Theorem \ref{chap2:sec10:thm33}, it
  is easy to deduce 
\end{theorem}

\begin{theorem}[Liapounoff]\label{chap2:sec10:thm36} %%% 36
  If $x _{n \nu}$ has mean o and finite variance $\sigma^2_{n \nu}$\break
  with $\sum _\nu \sigma^2 _{n \nu}=1$ 
  a necessary and sufficient condition that $x_n$ should tend to
  normal $(0, 1)$ distribution is that for every $\in > 0$, 
  $$
  \sum_\nu \int\limits_{|\rm x|\ge \in} x^2 ~d~F_{n\nu}~(
  x)\rightarrow 0 ~\text{as}~ n \to \infty.  
  $$
  
  The distributions for which log $\varphi(t)$ is a $K-L$ function can be
  characterized by another property. We say that a distribution is   
  \textit{infinitely divisible} (i.d.) if, for every $n$ we can write
  $$
  \phi(t) = (\phi_n(t))^n
  $$\pageoriginale
  where $\phi_n  (t)$ is a characteristic function.  This means that it
  is the distribution of the sum of $n$ independent random variables with
  the same distribution. 
\end{theorem}

\begin{theorem}\label{chap2:sec10:thm37} %%%% 37
A distribution is i.d. if and only if $\log \phi(t)$ is a $K-L$ function.

  This follows at once from Theorem \ref{chap2:sec10:thm32}.  That the condition that a
  distribution be i.d. is equivalent to a lighter one is shown by the
  following 
\end{theorem}

\setcounter{corollary}{0}
\begin{corollary}\label{chap2:sec10:thm37:coro1}
$\phi (t)$ is i.d. if there is a sequence of decompositions (not
  necessarily with identically distributed components) in which the
  terms are u.a.n.  
\end{corollary}

\begin{corollary}\label{chap2:sec10:thm37:coro2}
If a distribution is i.d., $\phi (t) \neq 0$.
\end{corollary}

\begin{theorem}\label{chap2:sec10:thm38} %%% 38
A distribution is i.d. if and only if it is the limit of finite
compositions of u.a.n.  Poisson distributions.   
\end{theorem}

\begin{proof}
The result is trivial in one direction.  In the other, we observe that
the integral 
$$
\int\limits^\infty_{-\infty} \left(e^{itx} - 1 -
\frac{itx}{1+x^2}\right) \frac{1+x^2}{x^2} d G(x)
$$
can be interpreted as a Riemann - Stieltjes integral and is the limit 
of finite sums of the type 
$$
\sum_j   b_j\left(e^{it \xi_j} -1 - \frac{it
  \xi_j}{1+\xi^2_j}\right)
$$
each\pageoriginale term of which corresponds to a Poisson distribution.
\end{proof} 

\section{Cumulative sums}\label{chap2:sec11}

Cumulative sums
$$
x_n = \frac{\xi_1 + \xi_2 + \ldots + \xi_n}{n}
$$
in which $\xi_1, \xi_2, \dots \xi_n$ are independent and have
distribution functions $B_1(x), B_2(x),\ldots , B_n(x)$ and
characteristic functions  $\beta, (t), \beta(t), \ldots , \beta_n (t)$
are included in the more general sums considered in the central limit
theorem.  It follows that the limiting distribution of $x_n$ is
i.d. and if $\varphi (t)$ is the limit of the characteristic functions
$\phi_n (t)$ of $x_n$, then $\log \varphi (t)$ is a $K-L$
function function. These limits form, however, only a proper subclass
of the $K-L$ class and the problem of characterizing this subclass was
proposed by Khint chine and solved by L\'evy.  We denote the class by
$L$. 
 
 As in the general case, it is natural to assume always that the
 components $\frac{\xi _\nu}{\lambda_n}$ are u.a.n. 

 \begin{theorem}\label{chap2:sec11:thm39} %%%% 39
 If $\xi_ \nu/\lambda_n$ are u.  a.  n.  and $\phi_n (t) \rightarrow
 \phi (t)$ where $\phi (t)$ is non-singular, then $\lambda_n \rightarrow
 \infty, \frac{\lambda_{n+1}}{\lambda_n}\rightarrow 1$.  
 \end{theorem}

\begin{proof}
Since $\lambda_n> 0$, either $\lambda_n \rightarrow \infty$ or
$(\lambda_n)$ contains a convergent subsequence $(\lambda_{n_k})$
with limit $\lambda$.  The u.a.n. condition implies that $\beta_
\nu\left(\frac{t \lambda}{\lambda_n}\right) \rightarrow 1$ for every $t$ and
therefore, by the continuity of $\beta_\nu (t)$, 
 $$
\beta_\nu(t) = \lim\limits_{k \rightarrow \infty}\beta_\nu
\left(\frac{t\lambda}{\lambda_{n_k}}\right) = 1
$$\pageoriginale 
for all $t$.  This means that every $\beta_\nu (t)$ is singular, and
this is impossible as $\varphi (t)$ is not singular. 
 
For the second part, since
 $$
\frac{\lambda_nx_n}{\lambda{n+1}}=
\frac{\xi_1+\xi_2+\dots+\xi_n}{\lambda_{n+1}} =
 x_{n+1}-\frac{\xi_{n+1}}{\lambda_{n+1}}   
 $$
 and the last term is asymptotically negligible, $\frac{\lambda_n
   x_n}{\lambda_{n+1}}$ and $x_{n+1}$ 
 have the same limiting distribution $F(x)$, and therefore 
  $$
 F_n \left(\frac{x \lambda_{n+1}}{\lambda_n}\right) \rightarrow F(x), F_n(x)
 \rightarrow F(x)
 $$

 Now if $\frac{\lambda_{n+1}}{\lambda_n} = \theta_n$, we can choose a
 subsequence ($\theta_{n_k}$) which either tends to $\infty$ or to some
 limit $\theta \ge 0$. In the first case  
 $F_{n_k} (x \theta_{n_k}) \rightarrow F (\pm \infty) \neq F(x)$
 for some $x$. In the other case 
  $$
 F(x) = \lim\limits_{k \rightarrow \infty} F_{n_k} (\theta_{n_k}
 x)=F(\theta x)
 $$ 
 whenever $x$ and $\theta x$ are continuity points of $F(x)$ and this is
 impossible unless $\theta = 1$. 
 
 A characteristic function $\phi (t)$ is called \textit{self-decomposable}
 (s.d) if, for every $c$ in $0 < c < 1$ it is possible to write  
 $$
 \phi(t) = \phi(ct) \phi_c (t)
 $$
 where\pageoriginale $\varphi_c (t)$ is a characteristic function
\end{proof}

 \begin{theorem}[P.L\'evy]\label{chap2:sec11:thm40} %%%% 40
   A function $\varphi (t)$ belongs to $L$ if and only
   if it is self-decomposable , and $\varphi_c (t)$ is then i.d 
 \end{theorem}

\begin{proof}
First suppose that $\varphi (t)$ is s.d. if it has a positive real
zero, it has a smallest, $2a$, since it is continuous, and so  
 $$
\varphi (2a)=0, \varphi(t)\neq 0 for 0 \le t  <  2a.
 $$

Then $\varphi (2a c) \neq 0$ if $0<c<1$, and since
$\varphi(2a)=\varphi(2a c) \varphi_c (2a)$ it follows that
$\varphi_c(2a)=0$. Hence 
\begin{align*}
  1&=1-\mathfrak{R}(\varphi_c(2a)) =
  \int\limits^\infty_{-\infty}(1-\cos 2 ax) d F_c(x)\\ 
  &=2 \int\limits^\infty_{-\infty}(1-\cos a x) (1+ \cos ax) d F_c(x)\le 4
  \int\limits^{\infty}_{-\infty}(1 - \cos a x) d F_c(x) \\
  & =4(1-\mathfrak{R}(\varphi_c(a)))=4(1-\mathfrak{R}(\varphi(a)1\varphi(c a)))
\end{align*}

 This leads to a contradiction since $\varphi(ca)\rightarrow \phi(a)$ as $c
 \rightarrow 1$, and it follows therefore that $\varphi(t)\neq 0$ for $t
 \ge 0$ and likewise for $t<0$. 
 
 If $1\le \mathcal{V}\le n$ it follows from our hypothesis that 
 $$
 \beta_\mathcal{V}(t)=\varphi
 _\frac{v-1}{v}(\mathcal{V}t)
 \frac{\varphi(\mathcal{V}t)}{\varphi((\mathcal{V}-1)t)}  
$$ 
is a characteristic function and the decomposition
 $$
 \varphi
 (t)=\varphi_n(t)=\displaystyle\prod^{n}_{=1}\beta\mathcal{V}(t/n) 
 $$ 
 shows that $\varphi (t)$ is of type $L$ with $\lambda_n =n$
 
 Conversely if we suppose that $\varphi (t)$ is of type $L$ we have 
 \begin{align*}
 \varphi_n (t)& =\displaystyle\prod^{n}_{r=1}\beta_r(t \lambda_n)\\ 
 \varphi_{n+m}(t)& =\displaystyle\prod^{n+m}_{v=1}\beta_\gamma(t/
 \lambda_{n+m})=\varphi_n(\lambda_n t/ \lambda_{n+m})\chi_{n,m}(t), 
 \end{align*}
 where\pageoriginale 
 $$
 \chi_{n,m}(t)=\displaystyle\prod^{n+m}_{v=n+1}\beta_\gamma(t/
 \lambda_{n+m})
 $$
 
 Using theorem \ref{chap2:sec11:thm39}, we can those $n$, $m(n) \rightarrow \infty$ so that
 $\lambda_n/ \lambda_{n+m}\rightarrow c (0 <c<1)$ and then
 $\varphi_{n+m}(t)\rightarrow\varphi (t)$.  Since
 $\varphi_n(t)\rightarrow\varphi(t)$ uniformly in any finite
 $t$-interval $\varphi_n(\lambda_n t/ \lambda_{n+m})\rightarrow \varphi
 (ct)$. It follows that $\chi_{n,m}(t)$ has a limit
 $\phi_c (t)$ which is continuous at $t=0$ and is therefore a
 characteristic function by theorem \ref{chap2:sec6:thm16}. Moreover the form of
 $\chi_{n,m} (t)$ shows that $\varphi_c (t)$ is i.d. 
 
 The theorem characterizes $L$ by a property of $\varphi (t)$. It is also
 possible to characterize it by a property of $F(x)$. 
\end{proof}

\begin{theorem}[P.L\'evy]\label{chap2:sec11:thm41}
 The function $\varphi (t)$ of $L$ are those for which $\log \varphi (t)$
 has a $K-L$ representation ($a, G)$ in which $\frac{x^2+1}{x} G^{'}(x)$
 exists and decreases outside a countable set of points. 
  \end{theorem}
   
\begin{proof}
  If we suppose that $\varphi (t)$ is of class $L$ and $0<c<1$ and
  ignore terms  of the form iat we have  
  \begin{multline*}
    \log
    \varphi_c(t)=\int\limits^{\infty}_{-\infty}
    \left(e^{itx}-1-\frac{itx}{1+x^2}\right) \frac{1+x^2}{x^2}d G(x) \\ 
    -\int\limits^{\infty}_{-\infty} \left[e^{itx}-1-\frac{itc^2x}{c^2+x^2}\right]
    \frac{x^2+c^2}{x^2} d G(x/c)  
  \end{multline*}
  and\pageoriginale the fact that $\varphi_c (t)$ is i.d. by
  Theorem \ref{chap2:sec11:thm40} implies that
  $Q(x)-Q(bx)$ decreases, where  $b=1/c>1$, $x>0$ and  
  $$
  Q(x)=\int\limits^{\infty}_{x}\frac{y^2+1}{y^2} d G (y)
  $$
  
  If we write $q(x)=Q(e^x)$ this means that $q(k)-q(x+d)$ decrease for
  $x>0$ if $d >0$. If follows that $q'(x)$ exists and decreases outside a
  countable set (see for instance Hardy, Littlewood, Polya: Inequalities
  , P.91 together with Theorem \ref{chap1:sec18:thm48} Chapter I on page 51)  
  
  Then since  
  \begin{multline*}
  \frac{x^2}{x^2+1}\frac{Q(x)-Q(x+h)}{h} \le \frac{G(x+h)-G(x)}{h}\\
  \le \frac{(x+h)^2}{(x+h)^2+1}\frac{Q(x)-Q()x+h}{h}
  \end{multline*}
  
  We have $G'(x)=\frac{x^2}{x^2+1} Q' (x)$ ~and~ $\frac{x^2+1}{x} G'
  (x)=x Q'(x)$ which also exists and decreases outside a countable
  set. The same
  argument applies for $x<0$. The converse part is trivial. 
  
  A more special case arises if we suppose that the components are
  identically distributed and the class $L^*$ of limits for sequences of
  such cumulative sums can again be characterized by properties of the
  limits $\varphi (t)$ or $G(x)$ 
  
  We say that $\varphi (t)$ is \textit{stable}, if for every positive constant
  $b$, we can find constants a, $b'$ so that    
  $$
  \varphi(t)\varphi(bt)=e^{iat}\varphi (b^{'}t)
  $$
  
  This implies, of course, that $\varphi$ (t) is s.d. and that
  $\varphi_c$ has the form $e^{ia't} \varphi (c't)$. 
\end{proof}

  \begin{theorem}[P.L\'evy]\label{chap2:sec11:thm42} %%%% 42
    A\pageoriginale characteristic function
    $\varphi (t)$ belongs to $L^*$ if and only if it is stable. 
  \end{theorem}

  \begin{proof}
    If $\varphi (t)$ is stable, we have on leaving out the inessential
    factors of the form $e^{i\alpha t}$, $(\varphi(t))^n = \varphi
    (\lambda_{n}t)$ for some $\lambda_{n}>0$ and so 
    $$
    \varphi(t)=(\varphi(t/\lambda_{n}))^n =
    \prod\limits^{n}_{\nu=1} \beta_{\nu}(t/\lambda_{n})~
    \text{with} ~\beta_{\nu}(t) = \varphi(t),
    $$
which is enough to show that $\varphi (t)$ belongs to $L^*$. 

Conversely, if we suppose that a sequence $\lambda_{n}$ can be found so that
   $$
   \varphi_{n}(t)= (\varphi(t/\lambda_{n}))^n \rightarrow \varphi(t),
   $$
we write $n = n_1 + n_2$,
   $$
\varphi_n(t) = (\varphi (t/\lambda_n))^{n_{1}}
(\varphi(t/\lambda_{n}))^{n_{2}} = \varphi_{n_{1}} (t
\lambda_{n_{1}}/\lambda_n)\varphi_{n_{2}} (t
\lambda_{n_{2}}/\lambda_n).
   $$

 Then, if   $0 < c < 1$, we choose $n_1$ so that
 $\lambda_{n_{1}}/\lambda_n \rightarrow c$ and it follows that
 $\varphi_{n_{1}}(t \lambda_{n_1}/\lambda_n) 
 \rightarrow \phi(ct)$  and  $\varphi_{n_{2}}(t \lambda_{n_2}/\lambda_n)
 \rightarrow\varphi_c (t)$. It is easy to show that this implies that
 $\varphi_c(t)$ has the form $e^{ia''t}\phi (c' t)$. 

It is possible to characterize the stable distributions in terms of
log $\varphi (t)$ and $G(x)$. 
  \end{proof}

\begin{theorem}[P.L\'evy]\label{chap2:sec11:thm43}
  The characteristic function $\varphi (t)$ is stable if and only if
  \begin{align*}
    \log \phi t & = iat - A \mid t \mid^{\alpha}  (1+\frac{i \theta
      t}{t} \tan \frac{\pi\alpha}{2})\\ 
    & \hspace{2cm}0 < \alpha < 1 ~\text{or}~ 1 < \alpha < 2\\
         {\rm or} \log \varphi(t) & = iat - A \mid t \mid (1 + \frac{i\theta
           t}{t} \frac{2}{\pi} \log \mid t \mid) {\rm with}\, A > 0
         \,{\rm and} -1 \leq \theta \leq + 1. 
  \end{align*}
\end{theorem}\pageoriginale

\begin{coro*}
  The real stable distributions are given by $\varphi (t) = e^{-A ~\mid
    t \mid^{\alpha}}(0<\alpha\leq 2)$.  
\end{coro*}

\begin{proof}
In the notation of Theorem \ref{chap2:sec11:thm41}, the stability condition implies that,
for every $d>0$, we can define $d'$ so that  
$$
q(x) = q(x+d) + q(x+d') (x>0).
$$

Since $q(x)$ is real, the only solutions of this difference equation,
apart from the special case $G(x) = AD(x)$ are given by  
$$
q(x) = A_1 e^{-\alpha x}, Q(x) = A_1 x^{-\alpha}, G'(x) = \frac{A_1
  x^{1-\alpha}}{1+x^{2}}, x>0
$$ 
and $\alpha$ satisfies 1 = e$^{-\alpha \text{d}} +
e^{-\alpha d'}$.  We can use a similar argument for x $<$0
and we have also 
$$
q(x) =A_2 e^{-\alpha\mid\text{x}\mid},Q(x) = A_2
\mid\text{x}\mid^{-\alpha}, G'(x) = \frac{A_2
  \mid\text{x}\mid^{1-\alpha}}{1+x^2},x<0
$$ 
where $d$, $d'$, $\alpha$ are the same. Moreover, since $G(x)$ is
bounded, we must have $0<\alpha\leq 2$, and since the case $\alpha =2$
arises when $G(x)= AD(x)$ and the distribution is normal, we can suppose
that $0<\alpha<2$.  Hence 
\begin{multline*}
  \log \varphi(t) = iat  + A_1  \int^{\infty}_0
  \left(e^{itx}-1-\frac{itx}{1+x^2}\right)\frac{dx}{x^{\alpha +1}}\\
  + A{_2} \int^{0}_{-\infty} \left(e^{itx}-1- \frac{itx}{1+x^2}\right)
  \frac{dx}{\mid \text{x}\mid^{\alpha+1}} 
\end{multline*}

The\pageoriginale conclusion follows, if $\alpha \neq  1 $ form the formula
\begin{align*} 
  \int\limits^{\infty}_{0}(e^{itx}-1) \frac{dx}{x^{\alpha +1}}  &= \mid
  t \mid^{\alpha} e^{-\alpha \pi i/2} \ulcorner (-\alpha)   ~\text{if}~ 0 <
  \alpha < 1,\\
  \int\limits^{\infty}_{0} \left(e^{itx} - 1 - \frac{itx}{1 + x^{2}}\right)
  \frac{dx}{x^{\alpha + 1}} &= \mid t \mid^{\alpha}  e ^{- \alpha \pi
    i/2} \ulcorner(-\alpha)   ~\text{if}~  1 < \alpha < 2
\end{align*} 
(easily proved by contour integration), Since the remaining components
$ \frac{itx}{1+x^{2}}$  or  $\frac{itx}{1 + x^{2}} - tx$  merely add
to the term iat.  If  $\alpha = 1$,  we use the formula 
$$
\int \limits^{\infty}_{0} \left(e^{itx} - 1- \frac{itx}{1 + x^{2}}\right)
\frac{dx}{x^{2}} = - \frac{\pi}{2}  \mid t \mid - \text{it}~  \log   \mid t
\mid +  i  a_{1}  t,  
$$
which is easy to Verify.
\end{proof}

\section{Random Functions}\label{chap2:sec12}

In our discussion of random functions, we shall not give proofs for
all theorems, but shall content ourselves with giving references, in
many cases.
		
Let $\Omega$ be the space of functions $x(t)$ defined on some space $T$
and taking values in a space $\mathfrak{X}$. Then we call $x(t)$ a
\textit{random function} (or \textit{process}) if a probability
measure is defined in 
$\Omega$. We shall suppose here that $\mathfrak{X}$ is the space of
real numbers and that $T$ is the same space or some subspace of it. 
		
The basic problem is to prove the existence of measures in $\Omega$ with
certain properties - usually that certain assigned sets in $\Omega$ are
measurable and have assigned measures. These sets are usually  
associated\pageoriginale with some natural property of the functions $x(t)$. It is
sometimes convenient to denote the function (which is a point $\Omega
$) by $\omega$ and the value of the function at $t$ by $x(t,\omega)..$ 

A basic theorem is
\begin{theorem}[Kolomogoroff]\label{chap2:sec12:thm44}
  Suppose that for every finite set of distinct real numbers  $t_1, t_2,
  \ldots , t_n$ we have a joint distribution function\- $ F_{t_1, t_2 ,
    \ldots, t_n} (\xi_1, \xi_2, \ldots , \xi_n)$ in $R_n$ and that these
  distribution functions are \textit{consistent} in the sense that their values
  are unchanged by like permutations of $(t_i)$ and $(\xi_i)$ and, if 
  $n>m$,
  $$ 
  F_{t_1, t_2 , \ldots, t_n} (\xi_1, \xi_2, \ldots , \xi_m, \infty ,
  \ldots, \infty) =  F_{t_1, t_2 , \ldots, t_m} (\xi_1, \ldots, \xi_m).
  $$
  
  Then a probability measure can be defined in $\Omega$ in such a way that
  \begin{equation*}
    \text{Prob} (x(t_i)\leq \xi_i, i=1, 2, \ldots, n)=  F_{t_1, t_2 , \ldots
      , t_n} (\xi_1, \ldots , \xi_n).\tag{1}\label{chap2:sec12:thm44:eq1}
  \end{equation*}
\end{theorem}

\begin{proof}
The set of functions defined by a finite number of conditions
$$ 
a_i \leq x(t_i)\leq b_i 
$$
is called a \textit{rectangular set} and the union of a finite number of
rectangular sets is called a figure. It is plain that intersections of
figures are also figures and that the system $S_o$ of figures 1 and
their complements is additive. Moreover, the probability measure $\mu$
defined in $S_o$ by \ref{chap2:sec12:thm44:eq1} is additive in $S_o$ , and it is therefore
enough, after Theorem \ref{chap1:sec7:thm7} of Chapter 1, to show that $\mu$ is completely
additive in $S_o$. It\pageoriginale is enough to show that if $I_n$  are figures and
$I_n \downarrow 0$, then $\mu (I_n)\rightarrow 0$. 

We assume that $\lim \mu (I_n)>0$, and derive a contradiction. Since
only a finite number of points $t_i$ are associated with each $I_n$,
the set of all these $t'_{i}s$ is countable and we can arrange them in
a sequence $(t_i)$. Now each $I_n$ is the union of a finite number of
the rectangular sets in the product space of finite number of the
space of the variables $x_i = x(t_i)$ and we can select one of these
rectangles, for $n = 1, 2, \ldots$ so that it contains a \textit{closed}
rectangle $J_n$ with the property that $\lim\limits_{m\to\infty}\mu
(J_n I_m)>0$. Also we may choose the $J_n$ so that $J_{n+1}\subset
J_n$. We then obtain a decreasing sequence of closed non empty
rectangles $J_n$ defined by 
$$ 
a_{in}\leq y_{i} \leq b_{in} ( i = 1, 2, \dots,
i_n) 
$$ 
For each i there is at least one point $y_i$ which is contained in all
the intervals $[a_{in}, b_{in}]$ , and
any function $x(t)$ for which $x(t_i)=y_i$ belongs to all $I_n$. This
impossible since $ I_n \downarrow 0$, and therefore we have $\mu
(I_n)\rightarrow 0$. 

As an important special case we have the following theorem on random
sequences. 
\end{proof}

\begin{theorem}\label{chap2:sec12:thm45} %%%% 45
  Suppose that for every $N$ we have joint distribution functions
  $F_N(\xi_1 , \ldots, \xi_N)$ in $R_N$ which are consistent in the sense
  of Theorem \ref{chap2:sec12:thm44}. Then a probability measure can
  be defined in the space 
  of real sequences $(x_1, x_2, \ldots)$ in such a way that 
  \begin{align*} 
    P(x_{i} \leq \xi_{i} i & = 1, 2, \ldots , N)\\
    &  = F_N(\xi_1, \ldots , \xi_N)
  \end{align*}
\end{theorem}

\begin{coro*}
  If\pageoriginale $\big\{ F_n(x)\big\}$ is a sequence of distribution functions, a
  probability measure can be defined in the space of real sequence so
  that if $I_n$ are any open or closed intervals, 
  $$ 
  P(x_{n} \in I_{n}, n=1, 2, \ldots , N)=
  \prod_{n=1}^{N} F_{n}(I_{n}).
  $$
\end{coro*}

  The terms of the sequence are then said to be \textit{independent}, and the
  measure is the product measure of the measures in the component
  spaces. The measures defined by Theorem \ref{chap2:sec12:thm44} will be called
  $K$-measure. The probability measures which are useful in practice are
  generally extensions of $K$-measure, since the latter generally fails to
  define measures on important classes of functions. For example, if $I$ is
  an interval, the set of functions for which  $a \leq x(t)\leq b$
  for $t$ in $I$ is not $K$-measurable. 
  
  In the following discussion of measures with special properties, we
  shall suppose that the basic $K$-measure can be extended so as to make
  measurable all the sets of functions which are used. 
  
  A random function $x(t)$ is called \textit{stationary} (in the
  strict sense) if 
  the transformation $x(t) \rightarrow x(t + a)$ preserves measures for
  any real a (or integer a in the case of sequences). 
  
  A random function $x(t)$ is said to have \textit{independent
    increments} if the 
  variables $x(t_i) - x(s_i)$ are independent for non-overlapping 
  intervals\pageoriginale $(s_i , t_i)$. It is called \textit{Gaussian} if the joint
  probability distribution for $x(t_1) , x(t_2), \ldots , x(t_n)$ for
  any finite set $t_1, t_2 , \ldots , t_n$ is Gaussian in $R_n$. That is
  , if the functions F of Theorem \ref{chap2:sec12:thm44} are all Gaussian. A random function
  $x(t)$ is called an $L_2$- function if it has finite variance for every
  $t$. This means that $x(t, w)$ belongs to $L_2(\Omega)$ as a function of
  $w$ for each $t$, and the whole function is described as a
  \textit{trajectory} in 
  the Hilbert space $L_2 (\Omega)$. 
  
  Many of the basic properties of an $L_2$-function can be described in
  terms of the \textit{auto-correlation} function  
  \begin{align*}
    r(s, t) &  =E ((x(s) - m (s)) \overline{(x(t) - m (t) }))\\
    &= E (x(s) \overline{x(t)}) - m(s) \overline{m(t)}
  \end{align*}
  where $m(t) = E ((x(t))$.
  
  A condition which is weaker than that of independent  increments is
  that the increments should be \textit{uncorrelated}. This is the
  case if $E(x(t) 
  - x(s)) \overline{(x(t')- x(s'))}) =E (x(t) - x(s)) \overline{E
    (x(t') -x (s'))}$ for non- overlapping intervals $(s, t)$, $(s' ,
  t')$. If an $L_2$-function is \textit{centred} so that $m(t) =0$ (which can
  always be done trivially by considering $x(t) - m(t))$,  a function
  with uncorrelated increments has \textit{orthogonal} increments, that is  
  $$ 
  E ((x(t) - x(s)) \overline{(x(t') - x (s')})) =0 
  $$
  for non-overlapping intervals. The function will then be called an
  \textit{orthogonal random function}. 
  
  The\pageoriginale idea of a \textit{stationary} process can also be weakened in the same
  way. An $L_2$- function is \textit{stationary in the weak sense} or
  stationary, if $r(s , t)$ depends only on $t - s$. We then write $\rho(h) =
  r (s,s + h)$. 
  
  We now go on to consider some special properties of random functions.

\section{Random Sequences and Convergence Properties}\label{chap2:sec13}

The problems connected with random sequences are generally much
simpler than those relating to random  functions defined over a non-
countable set. We may also use the notation $w$ for a sequence and $x_n
(w)$ for its $n^{th}$ term. 

\begin{theorem}[The 0 or 1 principle: Borel,
    Kolmogoroff]\label{chap2:sec13:thm46} 
  The probability that a random sequence of \textit{independent} variables have a
  property (e.g. convergence) which is not affected by changes in the
  values of any finite number of its terms is equal to 0 to 1. 
\end{theorem}

\begin{proof}
  Let $E$ be the set of sequences having the given property, so that our
  hypothesis is that, for every $N \geq 1$,  
  $$ 
  E= \mathfrak{X}_1 ~x~ \mathfrak{X}_2 ~x~ \ldots ~x~ \mathfrak{X}_n ~x~ E_N
  $$
  where $E_N$ is a set in the product space $\mathfrak{X}_{N+1} ~x~
  \mathfrak{X}_{N+2} ~x~ \ldots $ 
  
  It follows that if $F$ is any figure, $F ~E = F ~x~ E_N$ for large
  enough $N$ and  
  $$ 
  \mu (FE) = \mu (F) \mu (E_N) = \mu (F) \mu (E) 
  $$ 
  and\pageoriginale since this holds for all figures F , it extends to measurable
  sets F. In particular, putting F=E, we get 
  $$
  \mu (E) = (\mu(E))^2 , \mu(E) =0 or 1.
  $$
  
  We can now consider questions of convergence of series
  $\sum\limits^{\infty}_{\nu = 1} x_\nu$ of \textit{independent}
  random variables. 
\end{proof}

\begin{theorem}\label{chap2:sec13:thm47} %%%% 47
  If $s_n = \sum\limits^{\infty}_{\nu= 1} x_\nu \rightarrow s$ p. p., then $s_n-s
  \rightarrow 0$ in probability and the distribution function of $s_n$
  tends to that of $s$ . (This follows from Egoroff's theorem) 
\end{theorem}

\begin{theorem}[Kolmogoroff's inequality]\label{chap2:sec13:thm48} %%%% 48
  If $x_\nu$ are independent, with means 0 and standard deviations
  $\sigma_\nu$ and if  
  $$
  \displaylines{\hfill
  T_N = \sup_{n \leq N} \mid s_n \mid ,  s_n =
  \sum\limits^{n}_{\nu = 1} x_\nu , \in > 0\hfill\cr
  \text{then}\hfill 
  P(T_N \geq \in) \leq \in^{\frac{1}{2}}\sum\limits^{N}_{\nu =
    1} \sigma_\nu ^2\hfill }
  $$
\end{theorem}

 \begin{proof}
   Let
   $$
   \displaylines{\hfill 
   E = \in [ T_N \geq \in ]= \sum\limits^{N}_{k = 1} E_k\hfill\cr
   \text{where }\hfill
   E_k = \in [ \mid s_k \mid \geq \in ,  T_{k-1} < \in]\hfill }
   $$
   It is plain that the $E_k$ are disjoint .
   Moreover  $\sum\limits^{N}_{\nu = 1} \sigma_\nu ^2 =
   \int\limits_{\Omega} s^2 _ N d \mu$, since the
   $x_\nu $ are independent,
   \begin{align*}
     & \geq \int_{E} s^2_N d \mu= \sum\limits^N_{k=1}
     \int\limits_{E_K} s^2_N d \mu\\ 
     &=\sum\limits^N_{k=1} \int\limits_{E_K}(s_k+x_{k+1}+... + x_N)^2 d \mu\\
     &=\sum\limits^N_{k=1} \int\limits_{E_K} s^2_k  d\mu
     +\sum\limits^N_{k=1} \mu(E_k) \sum^N_{i=k+1} \sigma^2_i 
   \end{align*}
   since\pageoriginale $E_k$ involves only $x_1,\ldots,x_k$. 
   
   Therefore
   $$
   \sum\limits^N_{\nu=1} \sigma^2_\nu \ge \sum\limits^N_{k=1}
   \int\limits_{E_k} s^2_k d\mu \ge \in^2 \sum\limits^N_{k=1}\mu(E_k)=\in^2 \mu(E)
   $$
   as we require.
 \end{proof}

\begin{theorem}\label{chap2:sec13:thm49}%%% 49
  If $x_\nu$ are independent with means $m_\nu$ and
  $\displaystyle\sum^\infty_{\nu=1} \sigma^2_\nu < \infty$ then $\sum^\infty_1
  (x_\nu-m_\nu)$ converges p.p. 
\end{theorem}

\begin{proof}
  It is obviously enough to prove the theorem in the case $m_\nu=0$. 

  By theorem \ref{chap2:sec13:thm48}, if $\in > 0$
  $$
  P \left(\sup\limits_{\mid \leq n \leq N}\mid s_{m+n}-s_m\mid < \in \right) \geq
  \frac{1}{\in ^2} \sum\limits^{m+n}_{\nu=m+1} \sigma^2_\nu
  $$
  and therefore 
  $$
  P\left(\sup\limits_{n \geq 1}\mid s_{m+n} -s_m\mid > \in\right) \leq
  \frac{1}{\epsilon^2} \sum\limits^\infty_{\nu=m+1} \sigma^2_\nu
  $$
  and this is enough to show that  
  $$
  \lim\limits_{m \rightarrow \infty} \sup\limits_{n \geq 1} \mid
  s_{m+n}-s_m\mid=0 p.p.
  $$
  and\pageoriginale by the general principle of convergence, $s_n$
  converges p.p. 
  
  As a partial converse of this, we have
\end{proof}

\begin{theorem}\label{chap2:sec13:thm50} %%% 50
  If $x_\nu$ are independent with means $m_\nu$ and standard deviations
  $\sigma_\nu, \mid x_\nu \mid \le c$ and $\displaystyle\sum^\infty_{=1}
  x_\nu $ converges in a set of positive measure (and therefore p.p. by
  Theorem \ref{chap2:sec13:thm46}), then $\displaystyle\sum^\infty_{\nu=1} \sigma^2_\nu$ and
  $\displaystyle\sum^\infty_{\nu=1} m_\nu$ converge. 
\end{theorem}

\begin{proof}
  Let $\varphi_\nu (t), \vartheta(t)$ be the characteristic functions of
  $x_\nu$ and $s=\displaystyle\sum^\infty_{\nu=1} x_\nu$. Then it follows from
  Theorem \ref{chap2:sec13:thm47} that  
  $$
  \prod^\infty_{\nu=1} \varphi_\nu(t)=\vartheta(t)
  $$
  where $\vartheta(t)\neq 0$ in some neighbourhood of t=0, the product
  being uniformly convergent over every finite t-interval. Since 
  $$
  \varphi_\nu(t)=\int\limits^c_{-c} e^{\rm itx} d F_\nu(x)
  $$
  it is easy to show that
  $$
  \sigma^2_\nu \le-K \log \mid \varphi_\nu(t)\mid
  $$
  if t is in some sufficiently small interval independent of $\nu$, and
  it follows that $\displaystyle\sum^\infty_{\nu=1} \sigma^2_\nu <
  \infty$. Hence $\displaystyle\sum^\infty_{\nu=1}(x_\nu-m_\nu)$
  converges p.p. by Theorem 49, and since $\sum x_\nu$ converges p.p.,
  $\sum m_\nu$ also converges. 
\end{proof}

\begin{theorem}[Kolmogoroff's three series
    theorem]\label{chap2:sec13:thm51} %%% 51 
  Let $x_\nu$ be independent and $c> o,$
  \begin{align*}
    x'_\nu& = x_\nu if\mid x_\nu \mid \leq c\\
    & = 0 if \mid x \mid > c.
  \end{align*}\pageoriginale
  
  Then $\sum\limits_1^\infty x_\nu$ converges p.p. if and only if the
  three series 
  $$
  \sum^{\infty}_{1} P(\mid x_\nu \mid > c),\sum^{\infty}_{1} m_\nu^1 ,
  \sum^{\infty}_{1} \sigma'^2_\nu 
  $$
  converge, where $m '_\nu$ , $\sigma '_\nu$ are the means and standard
  deviations of the $x'_\nu$. 
\end{theorem}

\begin{proof}
  First,if $\sum x_\nu$ converges p.p., $x_\nu \rightarrow 0$ and
  $x'_\nu = x_\nu$ , $\mid x_\nu \mid < c$ for large enough $\nu$ for
  almost all sequences.  
  
  Let $p_\nu =P(\mid x_\nu \mid > c)$. 
  
  Now
  \begin{align*}
    \varepsilonup \left[\lim\limits_{\nu \rightarrow} ~\sup\limits_{\infty}\mid
      x_\nu \mid < c\right] 
    &=\lim\limits_{N \rightarrow \infty} \varepsilonup[\mid x_\nu \mid <
      c] ~\text{for}~ n \geq N]\\ 
      &= \lim\limits_{N \rightarrow \infty} \bigcap^{\infty}_{\nu = N}
      \varepsilonup [\mid x_\nu \mid < c]. 
  \end{align*}
  Therefore
  $$
  1= P(\lim \sup \mid x_\nu \mid < c) = \lim\limits_{N \rightarrow
    \infty} \prod^{\infty}_{\nu =N} (1-p_\nu)
  $$
  by the independence of the $x_\nu$. Hence $\prod\limits^{\infty} 
  _{\nu =1}  (1-p_\nu)$ so $\sum\limits^{\infty}_{\nu =1}p_\nu$ 
  converge. The convergence of the other two series follows from
  Theorem \ref{chap2:sec13:thm50}.
  
  Conversely,\pageoriginale suppose that the three series converge, so that, by
  Theorem \ref{chap2:sec13:thm50}, $\sum\limits^{\infty}_{1} x'_\nu$
  converges p.p. But it 
  follows from the convergence of $\sum\limits_1^\infty p_\nu$ that $x_\nu =
  \mathfrak{X} '_\nu$ for sufficiently large $\nu$ and almost all
  series, and therefore $\sum\limits^{\infty}_{1} x_\nu$
  also converges p.p. 
\end{proof}

\begin{theorem}\label{chap2:sec13:thm52} %%%% 52
  If $x$ are independent, $s_n = \sum\limits^{n}_{\nu=1}
  x_\nu $ converges if and only if $\prod^{\infty}_{\nu=1}
  \varphi_\nu(t)$ converges to a characteristic function.  

  We do not give the proof. For the proof see j.L.Doob, Stochastic
  processes, pages 115, 116. If would  seem natural to ask whether there
  is a direct proof of Theorem \ref{chap2:sec13:thm52} involving some
  relationship between 
  $T_N$ in Theorem \ref{chap2:sec13:thm48} and the functions $\varphi_\nu (t)$. This might
  simplify the whole theory. 
  
  Stated differently, Theorem \ref{chap2:sec13:thm52} reads as follows:
\end{theorem}

\begin{theorem}\label{chap2:sec13:thm53}%%% 53
  If $x_\nu$ are independent and the distribution functions of $s_n$
  converges to a distribution function, then $s_n$ converges p.p. 
  
  This is a converse of Theorem \ref{chap2:sec13:thm47}.
\end{theorem}

\begin{theorem}[The strong law of large numbers]\label{chap2:sec13:thm54}%%% 54
  If $x_\nu$ are independent, with zero means and standard deviations
  $\sigma_\nu$, and if 
  $$
  \displaylines{\hfill
  \sum\limits^{n}_{\nu=1} \frac{\sigma_\nu^2}{\nu^2 } < \infty\hfill\cr
  \text{then}\hfill 
  \lim\limits_{n \rightarrow \infty} \frac{1}{n}
  \sum\limits^{n}_{\nu=1} x_\nu =0 ~\text{p.p.} \hfill }
  $$
\end{theorem}

\begin{proof}
  Let\pageoriginale $y\nu =  \dfrac{x \nu}{\nu}$, so that $y_\nu$ has
  standard deviation 
  $\sigma_\gamma/\nu$. It follows then from Theorem
  \ref{chap2:sec13:thm49} that   
  $\sum y_\nu = \sum (x_\nu / \nu)$ converges p.p.

  If we write $x_\nu = \sum\limits^{\nu}_{j = 1} x_j/j$,
  \begin{align*}
    x_\nu & = \nu X_\nu - \nu X_{ \nu = 1},\\
    \frac{1}{2} \sum\limits^{n}_{\nu = 1} x_\nu & = 
    \frac{1}{n} \sum\limits^{n}_{\nu = 1} (\nu X_\nu - \nu X_{\nu =1})\\
    & = X_n - \frac{1}{n} \sum\limits^{n}_{\nu -1} X_{\nu-1}\\
    & = 0(1)
  \end{align*}
  if $X_n$ converges, by the consistency of (C, 1) summation.
  
  If the series $\sum\limits^{\infty}_{1} x_\nu$ does not
  converge p.p. it is possible to get results about the order of
  magnitude of the partial sums $s_n = \sum\limits^{n}_{1}
  x_\nu$. The basic result is the famous \textit{law of the iterated logarithm}
  of Khintchine. 
\end{proof}

\begin{theorem}[Khintchine; Law of the iterated
    logarithm]\label{chap2:sec13:thm55} %%% 55
  Let $x_\nu$ be independent, with zero means and standard deviations
  $\sigma_\nu$ 
  
  Let 
  $$
  B_n = \sum\limits^{n}_{\nu=1} \sigma^2_\nu
  \longrightarrow \infty  ~\text{as}~ n \longrightarrow \infty.
  $$ 
  
  Then\pageoriginale
  $$
  \limsup_{n \to \infty}\frac{\mid s_n \mid}{\sqrt(2 B_n \log
    \log B_n)} = 1 p.p.
  $$
\end{theorem}

\begin{coro*}
  If $x_\nu$ have
  moreover the same distribution, with $ \sigma_\nu = \sigma$ then 
  $$
  \limsup_{n \to \infty} \frac{\mid s_n \mid}{\sqrt(2n \log \log n)} =
  \sigma ~\text{p.p.}
  $$
  
  We do not prove this here. For the proof, see M. Loeve: Probability
  Theory, Page 260 or A. Khintchine : Asymptotische Gesetzeder
  Wahrsheinlichkeit srechnung, Page 59. 
\end{coro*}

\section{Markoff Processes}\label{chap2:sec14}

A random sequence defines a \textit{discrete Markoff process} if the behaviour
of the sequence $x_\nu$ for $\nu \geq n$ depends only on $x_n$ (see
page 123). It is called a \textit{Markoff chain} if the number of possible
values (or states) of $x_\nu$ is finite or countable. The states can
be described by the \textit{transition probabilities} ${}_np_{ij}$
defined as the probability that a sequence for which $x_n = i$ will
have $x_{n+ 1}=j$.  Obviously 
$$
{}_{n}p_{ij} \geq 0, \underset{J}\sum {}_{n}p_{ij} = 1.
$$

If ${}_{n}p_{ij}$ is independent of n, we say that the transition
probabilities are stationary and the matrix $P = (p_{ij})$ is called a
\textit{stochastic} matrix. It follows that a stationary Markoff chain must
have stationary transition probabilities, but the converse is not 
necessarily\pageoriginale true. 

It is often useful to consider one sided chains, say for $n \ge 1$ and
the behaviour of the chain then depends on the \textit{initial state} or the
initial probability distribution of $x_1$.
 
 The theory of Markoff chains with a finite number of states can be
 treated completely (see for example J.L.Doob, Stochastic processes
 page 172). In the case  of \textit{stationary} transition probabilities, the
 matrix $(p^n{}_{ij})$ defined by
 $$
 p^1_{ij} =p_{ij},\;p^{n+1}_{ij}=\sum\limits_{k}p^n_{ik} \; p_{kj} 
$$  
satisfies  $p^n_{ij}\ge 0$, $\sum_{j} p^n_{ij}=1$ and gives
the probability that a sequence with $x_1=i$ will have $x_n=j$. The
main problem is to determine the asymptotic behaviour of
$p_{ij}^n$. The basic theorem is

\begin{theorem}[For the proof see J.L.Doob, Stochastic Processes page
    175]\label{chap2:sec14:thm56}%%%% 56 
  $$
  \lim\limits_{n \rightarrow \infty} 
  \frac{1}{n} \sum\limits_{m=1}^{n}~p_{ij}^m~=q_{ij}
  $$
  where $Q=(q_{ij})$ is a stochastic matrix and  $QP=PQ=Q,Q^2=Q $.

  The general behaviour of $p_{ij}^n$ can be described by dividing
  the states into \textit{transient} states and disjoint \textit{ergodic sets of
    states}. Almost all sequences have only a finite number of terms in
  any one of the transient states and almost all sequences for which
  $x_n$ lies in 
  an ergodic set will have all its subsequent terms in the same set.
  
  A\pageoriginale random function of a continuous variable $t$ is called a Markoff if ,
  for $t_1 <t_2 \cdots <t_n <t$ and intervals (or Borel sets)
  $I_1,I_2,\ldots ,I_n,$ we have 
  \begin{align*}
    P(x(t)\in I/x(t_1)& \in I_1, x(t_2)\in I_2,..., x(t_n)\in I_n)\\
    &=P(x(t) \in I/x(t_n)\in I_n).
  \end{align*}
  
  Part of the theory is analogous to that of Markoff Chains, but the
  theory is less complete and satisfactory. 
\end{theorem}

\section{\texorpdfstring{$L_2$}{L2}-Processes}\label{chap2:sec15} 

\begin{theorem}\label{chap2:sec15:thm57} %%% 57 
  If $r(s,t)$ is the auto correlation function of an $L_2$ function,
  $$
  r(s,t)=\overline{r(t,s)}
  $$
  and if $(z_i)$ is any finite set of complex numbers, then
  $$
  \sum_{i,j} ~r(t_i,t_j) z_i \overline{z}_j \ge 0.
  $$

  The first part is trivial and the second part follows from the identity
  $$
  \sum\limits_{i,j}~r(t_i,t_j)z_i \overline{z}_j
  =E(~|(x(t_i)-m(t_i))z_i |~^2) \ge 0.
  $$ 
\end {theorem}

\begin{theorem}[For proof see J.L.Doob, Stochastic processes page
    72]\label{chap2:sec15:thm58}  %% 58
  If $m(t),r(s,t)$ are given and $r(s,t)$ satisfies the conclusion of
  theorem \ref{chap2:sec15:thm57}, then there is a unique Gaussian function $x(t)$ for which  
  $$
  E(x(t))=m(t),E(x(s) ~~\overline{x(t))} -m(s)\overline{m(t))}= r(s,t).
  $$\pageoriginale 
  
  The uniqueness follows from the fact that a Gaussian process
  is determined by its first and second order moments given $r(s,t)$.  Hence,
  if we are concerned only with properties depending on $r(s,t)$ and $m(t)$
  we may suppose that all our processes are Gaussian.
\end{theorem}

\begin{theorem}\label{chap2:sec15:thm59} %%% 59
  In order that a centred $L_2$-process should be 
  orthogonal, it is necessary and sufficient that 
  \begin{equation*}
    E(|x(t)-x(s)|^2)= F(t)-F(s)(s<t)\tag{1}\label{chap2:sec15:thm59:eq1}
  \end{equation*}
  where $F(S)$ is a non -decreasing function. In particular, if $x(t)$ is
  stationary $L_2$, then $E(|x(t)-x(s)|^2)= \sigma^2(t-s)(s<t)$
  for some constant $\sigma^2$.
\end{theorem}

\begin{proof}
  If $s<u<t$ the orthogonality condition implies that  $
  E(|x(u)-x(s)|^2)+E(|x(t)-x(u)|^2)= E(|x(t)-x(s)|^2)$ 
  which is sufficient to prove (\ref{chap2:sec15:thm59:eq1}). The
  converse is trivial. 

  We write
  $$
  dF=E(|dx|^2)
  $$
  and for stationary functions,
  $$
  \sigma^2 dt=E(|dx|^2).
  $$
  
  We say that an $L_2$ - function is \textit{continuous} at $t$ if 
  $$
  \lim\limits_{h\longrightarrow 0} E (|x(t+h)-x(t)|^2)=0,
  $$ 
  and that it is continuous if it is continuous for all $t$.
  Note that this does not imply that the individual $x(t)$ are
  continuous at $t$.
\end{proof}

\begin{theorem}[Slutsky]\label{chap2:sec15:thm60}%%% 60
  In\pageoriginale order that $x(t)$ be continuous
  at $t$, it is necessary and sufficient that $r(s,t)$ be continuous at
  $t=s$.
  
  It is continuous (for all t) if $r(s,t)$ is continuous on 
  the line t=s and then $r(s,t)$  is continuous in the whole plane.
\end{theorem}

\begin{proof}
  The first part follows from the relations
  \begin{align*}
    E (|x(t+h) & -x(t)|^2)\\ 
    &= r(t+h,t+h)-r(t+h,t)-r(t,t+h)+r(t,t)\\
    & =o(1) \text{ as } h\longrightarrow 0 ~\text{if}~ r(s,t)~\text{
      is continuous for}\\ 
    & \hspace{4cm}{t=s; r(t+h,t+k)-r(t,t)}\\
    & =E(x(t+h) \overline{x(t+k)}-x(t)\overline{x(t)})\\
    & =E(x(t+h)
    \overline{x(t+k)}-\overline{x(t)})+=((x(t+h)-x(t))\overline{x(t)})\\ 
    & =o(1) \text{ as } h,k\longrightarrow 0\\ 
    & \hspace{1cm}\text{ by the Schwartz inequality if} ~x(t) ~\text{is
      continuous at}~ t. 
  \end{align*}
  For the second part, we have
  \begin{align*}
    r(s+h, t+k)& -r(s,t)\\
    &=E(x(s+h)
    \overline{x(t+k)}-\overline{x(t)}))+E((x(s+h)-x(s))\overline{x(t)})\\
    &=o(1)\rm as~~ h,k\longrightarrow 0\text { by Schwarz's inequality},\\
    &\hspace{3cm} \text { if x(t)  is continuous at t  and s.}
  \end{align*}
\end{proof}

\begin{theorem}\label{chap2:sec15:thm61} %%% 61
   If $x(t)$ is continuous and stationary $L_2$, with 
   $\rho(h)=r(s,s+h)$, then
   $$
   \rho(h)= \int\limits^{\infty}_{-\infty} e^{i \lambda h} dS
   (\lambda)
   $$
   where $S(\lambda)$ is non- decreasing and bounded.
   
   Moreover,
   $$
   S(\infty)-S(-\infty)= \rho(0)=E(|x(t)|^2) ~\text {for all }t.
   $$
\end{theorem}
 
\begin{proof}
  We\pageoriginale have ${\rho(-h)= \overline{\rho(h)},\rho(h)}$ is continuous at 0 and  
  $$ 
  \sum\limits_{i,j} \rho (t_i-t_j) z_i \overline{z_j} \geq 0
  $$
  for all complex ${z_i}$ by Theorem \ref{chap2:sec15:thm57} and the conclusion follows from
  Bochner's theorem (Loeve, Probability theory, p.  
  207-209, and Bochner Harmonic analysis  and Probability, page 58).
  
  The theorem for sequences is similar. 
\end{proof}

\begin{theorem}\label{chap2:sec15:thm62} %%% 62
  If $x_n$ is stationary $L_2$, with $\rho_n=E(x_m.\overline{x_{m+n}})$
  then
  $$ 
  \rho_n = \int^\pi_{-\pi} e^{in \lambda} dS(\lambda)
  $$
  where ${S(\lambda)}$ increases and ${S(\pi)-S(-\pi) = \rho_o = E
    (\mid x_m \mid^2)}$ 
  
  We say that an ${L_2}$-random function ${x(t)} $ is
  differentiable at t with derivative ${x'(t)}$ (a random variable)if  
  $$
  {E\left(\mid \frac{x(t+h)-x(t)}{h} - \dot{x}(t) \mid
    ^2\right)\rightarrow 0~\text{as}~h \rightarrow 0}
  $$
\end{theorem}

\begin{theorem} \label{chap2:sec15:thm63}%%% 63
  In order that $x(t)$ be differentiable at t it is necessary and
  sufficient that $\frac{\partial^2 r}{\partial s \partial t}$
  exists when $t = s$. Moreover, if $x(t)$ is differentiable for all
  $t$, $\frac{\partial^2 r}{\partial s \partial t}$ exists on the whole plane. 
  
  (The proof is similar to that of Theorem \ref{chap2:sec15:thm60}.)
  
  Integration of $x(t)$ can be defined along the same lines.
  
  We say that $x(t)$ is R-integrable in $a \leq t \leq
  b$ if $\sum_i  x (t_i) \delta_i $ tends
  to a limit in $L_2$ for any sequence of sub-divisions of
  $(a, b)$ into intervals\pageoriginale of lengths $\delta_i$
  containing points $t_i$ 
  respectively. The limit is denoted by $\int^b_a x(t) dt$. 
\end{theorem}

\begin{theorem}\label{chap2:sec15:thm64}
  In order that $x(t)$ be $R$-integrable
  in a $\leq t \leq  b$ it is necessary and sufficient that 
  $\int^b_a \int^b_a r(s, t)ds dt$ exists as a Riemann integral.
\end{theorem}

Riemann - Stieltjes integrals can be defined similarly.

The idea of integration with respect to a random function $Z(t)$ is
deeper (see e.g. J.L.Doob, Stochastic processes, chap. IX $\S 2$). In
the important cases, $Z(t)$ is orthogonal, and then it is easy to
define the integral 
$$
\int^b_a  \phi(t) dZ(t)
$$
the result being a random variable. Similarly 
$$
\int^b_a  \phi(s,t) dZ(t)
$$
will be a random function of s under suitable integrability conditions.

The integral of a random function $x(t)$ with respect to a random
function $Z(t)$ can also be defined (Doob, Chap. IX $\S$ 5). 

The most important application is to the spectral representation of a
stationary process. 

\begin{theorem}[Doob, page 527]\label{chap2:sec15:thm65}%%% 65
  A continuous stationary $(L_2)$ function $x(t)$ can be represented
  in\pageoriginale the form
  $$
  x(t)=  \int\limits^{\infty} _{-\infty}e ^ {i  \lambda t}  d Z
  (\lambda)
  $$
  where $Z(\lambda)$ has orthogonal increments and
  $$ 
  E (\mid dZ \mid^2)= dS 
  $$ 
  where $S(\lambda)$ is the function defined in
  Theorem \ref{chap2:sec15:thm61}.
  
  The formula gives the \textit{spectral} decomposition of $x (t)$. $S(\lambda)$
  is its \textit{spectral distribution}.
  
  The corresponding theorem for random sequence is 
\end{theorem}

\begin{theorem}[Doob, page 481]\label{chap2:sec15:thm66}%%% 66
  A stationary $(L_{2})$ sequence $ \big\{x_n\big\}$ has spectral representation
  $$ 
  x_n = \int\limits^\pi _{-\pi} e ^{i \lambda n}   dZ (\lambda)
  $$
  where $Z(\lambda)$ has orthogonal increments and
  $$ 
  E (\mid dZ \mid)^2 = dS,
  $$
  $S (\lambda)$ being defined by Theorem \ref{chap2:sec15:thm62}.
  
  Two or more random function $x_i (t)$ are \textit{mutually orthogonal} if $E
  (x_i (t)$ $  \overline {x_j(s))} =0 $ for $i \neq j$ and $s$, $t$.   
\end{theorem}

\begin{theorem}\label{chap2:sec15:thm67} %%% 67
  Suppose that $x(t)$ is a continuous, stationary $(L_2)$ process and that
  $E_1, E_2 \ldots , E$ are measurable, disjoint 
  sets whose union is the whole real line. Then we can  write
  $$ 
  x (t)= \displaystyle \sum_{i=1}^ \nu x_i (t) 
  $$ 
  where\pageoriginale $x_i (t)$ are mutually orthogonal and
  $$ 
  x_i (t)= \int\limits^\infty_{-\infty} e^{i \lambda t}  dZ_i (\lambda) =
  \int_{E_i} e^{i \pi  t} d Z (\lambda)
  $$
  and $E(\mid dZ_i \mid)^2=0$ outside $E_i $. 
  
  The theorem for sequences is similar. In each case, a particularly
  important decomposition is that in which three sets $E_i$ are
  defined by the Lebesgue decomposition of $S(\lambda)$ into absolutely
  continuous, discontinuous and singular components. For the second
  component, the auto-correlation function $\rho(n)$ has the form 
  $$ 
  \rho (h)=\sum\limits_{i} d_i e^{ih \lambda i}
  $$
  where $d_i$ are the jumps of $S(\lambda)$ at the discontinuities
  $\lambda_i$, and is uniformly almost periodic. 
  
  We can define liner operations on stationary functions (Doob, page
  534). In particular, if $k(s)$ of bounded variation  in  $(- \infty ,
  \infty)$, the  random function 
  $$ 
  y(t)= \int\limits^\infty_{-\infty} x (t-s)d k (s) 
  $$ 
  can be defined and it is easy to show that y(t) has spectral representation
  $$ 
  \displaylines{\hfill
  y(t)=\int\limits^\infty _{-\infty}e ^{i \lambda t} K (\lambda)  dZ (\lambda)
  = \int\limits^\infty_{-\infty} e^{i \lambda t}  d z_1(\lambda)
  \hfill \cr
  \text{where}\hfill 
  K(\lambda) = \int\limits^\infty_{- \infty}~ e^{i \lambda s} dk(s),
  E(~\mid dZ_1 ~(\lambda) \mid^2) = (K (\lambda))^2 d S (\lambda).\hfill}
  $$\pageoriginale 
  
  If ~$k(s) = 0$ for $s < \tau$, $\tau > 0$ we have
  $$ 
  y(t + \tau) = \int^\infty_0 ~~ x (t-s) d k(s - \tau) 
  $$
  which depends only on the "part" of the function $x(t)$ "before time $t$".
  The \textit{linear prediction problem} (Wiener) is to determine
  $k(s)$ so as to 
  minimise (in some sense) the difference between $y(t)$ and $x(t)$. In so 
  far as this difference can be made small, we can regard $y(t + \tau)$ as a 
  \textit{prediction} of the value of $x(s)$ at time $t + \tau$ based on our 
  knowledge of its behaviour before $t$.
\end{theorem}

\section{Ergodic Properties}\label{chap2:sec16}

We state first the two basic forms of the ergodic theorem.

\begin{theorem}[G.D Birkhoff, 1932]\label{chap2:sec16:thm68}%%% 68 
  Suppose that for $\lambda \ge 0, T^\lambda$ is a measure preserving (1-1)
  mapping of a measure space $\Omega$ of measure 1 onto itself and that 
  $T^0 = I,~~T^{\lambda + \mu} = T^\lambda \circ T^\mu$. Suppose that
  $f(\omega) \in L(\Omega) and $ 
  that $f(T^\lambda \Omega)$  is a measurable function of $ (\lambda ,
  \omega)$ in the product 
  space $R \times \Omega$. Then
  $$
  f^*(\omega) =  \lim\limits_{\lambda \rightarrow \infty} \frac
  {1}{\lambda} ~ \int^\lambda_0 ~ f(T^\Lambda \omega) d \lambda
  $$ 
  exists for almost all $\omega, f^* (\omega) \in L(\Omega)$ and 
  $$
  \int_ \Omega f^* (\omega) d \omega =  \int_ \Omega f(\omega) d \omega
  $$
  
  Moreover,\pageoriginale if $\Omega$ has no subspace of measure $> 0$ and $< 1$ invariant
  under all $T^\lambda$
  $$ 
  f^* (\omega) = \int_ \Omega ~ f (\omega)d \omega ~\text{ for almost all}~
  \omega
  $$
  
  There is a corresponding discrete ergodic theorem for transformations
  $T^n =  (T)^n$ where n is an integer, the conclusion then being that
  $$ 
  f^* (\omega) =  \lim\limits_{n \rightarrow \infty} ~ \frac {1}{N} ~~
  \sum\limits_{n=1}^N ~ f(T^n \omega)
  $$
  exists\pageoriginale for almost all $\omega$. In this case, however, the
  memorability condition on $f(T^\lambda \omega)$ may be dispensed
  with. 
\end{theorem}

\begin{theorem}[Von Neumann]\label{chap2:sec16:thm69} %%% 69
  Suppose that the conditions of Theorem \ref{chap2:sec16:thm68} hold and that $f(\omega) \in
  L_2 (\Omega)$.  Then 
  $$
  \int_ \Omega \bigg|
  \frac{1}{\Lambda}~\int\limits_0^{\lambda} f(T^{\lambda}\omega)d
  \lambda-f^*(\omega)\bigg|^2 d \omega \rightarrow 0 ~\text{as}~
  \wedge \rightarrow \infty. 
  $$ 

  For proofs see Doob page 465, 515 or P.R. Halmos, Lectures on Ergodic
  Theory, The math.Soc. of Japan, pages 16,18. The simplest proof is due
  to F.Riesz (Comm. Math.Helv. 17 (1945)221-239). 
  
  Theorems \ref{chap2:sec16:thm68} is much than
  Theorem \ref{chap2:sec16:thm69}.
\end{theorem}

The applications to random functions are as follows

\begin{theorem}\label{chap2:sec16:thm70}%%% 70
  Suppose that $x(t)$ is a strictly stationary random function and that
  $x(\omega, t) \in L (\Omega)$ for each $t$, with 
  $\int_\Omega x (\omega, t) d \omega = E (x(t)) = m$. Then
  $$
  \lim\limits_{\Lambda  \rightarrow \infty} ~ \frac {1}{\Lambda} ~~
  \int^\Lambda_0 x (\omega, t) dt = x^* (\omega) 
  $$ 
  exists for almost all $\omega$ If $x(t)$ is an $L_2$- function we have
  also convergence in mean . 
  
  This follows  at once from Theorem
  \ref{chap2:sec16:thm68}, \ref{chap2:sec16:thm69} if we define 
  $$
  f(\omega)=x(\omega,0),T^\lambda(x(t))=x(t+\lambda).
  $$
\end{theorem}  

\begin{coro*}
  If a is real
  $$
  \lim\limits_{\Lambda\rightarrow \infty}
  \frac{1}{\Lambda}\int\limits^\Lambda_0x(\omega, t)e^{iat} dt = x^* (\omega, a)
  $$
  exists for almost all $\omega$.
\end{coro*}

Theorem \ref{chap2:sec16:thm70} is a form of the strong law of large
number for random functions. There is an analogue for sequences.  

A particularly important case  arises if the translation operation
$x(t)\break \rightarrow x (t+\lambda)$ has no invariant subset whose
measure  is $>$0 and $<1$. In this case we have 
$$
\lim\limits_{\Lambda \rightarrow \infty}
\frac{1}{\Lambda}\int\limits^\Lambda_0 x(\omega, t) dt = \int_\Omega x (\omega
, t)d \omega =m 
$$ 
for almost all $\omega$. In other words almost all functions have
limiting ``time averages'' equal to the mean of the values of the
function  at any fixed time.  

\section{Random function with independent increments}\label{chap2:sec17}

The basic condition is that if $t_1<t_2<t_3$, then $x(t_2)-x(t_1)$ and
$x(t_3) - x(t_2)$ are independent (see page 114) so that the distribution
function of $x(t_3)-x(t_1) $ is the convolution of those of
$x(t_3)-x(t_1)$ and $x(t_3)-x(t_2)$. We are generally interested only in
the increments, and it is convenient to consider the  
behaviour\pageoriginale of the function from some base point, say 0, modify the
function by subtracting a random variable so as to make
$x(0)=0$, $x(t)=x(t)-x(0)$. Then if $F_{t_1,t_2}$ is the distribution
function for the increment $x(t_2)-x(t_1)$ we have, 
$$
F_{t_1,t_3}(x)=F_{t_1,t_2} * F_{t_2,t_3}(x).  
$$

We get the stationary case if $F_{t_1,t_2}(x)$ depends only on
$t_2-t_1$. (This by itself is not enough, but together with independence,
the condition is \textit{sufficient} for stationary) If we put 
$$
 F_t (x)=F_{o.t}(x),
$$
we have in this case
$$
F_{t_1+t_2}(x)=F_{t_1}* F_{t_2}(x),
$$
for all $t_1$,$t_2>0$.

If $x (t)$ is also an $L_2$  function with $x(0)=0$, it follows
that
$$
\displaylines{\hfill 
  E(|x(t_1+t_2)|^2) = E (|x(t_1)|^2)+ E (|x(t_2)|^2)\hfill\cr
  \text{so that }\hfill
  E(|x(t)|^2)  = t\sigma^2\hfill\cr
  \text{where}\hfill 
  \sigma^2 =E(|x(1)|^2)\hfill }
$$

\begin{theorem}\label{chap2:sec17:thm71}%%% 71
   If $x (t)$ is stationary with independent increments, its distribution
   function $F_t (x)$ infinitely divisible  and its characteristic function
   $\varphi_t (u)$ has the form $e^{t\psi (u)}$, where 
   $$
   \psi(u) = iau + \int\limits^\infty_{-\infty}\left[e^{itx} - 1 - \frac{iux}{1 +
       x^2}\right]\frac{1 + x^2}{x^2} dG(x)
   $$\pageoriginale 
   $G(x)$ being non-decreasing and bounded,
\end{theorem}
 
\begin{proof}
  The distribution function is obviously infinitely divisible for every
  $t$ and it follows from the stationary property that  
  $$
  \varphi_{t_1+t_2}(u) = \varphi_{t_1} (u) \varphi_{t_2}(u)
  $$
  so that $\varphi_t(u)=e^{t \psi(u)}$ for some $\psi(u)$, which must
  have the $K-L$ form which is seen by putting $t=1$ and using Theorem
  \ref{chap2:sec10:thm37}.  
\end{proof}

Conversely, we have also the 

\begin{theorem} \label{chap2:sec17:thm72}%%% 72
  Any function $\varphi_t (u)$ of this form is the characteristic
  function of a stationary random function with independent increments. 
\end{theorem}

\begin{proof}
  We observe that the conditions on $F_t(x)$ gives us a system of joint
  distributions over finite sets of points $t_i$ which is consistent in
  the sense of Theorem \ref{chap2:sec12:thm44} and the random function
  defined by the 
  Kolmogoroff measure in Theorem \ref{chap2:sec12:thm44} has the required properties. 
\end{proof}

\setcounter{example}{0} %%%% 1
\begin{example}[Brownian motion : Wiener]\label{chap2:sec17:thm72:exp1}
  The increments all have normal distributions, so that 
  $$ 
  F_t (x) =\frac{1}{\sigma \sqrt{2 \pi t}} e^{-x^2/2 t \sigma^2}
  $$
\end{example}

\begin{example}[Poisson]\label{chap2:sec17:thm72:exp2} %%% 2
  The increments $x(s + t) -x(s)$ have integral values $\nu \ge 0$ with
  probabilities $e^{-ct}\frac{(ct)^\nu}{\nu !}$  
  
  Both\pageoriginale are $L_2-$ Processes.
\end{example}

\begin{theorem}\label{chap2:sec17:thm73}%%% 73
  Almost all functions
  $x(t)$ defined by the Kolmogoroff measure defined by the Wiener function,
  or any extension of it, are everywhere non-differentiable. In fact,
  almost all functions fail to satisfy a Lipschitz condition of order
  $\alpha (x(t-h) - x(t) =0 (\mid h \mid^\alpha))$ if $\alpha >
  \frac{1}{2}$ and are not of bounded variation. 
\end{theorem}

\begin{theorem}\label{chap2:sec17:thm74} %%%% 74
  The Kolmogoroff measure defined by the Wiener function can be extended
  so that almost all functions $x(t)$ Satisfy a Lipschitz condition of any
  order $\alpha < \frac{1}{2}$ at every point, and are therefore
  continuous at every point. 
  
  For proofs, see Doob, pages 392-396 and for the notion of extension,
  pages 50-71. 
\end{theorem}

\begin{theorem}\label{chap2:sec17:thm75} %%% 75
  The $K$-measure defined by the Poisson function can be extended so that
  almost functions $x(t)$ are step functions with a finite number of
  positive integral value in any finite interval. 
  
  The probability that $x(t)$ will be constant in an interval of
  length t is $e^{-ct}$. 
\end{theorem}

\section{Doob Separability and extension theory}\label{chap2:sec18}

The $K$-measure is usually not extensive enough to give probabilities
to important properties of the functions $x(t)$, e.g. continuity etc. 

Doob's solution is to show that a certain subject $\Omega_\circ$ of
$\Omega$ has outer $K$-measure 1, $\mu (\Omega_\circ)=1$.  Then, if
$X_1$ is any $K$-measurable\pageoriginale set, Doob defines
$$
\mu \star(X)=\mu(X_1)\text{when}\; X=\Omega_o X_1
$$
and shows that $\mu^\star$ is completely additive and defines a
probability measure in a Borel system \textit{containing} $\Omega_0$, and
$\mu^\star(\Omega_0)=1$.  

Doob now defines a \textit{quasi-separable} $K$-measure as one for
which there is 
a subset $\Omega_0$ of outer $K$-measure 1 and a countable set $R_0$ of
real numbers with the property that 
\begin{align*}
  \sup_{t \epsilon I} x(t)\; & = \sup_{t \epsilon I.R_\circ} x(t)\\
  \inf_{t\epsilon I}~ x(t)& = \inf_{t \epsilon I.R_\circ}
  x(t)\tag{$\alpha$}\label{chap2:sec18:eq1} 
\end{align*}
for every $x(t) \in \Omega_\circ$ and every open interval $I$.

If the $K$-measure has this property, it can be extended to a measure so
that almost all functions $x(t)$ have the property
(\ref{chap2:sec18:eq1}). 

All conditions of continuity, differentiability and related
concepts can be expressed then in terms of the countable set $R_0$ and
the sets of functions having the corresponding property then become
measurable. Thus, in the proofs of Theorem \ref{chap2:sec17:thm74} we have only to show
that the set of functions having the required property (of continuity
or Lipschitz condition) has outer measure 1 with respect to the basic
Wiener measure. 

For Theorem \ref{chap2:sec17:thm73}, there is no need to extended the
measure, for if the 
set of functions $x(t)$ which are differentiable at least at one 
point\pageoriginale has measure zero, with respect to Wiener measure,
it has measure 
zero with respect to any extension of Wiener measure. 
   
   For a fuller account, see Doob, Probability in Function Space,
   Bull. Amer. Math. Soc. Vol. 53 (1947),15-30. 
