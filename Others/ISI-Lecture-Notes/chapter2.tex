\chapter{}\label{chap2}

\section{Generalities on Resultants}\label{c2:s1}

In this section we give some generalities about resultant which we
need in the next section.

Let $A$ be a commutative ring and $f,g \varepsilon A[X]$. Suppose
\begin{align*}
f&=a_0X^{n}+a_1X^{n-1}+\cdots+a_n\\
g&=b_0X^{m}+b_1X^{m-1}+\cdots+b_m
\end{align*}

\begin{dfn}\label{c2:dfn1.1}
The $(n,m)$ - \textit{resultant} of $f,g$ denoted by $R_{n,m}(f,g)$
is defined to be the determinant of the following $(n+m)\times (n+m)$
matrix
%\begin{bmatrix}
%a_0 & a_1 & &\cdots & a_n &0 & \cdots & 0\\
%0 & a_0& a_1& \cdots & & a_n & 0 &\cdots & 0\\
%& & & \cdots & & & \cdots &\\
%0 & \cdots &  &0 & a_0 & a_1 & & \cdots & a_n\\
%b_0 & b_1 & \cdots & b_m & 0 & & &\cdots & 0\\
%0 & b_0 & b_1 & \cdots & b_m & 0 && \cdots & 0\\
%& & & \cdots & & & \cdots &\\
%0 & &  \cdots &  &0 & b_0& b_1 & \cdots & b_m
%\end{bmatrix}
{\large\bf Not able to identify how many columns and how many rows are
there in this matrix}


We remark that when $a_o\neq 0$, $b_o\neq 0$, we write
$R_{n,m}(f,g)=R(f,g)$ and call it the resultant of $f,g$.
\end{dfn}

If we multiply the $i$-th column by $X^{m+n-i}$ for
$i=1,2,\ldots,m+n-1$ and add this to the last column, then the last
column is
$$
\begin{bmatrix}
X^{m-1}f\\
X^{m-2}f\\
\vdots\\
f\\
X^{n-1}g\\
X^{n-2}g\\
\vdots\\
g
\end{bmatrix}
$$

Now expanding the resulting matrix by the last column, we obtain
$$
R_{n,m}(f,g)=f'f+g'g
$$
where $f'$, $g'\varepsilon A[X]$ and $\deg f'<m$, $\deg g'<n$. Hence
we have the following

\begin{lem}\label{c2:lem1.2}
Let $A \subset B$ and there exist $b\varepsilon B$ such that
$f(b)=g(b)=0$ , then $R_{n,m}(f,g)=0$.
\end{lem}

\begin{Prop}\label{c2:Prop1.3}
Let $A$ be a field and $f,g$ as above. Suppose $(a_o,b_o)\neq
(0,0)$. Then $f$ and $g$ have a common factor in $A[X]$ if and only if $R_{n,m}(f,g)=0$.
\end{Prop}

\begin{Proof}
If $f,g$ have a common factor, then $f,g$ have a common zero in an
extension field of $A$. Hence by Lemma~\ref{c2:lem1.2},
$R_{n,m}(f,g)=0$. Conversely if $R_{n,m}(f,g)=0$. Then since
$R_{n,m}(f,g)=f'f+g'g$ with $\deg f'<m$  and $\deg g' <n$. Say
$a_o\neq 0$ i.e. $\deg f=n$. Then $g$ and $f$ have a common factor
since $\deg g' <n$.
\enprf
\end{Proof}

\begin{rem}\label{c2:rem1.4}
Let $\phi:A\rightarrow B$ be a homomorphism of rings and $f,g$ be as
above. Then 
$$
\phi(R_{n,m}(f,g))=R_{n,m}(\phi(f), \phi(g)).
$$
\end{rem}

\section{Serre's Problem}\label{c2:s2}

The purpose of this section is to give Suslin's direct proof of the
following result
(Serre problem): Let $A=k[X_1,\ldots,X_n], k$ a field and
$f_1,\ldots,f_r \varepsilon A$ be such that
$\sum\limits_{i=1}^{r}Af_i=A$. Then there exists an invertible matrix
over $A$ with $(f_1,\ldots,f_r)$ as its first column.

For an interpretation of this result in terms of projective modules
see Chapter~\ref{chap4}.

Let $A$ be any commutative ring. We recall from section~\ref{c1:s4} of
Chapter~\ref{chap1} that $\underline{a}=(a_1,\ldots,a_n) \varepsilon
A^{n}$ is said to be unimodular if $\sum\limits_{i=1}^{n}Aa_i=A$ Let 
$$
U_n(A)=\left\{\underline{a} \varepsilon A^{n}\mid \underline{a} \text{
  is unimodular }\right\}
$$

We remark that, whenever convenient, the elements of $U_n(A)$ will be
written as columns.

\begin{dfn}\label{c2:dfn2.1}
Let $\underline{a}=(a_1,\ldots,a_n)\varepsilon A^{n}$. Then
$\underline{a}$ is said to be \textit{completable} if
$(a_1,\ldots,a_n)$ is the first column of an $n\times n$ invertible
matrix with entries in $A$.

It is clear that is $\underline{a}=(a_1,\ldots,a_n)$ is completable
then it is unimodular. But not every unimodular element is completable
in general. However it is obvious that any $\underline{a}\varepsilon
U_2(A)$ is completable.

Let $\alpha \varepsilon GL_n(A)$ and $\underline{a}\varepsilon
U_n(A)$. Then it is easy to see that 
$$
\alpha\cdot a=\left(\alpha\begin{bmatrix}
a_1\\
\vdots\\
a_n
\end{bmatrix}\right)^{t} \varepsilon U_n(A)
$$
(where $t$ denotes the transpose) and $GL_n(A)$ defines an action on
$U_n(A)$.

We remark that $\underline{a}=(a_1,\ldots,a_n)\varepsilon A^{n}$ is
completable if and only if it lies in the orbit of $(1,0,\ldots,0)$
under the above action of $GL_n(A)$ on $U_n(A)$.
\end{dfn}


\begin{thm}[\citeauthor{Quillen}-\citeauthor{Suslina}]\label{c2:thm2.2}
Let $k$ be a field and $A=k[X_1,\ldots,X_n]$. Then every
$\underline{a}\varepsilon U_r(A)$ is completable (or equivalently
$GL_r(A)$ acts transitively on $U_r(A)$).
\end{thm}

For a proof of this theorem, we need

\begin{lem}\label{c2:lem2.3}
Let  $A$ be any principal ideal domain. Let
$\underline{a}=\\(a_1,a_2,\ldots,a_n), a_1\neq 0$, $n\geq 3$ be
unimodular. Then there exists $\lambda \varepsilon A$, $i\geq 3$ such
that 
$$
\left(a_1,a_2+\sum\limits_{i=3}^{n}\lambda_ia_i\right)
$$
is unimodular.
\end{lem}

\begin{Proof}
Since $\underline{a}=(a_1,a_2,\ldots,a_n)$ is unimodular,
$$
\text{g.c.d}\{a_1,a_2,\ldots,a_n\} = 1
$$
let
$$
\text{g.c.d}\{a_3,a_4,\ldots,a_n\} = d
$$

So there exist $b_i\varepsilon A$, $3\leq i\leq n$ such that
$$
\sum\limits_{i=3}^{n}b_ia_1=d
$$

Clearly g.c.d.$\{a_1,a_2,d\} = 1$. Therefore $(a_1,a_2,d)\varepsilon
U_3(A)$. Let $P_1,\\P_2,\ldots,P_r$ be all the distinct prime divisors
of $a_1$. Suppose among these $p_1,\ldots,p_k$ divide $a_2$ and the
rest do not divide $a_2$. Therefore none of the primes
$p_1,p_2,\ldots,p_k$ divide $d$. Let 
\begin{align*}
a_2'&=a_2+p_{k+1}\cdots p_{r^{d}}\\
&=a_2+\lambda d, \text{ where } \lambda=p_{k+1}\cdots P_r\\
&=a_2+\lambda\sum\limits_{i=3}^{n}b_ia_i
\end{align*}

Clearly $P_i, 1\leq i\leq r$ do not divide $a'_2$. So
$(a_1,a'_2)\varepsilon U_2(A)$. Hence the assertion follows.
\enprf
\end{Proof}

\begin{coro}\label{c2:coro2.4}
Let $A$ be a principal ideal domain. Then $E_n(A)$ acts transitively
on $U_n(A)$ for $n\geq 3$.
\end{coro}

\begin{Proof}
It is enough to prove that
$\underline{a}=(a_1,a_2,\ldots,a_n)\varepsilon U_n(A)$ can be brought
to the form $(1,0,\ldots,0)$ by elementary transformations. In view of
Lemma~\ref{c2:lem2.3}, we may assume that
g.c.d. $\{a_1,a_2\}=1$. Therefore adding a suitable linear
combination of $a_1$ and $a_2$ to $a_n$, we may assume $a_n=1$. Then
by further elementary transformations. 
$$
\begin{bmatrix}
a_1\\
a_2\\
\vdots\\
1
\end{bmatrix} \xrightarrow{E_n(A)} \begin{bmatrix}
0\\
0\\
\vdots\\
1
\end{bmatrix} \xrightarrow{E_n(A)} \begin{bmatrix}
1\\
0\\
\vdots\\
0
\end{bmatrix}
$$
we can reduce it to $(1,0,\ldots,0)$.

The notation ``$\xrightarrow{E_n(A)}$'' indicates that we are applying
elementary\\ transformation.
\enprf
\end{Proof}

\begin{dfn}\label{c2:dfn2.5}
A polynomial $f(X)\varepsilon A[X]$ is said to be \textit{unitary} if
its leading coefficient is a unit. Let 
$$
f=\sum\limits_{i_1\ldots,i_n}a_{i_{1}i_{2}\ldots
  i_{n}}X^{i_{1}}_1\ldots X^{i_{n}}_n\varepsilon A[X_1,\ldots,X_n].
$$
and $S=\{(i_1,i_2,\ldots,i_n)\}$. We totally order $S$ by
lexicographic ordering in the reverse way, i.e. we say
$$
(i_1,i_2,\ldots,i_n)>(j_1,j_2,\ldots,j_n)
$$
if there exists $k$ such that 
$$
i_n=j_n,\ldots,i_k,i_{k-1}>j_{k-1}
$$

It is easy to see that 
$$
(i_1,i_2,\ldots,i_n)>(j_1,j_2,\ldots,j_n)
$$
if and only if
$$
\sum\limits_{k=1}^{n}i_kd^{k}>\sum\limits_{k=1}^{n} j_kd^{k} \text{
  for sufficiently large } d.
$$
\end{dfn}

\begin{lem}\label{c2:lem2.6}
Let 
\begin{align*}
f&=\sum\limits_{i_1,\ldots,i_n}a_{i_1,\ldots,i_n}X^{i_1}\cdots
X^{i_n}_n\\
&=\sum\limits_{\mu}a_{\mu}X^{\mu}\varepsilon A[X_1,\ldots,X_n]
\end{align*}
where $\mu=(\mu_1,\ldots,\mu_n)$ and $X^{\mu}=X^{\mu_1}_1\cdots
X^{\mu_n}_n$. Suppose \\$\nu=\sup\left\{\mu\mid a_{\mu}\neq
0\right\}$ is such that $a_{\nu}\varepsilon A^{\ast}$. If
$X_i=Y_i+X^{d^{i}}_1$, $2\leq \\i\leq n$, $d$ sufficiently large, then
$$
f(X_1,\ldots,X_n)=f\left(X_1,Y_2+X^{d^{2}}_1,\ldots,Y_n+X^{d^{n}}_1\right)
$$
is unitary in $X_1$ with coefficients in $A[Y_2,\ldots,Y_n]$.
\end{lem}

\begin{Proof}
Substituting $X_i=Y_i+X^{d^{i}}_1$, $2\leq i\leq n$ in $f$, we get 
$$
\begin{aligned}
f=&\sum\limits_{\mu}a_{\mu}X^{\mu_1}_1\cdots X^{\mu_n}_n\\
=&\sum\limits_{\mu}a_{\mu}X^{\mu_1}_1\left(Y_2+X^{d^{2}}_1\right)^{\mu_2}\cdots\left(Y_n+X^{d^{n}}_1\right)^{\mu_n}\\
&\varepsilon B[X_1] \text{ where } B=A[Y_2,\ldots,Y_n]
\end{aligned}
$$
is of degree $\nu_1+\nu_2+d^{2}+\cdots+\nu_nd^{n}$ and its leading
coefficient is\\ $a_{\nu} \varepsilon A^{\ast}$.
\enprf
\end{Proof}

\begin{coro}\label{c2:coro2.7}
Let $k$ be a field and $f(X_1,\ldots,X_n)\varepsilon
k[X_1,\ldots,X_n]$ Then for some sufficiently large integer $d$, if we
let 
$$
X_i=Y_i+X^{d^{i}}_1, 2\leq i\leq n
$$
then
$$
f(X_1,\ldots,X_n)=f\left(X_1,Y_2+X^{d^{2}}_1,\ldots,Y_n+X^{d^{n}}_1\right)
$$
is unitary in $X_1$ with coefficients in $k[Y_2,\ldots,Y_n]$.

Let $A$ and $B$ be commutative rings such that $B$ is an
$A$-algebra. So we have a ring homomorphism. $\phi: A\rightarrow B$
and $B$ can be regarded as an $A$-module (via $\phi$) by defining $ab=\phi(a)b$.
\end{coro}

\begin{lem}\label{c2:lem2.8}
Let $(f_1,f_2)\varepsilon (A[X])^{2}$ and $c\varepsilon A$ be such
that $c=f_1g_1+f_2g_2$ for some $g_1,g_2\varepsilon A[X]$. Let $B$ be
an $A$-algebra and $b,b'\varepsilon A$ be such that $b'\equiv b
\pmod{cB}$. Then there exists $\alpha\varepsilon SL_2(B)$ such that 
$$
\alpha
\begin{bmatrix}
f_1(b)\\
f_2(b)
\end{bmatrix} = \begin{bmatrix}
f_1(b')\\
f_2(b')
\end{bmatrix}
$$
\end{lem}

\begin{Proof}
We first prove for the case when $c$ is not a zero divisor in
$B$. Since $c=f_1g_1+f_2g_2$, we have
\begin{align*}
c&=f_1(b)_{g_1}(b)+f_2(b)_{g_2}(b)\\
&=f_1(b')_{g_1}(b')+f_2(b')_{g_2}(b')
\end{align*}

Let 
$$
\alpha=\frac{1}{c}
\begin{bmatrix}
f_1(b') & -g_2(b')\\
& \\
f_2(b') & g_1(b')
\end{bmatrix} \begin{bmatrix}
g_1(b) & g_2(b)\\
& \\
-f_2(b) & f_1(b)
\end{bmatrix} \varepsilon M_2(B_c).
$$
Clearly $\det \alpha=1$, therefore $\alpha\varepsilon SL_2(B_c)$. We
shall show that in fact $\alpha\varepsilon SL_2(B)$. For this it is
enough to show that each entry of the product
$$
\beta=
\begin{bmatrix}
f_1(b') & -g_2(b')\\
&\\
f_2(b') & g_1(b')
\end{bmatrix} \begin{bmatrix}
g_1(b) & g_2(b)\\
&\\
-f_2(b) & f_1(b)
\end{bmatrix}
$$
is congruent to zero $\mod (cB)$. But this follows immediately from
the fact that $b'\equiv b\pmod{cB}$. Also it is easy to check that 
$$
\alpha
\begin{bmatrix}
f_1(b)\\
f_2(b)
\end{bmatrix} = \begin{bmatrix}
f_1(b')\\
f_2(b')
\end{bmatrix}
$$

Now we prove the result for any $c$. Let $X,Y,Z$ be indeterminate. We
have
\begin{align*}
f_i(X+ZY)&=f_i(X)+Z\phi_i(X,Y,Z)\\
g_i(X+ZY)&=g_i(X)+Z\psi_i(X,Y,Z)
\end{align*}
for $i=1,2$. Since $b'\equiv b\pmod{cB}$, $b'=b+cd$, $d\varepsilon
B$. Substituting $X=b$, $Y=d$, $Z=c$, we have 
$$
f_i(b')=f_i(b)+c\phi_i(b,d,c)
$$
and
$$
g_i(b')=g_i(b)+c\psi_i(b,d,c)
$$
for $i=1,2$. Let
$$
\alpha=
\begin{bmatrix}
1+(\phi_1(b,d,c)g_1(b) & (\phi_1(b,d,c)g_2(b)\\
 +\psi_2(b,d,c)f_2(b)) &  -\psi_2(b,d,c)f_1(b))\\
& \\
(\phi_2(b,d,c)g_1(b) & 1+(\phi_2(b,d,c)g_2(b)\\
 -\psi_1(b,d,c)f_2(b)) & +\psi_1(b,d,c)f_1(b))
\end{bmatrix}
$$

It is easy to see that
$$
\alpha
\begin{bmatrix}
f_1(b)\\
\\
f_2(b)
\end{bmatrix} = \begin{bmatrix}
f_1(b)+c\phi_1(b,d,c)\\
\\
f_2(b)+c\phi_2(b,d,c)
\end{bmatrix} = \begin{bmatrix}
f_1(b')\\
\\
f_2(b')
\end{bmatrix}
$$

Now we show that $\alpha\varepsilon SL_2(B)$. Note, since
$$
f_1(X+ZY)_{g_1}(X+ZY)+f_2(X+ZY)_{g_2}(X+ZY)=c
$$
i.e.
$$
\begin{aligned}
(f_1(X)+Z&\phi_1(X,Y,Z))(g_1(X)+Z\psi_1(X,Y,Z))\\
&+(f_2(X)+Z\phi_2(X,Y,Z))(g_2(X)+Z\psi_2(X,Y,Z))=c
\end{aligned}
$$

We have
$$
\begin{aligned}
f_1(X)\psi_1(X,Y,Z)&+\phi_1(X,Y,Z)_{g_1}(X)+f_2(X)\psi_2(X,Y,Z)\\
&{}+\phi_2(X,Y,Z)_{g_2}(X)+Z(\phi_1(X,Y,Z)\psi_1(X,Y,Z)\\
&{}+\phi_2(X,Y,Z)\psi_2(x,y,z))=0
\end{aligned}
$$

Substituting for $X=b$, $Y=d$, $Z=c$, this gives $\det \alpha=1$,
which proves the lemma.

For $r\geq 3$, we identify $SL_2(B)$ as the subgroup
$$
\begin{bmatrix}
SL_2(B) & 0\\
0 & I_{r-2}
\end{bmatrix}
$$
of $GL_r(B)$.
\enprf
\end{Proof}


\begin{lem}[\citeauthor{Vasersteina}]\label{c2:lem2.9}
Let $B$ be an $A$-algebra and 
$$
V(X)=
\begin{bmatrix}
f_1(X)\\
\vdots\\
f_r(X)
\end{bmatrix}
$$
where $f_i(X)\varepsilon A[X]$, $1\leq i\leq r$. Then 
$$
I(V)=
\left\{
\begin{aligned}
c\varepsilon A &\mid \text{ there exists } \alpha \varepsilon
SL_2(B)E_r(B) \text{ such that }\\
&\alpha V(b)=V(b') \text{ for } b\equiv b'\pmod{cB}
\end{aligned}\right\}
$$
is an ideal.
\end{lem}

\begin{Proof}
It is obvious that $c\varepsilon I\Rightarrow \lambda c \varepsilon
I$, for $\lambda \varepsilon A$. Now let $c_1,c_2\varepsilon I$ and
$b\equiv b'\pmod{(c_1+c_2)}$. So there exists $\lambda \varepsilon B$
such that 
$$
b-\lambda c_1=b'+\lambda c_2
$$
since there exist $\alpha_1,\alpha_2\varepsilon SL_2(B)E_r(B)$ such
that 
\begin{align*}
&\alpha_1V(b)=V(b-\lambda c_1)\\
&\alpha_2V(b-\lambda c_1)=\alpha_2 V(b'+\lambda c_2)=V(b')
\end{align*}
so if we let $\alpha=\alpha_2\alpha_1$ the lemma follows.

We remark that if $\beta \varepsilon SL_2(B)E_r(B)$ then clearly
$I(V)=I(\beta V)$.
\end{Proof}


\begin{lem}\label{c2:lem2.10}
Let $c=f_1g_1+f_2g_2\varepsilon A$, $f_i,g_i \varepsilon A[X]$, for
$i=1,2$ and 
$$
V(X)=
\begin{bmatrix}
f_1(X)\\
\vdots\\
f_r(X)
\end{bmatrix}
$$
where $f_i(X)\varepsilon A[X]$, $1\leq i\leq r$. Let $B$ be an
$A$-algebra and $b,b' \varepsilon B$ be such that $b\equiv
b'\pmod{cB}$. Then there exists $\alpha \varepsilon SL_2(B)E_r(B)$
such that
$$
\alpha V(b)=V(b')
$$
\end{lem}

\begin{Proof}
Since 
\begin{align*}
c&=f_1(b)g_1(b)+f_2(b)g_2(b)\\
&=f_1(b')_{g_1}(b')+f_2(b')_{g_2}(b'),
\end{align*}
by Lemma~\ref{c2:lem2.8}, it follows that 
$$
\begin{bmatrix}
f_1(b)\\
f_2(b)\\
f_3(b)\\
\vdots\\
f_r(b)
\end{bmatrix} \xrightarrow{SL_2(B)} \begin{bmatrix}
f_1(b')\\
f_2(b')\\
f_3(b)\\
\vdots\\
f_r(b)
\end{bmatrix}
$$

Since $b'\equiv b\pmod{cB}$, we have $f_i(b')\equiv f_i(b)\pmod{cB}$
for $1\leq i\leq r$. 

Let 
\begin{align*}
f_i(b')&=f_i(b)+\lambda_ic\\
&=f_i(b)+\lambda_i(f_1(b')_{g_1}(b')+f_2(b')_{g_2}(b'))
\end{align*}
where $\lambda_1\varepsilon B$. So it follows that 
$$
\begin{bmatrix}
f_1(b')\\
f_2(b')\\
f_3(b)\\
\vdots\\
f_r(b)
\end{bmatrix}\xrightarrow{E_r(B)} \begin{bmatrix}
f_1(b')\\
f_2(b')\\
f_3(b')\\
\vdots\\
f_r(b')
\end{bmatrix}
$$

This completes the proof of the lemma.
\enprf
\end{Proof}


\begin{thm}[\citet{Suslina}]\label{c2:thm2.11}
Let $A$ be a commutative ring and $B$ an $A$-algebra. Let
$(f_1,\ldots,f_r)\varepsilon U_r(A[X])$, where $f_1$ is unitary and
let 
$$
V(X)=
\begin{bmatrix}
f_1\\
\vdots\\
f_r
\end{bmatrix}
$$

Then given $b,b'\varepsilon B$, there exists $\alpha \varepsilon
SL_2(B)E_r(B)$ such that 
$$
\alpha V(b)=V(b')
$$
\end{thm}

\begin{Proof}
In view of Lemma~\ref{c2:lem2.9}, it is sufficient to prove that for
any maximal ideal $\underline{m}$ of $A$, there exists $c\varepsilon
I(V)$ such that $c\varepsilon A-\underline{m}$. 

Suppose $\underline{m}$ is a maximal ideal in $A$. Let $\overline{f}$
be the image of $f$ in $\left(\dfrac{A}{\underline{m}}\right)[X]$. By
Lemma~\ref{c2:lem2.3}, there exists polynomials
$\overline{h}_3,\ldots,\overline{h}_r \varepsilon
\left(\dfrac{A}{\underline{m}}\right)[X]$ such that 
$$
\left(\overline{f}_1,\overline{f}_2+\overline{h}_3\overline{f}_3+\cdots+\overline{h}_r\overline{f}_r\right)
$$
is unimodular. Let 
$$
f_2+h_3f_3+\cdots+h_rf_r=f'_2,\left(h_i\rightarrow \overline{h}_i\right)
$$

Clearly, we have
$$
\begin{bmatrix}
f_1\\
f_2\\
f_3\\
\vdots\\
f_r
\end{bmatrix} \xrightarrow{E_r(B)} \begin{bmatrix}
f_1\\
f'_2\\
f_3\\
\vdots\\
f_r
\end{bmatrix}
$$

Since $I(V)=I(\beta V)$ for any $\beta \varepsilon SL_2(B)E_r(B)$, we
may be replacing $f_2$ by $f'_2$ if necessary, assume that
$\left(\overline{f}_1,\overline{f}_2\right)$ is unimodular. Since
$f_1$ is unitary therefore by Proposition~\ref{c2:Prop1.3} and
Remark~\ref{c2:rem1.4}, we have 
$$
R(f_1,f_2)\nvar \underline{m}
$$

But by Lemma~\ref{c2:lem2.8}, 
$$
c=R(f_1,f_2)=f_1f'+f_2g'\varepsilon I(V).
$$

This completes the proof of the theorem.
\enprf
\end{Proof}

\begin{coro}\label{c2:coro2.12}
Let $B=A[X]$. Then there exists $\alpha \varepsilon
SL_2(A[X])E_r(A[X])$ such that 
$$
\alpha
\begin{bmatrix}
f_1(X)\\
\vdots\\
f_r(X)
\end{bmatrix} = \begin{bmatrix}
f_1(0)\\
\vdots\\
f_r(0)
\end{bmatrix}
$$
where $(f_1(X),\ldots,f_r(X)) \varepsilon U_r(A[X])$ and $f_1(X)$ is unitary.
\end{coro}


\begin{PRF}
Let $(f_1,\ldots,f_r) \varepsilon U_r(k[X_1,\ldots,X_n])$ and 
$$
V(X_1,\ldots,X_n)=
\begin{bmatrix}
f_1\\
\vdots\\
f_r
\end{bmatrix}
$$

We may assume that $f_1\neq 0$. Further in view of
Lemma~\ref{c2:lem2.6}, by change of variables if necessary, we may
assume that $f_1$ is unitary with coefficients in
$k[X_2,\ldots,X_n]$. We let $A=[[X_2,\ldots,X_n]$ and 

$B=A[X_1]=K[X_1,X_2,\ldots,X_n]$. By Corollary~\ref{c2:coro2.12},
  there exists $\alpha \varepsilon SL_2(A[X_1])E_r(A[X_1])$ such that 
$$
\alpha V(X_1,\ldots,X_n)=V(0,X_2,\ldots,X_n)
$$
Now the theorem follows by induction on $n$.
\enprf
\end{PRF}


\begin{rem}\label{c2:rem2.13}
Let $A=k[X_1,\ldots,X_n]$. We have shown in Theorem~\ref{c2:thm2.2}
that the group $SL_2(A)E_r(A)$ acts transitively on $U_r(A)$. In fact
even $E_r(A)$ acts transitively on $U_r(A)$ for $r\geq 3$. For let
$\alpha \varepsilon SL_2(A)$, $\beta \varepsilon E_r(A)$ be such that
$$
\begin{bmatrix}
f_1\\
\vdots\\
f_{r-1}\\
f_r
\end{bmatrix} =\beta\alpha \begin{bmatrix}
0\\
\vdots\\
0\\
1
\end{bmatrix}
$$
where $(f_1,\ldots,f_r)\varepsilon U_r(A)$. Since $r\geq 3$, it
follows that 
$$
\beta\alpha
\begin{bmatrix}
0\\
\vdots\\
0\\
1
\end{bmatrix} = \beta \begin{bmatrix}
0\\
\vdots\\
0\\
1
\end{bmatrix}
$$
and our assertion follows.
\end{rem}

\section{Special linear group over polynomial rings}\label{c2:s3}

The purpose of this section is to prove Theorem~\ref{c2:thm3.1}. This
is a particular case of a theorem of Suslin proved for arbitrary
commutative Noetherian rings of finite Krull dimension (see
\cite[][Theorem~6.3]{Suslina}).

We recall that Theorem~\ref{c2:thm3.1} is false for $m=2$, (see
Corollary~\ref{c1:coro3.5} of Chapter~\ref{chap1}).

\begin{thm}[\citet{Suslina}]\label{c2:thm3.1}
Let $k$ be a field and $A=k[X_1,\ldots,X_n]$. Then 
$$
SL_m(A)=E_m(A)\text{ for } m\geq 3.
$$

For a proof of this, we need
\end{thm}

\begin{lem}\label{c2:lem3.2}
Let $A$ be any commutative ring. Suppose $E_m(A)$ acts transitively
on $U_m(A)$ for $m\geq r+1$. Then 
$$
SL_m(A)=SL_r(A)E_m(A),\text{ for } m\geq 3.
$$
\end{lem}

\begin{Proof}
Let $\alpha \varepsilon SL_m(A)$. Suppose
$$
\alpha=
\begin{bmatrix}
\ast & \cdots & g_1 & f_1\\
\ast & \cdots & g_2 & f_2\\
\vdots & & \vdots & \vdots\\
\ast & \cdots & g_m & f_m
\end{bmatrix}
$$

Since $E_m(A)$ acts transitively on $U_m(A)$ for $m\geq r+1$, there
exists $\beta \varepsilon E_m(A)$ such that 
$$
\beta
\begin{bmatrix}
f_1\\
\vdots\\
f_{m-1}\\
f_m
\end{bmatrix} = \begin{bmatrix}
0\\
\vdots\\
0\\
1
\end{bmatrix}
$$

Then
$$
\beta\alpha=
\begin{bmatrix}
& & & 0\\
& \ast &  &\vdots\\
& & & 0\\
\ast & \cdots & \ast & 1
\end{bmatrix}
$$

Using $1$ in the last column, we may, by elementary column operations,
transform the last row to $(0,0,\ldots,0,1)$. Hence
$$
\beta\alpha\epsilon=
\begin{bmatrix}
\alpha' & 0\\
0 & 1
\end{bmatrix}
$$
for some $\alpha' \varepsilon SL_{m-1}(A)$, $\epsilon \varepsilon
E_m(A)$. Further since $E_m(A)$ is a normal subgroup of $SL_m(A)$ for
$m\geq 3$, we have 
$$
\beta'\alpha=
\begin{bmatrix}
\alpha'& 0\\
0 & 1
\end{bmatrix}, \text{ form some } \beta' \varepsilon E_m(A)
$$

By successively applying the above argument, it clearly follows that
there exists $\beta''$ in $E_m(A)$ such that 
$$
\beta''\alpha=
\begin{bmatrix}
\gamma & 0\\
0 &I_{m-r}
\end{bmatrix}
$$
where $\gamma \varepsilon SL_r(A)$. This proves the lemma.
\enprf
\end{Proof}


\begin{coro}\label{c2:coro3.3}
Let $k$ be a field and $A=k[X_1,\ldots,X_n]$. Then 
$$
SL_m(A)=SL_2(A)E_m(A),m\geq 3
$$
\end{coro}

\begin{Proof}
Follows from Remark~\ref{c2:rem2.13} and Lemma~\ref{c2:lem3.2}

We remark that in view of the above Corollary, it is clear that in
order to prove Theorem~\ref{c2:thm3.1}, it will suffice to show that 
$$
\alpha\varepsilon SL_2(A)\Rightarrow
\begin{bmatrix}
\alpha & 0\\
0 & 1
\end{bmatrix} \varepsilon E_3(A).
$$

To do this, we introduce the notion of Mennicke symbol and derive some
of its properties:
\enprf
\end{Proof}

\begin{dfn}\label{c2:dfn3.4}
Let $A$ be a commutative ring and $(a,b)\varepsilon U_2(A)$. Let
$c,d\varepsilon A$ be such that $ad-bc=1$. The \textit{Mennicke
  symbol} $\begin{pmatrix}
a\\
b
\end{pmatrix}$ is defined as: 
$$
\begin{pmatrix}
a\\
b
\end{pmatrix} = \text{ class of } \begin{bmatrix}
a & b & 0\\
c & d & 0\\
0 & 0 & -1
\end{bmatrix} \varepsilon \frac{SL_3(A)}{E_3(A)}.
$$

First we show that the Mennicke Symbol of $(a, b) \varepsilon U_2(A)$
is independent of the choice of $c,d$ in $A$. For let
$c',d'\varepsilon A$ be such that $ad'-bc'=1$. So
$$
\begin{aligned}
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix} \begin{bmatrix}
a & b\\
c' & d'
\end{bmatrix}^{-1} &= \begin{bmatrix}
a & b\\
c & d
\end{bmatrix} \begin{bmatrix}
d' & -b\\
-c'&  a
\end{bmatrix}\\ &= \begin{bmatrix}
1 & 0\\
cd'-dc' & 1
\end{bmatrix} \varepsilon E_2(A)
\end{aligned}
$$

Hence we have
$$
\begin{bmatrix}
a & b & 0\\
c & d & 0\\
0 & 0 & 1
\end{bmatrix} \equiv \begin{bmatrix}
a & b & 0\\
c' & d' & 0\\
0 & 0 & 1
\end{bmatrix} \pmod{E_3(A)}
$$
\end{dfn}

\begin{Prop}\label{c2:Prop3.5}
The Mennicke symbol satisfy the following properties: 
\begin{enumerate}[(i)]
\item $\begin{pmatrix}
u\\
b
\end{pmatrix}=1$ for $u\varepsilon A^{\ast}$
\item $\begin{pmatrix}
aa'\\
b
\end{pmatrix} = \begin{pmatrix}
a\\
b 
\end{pmatrix} \begin{pmatrix}
a'\\
b 
\end{pmatrix}$
\item $\begin{pmatrix}
a\\
b 
\end{pmatrix} = \begin{pmatrix}
b\\
a
\end{pmatrix}$
\item $\begin{pmatrix}
a+\lambda b\\
b
\end{pmatrix} = \begin{pmatrix}
a\\
b 
\end{pmatrix}, \lambda \varepsilon A$.
\end{enumerate}
\end{Prop}

\textbf{Proof}(i) Since
$$
\begin{bmatrix}
u & b\\
0 & u^{-1}
\end{bmatrix} \xrightarrow{E_2(A)} \begin{bmatrix}
u & 0\\
0 & u^{-1}
\end{bmatrix}
$$
and $\begin{bmatrix}
u & 0\\
0 & u^{-1}
\end{bmatrix} \varepsilon E_s(A)$ (see Lemma~\ref{c1:lem2.2} of
Chapter~\ref{chap1}).

Therefore $\begin{pmatrix}
u\\
b
\end{pmatrix}=1$.

(ii) Let $c,d,c',d'\varepsilon A$ be such that $ad-bc=1$ and
$a'b'-bc'=1$. Since
$$
\begin{aligned}
\begin{bmatrix}
1 & 0 & 0\\
0 & 0 & -1\\
0 & 1 & 0
\end{bmatrix} \begin{bmatrix}
a & b & 0\\
c & d & 0\\
0 & 0 & 1
\end{bmatrix}& \begin{bmatrix}
1 & 0 & 0\\
0 & 0 & 1\\
0 & -1 & 0
\end{bmatrix} \begin{bmatrix}
a & b & 0\\
c & d & 0\\
0 & 0 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0 & 0\\
0 & 0 & -1\\
0 & 1 & 0
\end{bmatrix} \begin{bmatrix}
1 & 0 & 0\\
0 & 1 & a\\
0 & 0 & 1
\end{bmatrix}\\
&{}=\begin{bmatrix}
aa' & b & 0\\
c' & 0 &-d'\\
a'c & d & 1
\end{bmatrix}\\ &\equiv{}\begin{bmatrix}
aa' & b & 0\\
\ast & \ast & 0\\
0 & 0 & 1
\end{bmatrix} \pmod{E_3(A)},
\end{aligned}
$$
we have
$$
\begin{pmatrix}
a\\
b
\end{pmatrix} \begin{pmatrix}
a'\\
b
\end{pmatrix} = \begin{pmatrix}
aa'\\
b
\end{pmatrix}
$$

(iii) Let $c,d\varepsilon A$ be such that $ad-bc=1$. Since
$$
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix} \begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix} = \begin{bmatrix}
-b & a\\
-d & c
\end{bmatrix}
$$

Therefore
\begin{align*}
\begin{pmatrix}
a\\
b
\end{pmatrix} = \begin{pmatrix}
-b\\
a
\end{pmatrix}&= \begin{pmatrix}
-1\\
a
\end{pmatrix} \begin{pmatrix}
b\\
a
\end{pmatrix}\quad \text{ by (ii) }\\
&=\begin{pmatrix}
b\\
a
\end{pmatrix} \quad\text{ by (i) }
\end{align*}

(iv) This is obvious.

\begin{lem}\label{c2:lem3.6}
Let $A$ be any commutative local ring. Suppose $(f,g)\varepsilon\\
U_2(A[X])$ where $f$ is unitary. Then 
$$
\begin{pmatrix}
f\\
g
\end{pmatrix} =1
$$
\end{lem}

\begin{Proof}
We shall prove this by induction on $\deg f$. By Lemma~\ref{c3:lem3.5}
the assertion holds if $\deg f=0$. Suppose $\deg f=n>0$ and $\deg
g=m$. Let $q,r\varepsilon A[X]$ be such that 
$$
g=qf+r, \deg r<n.
$$

Then by Lemma~\ref{c3:lem3.5}
$$
\begin{pmatrix}
f\\
g
\end{pmatrix} = \begin{pmatrix}
f\\
g-qf
\end{pmatrix} = \begin{pmatrix}
f\\ 
r
\end{pmatrix}
$$
so we may assume $\deg g<n$. Since $A$ is local and $(f,g)$ is
unimodular, it follows that either $f(0)$ or $g(0)$ is a unit. We
first suppose that $g(0)$ is a unit. Then we have 
$$
\begin{pmatrix}
f\\
g
\end{pmatrix} = \begin{pmatrix}
f-g(0)^{-1}f(0)g\\
g
\end{pmatrix}
$$
So we may assume that $f(0)=0$ Let $f=Xf'$. Now we have 
$$
\begin{pmatrix}
f\\
g
\end{pmatrix} = \begin{pmatrix}
Xf'\\
g
\end{pmatrix} = \begin{pmatrix}
X\\
g
\end{pmatrix} \begin{pmatrix}
f'\\
g
\end{pmatrix}
$$
since $\deg f'<n$, the proof is complete in this case. 

Now suppose that $g(0)$ is not a unit. Let $R(f,g)$ be the resultant
of $f,g$. Then 
$$
c=R(f,g)=f'f+g'g
$$
with $\deg f'<m$, $\deg g' <n$ (see section~\ref{c2:s1}). Since $f$ is
unitary and $(f,g)$ is unimodular, by Proposition~\ref{c2:Prop1.3} and
Remark~\ref{c2:rem1.4}, $R(f,g){\nvar} \underline{m}$, the maximal ideal
of $A$. Hence $c\varepsilon A^{\ast}$. By suitably changing $f'$ and
$g'$ (dividing by $c$ and changing sign of $g'$) we may assume that 
$$
f'f-g'g=1
$$

Clearly $f'(0)\nvar \underline{m}$ because $g(0)\varepsilon
\underline{m}$. Now 
$$
\begin{bmatrix}
f & g & 0\\
g' & f' & 0\\
0 & 0 & 1
\end{bmatrix}\equiv \begin{bmatrix}
f+g' & g+f' & 0\\
g' & f' & 0\\
0 & 0 & 1
\end{bmatrix} \pmod{E_3(A)}
$$

So 
$$
\begin{pmatrix}
f\\
g
\end{pmatrix} = \begin{pmatrix}
f+g'\\
g+f'
\end{pmatrix}
$$
since $g(0)+f'(0)$ is a unit and $f+g'$ is unitary, we are reduced to
the earlier case when $g(0)\varepsilon A^{\ast}$. This completes the proof.
\enprf
\end{Proof}

For any Mennicke symbol $\begin{pmatrix}
a\\
b
\end{pmatrix}$ and any maximal ideal $\underline{m}$ of $A$, we shall
denote by $\begin{pmatrix}
a\\
b 
\end{pmatrix}_{\underline{m}}$, the image of the symbol
$\begin{pmatrix}
a\\
b
\end{pmatrix}$ in $\dfrac{SL_3(A_{\underline{m}})}{E_3(A_{\underline{m}})}$.

\begin{lem}\label{c2:lem3.7}
Let $(f,g)\varepsilon U_2(A[X])$. Suppose that for every maximal ideal
$\underline{m}$ of $A$, 
$$
\begin{pmatrix}
f\\
g
\end{pmatrix}_{\underline{m}}=\begin{pmatrix}
a\\
b
\end{pmatrix},\quad \text{ for some } a,b\varepsilon A_{\underline{m}}
$$

Then
$$
\begin{pmatrix}
f\\
g
\end{pmatrix}= \begin{pmatrix}
f(0)\\
g(0)
\end{pmatrix}
$$
\end{lem}

\begin{Proof}
Let
$$
\alpha=
\begin{bmatrix}
f & g & 0\\
\ast & \ast & 0\\
0 & 0 & 1
\end{bmatrix} \varepsilon SL_3(A[X])
$$

By hypothesis $\alpha_{\underline{m}}\varepsilon
SL_3(A_{\underline{m}})E_3(A_{\underline{m}}[X])$, for any maximal
ideal $\underline{m}$ of $A$. Hence by Theorem~\ref{c1:thm5.3} of
Chapter~\ref{chap1},
$$
\alpha=\alpha(0)\epsilon,\text{ where }\epsilon \varepsilon E_3(A[X]).
$$

This implies that 
$$
\begin{pmatrix}
f\\
g
\end{pmatrix} = \begin{pmatrix}
f(0)\\
g(0)
\end{pmatrix}
$$
\enprf
\end{Proof}

\begin{coro}\label{c2:coro3.8}
Let $(f,g)\varepsilon U_2(A[X])$ with $f$ unitary. Then 
$$
\begin{pmatrix}
f\\
g
\end{pmatrix} = \begin{pmatrix}
f(0)\\
g(0)
\end{pmatrix}
$$
\end{coro}

\begin{Proof}
Immediate from Lemma~\ref{c2:lem3.6} and Lemma~\ref{c2:lem3.7}
\enprf
\end{Proof}

\begin{PRFF}
By the remark following Corollary~\ref{c2:coro3.3}, it suffices to
show that for every $(f,g)\varepsilon U_2(A)$,
$(A=k[X_1,\ldots,X_n])$, the Mennicke symbol
$$
\begin{pmatrix}
f\\
g
\end{pmatrix} =1
$$

This we do by induction on $n$. By Corollary~\ref{c2:coro2.7}, we may
assume by change of variables that $f$ is unitary in $B[X]$, where
$B=k[X_1,\ldots,X_{n-1}]$ and $X=X_n$. Hence by
Corollary~\ref{c2:coro3.8},
$$
\begin{pmatrix}
f\\
g
\end{pmatrix} = \begin{pmatrix}
f_1\\
g_1
\end{pmatrix}, f_1,g_1\varepsilon B.
$$
This finishes the proof.
\enprf
\end{PRFF}
