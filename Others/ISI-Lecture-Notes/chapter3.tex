\chapter{}\label{chap3}

\section{Suslin's Analogue of Horrocks's Theorem}\label{c3:s1}

In this section we prove a result of \citeauthor{Suslina} (Theorem~\ref{c3:thm1.1}),
which is an analogue of Horrocks's theorem on projective modules. We
use the following Theorem~\ref{c3:thm4.1}, which we shall prove in section~\ref{c3:s4}.

%\begin{THM}
%Let $A$ be any commutative local ring with maximal ideal
%$\underline{m}$. Then
%$$
%E_n(A[X,X^{-1}])\cap GL_n(A[X,X^{-1}],\underline{m}[X,X^{-1}])=G_{+}G_{-}
%$$
%where
%\begin{align*}
%G_+&=E_n(A[X])\cap GL_n(A[X],\underline{m}[X])\\
%G_-&=E_n(A[X^{-1}])\cap GL_n(A[X^{-1}], \underline{m}[X^{-1}]).
%\end{align*}
%\end{THM}

\begin{thm}[\citeauthor{Suslina}]\label{c3:thm1.1}
Let $A$ be any commutative ring and $\alpha \varepsilon\\ GL_n(A[X])
n\geq 3$. . Suppose there exists $\beta \varepsilon GL_n(A[X^{-1}])$
such that $\alpha\beta^{-1}\varepsilon E_n\\(A[X,X^{-1}])$. Then $\alpha
\varepsilon GL_n(A)E_n(A[X])$. 
\end{thm}

\begin{Proof}
By the localization theorem~\ref{c1:thm5.3} of Chapter~\ref{chap1}, it
is enough to prove the theorem for a local ring. So we can assume that
$A$ is local. Also without loss of generality we can assume that
$\alpha(0)=1$ (by replacing $\alpha$ by $\alpha\cdot \alpha (0)^{-1}$
and $\beta$ by $\beta\cdot \alpha(0)^{-1}$). Since $\det
(\alpha\beta^{-1})=1$, we have 
$$
\det \alpha =\det \beta \varepsilon A[X]\cap A[X^{-1}]=A
$$
and therefore $\det \alpha=1=\det \beta$.

Now let $\underline{m}$ be the maximal ideal of $A$ and
$k=\frac{A}{\underline{m}}$. We have a canonical mapping
$$
SL_n(A[X])\rightarrow SL_n(k[X])
$$

Let $\overline{\alpha}$ be the image of $\alpha$ in
$SL_n(k[X])$. Since $k[X]$ is a Euclidean domain, by
Proposition~\ref{c1:Prop2.4} of Chapter~\ref{chap1}, we have
$SL_n(k[X])=E_n(k[X])$. Let $\gamma$ be a list of $\overline{\alpha}$
in $E_n(A[X])$. Then ${\overline{\gamma}^{-1}}\overline{\alpha}=1$. Therefore
$$
\gamma^{-1}\alpha\varepsilon GL_n(A[X],\underline{m}[X]).
$$

Similarly, we can find $\delta \varepsilon E_n(A[X^{-1}])$ such that 
$$
\delta^{-1}\beta \varepsilon GL_n\left(A[X^{-1}], \underline{m}[X^{-1}]\right).
$$

So we may assume that 
$$
\alpha\varepsilon GL_N(A[X],\underline{m}[X])
$$
and 
$$
\beta \varepsilon GL_n\left(A[X^{-1}], m[X^{-1}]\right)
$$

Now
\begin{align*}
\alpha\beta^{-1}&\varepsilon E_n(A[X,X^{-1}])\cap
GL_n(A[X,X^{-1}],\underline{m}[X,X^{-1}])\\
&=G_+G_{-}\qquad ( \text{ by Theorem~\ref{c3:thm4.1} }).
\end{align*}

Therefore
$$
\alpha\beta^{-1}=\alpha_+\alpha_{-},
$$
where
\begin{align*}
\alpha_+\varepsilon G_+ &=E_n(A[X])\cap GL_n(A[X],\underline{m}[X])\\
\alpha_{-}\varepsilon G_{-}&=E_n\left(A[X^{-1}]\right) \cap GL_n\left(A[X^{-1}], \underline{m}[X^{-1}]\right).
\end{align*}

So 
$$
\begin{aligned}
\alpha^{-1}_+\alpha=\alpha_{-}\beta&=\alpha_0( \text{ say })\\
&{}\varepsilon GL_n(A[X])\cap GL_n\left(A[X^{-1}]\right)\\
&{}=GL_n(A).
\end{aligned}
$$

Therefore, $\alpha\varepsilon E_n(A[X])GL_n(A)=GL_n(A)E_n(A[X])$. This
proves the theorem.
\enprf
\end{Proof}

We shall now give some applications of Theorem~\ref{c3:thm1.1} (see
Corollary~\ref{c3:coro1.4} and \ref{c3:coro1.5}). For this we need 

\begin{lem}\label{c3:lem1.2}
Let $G$ be any group and $a_i$, $b_j \varepsilon G$ for $1\leq i,j\leq
m$. Then  
$$
\prod\limits_{i=1}^{m}(a_ib_i)=\prod\limits_{i=0}^{m-1}\left(\gamma_{m-i}a_{m-i}\gamma^{-1}_{m-i}\right)
\prod\limits_{i=1}^{m} b_i
$$
where 
$$
\gamma_k=\prod\limits_{i=1}^{k-1}(a_ib_i) \text{ and }
\gamma_1=1,2,\leq k\leq m.
$$
\end{lem}

\begin{Proof}
Obvious.
\enprf
\end{Proof}

\begin{lem}\label{c3:lem1.3}
Let $A$ be any commutative ring. Let $a,b \varepsilon A$ be such that
$Aa+Ab=A$ and $\alpha \varepsilon E_n(A_{ab})$. Then $\alpha$ can be
written as 
$$
\alpha=(\alpha_1)(\alpha_2)_b
$$
where $\alpha_1\varepsilon E_n(A_b)$ and $\alpha_2 \varepsilon E_n(A_a)$.
\end{lem}

\begin{Proof}
Since $\alpha \varepsilon E_n(A_{ab})$, we can write
$$
\alpha=\prod\limits_{k=1}^{m}e_{i_{k}j_{k}}(\lambda_k),\lambda_k\varepsilon A_{ab}.
$$

Let 
\begin{align*}
\gamma_1&=1\\
\gamma_p&=\prod\limits_{k=1}^{p-1}e_{i_kj_k}(\lambda_k)\text{ for }
2\leq p \leq m
\end{align*}
and
$$
\tau_p(Z)=\gamma_pe_{i_pj_p}(\lambda_{p}Z)\gamma^{-1}_p\varepsilon
E_n(A_{ab}[Z]),1\leq p\leq m.
$$

Clearly
$$
\tau_P(0)=1.
$$

By Lemma~\ref{c1:lem5.5} of Chapter~\ref{chap1}, there exists
$\epsilon_P(Z)\varepsilon E_n(A_b[Z])$ such that 
$$
(\epsilon_P(Z))_a=\tau_p\left(a^{N}Z\right)
$$
for $1\leq p\leq m$ and for sufficiently large $N$. By choosing $N$
still bigger if necessary, we may assume that 
$$
b^{n}\lambda_k\varepsilon A_a\text{ for }1\leq k\leq m.
$$

Now since $Aa+Ab=A$, we have $Aa^{N}+Ab^{N}=A$. So there exist
$\lambda,\mu\varepsilon A$ such that $\lambda a^{N}+\mu b^{4}=1$. We
have
\begin{align*}
\alpha&=\prod\limits_{k=1}^{m}e_{i_kj_k}\left(\lambda
a^{N}\lambda_k+\mu b^{N}\lambda_k\right)\\
&=\prod\limits_{k=1}^{m}\left(e_{i_kj_k}\left(\lambda
a^{N}\lambda_k\right)e_{i_kj_k}\left(\mu
b^{N}\lambda_{k}\right)\right)\\
&=\prod\limits_{k=0}^{m-1}\left(\gamma_{m-k}e_{i_{m-k}j_{m-k}}\left(\lambda
a^{N}\lambda_{m-k}\right)\gamma^{-1}_{m-k}\right)
\prod\limits_{k=1}^{m}e_{i_kj_k}\left(\mu b^{N}\lambda_k\right)
\end{align*}
by Lemma~\ref{c3:lem1.2},
\begin{align*}
&=\prod\limits_{k=0}^{m-1}\tau_{m-k}\left(\lambda a^{N}\right)
  \prod\limits_{k=1}^{m}e_{i_kj_k}\left(\mu b^{N}\lambda_k\right)\\
&=\prod\limits_{k=0}^{m-1}\left(\epsilon_{m-k}(\lambda)\right)_a\prod\limits_{k=1}^{m}e_{i_kj_k}\left(\mu
  b^{N}\lambda_k\right)\\
&=(\alpha_1)_a(\alpha_2)_b
\end{align*}
where
\begin{align*}
\alpha_1&=\prod\limits_{k=0}^{m-1}\epsilon_{m-k}(\lambda)\varepsilon
E_n(A_b)\\
\alpha_2&=\prod\limits_{k=1}^{m}e_{i_kj_k}\left(\mu
b^{N}\lambda_k\right) \varepsilon E_n(A_a)
\end{align*}

This completes the proof.
\enprf
\end{Proof}

\begin{coro}\label{c3:coro1.4}
Let $f\varepsilon A[X]$ be a monic polynomial. Let $\alpha \varepsilon
GL_n(A[X])$, $n\geq 3$, be such that $\alpha\varepsilon
E_n\left(A\left[X,\frac{1}{f}\right]\right)$. Then $\alpha\varepsilon E_n(A[X])$.
\end{coro}

\begin{Proof}
Let
$$
f=X^{n}+a_{n-1}X^{n-1}+\cdots+a_{o},a_i\varepsilon A.
$$

Then we can write
$$
f=X^{n}g(Y)
$$
where $Y=X^{-1}$ and $g(Y)=1+a_{n-1}Y+\cdots+a_oY^{n}$. We note that 
$$
A\left[X,X^{-1},\frac{1}{f}\right]=A\left[Y,Y^{-1},\frac{1}{Y^{-n}}g(Y)\right]=A\left[Y,Y^{-1},\frac{1}{g}(Y)\right]
$$
and 
$$
\alpha \varepsilon E_n\left(A\left[X,\frac{1}{f}\right]\right)\subset
E_n\left(A\left[X,X^{-1},\dfrac{1}{f}\right]\right)=E_n\left(A\left[Y,Y^{-1},\frac{1}{g}(Y)\right]\right).
$$

Since
$$
g(Y)A[Y]+YA[Y]=A[Y],
$$

we have by Lemma~\ref{c3:lem1.3},
$$
\alpha=\alpha_1\alpha_2
$$
where
$$
\alpha_1\varepsilon E_n\left(A\left[Y,Y^{-1}\right]\right),
\alpha_2\varepsilon E_n\left(A\left[Y,\frac{1}{g}(Y)\right]\right).
$$

Further since
$$
A[Y,Y^{-1}]\cap A\left[Y,\frac{1}{g}(Y)\right]=A[Y],
$$
we have
$$
GL_n\left(A\left[Y,Y^{-1}\right]\right)\cap GL_n\left(A\left[Y\frac{1}{g}(Y)\right]\right)=GL_n(A[Y]).
$$

Therefore
$$
\alpha^{-1}_1\alpha=\alpha_2\varepsilon
GL_n\left(A\left[Y,Y^{-1}\right]\right)\cap GL_n\left(A\left[Y,\frac{1}{g}(Y)\right]\right)=GL_n(A[Y]).
$$

Now by Theorem~\ref{c3:thm1.1}, we have 
$$
\alpha\varepsilon GL_n(A)E_n(A[X]).
$$

Let $\alpha=\alpha_{o}\epsilon, \alpha_o\varepsilon GL_n(A)$, $\epsilon\varepsilon
E_n(A[X])$. So we have $\alpha(1)=\alpha_o\epsilon(1)$. 

Therefore to show $\alpha \varepsilon E_n(A[X])$, it is enough to show
that $\alpha(1)\varepsilon\\ E_n(A)$. By applying
Theorem~\ref{c3:thm1.1} to $\alpha_2 \varepsilon GL_n(A[Y])$, we see
that $\alpha_2\varepsilon \\GL_n(A)E_n(A[Y])$. But $\alpha_2(0)
\varepsilon E_n(A)$ and so $\alpha_2 \varepsilon
E_n(A[Y])$. Now \\$\alpha(1)=\alpha_1(1)\alpha_2(1)\varepsilon
E_n(A)$, and hence $\alpha \varepsilon E_n(A)$.
\enprf
\end{Proof}

\begin{coro}\label{c3:coro1.5}
Let $k$ be a field and $A=k[X_1,\ldots,X_n]$. Then
$$
SL_m(A)=E_m(A)\text{ for } m\geq 3.
$$
\end{coro}

\begin{Proof}
We prove the result by induction on $n$. For $n=0$, this is clear. Let 
$$
\alpha\varepsilon SL_m(k[X_1,\ldots,X_n]).
$$

Then since
$$
SL_m(k[X_1,\ldots,X_n])\subset SL_m(k(X_1)[X_2,\ldots,X_n]),
$$
we have by induction
$$
a\varepsilon E_m(k(X_1)[X_2,\ldots,X_n]).
$$

So there exists $f\varepsilon k[X_1]$ such that 
$$
\alpha\varepsilon E_m\left(k[X_2,\ldots,X_n]\left[X_1,\frac{1}{f}\right]\right).
$$

Now by Corollary~\ref{c3:coro1.4}, $\alpha \varepsilon
E_m(k[X_1,\ldots,X_n])$ for $m\geq 3$. 
\enprf
\end{Proof}


\section{Structure of \texorpdfstring{$SL_n(k)$}{SLnk}(Bruhat Decomposition)}\label{c3:s2}

In this section we give a structure theorem for $SL_n(k)$ (see
Theorem~\ref{c3:thm2.5}), for any field $k$. The proof given here is
lifted from \cite{Milnor}.

We first introduce some notation. For any commutative ring $A$, we let

\begin{tabular}{lll}
$T(A)$&=&subgroup of $SL_n(A)$ consisting of all upper \\
&& triangular matrices with $1$ in the main diagonal.\\
$\tilde{T}(A)$&=&subgroup of $SL_n(A)$ consisting of all upper\\
& &triangular matrices.\\
$D(A)$&=& subgroup of $SL_n(A)$ consisting of all\\
& &diagonal matrices.\\
$\tilde{D}(A)$&=&subgroup of $GL_n(A)$ consisting of all \\
&&diagonal matrices.\\
\end{tabular}

We remark that $D(A)$ normalises $T(A)$ i.e. if $\alpha\varepsilon
D(A)$, $t\varepsilon T(A)$ then $\alpha t\alpha^{-1}$. Also
$$
\tilde{T}(A)=T(A)D(A)=D(A)T(A)
$$

We denote by $S_{n}$ the group of all permutations of the set
$\{1,2,\ldots,n\}$. There is a standard monomorphism of $S_n$ into
$GL_n(A)$ given by
$$
\pi\mapsto\left[e_{\pi(1)},\ldots,e_{\pi(n)}\right]=\begin{bmatrix}
e^{t}_{\pi^{-1}}(1)\\
\vdots\\
e^{t}_{\pi^{-1}}(n)
\end{bmatrix}
$$

$$
\text{ where } e_i= i-th
\begin{bmatrix}
0\\
\vdots\\
0\\
1\\
0\\
\vdots\\
0
\end{bmatrix} \text{ and } e^{t}_i=\displaystyle\mathop{[0,\ldots,0,1,0,\ldots,0]}_{i-th}
$$

We identify $S_n$ as a subgroup of $GL_n(A)$ via $\rho$. For $\pi
\varepsilon S_n,\rho(\pi)$ is usually called the permutation matrix
corresponding to the permutation $\pi$. 

We remark that 
\begin{enumerate}[(i)]
\item If $\pi \varepsilon S_n$, then 
\begin{align*}
\det \pi&=1\text{ if } \pi \text{ is an even permutation }\\
&=-1\text{ if } \pi\text{ is an odd permutation. }
\end{align*}

\item If $d=\begin{bmatrix}
d_1 & &0\\
& \ddots \\
0 & &d_n
\end{bmatrix} \varepsilon\tilde{D}(A)$, and $\pi \varepsilon S_n$, 
then
$$
\pi^{-1}d\pi=\begin{bmatrix}
d_{\pi(1)} & & 0\\
& \ddots\\
0 & & d_{\pi(n)}
\end{bmatrix}
$$
\end{enumerate}

\begin{dfn}\label{c3:dfn2.1}
$\alpha \varepsilon GL_N(A)$ is said to be a \textit{monomial matrix}
  if $\alpha=\pi d$
$$
\begin{aligned}
\text{ where } &\pi \text{ is a permutation matrix, }\\
&{}d \text{ is a diagonal matrix. }
\end{aligned}
$$

Set
\begin{tabbing}
$\tilde{M}(A)$\= = subgroup of $GL_n(A)$ consisting of all monomial
  matrices.\\[5pt]
\>=$S_{\pi}\tilde{D}(A)=\tilde{D}(A)S_n$\quad(\text{ by (ii) above
}). \\[5pt]
$M(A)$\= = subgroup of $SL_n(A)$ of all monomial matrices
\end{tabbing}
$$
=
\left\{\begin{aligned}
\pi d &\mid \pi \varepsilon S_n, d\varepsilon \tilde{D}(A) \text{ and }\\
&\det d=1 \text{ if } \pi \text{ is even }\\
&{} \det d= -1 \text{ if } \pi \text{ is odd. }
\end{aligned}\right\}
$$
Note that $\pi d\pi' d'=\pi\pi' d''$ where
$d''=\pi'^{-1}d\pi'd'\varepsilon \tilde{D}(A)$.

For $a\varepsilon A^{\ast}$, we define
$$
w_{ij}(a)=e_{ij}(a)e_{ji}\left(-a^{-1}\right)e_{ij}(a),
$$
for $i\neq j$, $1\leq i$, $j\leq n$
$$
\begin{matrix}
ith\\
jth
\end{matrix} 
\begin{bmatrix}
1 & a\\
0 & 1
\end{bmatrix}\begin{bmatrix}
1 & 0\\
-a^{-1} & 1
\end{bmatrix} \begin{bmatrix}
1 & a\\
0 & 1
\end{bmatrix} = \begin{matrix}
ith\\
jth
\end{matrix} \begin{bmatrix}
0 & a\\
-a^{-1} & 0
\end{bmatrix}
$$
%&(i,j)\begin{bmatrix}
%1 & 0\\
%a & -a^{-1}
%\end{bmatrix}, (i,j)\varepsilon S_n.

{\large\bf Not able to identify how many columns and how many rows are
there in this matrix}


It is clear that for $a\varepsilon A^{\ast}$, 
\begin{enumerate}[(i)]
\item $w_{ij}(a)w_{ij}(-a)=1$
\item $w_{ij}(a)=(i,j)w_{ji}(a)(i,j)$
\item $w_{ij}(a)\varepsilon M(A)$
\end{enumerate}
\end{dfn}

\begin{lem}\label{c3:lem2.2}
The set $\{w_{ij}(a)\mid a\varepsilon A^{\ast}\}$ generates $M(A)$. In
particular $M(A)\subset E_n(A)$. 
\end{lem}

\begin{Proof}
Let $W$ be the subgroup of $E_n(A)$ generated by $\{w_{ij}(a)\mid
a\varepsilon A^{\ast}\}$. We note that for $i<j$, we have
\begin{align*}
d_{ij}(a)&=w_{ij}(a)w_{ij}(-1)\\
&=\begin{bmatrix}
0 & a\\
-a^{-1} & 0
\end{bmatrix} \begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix} = \begin{bmatrix}
a & 0\\
0 & a^{-1}
\end{bmatrix} \begin{matrix}
ith \text{ row }\\
jth \text{ row }
\end{matrix}
\end{align*}
{\large\bf Not able to identify how many columns and how many rows are
there in this matrix}

%=\begin{bmatrix}
%1 & 0\\
%0 & 1
%\end{bmatrix} \varepsilon W
%\end{align*}

Similarly for $i>j$, $d_{ij}(a)\varepsilon w$. It is clear that $D(A)$
is generated by $d_{ij}(a)$; $i\neq j$, $a\varepsilon A^{\ast}$. Hence
$D(A)\subset W$.

Now for $\pi d\varepsilon M(A)$, with $\pi$ an even permutation, it is
enough to show that $\pi \varepsilon W$. For this it is sufficient to
prove that any 3-cycle $(i,j,k)=(i.j)(j,k)\varepsilon W$, where
$i,j,k$ are distinct. We note that 
$$
w_{ij}(1)w_{jk}(1)=(i,j(j,k)d
$$
where 
{\fontsize{10}{12}\selectfont$$
\begin{matrix}
d=(j,k)^{-1}\\
i-th\text{ row }
\end{matrix} \begin{bmatrix}
1 & & & &\\
&\ddots & & &0\\
& & -1 & &\\
& 0 & &\ddots &\\
& & & & 1
\end{bmatrix} \begin{matrix}
(j,k)\\
j-th \text{ row }
\end{matrix} \begin{bmatrix}
1 & & & &\\
&\ddots & &&\\
& &-1 & &\\
& & &\ddots &\\
&& & &  1
\end{bmatrix} \varepsilon D(A)
$$}

So $(i,j)(j,k)\varepsilon W$. 

Now let $\pi \varepsilon S_n$ be an odd permutation and $d\varepsilon
\tilde{D}(A)$ be such that \\$\det d=-1$. Then 
$$
w_{ij}(1)\pi d=(i,j)\pi d'\varepsilon W,
$$
where $d'\varepsilon D(A)$. Now since $(i,j)\pi$ is an even
permutation, the\\ lemma follows.
\enprf
\end{Proof}

We remark that 
\begin{enumerate}[(i)]
\item $M(A)$ normalises $E_n(A)$, $n\geq 3$. For  if $w=\pi d
  \varepsilon M(A)$, 
$$
\text{ where } d=\begin{bmatrix}
d_1 & & 0\\
& \ddots \\
0 & &d_n
\end{bmatrix} \text{ with }\det d=1 \text{ if } \pi \text{ is even },
$$
$\det d=-1$ if $\pi$ is odd and $e_{ij}(\lambda)\varepsilon E_n(A)$,
then 
\begin{align*}
we_{ij}(\lambda)w^{-1} &=\pi\begin{bmatrix}
d_1 && 0\\
& \ddots &\\
0 && d_n
\end{bmatrix} e_{ij}(\lambda) \begin{bmatrix}
d^{-1}_1 && 0\\
& \ddots &\\
0 && d^{-1}_n
\end{bmatrix}\pi^{-1}\\
&=\pi e_{ij}\left(d_i\lambda d^{-1}_j\right)\pi^{-1}\\
&=e_{\pi(i)\pi(j)}\left(d_i\lambda d^{-1}_j\right)\varepsilon E_n(A).
\end{align*}

\item If $\alpha \varepsilon T(A)$, then
  $\alpha=\prod\limits_{i<j}e_{ij}(a_{ij})$ ordered lexicographically,
  because
$$
\begin{bmatrix}
1 & &a_{12}&\cdots &a_{1n}\\
& & & &\vdots\\
& &1 & &a_{n-1}\\
& 0 & &\ddots  \\
& & & &1
\end{bmatrix}e_{12}(-a_{12})e_{13}(-a_{13})\ldots = I_n
$$
\end{enumerate}


\begin{lem}\label{c3:lem2.3}
$E_n(A)$ is generated by the set
$$
\{e_{i,i+1}(\lambda)\text{ and } w_{i,i+1}(1),1\leq i \leq n-1,
\lambda \varepsilon A\}
$$
\end{lem}

\begin{Proof}
First we note that $E_n(A)$ is generated by $e_{ij}(\lambda),j=i\pm 1,
\lambda \varepsilon A$. This is clear, since for $j>i+1$, 
$$
e_{ij}(\lambda)=[e_{i,j-1}(\lambda), e_{j-1,j}(1)]
$$
and $j<i-1$
$$
e_{ij}(\lambda)=[e_{i,j+1}(\lambda), e_{j+1,j}(1)].
$$

Further since
$$
w_{i,i+1}(1)e_{i,i+1}(\lambda)w_{i,i+1}(-1)=e_{i+1,i}(-\lambda),
$$
the lemma follows.
\enprf
\end{Proof}

\begin{lem}\label{c3:lem2.4}
Let $t\varepsilon T(A)$, then 
$$
t=e_{i,j+1}(\lambda)t'
$$
where $\lambda \varepsilon A$ and $t'$ is a product of
$e_{kl}(\mu),\mu \varepsilon A$, $k<l$ and $(k,l)\neq
(i,i+1)$. 
\end{lem}

\begin{Proof}
Since for any $t\varepsilon T(A)$, we can write 
$$
t=\prod\limits_{i<j}e_{ij}(\lambda_{ij}), \lambda_{ij}\varepsilon A
$$

Therefore it is enough to show that for any $e_{kl}(\mu)$,
$k<l$, $(k,l)\neq (i,i+1)$, $\mu \varepsilon A$, we can
write
$$
e_{kl}(\mu)e_{i,i+1}(\lambda)=e_{i,i+1}(\lambda)e_{k'l'}(\mu'),
$$
with $k'< l'$ and $(k',l') \neq (i,i+1),\mu'\varepsilon
A$. Since 
\begin{align*}
[e_{kl}(\mu),e_{ij}(\lambda)]&=1\text{ if } l \neq i, k\neq
j\\
&= e_{kj}(\lambda \mu)\text{ if } l=1, k\neq j\\
&=e_{il}(-\lambda \mu) \text{ if } l \neq, k=j.
\end{align*}

Therefore considering the different cases, we have
\begin{enumerate}[(i)]
\item For $k=1$ (then $l \geq i+2$) $[e_{il}(\mu),e_{i,i+1}(\lambda)]=1$.
\item For $k=i+1$, $l\geq 1+2\quad [e_{i+1,l}(\mu),
  e_{i,i+1}(\lambda)]=e_{il}(-\lambda \mu)$
\item For $k<i,l=i
  [e_{ki}(\mu),e_{i,i+1}(\lambda)]=e_{k,i+1}(\mu\lambda)$
\item For $k\neq i+1, l\neq i$, $[e_{kl}(\mu),e_{i,i+1}(\lambda)]=1$.
\end{enumerate}
This proves the lemma.
\enprf
\end{Proof}

\begin{thm}\label{c3:thm2.5}
Let $k$ be any field, then 
$$
SL_n(k)=\text{ TMT }
$$
(where T=T(k), M=M(k)).
\end{thm}

\begin{Proof}
Since for any field $k, SL_n(k)=E_n(k)$, By Lemma~\ref{c3:lem2.3},
$SL_n(k)$ is generated by $\left\{e_{i,i+1}(\lambda),w_{i,i+1}(1)\mid
1\leq i \leq n-1,\lambda \varepsilon k\right\}$. Let 
$$
W=\text{ TMT }
$$

Clearly
$$
We_{i,i+1}(\lambda)\subset \text{ TMT }.
$$

So it is enough to show that 
$$
Ww_{i,i+1}(1)\subset \text{ TMT }
$$

Let $t_1wt_2\varepsilon W$, where $t_1,t_2\varepsilon T$, $
w\varepsilon M$. Now 
$$
t_1wt_{2}w_{i,i+1}(1)=t_1we_{i,i+1}(\lambda) t'_2w_{i,i+1}(1)
$$
by Lemma~\ref{c3:lem2.4} where $t'_2$ is a product of
$e_{kl}(\lambda'),\lambda' \varepsilon A$, $k < l$ and $(k,l)\neq (i,
i+1)$.

So
$$
t_1wt_2w_{i,i+1}(1)=t_1we_{i,i+1}(\lambda)w_{i,i+1}(1)w_{i,i+1}(-1)t'_2w_{i,i+1}(1).
$$

But
$$
w_{i,i+1}(-1)t'_2w_{i,i+1}(1)\varepsilon T
$$
which follows from the remarks following Lemma~\ref{c3:lem2.2}. Hence
it suffices to show that 
$$
t_1we_{i,i+1}(\lambda)w_{i,i+1}(1)\varepsilon \text{ TMT }
$$

Now
\begin{align*}
&t_1we_{i,i+1}(\lambda)w_{i,i+1}(1)\\
=& t_1we_{i,i+1}(\lambda) w^{-1}ww_{i,i+1}(1)\\
=& t_1e_{\pi(i),\pi(i+1)}(\ast)ww_{i,i+1}(1)
\end{align*}
for some $\pi \varepsilon S_n$ and $\ast \varepsilon k$. If
$\pi(i)<\pi(i+1)$, then 
$$
t_1e_{\pi(i),\pi(i+1)}(\ast) \varepsilon T.
$$
and since 
$$
ww_{i,i+1}(1)\varepsilon M.
$$
we have
$$
t_1wt_{2}w_{i,i+1}(1)\varepsilon \text{ TMT }, \text{ q.e.d. in case } \pi(i)<\pi(i+1).
$$

If $\pi(i)>\pi(i+1)$, then 
$$
t_we_{i,i+1}(\lambda)w_{i,i+1}(1)=t_1ww_{i,i+1}(\lambda)e_{i,i+1}(-\lambda)e_{i+1,i}\left(\lambda^{-1}\right)w_{i,i+1}(1)
$$

$$
\begin{aligned}
(\text{ because } &w_{i,i+1}(\lambda)=e_{i,i+1}(\lambda)e_{i+1,i}\left(-\lambda^{-1}\right)e_{i,i+1}(\lambda))\\
&{}=t_{1}ww_{i,i+1}(\lambda)e_{i,i+1}(-\lambda)
  w_{i,i+1}(-\lambda)w^{-1}ww_{i,i+1}(\lambda)w_{i,i+1}(1)\times\\
&{} w_{i,i+1}(-1)e_{i+1,i}\left(\lambda^{-1}\right)w_{i,i+1}(1).
\end{aligned}
$$

Since 
\begin{enumerate}[(i)]
\item $$
\begin{aligned}
&t_1ww_{i,i+1}(\lambda)e_{i,i+1}(-\lambda)w_{i,i+1}(-\lambda)w^{-1}\\
=&{}t_1we_{i+1,i}\left(\lambda^{-1}\right) w^{-1}\\
=&{}t_1e_{\pi(i+1),\pi(i)}(\ast),\ast \varepsilon k\\
&{}\varepsilon T,\qquad (\text{ since } \pi(i+1)<\pi(i))
\end{aligned}
$$
\item $ww_{i,i+1}(\lambda)w_{i,i+1}(1)\varepsilon M$
\item $w_{i,i+1}(-1)e_{i+1,i}\left(\lambda^{-1}\right)w_{i,i+1}(1)=e_{i,i+1}(\ast)\varepsilon T.$
\end{enumerate}

We have
$$
t_1we_{i,i+1}(\lambda w_{i,i+1}(1)\varepsilon \text{ TMT }
$$
This proves the theorem.
\enprf
\end{Proof}

Now for any commutative ring $A$, and $1\leq i \leq n$, we define
$$
\begin{aligned}
N_i(A)=& \text{ subgroup of } E_n(A) \text{ consisting of the matrices
  }\alpha
  \varepsilon E_n(A)\\ 
&{} \text{ such that } e^{t}_i\alpha=ae^{t}_i, a\varepsilon A^{\ast}\\
=&{}\text{ sub group of } E_n(A) \text{ consisting of matrices } \alpha\varepsilon
E_n(A)\\ 
&{}\text{ such that } \alpha \text{ has i-th row as } \displaystyle\mathop{(0,\ldots,0, a,
0,\ldots,0)}_{i-th},\\
&{}\text{ for some a }\varepsilon A^{\ast}.
\end{aligned}
$$
and 

$$
\begin{aligned}
N^{(i)}(A)=&\text{ subgroup of } E_n(A) \text{ consisting of the
  matrices }\\
&{} \alpha \varepsilon E_n(A) \text{ such that }\alpha e_i=e_ia,a\varepsilon A^{\ast}\\
=&{} \text{ subgroup of } E_n(A) \text{ consisting of matrices }
\alpha \varepsilon E_n(A)'\\
&{}\text{ such that } \alpha \text{ has } i-th \text{ column as }\\
\end{aligned}
$$

$$
\begin{bmatrix}
0\\
\vdots\\
0\\
a\\
0\\
\vdots\\
0
\end{bmatrix} i-th \qquad, \text{ for some } a\varepsilon A^{\ast}.
$$

\begin{coro}\label{c3:coro2.6}
Let $k$ be any field. Then for $1\leq i\leq n$,
\begin{align*}
SL_n(k)&=N^{i}(k)M(k)T(k)\\
&=N_i(k)M(k)T(k).
\end{align*}
\end{coro}

\begin{Proof}
Clearly
$$
T(k)\subset N^{1}(k)\cap N_n(k).
$$

Since
$$
SL_n(k)=T(k)M(k)T(k)\quad (\text{ by Theorem~\ref{c3:thm2.5} }),
$$
we have
\begin{align*}
SL_n(k)&=N^{1}(k)M(k)T(k)\\
&=N_n(k)M(k)T(k).
\end{align*}

Now if $i>1$, we have
\begin{align*}
SL_n(k)&=T(k)M(k)T(k)\\
&=w_{il}(1)T(k)w_{il}(1)M(k)T(k).
\end{align*}

Observing that
$$
w_{il}(1)T(k)w_{i1}(1)\subset w_{il}(1)N^{1}(k)w_{i1}(1)\subset N^{i}(k),
$$
we have
$$
SL_n(k)\subset N^{i}(k)M(k)T(k).
$$

Similarly if $1<n$, we have 
\begin{align*}
SL_n(k)&=T(k)M(k)T(k)\\
&=w_{\text{ in }}(1)T(k)w_{\text{ in }}(1)M(k)T(k)\\
&\subset w_{\text{ in }}(1)N_n(k)w_{\text{ in }}(1)m(k)T(k)\\
&\subset N_i(k)M(k)T(k).
\end{align*}

This proves the corollary.
\enprf
\end{Proof}


\section{Structure of \texorpdfstring{$E_n\left(A\left[X,X^{-1}\right]\right)$}{En}}\label{c3:s3}

The main aim of this section is to prove Theorem~\ref{c3:thm3.8},
which gives the structure of $E_n\left(A\left[X,X^{-1}\right]\right)$,
for a commutative local ring $A$. We need this for a proof of
Theorem~\ref{c3:thm4.1}, in the next section.

We set
$$
\delta_i=
\begin{bmatrix}
1 & & & & & &0\\
& \ddots & && & &\\
& & 1 & & &\\
& & & X & &\\
& & & & 1 &\\
& & & & & \ddots\\
0 & & & & & &1
\end{bmatrix} i-th \text{ row }\qquad \varepsilon GL_n\left(A\left[X,X^{-1}\right]\right)
$$

\begin{Prop}\label{c3:Prop3.1}
Let $A$ be any commutative ring. Let $\alpha \varepsilon E_n(A[X])$,
and $n\geq 3$.
$$
\begin{aligned}
(a)\quad&\delta_i\alpha \delta^{-1}_i\varepsilon GL_n(A[X])\Rightarrow
  \delta_i\alpha\delta^{-1}_i\varepsilon E_n(A[X])\\
(b)\quad& \delta^{-1}_i\alpha\delta_i\varepsilon GL_n(A[X])\Rightarrow
  \delta^{-1}_i\alpha\delta_i\varepsilon E_n(A[X]).
\end{aligned}
$$
\end{Prop}

\begin{Proof}
First we note that $(a)\Rightarrow (b)$. For if $\alpha \varepsilon
E_n(A[X])$ and $\delta^{-1}_i\alpha\delta_i\varepsilon\\ GL_n(A[X])$,
then 
$$
\left(\delta^{-1}_i\alpha\delta_i\right)^{t}=\delta_i\alpha^{t}\delta^{-1}_i\varepsilon GL_n(A[X]),
$$
(t denotes transpose). So 
$$
\delta_i\alpha^{t}\alpha^{-1}_i\varepsilon E_n(A[X])
$$
by (a) and since $\alpha^{t}\varepsilon
(E_n(A[X]))^{t}=E_n(A[X])$. Therefore 
$$
\left(\delta_i\alpha^{t}\delta^{-1}_i\right)^{t}=\delta^{-1}_i\alpha\delta_i\varepsilon E_n(A[X]).
$$
So it is enough to prove (a).

For, let $i<n$, since 
$$
\begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix} \begin{bmatrix}
1 & 0\\
0 & X
\end{bmatrix} \begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix} = \begin{bmatrix}
X & 0\\
0 & 1
\end{bmatrix}
$$
we have 
$$
w_{\text{ in }}(1)\delta_nw_{\text{ in }}(-1)=\delta_i.
$$

Therefore
$$
\delta_i\alpha\delta^{-1}_i=w_{\text{ in }}(1)\delta_nw_{\text{ in
}}(-1)\alpha w_{\text{ in }}(1)\delta^{-1}_nw_{\text{
    in }}(-1)\varepsilon GL_n(A[X]).
$$

Hence
$$
\delta_i\alpha\delta^{-1}_i\varepsilon E_n(A[X])\Leftrightarrow
\delta_n\alpha'\delta^{-1}_n\varepsilon E_n(A[X])
$$
where $\alpha'=w_{\text{ in }}(-1)\alpha w_{\text{ in }}(1)$. So we
may assume $i=n$. 
\enprf
\end{Proof}

Now we prove

\begin{assr}\label{c3:assr1}
If $\alpha \varepsilon E_n(A[X])$, $\alpha \equiv 1\pmod X$ then
$\delta_n\alpha\delta^{-1}_n\varepsilon E_n(A[X])$. 

Since we have
$$
\begin{aligned}
&\alpha \varepsilon E_n(A[X]) \cap GL_n(A[X],(X))\\
=& E_n(A[X],(X))\\
=& \text{ subgroup of } E_n(A[X]) \text{ generated by } \gamma
  e_{ij}(Xf)\gamma^{-1}, for\\
&\gamma \varepsilon E_n(A), f\varepsilon A[X],
\end{aligned}
$$
by Lemma~\ref{c1:lem5.4} of Chapter~\ref{chap1}, we can write $\alpha$
as a product of matrices of the type $\gamma
e_{ij}(Xf)\gamma^{-1}$. Clearly to show
$\delta_n\alpha\delta_n^{-1}\varepsilon E_n(A[X])$, it is enough to
show $\delta _n\gamma e_{ij}(Xf)\gamma^{-1}\delta^{-1}_n\varepsilon
E_n(A[X])$. So we can assume $\alpha=\gamma e_{ij}(Xf)\gamma^{-1}$. 

Let 
$$
v=i-th \text{ column of } \gamma=
\begin{bmatrix}
v_1\\
\vdots\\
v_n
\end{bmatrix}
$$
and 
$$
w=j=th \text{ row of }\gamma^{-1}=[w_1,\ldots, w_n].
$$

Then $\alpha=\gamma e_{ij}(Xf)\gamma^{-1}=I_n+Xfw$. Since $wv=0$, by
Lemma~\ref{c1:lem4.6} (see also proof of Proposition~\ref{c1:Prop4.7}
of Chapter~\ref{chap1}, we can assume that 
$$
\alpha=I_n+Xf
\begin{bmatrix}
v_1\\
\vdots\\
v_n
\end{bmatrix} [w'_1,\ldots, w'_n]
$$
where $(w'_1,\ldots,w'_n) \varepsilon A^{n}$ with $w'_k=0$ for some
$k$ and $\sum\limits_{1\leq i \leq n}w'_iv_i=0$. 

Now clearly
$$
\delta_n\alpha\delta^{-1}_n=I_n+
\begin{bmatrix}
fv_1\\
\vdots\\
fv_{n-1}\\
Xfv_n
\end{bmatrix} [Xw'_1,\ldots, Xw'_{n-1},w_n]
$$
with
$$
[Xw'_1,\ldots,Xw'_{n-1},w_n]
\begin{bmatrix}
fv_1\\
\vdots\\
fv_{n-1}\\
Xfv_n
\end{bmatrix}=0
$$

Therefore by Corollary~\ref{c1:coro4.4} of Chapter~\ref{chap1}, we
have $\delta_n\alpha \delta^{-1}_n\varepsilon E_n(A[X])$. This proves
assertion 1.
\end{assr}


\begin{assr}\label{c3:assr2}
If $\alpha \varepsilon E_n(A[X])$, $\delta_n\alpha
\delta^{-1}_n\varepsilon GL_n(A[X])$ then $\delta_n\alpha
\delta^{-1}_n\varepsilon E_n(A[X])$.

Let $\alpha_o$ be the image of $\alpha$ in $GL_n(A)$ under the mapping
$GL_n\\(A[X])\rightarrow GL_n(A)$ given by $x\rightarrow 0$. We can
write
$$
\alpha=\alpha_0\alpha_1
$$
where $\alpha_1\varepsilon E_n(A[X])$, $\alpha_1\equiv 1 \pmod X$. So 
$$
\delta_n\alpha\delta^{-1}_n=\delta_n\alpha_o\delta^{-1}_n\delta_n\alpha_1\delta^{-1}_n
\varepsilon GL_n(A[X]). 
$$

By assertion~\ref{c3:assr1}, $\delta_n\alpha_n\delta^{-1}_n\varepsilon
E_n(A[X])$. So it is enough to show that if $\alpha \varepsilon
E_n(A)$, $\delta_n\alpha \delta^{-1}_n\varepsilon GL_n(A[X])$ then
$\delta_n\alpha \delta^{-1}_n\varepsilon E_n (A[X])$. Now let $\alpha=
(\alpha_{ij})\varepsilon E-n(A)$ , then 
$$
\delta_n\alpha\delta^{-1}_n=
\begin{bmatrix}
\alpha_{11} &\cdots & \alpha_{1,n-1} & \alpha_{1n}X^{-1}\\
\vdots & & \vdots & \vdots\\
& & & \alpha_{n-1,n^{X^{-1}}}\\
X\alpha_{nl}&\cdots &X\alpha_{n,n-1} & \alpha_{nn}
\end{bmatrix} \varepsilon GL_n(A[X])
$$

So $\alpha_{\text{ in }}=0$ for $i<n$ and $\alpha_{nn}\varepsilon
A^{\ast}$. Now choosing a suitable $\epsilon'\varepsilon \\E_n(A[X])$, we
get
$$
\delta_n\alpha \delta^{-1}_n\epsilon'=
\begin{bmatrix}
\alpha_{11}& \cdots & \alpha_{1,n-1} &0\\
\vdots & & \vdots & \vdots\\
\alpha_{n-1,1} & \cdots & \alpha_{n-1,n-1}& 0\\
0 & \cdots & 0 &  \alpha_{nn}
\end{bmatrix}
$$

On the other hand, since
$$
\alpha=
\begin{bmatrix}
\alpha_{11}&\cdots & \alpha_{1,n-1} & 0\\
\vdots & & \vdots & \vdots\\
\alpha_{n-1,1} & \cdots & \alpha_{n-1,n-1} &0\\
\alpha_{n1}& \cdots & \alpha_{n,n-1} & \alpha_{nn}
\end{bmatrix}
$$
we can find $\epsilon \varepsilon E_n(A)$ such that 
$$
\delta_n\alpha\delta^{-1}_n\epsilon'=\alpha\epsilon \varepsilon E_n(A).
$$

Therefore $\delta_n\alpha\delta^{-1}_n=\alpha\epsilon
\epsilon'^{-1}\varepsilon E_n(A[X])$. 
This proves assertion~\ref{c3:assr2} and hence the proposition.
\end{assr}


\begin{Prop}\label{c3:Prop3.2}
Let $A$ be any commutative local ring with maximal ideal
$\underline{m}$. Let $n\geq 3$. Suppose
$$
V=E_n(A[X])\tilde{T}\left(A\left[X,X^{-1}\right]\right)E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
$$
Then 
\begin{enumerate}[(i)]
\item $\delta_iV\delta^{-1}_i\subset V$

\item $\delta^{-1}_iV\delta_i\subset V$.
\end{enumerate}

For a proof of this , we need
\end{Prop}

\begin{lem}\label{c3:lem3.3}
Let $A$ be commutative local ring. Then
$$
SL_n(A)=E_n(A),n\geq 1.
$$
\end{lem}

\begin{Proof}
Since $A$ is local, every $\alpha \varepsilon SL_n(A)$ has a unit in
each row and column. Hence by elementary transformations $\alpha$ can
be brought to diagonal form. Now by Lemma~\ref{c3:lem2.2}, $\alpha
\varepsilon E_n(A)$.
\enprf
\end{Proof}

\begin{lem}\label{c3:lem3.4}
(Whitehead Lemma). Let $A$ be any commutative ring and $J$ be an ideal
  of $A$. Let $\alpha=(I+\beta)\varepsilon GL_n(A)$ where
  $\beta=(\beta_{ij})$, $\beta_{ij}\varepsilon J$. Then 
$$
\begin{bmatrix}
\alpha & 0\\
0 & \alpha^{-1}
\end{bmatrix} \varepsilon E_{2n}(A,J)
$$
\end{lem}


\begin{Proof}
Since 
$$
\begin{bmatrix}
\alpha & 0\\
0 & \alpha^{-1}
\end{bmatrix}= \begin{bmatrix}
I & I\\
0 & I
\end{bmatrix} \begin{bmatrix}
I & 0\\
\beta & I
\end{bmatrix} \begin{bmatrix}
I & -I\\
0 & I
\end{bmatrix} \begin{bmatrix}
I & \alpha^{-1}\beta\\
0 & I
\end{bmatrix} \begin{bmatrix}
I & 0\\
-\alpha\beta & I
\end{bmatrix}
$$
and each matrix on the right hand side is a product of elementary
matrices. So it is clear from the definition of $E_{2n}(A,J)$ that 
$$
\begin{bmatrix}
\alpha & 0\\
0 & \alpha^{-1}
\end{bmatrix} \varepsilon E_{2n}(A,J).
$$
\enprf
\end{Proof}


\begin{lem}\label{c3:lem3.5}
Let $A$ be any commutative ring and $J$ be an ideal of $A$. Then
$\tilde{T}(A)\cap GL_n(A,J)\subset E_n(A,J)$.
\end{lem}


\begin{Proof}
Let 
$$
\alpha=
\begin{bmatrix}
\alpha_{11}&\alpha_{12}&\cdots & \alpha_{1n}\\
& \alpha_{22}& &\vdots\\
& 0 & \ddots &\\
& & & \alpha_{nn}
\end{bmatrix} \varepsilon \tilde{T}(A)\cap GL_n(A,J)
$$

Since $\det \alpha=\prod\limits_{i=1}^{n}\alpha_{ii}=1$ we have
$\alpha_{ii}\varepsilon A^{\ast}$, $1\leq i\leq n$. We can write
$$
\alpha=
\begin{bmatrix}
\alpha_{11}& 0\\
0 & \alpha_{nn}
\end{bmatrix} \begin{bmatrix}
1 & \alpha'_{12}&\cdots & \alpha'_{1n}\\
& 1 & &\vdots\\
& &  \ddots &\alpha'_{1,n-1}\\
& 0 && 1
\end{bmatrix}
$$
where $\alpha'_{ij}\varepsilon J$, $i<j$. But
$$
\begin{bmatrix}
1 & \alpha'_{12}& \cdots & \alpha'_{1n}\\
& 1 & & \vdots\\
& & \ddots & \alpha'_{1,n-1}\\
& 0 & & 1
\end{bmatrix} \varepsilon E_n(A,J)
$$
because it is a product of elementary matrices of the type $1+\lambda
E_{ij}$, $\lambda \varepsilon J$. 
 
Moreover
$$
\begin{bmatrix}
\alpha_{11} & 0\\
0 & \alpha_{nn}
\end{bmatrix}= \begin{bmatrix}
\alpha_{11} & & & &\\
& & 0 &&\\
& \alpha^{-1}_{11} & & &\\
& & 1 & &\\
& 0 & &\ddots & \\
&& & &1
\end{bmatrix} \begin{bmatrix}
1 & & & &0\\
& \alpha_{11}&\alpha_{22} & & \\
& & \alpha_{33} & & \\
& & &\ddots & \\
0 & & & &\alpha_{mm}
\end{bmatrix}
$$

Since for $\alpha\varepsilon E_n(A,J),\begin{bmatrix}
\alpha & 0\\
0 & I_m
\end{bmatrix} \varepsilon E_{n+m}(A,J)$, we have by
Lemma~\ref{c3:lem3.4}, 
$$
\begin{bmatrix}
\alpha_{11} & & & &\\
& \alpha^{-11} & &0 &\\
& &  1 & \\
& & & \ddots &\\
& 0&& & &1
\end{bmatrix}\varepsilon E_n(A,J).
$$
Hence the lemma follows by induction.
\enprf
\end{Proof}

\begin{lem}\label{c3:lem3.6}
Let $A$ be a commutative local ring with maximal ideal
$\underline{m}.$ Then 
$$
SL_n(A,\underline{m})=SL_{n}(A)\cap GL_n(A,\underline{m})=E_n(A,\underline{m}).
$$
\end{lem}

\begin{Proof}
Let $\alpha\varepsilon SL_n(A)\cap GL_n(A,\underline{m})$. Then 
$$
\alpha=
\begin{bmatrix}
1+a_{11}& & a_{12}&\cdots & a_{1n}\\
& && &\vdots\\
a_{21} && 1+a_{22} & &\\
\vdots & & &\ddots &\\
a_{n1}& \cdots & & & 1+a_{nn}
\end{bmatrix}, a_{ij}\varepsilon\underline{m}.
$$

Since $1+a_{ii}$ is a unit for $1\leq i \leq n$, by applying
elementary transformations congruent to $1\pmod{\underline{m}}$, we
can reduce $\alpha$ to a diagonal matrix congruent to
$1\pmod{\underline{m}}$. So we can write
$$
\alpha \epsilon'=
\begin{bmatrix}
d_1 & &0\\
& \ddots &\\
0 & & d_n
\end{bmatrix} \varepsilon \tilde{T}(A)\cap GL_n(A,\underline{m}),
\epsilon \varepsilon E_n(A,\underline{m}).
$$

But by Lemma~\ref{c3:lem3.5} $\tilde{T}(A)\cap
GL_n(A,\underline{m})\subset E_n(A,\underline{m})$. So $\alpha
\varepsilon E_n(A,\underline{m})$. This proves the lemma.
\enprf
\end{Proof}

\begin{lem}\label{c3:lem3.7}
Let $A$ be any local ring. Let $\underline{m}$ be the maximal ideal of
$A$ and $k=\frac{A}{m}$. Then the cannonical maps
\begin{enumerate}[(i)]
\item $\tilde{T}(A)\rightarrow \tilde{T}(k)$
\item $M(A)\rightarrow M(K)$
\item $N^{i}(A)\rightarrow N^{i}(k), 1\leq i \leq n$
\item $T(A)\rightarrow T(k)$
\end{enumerate}
are surjective.
\end{lem}

\begin{Proof}
\begin{enumerate}[(i)]
\item Let $\beta \varepsilon \tilde{T}(k)\subset GL_n(k)$. We choose a
lift $\gamma$ of $\beta$ in $GL_n(A)$, which is an upper triangular
matrix. Let $\det \gamma=1+\lambda,\lambda \varepsilon
\underline{m}$. Take 
$$
\gamma'=
\begin{bmatrix}
1 & & &\\
& \ddots &0 &\\
&& 1 &\\
& 0 &  &\\
& & & (1+\lambda)^{-1}
\end{bmatrix}
$$

Then $\det(\gamma' \gamma)=1$, $\gamma'\gamma \varepsilon
\tilde{T}(A)$ and $\gamma'\gamma$ maps to $\beta$.

\item Let $d\varepsilon \tilde{D}(k)$ with $d=\pm 1$, lift $d$ to a
  diagonal matrix in $GL_n(A)$ and modify it  suitably as in (i).
\item Enough to prove for $i=n$. Let 
$$
\alpha=
\begin{bmatrix}
\alpha_{11}& \cdots & \alpha_{1,n-1} &0\\
\vdots & & \vdots & \vdots\\
\alpha_{n-1,1}& \cdots & \alpha_{n-1,n-1} & 0\\
\alpha_{n1} & \cdots & \alpha_{n,n-1} & \alpha_{nn}
\end{bmatrix} \varepsilon N^{n}(k)
$$
and 
$$
\alpha'
\begin{bmatrix}
\alpha_{11} & \cdots & \alpha_{1,n-1}\\
\vdots & &\vdots\\
\alpha_{n-1,1} & \cdots & \alpha_{n-1,n-1}
\end{bmatrix} \varepsilon GL_n(k)
$$

Let $\beta' \varepsilon GL_n(A)$ be a lift of $\alpha'$ and
$\beta_{nl},\ldots,\beta_{n,n-1}$ be lifts of\\
$\alpha_{n1},\ldots,\alpha_{n,n-1}$ respectively, in $A$. Let 
$$
\beta=
\begin{bmatrix}
&& &0\\
& \beta' & & \vdots\\
& & & 0\\
\beta_{nl} & \cdots & \beta_{n-1,n} & (\det \beta')^{-1}
\end{bmatrix} \varepsilon SL_n(A)
$$

But $SL_n(A)=E_n(A)$ by Lemma~\ref{c3:lem3.3} Therefore $\beta
\varepsilon N^{n}(A)$ and clearly it maps to $\alpha$.

\item Obvious.
\end{enumerate}
\enprf
\end{Proof}

\begin{PRFF1}
Let 
$$
\alpha=\alpha_1\alpha_2\alpha_3
$$
where
$$
\begin{aligned}
&\alpha_1\varepsilon E_n(A[X])\\
&{}\alpha_2\varepsilon \tilde{T}\left(A\left[X,X^{-1}\right]\right)\\
&{}\alpha_3\varepsilon  E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
\end{aligned}
$$

Then
$$
\delta_i\alpha\delta^{-1}_i=\delta_i\alpha_1\delta^{-1}_i\delta_i\alpha_2\delta^{-1}_i\delta_i\alpha_3\delta^{-1}_i,1\leq
i \leq n.
$$

Clearly $\delta_i\alpha_2\delta^{-1}_i\varepsilon
\tilde{T}\left(A\left[X,X^{-1}\right]\right)$. Further by definition
$\alpha_3$ is\\ a product of elements of the form $\gamma
e_{kl}(f)\gamma^{-1}$ where $\gamma \varepsilon
\\E_n\left(A\left[X,X^{-1}\right]\right)$, $f\varepsilon
\underline{m}\left[X,X^{-1}\right]$. Now writing 
$$
\delta_i\gamma
e_{kl}(f)\gamma^{-1}\delta^{-1}_i=\delta_i\gamma\delta^{-1}_1\delta_ie_{kl}(f)\delta^{-1}_1\delta_{i}\gamma^{-1}\delta^{-1}_i,
$$
and observing that 
$$
\delta_i\gamma \delta^{-1}_i\varepsilon E_n\left(A\left[X,X^{-1}\right]\right)
$$
and 
$$
\delta_ie_{kl}(f)\delta^{-1}_i\varepsilon
E_n\left(A\left[X,X^{-1}\right], \underline{m}\left[X,X^{-1}\right]\right)
$$
we have
$$
\delta_i\gamma e_{kl}(f)\gamma^{-1}\delta^{-1}_i\varepsilon
E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
$$

Hence
$$
\delta_i\alpha_3\delta^{-1}_i\varepsilon E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
$$

Now to prove the proposition, it is enough to show that 
$$
\delta_i\alpha_1\delta^{-1}_i\varepsilon V
$$
because
$$
\begin{aligned}
&V\tilde{T}\left(A\left[X,X^{-1}\right]\right)E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\\
&{}= VE_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\tilde{T}\left(A\left[X,X^{-1}\right]\right)
\end{aligned}
$$

$E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)$
being a normal subgroup of \\$E_n\left(A\left[X,X^{-1}\right]\right)$
and $\tilde{T}\left(A\left[X,X^{-1}\right]\right)\subset
  E_n\left(A\left[X,X^{-1}\right]\right)$ $=
  \\V\tilde{T}\left(A\left[X,X^{-1}\right]\right)=V$.

So we may assume that $\alpha \varepsilon E_n(A[X])$. Let $\alpha_o$
be the image of $\alpha$ in $E_n(A)$ under the mapping $X\rightarrow
0$. Write
$$
\alpha=\alpha'\alpha_o\text{ where }
\alpha'=\alpha\alpha^{-1}_o\varepsilon E_n(A[X]).
$$
Now
$$
\delta_i\alpha \delta^{-1}_i=\delta_i\alpha'\delta^{-1}_i\delta_i\alpha_o\delta^{-1}_i
$$

Since $\alpha' \varepsilon E_n(A[X])$, $\alpha'\equiv 1\pmod X$. so
$\delta_i\alpha' \delta^{-1}_i\varepsilon \delta^{-1}_i\varepsilon
GL_n(A[X])$ and hence $\delta_i\alpha'\delta^{-1}_i\varepsilon
E_n(A[X])$ by Proposition~\ref{c3:Prop3.1}. So if we show that
$\delta_i\alpha_o\delta^{-1}_1\varepsilon V$, then we are through. Let
$k=\frac{A}{\underline{m}}$ be the quotient field of $A$. By
Corollary~\ref{c3:coro2.6}, we have
$$
SL_n(k)=N^{i}(k)M(k)T(k),1\leq i\leq n.
$$

Now by surjectivity of maps in Lemma~\ref{c3:lem3.7}. we can write
$$
\alpha_o=\beta_1\beta_2\beta_3\gamma.
$$
where $\beta_1\varepsilon N^{i}(A)$, $\beta_2\varepsilon M(A)$,
$\beta_3\varepsilon T(A)$ and $\gamma \varepsilon
SL_n(A,\underline{m})$. Therefore 
$$
\delta_i\alpha_o\delta^{-1}_i=\delta_i\beta_1\delta^{-1}_i\delta_i\beta_2\delta^{-1}_i\delta_i\beta_3\delta^{-1}_i\delta_i\gamma\delta^{-1}_i.
$$

We note 
\begin{enumerate}[(i)]
\item By Lemma~\ref{c3:lem3.6}, $\gamma \varepsilon
  E_n(A,\underline{m})$ and hence
$$
\delta_i\gamma \delta^{-1}_i\varepsilon E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
$$
\item $\beta_3\varepsilon T(A)$ implies that
  $\delta_i\beta_3\delta^{-1}_i\varepsilon
  \tilde{T}\left(A\left[X,X^{-1}\right]\right)$. 
\item For any ring $A$, $\tilde{D}(A)$ normalises $M(A)$, and hence
$$
\delta_iM(A)\delta^{-1}_i\subset M\left(A\left[X,X^{-1}\right]\right).
$$
So
$$
\delta_i\beta_2\delta^{-1}_i\varepsilon
M\left(A\left[X,X^{-1}\right]\right).
$$

Also for any ring $A$,
$$
M(A)\subset S_n\tilde{D}(A)\subset E_n(\mathbb{P})\tilde{D}(\mathbb{P})\tilde{D}(A).
$$
where $\mathbb{P}$ is the prime subring of $A$. Hence $M(A)\subset
E_n(\mathbb{P}T(A)$. Therefore 
$$
\delta_i\beta_2\delta^{-1}_i\varepsilon E_n(A[X])\tilde{T}\left(A\left[X,X^{-1}\right]\right).
$$
\item Since $\beta_1\varepsilon N^{i}(A)\subset E_n(A)$, it easily
  follows that $\delta_i\beta_1\delta_i^{-1}\\\varepsilon
  GL_n(A[X])$. Hence by Proposition~\ref{c3:Prop3.1}
$$
\delta_i\beta_1\delta^{-1}_i\varepsilon E_n(A[X]).
$$ 
So
$$
\delta_i\alpha_o\delta^{-1}_i\varepsilon
(A[X])\tilde{T}\left(A\left[X,X^{-1}\right]\right)E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
$$

This proves the proposition.
\end{enumerate}
\enprf
\end{PRFF1}

\begin{thm}\label{c3:thm3.8}
Let $A$ be any commutative local ring and $\underline{m}$ be the
maximal ideal. Suppose $n\geq 3$. Then 
$$
E_n\left(A\left[X,X^{-1}\right]\right)=E_n(A[X])\tilde{T}\left(A\left[X,X^{-1}\right]\right)E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)
$$
\end{thm}

\begin{Proof}
Since $E_n\left(A\left[X,X^{-1}\right]\right)$ is generated by the
matrices $e_{ij}(f)$,\\ $f\varepsilon A\left[X,X^{-1}\right]$ and 
$$
e_{ij}(\lambda)e_{ij}(\mu)=e_{ij}(\lambda+\mu),\lambda,\mu \varepsilon A[X,X^{-1}]
$$
therefore $e_{ij}(f)$ is a product of elements of the form
$e_{ij}\left(aX^{tn}\right)$, $a\varepsilon A$. Moreover
$$
[e_{ij}(\lambda),e_{jk}(\mu)]=e_{ik}(\lambda \mu)
$$
for $i\neq k$ and $\lambda,\mu\varepsilon
A\left[X,X^{-1}\right]$. Therefore $E_n\left(A\left[X,X^{-1}\right]\right)$
is generated by 
$$
e_{ij}(a),e_{ij}(aX), e_{ij}\left(aX^{-1}\right)
$$
for $a\varepsilon A$, $1\leq i\neq j\leq n$. Let 
$$
V=E_n(A[X])\tilde{T}\left(A\left[X,X^{-1}\right]\right)E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right),
$$
and
$$
K=\left\{\alpha \varepsilon E_n\left(A\left[X,X^{-1}\right]\right) \mid
\alpha V\subset V\right\}.
$$

Let $\alpha, \beta \varepsilon K$, then
$$
\alpha\beta V\subset \alpha V\subset V
$$
and hence $\alpha\beta \varepsilon K$, so $K$ is
multiplicative. Moreover $K\subset V$ because $1\varepsilon V$. Hence
to prove the theorem it is enough to show
$$
e_{ij}(a),e_{ij}(aX),e_{ij}\left(aX^{-1}\right)\varepsilon K.
$$

Clearly $e_{ij}(a)$, $e_{ij}(aX)\varepsilon K$ because
$E_n(A[X])V\subset V$. Further since 
$$
e_{ij}\left(aX^{-1}\right)=\delta^{-1}_ie_{ij}(a)\delta_i,
$$
we have
$$
\begin{aligned}
e_{ij}\left(aX^{-1}\right)V&=\delta^{-1}_ie_{ij}(a)\delta_iV\\
&{}=\delta^{-1}_ie_{ij}(a)\delta_iV\delta^{-1}_i\delta_i\\
&{}\subset \delta^{-1}_ie_{ij}(a)V\delta_i, \\
&{}V\qquad (\text{ by Proposition~\ref{c3:Prop3.2} }).
\end{aligned}
$$
This completes the proof.
\enprf
\end{Proof}


\begin{coro}\label{c3:coro3.9}
Let $A$ be a commutative local ring with maximal ideal
$\underline{m}$. Suppose $n\geq 3$. Then 
$$
\begin{aligned}
&E_n\left(A\left[X,X^{-1}\right]\right)\cap
GL_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\\
&=G_+E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
\end{aligned}
$$
where $G_+=E_n(A[X])\cap GL_n(A[X],\underline{m}[X])$. 
\end{coro}

\begin{Proof}
Let 
$$
\alpha=\alpha_1\alpha_2\alpha_3
$$
where
$$
\begin{aligned}
&\alpha_1\varepsilon E_n(A[X])\\
&\alpha_2\varepsilon \tilde{T}\left(A\left[X,X^{-1}\right]\right)\\
&\alpha_3\varepsilon E_n\left(A\left[X,X^{-1}\right], \underline{m}\left[X,X^{-1}\right]\right)
\end{aligned}
$$
and $\alpha \equiv 1
\pmod{\underline{m}\left[X,X^{-1}\right]}$. Let
$k=\frac{A}{\underline{m}}$ and $\overline{\alpha}$ be the image of
$\alpha$ in $GL_n\left(k\left[X,X^{-1}\right]\right)$. Then 
$$
\overline{\alpha}=\overline{\alpha_1}\overline{\alpha_2}\overline{\alpha_3}=1
\text{ in } GL_n\left(k\left[X,X^{-1}\right]\right).
$$

Since $\alpha_3\equiv
1\pmod{\underline{m}\left[X,X^{-1}\right]}$,
$\overline{\alpha_3}=1$. So 
$$
\overline{\alpha}=\overline{\alpha_1}\overline{\alpha_2}=1.
$$

Therefore
$$
\overline{\alpha}=\overline{\alpha}^{-1}_2\varepsilon E_n(k[X])\cap \tilde{T}\left(k\left[X,X^{-1}\right]\right).
$$

By the same argument as in Lemma~\ref{c3:lem3.7}(i),
$$
\tilde{T}(A[X])\rightarrow \tilde{T}(k[X])
$$
is surjective. Let $\beta$ be a lift of $\overline{\alpha_1}$ in
$\tilde{T}(A[X])\subset E_n(A[X])$. Then 
$$
\alpha_1\beta^{-1}\varepsilon E_n(A[X])\cap GL_n(A[X],\underline{m}[X]).
$$

Also since  $\overline{\beta}=\overline{\alpha}
=\overline{\alpha}_2^{-1}$, we have
$\overline{\beta}\overline{\alpha}_2=1$, so 
$$
\begin{aligned}
&\beta\alpha_2\varepsilon
\tilde{T}\left(A\left[X,X^{-1}\right]\right)\cap
GL_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\\
&{}\subset
E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)
\quad\text{ be Lemma~\ref{c3:lem3.5} }
\end{aligned}
$$

Therefore
$$
\begin{aligned}
\alpha&=\alpha_1\beta^{-1}\beta\alpha_2\alpha_3\varepsilon
(E_n(A[X])\cap GL_n(A[X],\\
&{}\underline{m}[X]))E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\\
&=G_+E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
\end{aligned}
$$
\enprf
\end{Proof}

\begin{coro}\label{c3:coro3.10}
Let $A$ be a commutative local ring with maximal ideal
$\underline{m}$. Suppose $n\geq 3$. Then 
$$
\begin{aligned}
&E_n\left(A\left[X,X^{-1}\right]\right)\cap
GL_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\\
&{}=\left(G_+\cap GL_n(A[X],(X))\right) E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
\end{aligned}
$$
where $G_+=E_n(A[X])\cap GL_n(A[X],\underline{m}[X]).$
\end{coro}

\begin{Proof}
Let 
$$
\begin{aligned}
&\alpha \varepsilon E_n\left(A\left[X,X^{-1}\right]\right)\cap
GL_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\\
&{}=G_+E_n\left(A\left[X,X^{-1}\right]\right),\underline{m}\left[X,X^{-1}\right]
\end{aligned}
$$
by corollary~\ref{c3:coro3.9}. So we can write
$$
\begin{aligned}
\alpha=\alpha_+\alpha_1,\alpha_+\varepsilon G_+\alpha_1 \varepsilon
E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
\end{aligned}
$$

Let $\alpha_+(0)$ be the image of $\alpha_+$ in $GL_n(A)$ under the
map $X\rightarrow 0$. We write 
$$
\alpha=\alpha_+\alpha_+(0)^{-1}\alpha_+(0)\alpha_1.
$$
Clearly
$$
\alpha_+\alpha_+(0)^{-1}\varepsilon G_+\cap GL_n(A[X],(X))
$$
and 
$$
\alpha_+(0)\alpha_1\varepsilon
E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\quad(\text{
  by Lemma~\ref{c3:lem3.6} })
$$

This completes the proof.
\enprf
\end{Proof}


\section{Structure of \texorpdfstring{\protect\eq}{eq}}\label{c3:s4}  
 
In this section we prove Theorem~\ref{c3:thm4.1}, due to
\citeauthor{Suslina}\cite{Suslina} stated in section~\ref{c3:s1}.


\begin{thm}\label{c3:thm4.1}
Let $A$ be any commutative local ring with maximal ideal
$\underline{m}$ and $n\geq 3$. Then 
$$
E_n\left(A\left[X,X^{-1}\right]\right)\cap GL_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)=G_+G_-
$$
where
\begin{align*}
G_+&=E_n(A[X])\cap GL_n(A[X],\underline{m}[X])\\
G_-&=E_n\left(A\left[X^{-1}\right]\right)\cap GL_n\left(A\left[X^{-1}\right],\underline{m}\left[X^{-1}\right]\right).
\end{align*}
\end{thm}

\begin{Proof}
Let $G=G_+G_-$ and 
$$
H=\left\{h \varepsilon E_n\left(A\left[X,X^{-1}\right]\right)\mid
hGh^{-1}\subset G\right\}
$$
\underline{Claim:} $H=E_n\left(A\left[X,X^{-1}\right]\right)$.


Clearly $h_1,h_2\varepsilon H\Rightarrow h_1h_2\varepsilon H$. So it
is enough to show that 
$$
e_{ij}(a),e_{ij}(aX),e_{ij}\left(aX^{-1}\right)\varepsilon H,
a\varepsilon A
$$
as any element of $E_n\left(A\left[X,X^{-1}\right]\right)$ can be written
as a product of $e_{ij}(a)$, $e_{ij}(aX), e_{ij}\left(aX^{-1}\right)$
for $1\leq i\neq j\leq n, a\varepsilon A$. Clearly $e_{ij}(a)\varepsilon
H$. Now considering the mapping $X\rightarrow X^{-1}$ from
$A\left[X,X^{-1}\right]\rightarrow A\left[X,X^{-1}\right]$, we note
that this induces a mapping $G_+\rightarrow G_{-}$ and from
$G_+G_{-}\rightarrow G_{-}G_{+}$. So if 
$$
e_{ij}\left(aX^{-1}\right)G_+G_{-}e_{ij}(\left(-aX^{-1}\right)\subset G_+G_{-}
$$
then
$$
e_{ij}(aX)G_{-}G_{+}e_{ij}(-aX)\subset G_{-}G_+.
$$

Now taking the inverse we get
$$
e_{ij}(aX)G_{+}G_{-}e_{ij}(-aX)\subset G_{+}G_{-}=G.
$$

So it is enough to show that 
$$
e_{ij}\left(aX^{-1}\right)\varepsilon H, a\varepsilon A.
$$

Let $\pi$ be a permutation matrix such that $\pi(1)=i$,
$\pi(n)=j$. Then 
$$
\pi e_{1n}\left(aX^{-1}\right)\pi^{-1}=e_{ij}\left(aX^{-1}\right)
$$
since $S_n$ normalises $G_+$ and $G_{-}$, it suffices to show that 
$$
e_{1n}\left(aX^{-1}\right)\varepsilon H, a\varepsilon A.
$$

Let
$$
\alpha=\alpha_+\alpha_-\varepsilon G_+G_{-}
$$
where $\alpha_+\varepsilon G_+,\alpha_{-}\varepsilon G_{-}$. Let
$\alpha_+(0)$ be the image of $\alpha$ in $GL_n(A,\underline{m})$
under the mapping induced by $X\rightarrow 0$. We can write 
$$
\alpha=\alpha_+\alpha_+(0)^{-1}\alpha_+(0)\alpha
$$

Since $\alpha_+\alpha_+(0)^{-1}\equiv 1\pmod{X}$, we may assume
$\alpha_+\equiv 1\pmod{X}$. Let 
$$
\alpha_+
\begin{bmatrix}
1+Xf_{11} & Xf_{12}&\cdots &Xf_{1n}\\
Xf_{21} & 1+Xf_{22} &\\
\vdots & \vdots & &\vdots\\
Xf_{n-1} & Xf_{n2}&\cdots& 1+Xf_{nn}
\end{bmatrix} \varepsilon G_+
$$

Then 
{\fontsize{10}{12}\selectfont$$
e_{1n}\left(aX^{-1}\right)\alpha_+=
\begin{bmatrix}
1+Xf_{11}+af_{n1} & Xf_{12}+af_{n2}& \cdots &
Xf_{1n}+aX^{-1}+af_{nn}\\
Xf_{21} & 1+Xf_{22}&\\
\vdots & \vdots & &\vdots\\
Xf_{n1}& Xf_{n2}&\cdots & 1+Xf_{nn}
\end{bmatrix}
$$}

Let $a'=f_{n1}(0)\varepsilon \underline{m}$. Therefore the constant
term of $1+Xf_{11}+af_{n1}$ is $(1+aa')\varepsilon A^{\ast}$. So we
can write
$$
1+Xf_{11}+af_{n1}=(1+aa')+X\phi, \phi \varepsilon \underline{m}[X].
$$

Now
$$
\begin{aligned}
&e_{1n}\left(aX^{-1}\right)\alpha_+e_{1n}\left(-a(1+aa')^{-1}X^{-1}\right)\\
&{}=
\begin{bmatrix}
(1+aa')+X\phi & Xf_{12}+af_{n2} &\cdots &
  -a(1+aa')^{-1}\phi+Xf_{1n}+af_{nn}\\
Xf_{21} & 1+Xf_{22} & \cdots & -a(1+aa')^{-1}f_{21}+Xf_{2n}\\
\vdots & \vdots & & \vdots\\
Xf_{n1} & Xf_{n2} &\cdots & -a(1+aa')^{-1}f_{n1}+1+Xf_{nn}
\end{bmatrix}\\
&\varepsilon GL_n(A[X],\underline{m}[X]).
\end{aligned}
$$

Since 
$$
e_{1n}\left(aX^{-1}\right)=\delta^{-1}_1e_{1n}(a)\delta_1,
$$

we have 
$$
\begin{aligned}
&e_{1n}\left(aX^{-1}\right)\alpha_+e_{1n}\left(-1\left(1+aa'\right)^{-1}X^{-1}\right)\\
&{}= \delta^{-1}_1e_{1n}(a)\delta_1\alpha_+\delta^{-1}_1e_{1n}\left(-a\left(1+aa'\right)^{-1}\right)\delta_1.
\end{aligned}
$$

But $\alpha_+\equiv 1\pmod{X}$, so
$\delta_1\alpha_+\delta^{-1}_1\varepsilon GL_n(A[X])$ and by\\
Proposition~\ref{c3:Prop3.1},
$\delta_1\alpha_+\delta^{-1}_1\varepsilon E_n(A[X])$. Therefore 
$$
e_{1n}(a)\delta_1\alpha_+\delta^{-1}_1e_{1n}\left(-a(1+aa')^{-1}\right)\varepsilon E_n(A[X]).
$$

Again by Proposition~\ref{c3:Prop3.1}, we get
$$
\delta^{-1}_1e_{1n}(a)\delta_1\alpha_+\delta^{-1}_1e_{1n}\left(-a(1+aa')^{-1}\right)\delta_1\varepsilon E_n(A[X]).
$$

Therefore
$$
\begin{aligned}
&e_{1n}\left(aX^{-1}\right)\alpha_+e_{1n}\left(-a(1+aa')^{-1}X^{-1}\right)\\
&{}\varepsilon E_n(A[X])\cap GL_n(A[X],\underline{m}[X])=G_+
\end{aligned}
$$

Now since 
$$
\begin{aligned}
e_{1n}\left(aX^{-1}\right)\alpha_+&\alpha_{-}e_{1n}\left(-aX^{-1}\right)\\
=e_{1n}\left(aX^{-1}\right)&{}\alpha_+e_{1n}\left(-a(1+aa')^{-1}X^{-1}\right)\\
&{}e_{1n}(a(1+aa')^{-1}X^{-1}\alpha_{-}e_{1n}\left(-aX^{-1}\right)
\end{aligned}
$$
and
$$
e_{1n}\left(a\left(1+aa'\right)^{-1}X^{-1}\right)\alpha_{-}e_{1n}\left(-aX^{-1}\right)\varepsilon G.
$$
So
$$
e_{1n}\left(aX^{-1}\right)\alpha e_{1n}\left(-aX^{-1}\right)
\varepsilon G.
$$

Therefore, for $\alpha\varepsilon
E_n\left(A\left[X,X^{-1}\right]\right)$, $\alpha G \alpha^{-1}\subset
G$. This proves our claim.

Now to complete the proof of the theorem, we note that by
Corollary~\ref{c3:coro3.9},
$$
\begin{aligned}
&E_n\left(A\left[X,X^{-1}\right]\right)\cap
GL_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\\
&{}=G_+E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right).
\end{aligned}
$$

Therefore it is enough to show that
$E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\subset
G$. But this follows from the fact that
$E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)$
is generated by the elements of the form
$$
\alpha\beta \alpha^{-1},\alpha\gamma\alpha^{-1}
$$
where
$$
\begin{aligned}
&\alpha \varepsilon E_n\left(A\left[X,X^{-1}\right]\right)\\
&\beta \varepsilon E_n(A[X],\underline{m}[X])\\
&\gamma\varepsilon E_n\left(A\left[X^{-1}\right],\underline{m}\left[X^{-1}\right]\right)
\end{aligned}
$$
and 
$$
\begin{aligned}
\alpha\beta \alpha^{-1} G&=\alpha\beta\alpha^{-1}G\alpha\alpha^{-1}\\
&{}\subset \alpha\beta G\alpha^{-1}\qquad (\text{ by above claim })\\
&{}\subset \alpha G\alpha^{-1}\\
&{} \subset G\\
\alpha\gamma\alpha^{-1}G&=\alpha\gamma\alpha^{-1}G\alpha\alpha^{-1}\\
&{}\subset \alpha\gamma G\alpha^{-1}=\alpha\gamma
G\gamma\gamma^{-1}\alpha^{-1}\\
&{}\subset (\alpha\gamma)G(\alpha\gamma)^{-1}\\
\end{aligned}
$$

This proves the theorem.
\enprf
\end{Proof}

\begin{rem}\label{c3:rem4.2}
In fact \citeauthor{Suslina} proves that for a commutative local ring $A$ with
maximal ideal $\underline{m}$,
%$$
\begin{align*}
&E_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\\
&=E_n\left(A\left[X,X^{-1}\right]\cap
  GL_n\left(A\left[X,X^{-1}\right],\underline{m}\left[X,X^{-1}\right]\right)\right)\\
&=G_+G_{-}\quad (n\geq 3)
\end{align*}

by showing that 
$$
\begin{aligned}
G_+&=E_n(A[X],\underline{m}[X])\\
G_-&=E+_n\left(A\left[X^{-1}\right],\underline{m}\left[X^{-1}\right]\right).
\end{aligned}
$$

Below we outline a proof for this.

For a field $k,k_{2,n}(k[X])$ is generated by symbols.

Hence if $A$ is local with maximal ideal $m$ and $k=\frac{A}{m}$, then
units of $k[X]$ lift to units of $A[X]$. So
$K_{2,n}(A[X]K_{2,n}(k[X])$ is surjective.

Now $G_{\pm}=E_n\left(A\left[X^{\pm 1}\right]\right)$,
$\underline{m}\left[X^{\pm}\right]$ follows from the following lemma.
\end{rem}

\begin{lem}[\citeauthor{Suslina}]\label{c3:lem4.3}
Let $A$ be any commutative ring and $I\subset A$ be an ideal of $A$
such that $K_{2,n}(A)\rightarrow K_{2,n}\left(\frac{A}{I}\right)$ is
surjective. Then $E_n(A,I)=E_n(A)\subset GL_n(A,I)$.
\end{lem}

\begin{Proof}
Note that we have a commutative diagram of short exact sequences

$$
\xymatrix{
& & 0\ar[d]&0\ar[d]&\\
& &\ar[r]St_n(A,I)\ar[d]\ar[r]^-\phi&(E_n(A)\cap GL_n(A,I))\ar[d]&\\
0\ar[r]&K_{2,n}(A)\ar[d]^-{\Psi}\ar[r]&St_n(A)\ar[d]\ar[r]&E_n(A)\ar[d]\ar[r]&0\\
0\ar[r]&K_{2,n}^{\dfrac{A}{I}}\ar[r]&St_n\left(\dfrac{A}{I}\right)\ar[r]&
E_n\left(\dfrac{A}{I}\right)\ar[r]&0}
$$

%$$
%\xymatrix{
%A\ar[d]_f\ar[r]^f&B\ar[dl]|{i_b}\ar[d]^g\\
%B\ar[r]_g&c}
%$$

Now observe that $St_n(A,I)$ is a normal subgroup of $St_n(A)$
generated by Steinberg symbols $x_{ij}(a)$, $a\varepsilon I$ and so
$\phi (St_n(A,I))=E_n(A,I)$. Therefore it is sufficient to show that
$\phi$ is surjective. But this follows easily from the above diagram,
because by hypothesis $\psi$ is surjective.
\enprf
\end{Proof}


