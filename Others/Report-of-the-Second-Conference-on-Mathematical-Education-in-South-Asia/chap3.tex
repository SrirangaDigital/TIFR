
\chapter{A Beginning Course in Analytic Geometry and Calculus}


\begin{center}
{\em By~} M. H. STONE
\end{center}

When\pageoriginale  I became Chairman of the Department of Mathematics
at the University of Chicago in 1946, the time seemed to be ripe for
revising the entire mathematical curriculum there. A long series of
discussions resulted in a carefully articulated scheme of courses
ranging from these offered first-year undergraduates to the most
advanced post-graduate lectures and seminars. I took a personal
interest in replacing the old first-year sequence of college algebra,
trigonometry and analytic geometry by a new one providing an
introduction to analytic geometry and calculus. After working out a
syllabus for the new sequence, I collaborated with Dr. Allen Strehler,
at that time an instructor in the Department, in preparing notes for
the firist part of the course, covering the co-ordinate geometry of
the straight line and circle and the basic principels of
trigonometry. We later decided to write a text for the entire
sequence, and are still engaged in doing so. In fact, we plan to go
considerably further in treating the calculus than required by our
original purpose.

This text is designed for students with only a modest preparation in
secondary school algebra and euclidean geometry. We presuppose a
knowledge of the algebraic and ordinal properties of the real numbers,
the principle of the least upper bound included, and a knowledge of
the first two books of Euclid and the analogous material on straight
lines and planes in three-dimensional space. It is necessary for us to
review and refine this material, as well as to anticipate deficiencies
in the student's preparation. We aim to provide an introductory text
covering the elements of analytic geometry, trigononetry, differential
calculus, and integral calculus, which will be logically correct but
not necessarily logically complete, which will make a strong appeal to
the intuition, and which will provide a convenient foundation for more
advanced instruction along\pageoriginale modern lines. We are finding
that there are numerous opportunities for simplifying and clarifying
the presentation of traditional material by rearrangement and by
appeal to new demonstrations. We seek to provide problems which will
develop the student's power and deepen his understanding. We are
trying to justify with adequate proofs as much as possible of the
theoretical development. Everything we propose to include has been or
will be tested in the class-room.

What I wish to do here is to describe some of the salient features of
this work in progress.

At the outset we face a difficult problem---how to introduce
co-ordinates in the euclidean plane. At first sight, it appears to be
a simple matter to define both cartesian and polar co-ordinates ; but
what turns out to be hard is to choose definitions leading to
relatively simple, straightforward proofs of the basic equations and
formulas of analytic geometry. It is well known that complications due
to the order or separation properties of the plane obstrude themselves
and force the consideration of numerous special cases unless good
definitions are taken as the starting point. It is also well known
that these complications  can be avoided in large measure by basing
the discussion on the concept of the ordered point-pair ($P_1$, $P_2$)
or, what amounts to the same thing, the directed  line segment with
$P_1$ and $P_2$ as initial and terminal points respectively. It is
therefore tempting from a mathematical point of view not only to make
use of ordered point-pairs but to go even a step farther and to
introduce the much more important vector concept, thus making
immediate contacts with elementary physics and modern algebra. Here,
however, the appropriate procedure of defining a vector as an
equivalence-class of ordered point-pairs seemed to us to require too
great a degree of sophistication on the student's part. Consequently
we have decided to adopt the physicist's definition of a vector as a
pair comprising a magnitude and a direction. It is necessary, of
course to define the terms ``magnitude'' and ``direction''. Only the
latter offers any difficulty. The concept of direction again involves
an equivalence-realtion, but is so familiar to the\pageoriginale
student from every-day life that it can now be analysed in a way
appealing quite naturally to his intuition and avoiding too great an
appearance of artificiality. We define two rays to have the same
direction, intuitively speaking, if and only if they contain points
less than some fixed distance apart but arbitrarily far away from
their initial points. Formally them, two rays $r_1$ and $r_2$ with the
respective initial points $O_1$ and $O_2$ have the same direction if
and only if there is a constant $C$ such that, whatever the points
$X_1$ and $X_2$ on $r_1$ and $r_2$ respectively, there are points
$Y_1$ and $Y_2$ with $X_k$ between $O_k$ and $Y_k, k = 1, 2,$ and
$|Y_1 Y_2| \leq C$. An equivalent affine definition is this : two rays
have the same direction if and only if every transversal to the
straight lines determined by the two rays determines a half-plane
which intersects each of the given rays in a ray. By a transversal to
two straight lines we mean a straight line which interesects both
without containing either. It is not difficult to set up the
correspondence between directions and angles which is embodied in the
familiar mariner's compass. Inevitably a full discussion of these
matters would require careful attention to order properties of the
plane ; and even a partial discussion, which is all we envisage,
necessitates a review of facts concerning parallel lines and
angles. An inconvenience which has to be accepted  in using the
physicist's definition of a vector is the necessity of identifying the
pairs comprising the magnitude zero and an arbitrary direction. This
identification demands the consideration of a number of special cases,
but its meaning is sufficiently clear both physically and
geometrically that no really serious difficulties have to be
overcome. Once the preliminary discussion has been completed the
introduction of the vector operations and the length or norm of a
vector is undertaken in the usual way ; and the algebraic and metric
properties of the resulting vector-system are developed. The
introduction of co-ordinates now follows : If O, X, Y are fixed points
in the plane and $P$ is an arbitrary point, the ordered point-pairs
($O,X$), ($O,Y$), ($O,P$) determine vectors $u, v, w$ respectively in
an obvious manner, viz. the magnitude associated with ($O,P$) is the
distance $\rho = |OP|$ between $O$ and $P$, and the direction
associated with ($O,P$) is that determined by a ray originating at $O$
and passing through\pageoriginale $P$, unique unless $P$ and $O$ coincide. The polar
co-ordinates ($\rho, \theta$) of $P$ are the magnitude $\rho$ of the
vector $\bw$ and the numerical angle $\theta$ between the directions
of the vectors $\bu$ and $\bw$, in the indicated order. This
definition presupposes an adequate treatment of the measurement of
angles. When $\bu$ and $\bv$ have unit magnitudes and orthogonal or
perpendiculare directions, the cartesian co-ordinates of $P$ are $x$
and $y$ where $\bw = x \bu + y \bv$.

The derivation of the standard formulas and equations for the analytic
geometry of the straight line and circle proceeds without further
difficulty. The correspondence between straight lines and linear
equations is almost immediate, as is the distance formula. If $(x,y)$
is an arbitrary point on a given line $Ax + By + C = 0$ and $(x_0 ,
y_0)$ is a given point, the identity
\begin{align*}
(A^2+ B^2) ((x-x_0)^2 + (y-y_0)^2) & = (A (x-x_0) + B(y-y_0))^2 +\\
& \qquad + (-B(x-x_0) + A (y - y_0))^2\\
& = (Ax_0 + By_0 + C)^2  +\\
& \qquad + (-B (x-x_0) + A(y-y_0))^2
\end{align*}
shows that the distance from ($x_0, y_0$) to ($x,y$) has its minimum
value $|Ax_0 + By_0 + C| / \surd (A^2+B^2)$ when $(x,y)$ is the point
$(x_1, y_1)$ such that 
$$
Ax_1 + By_1 + C = 0, \quad - Bx_1 + Ay_1 + (Bx_0 - Ay_0) = 0.
$$
From geometrical considerations it follows that the straight line with
the equation $-Bx + Ay + (Bx_0 - Ay_0) =0$ is the line through $(x_0,
y_0)$ perpendicular to the given line. Thus we may obtain quite easily
and without attention to any special cases the standard formula for
the distance from a point to a line and the essential elements of the
theory of orthogonal or perpendicular lines and directions in the plane.

So far as trigonometry is concerned we may define the basic
trigonometric functions $\cos \theta$ and $\sin \theta$ by the
equations $x = \rho \cos \theta$, $y = \rho \sin \theta$, connecting
the cartesian and the polar co-ordinates of a point $P$ in the
plane. Equivalently, we may define $\cos \theta$ and $\sin \theta$ as
the co-ordinates of the point $P : (x,y)$ on the unit circle, $x^2 +
y^2 = 1$. The\pageoriginale principal formulas of trigonometry can now
be derived without any trouble. Consider the points $P_1, P_2, P_3,
P_4$ on the unit circle with the respective co-ordinates (1,0), $(\cos
(\theta + \phi), \sin (\theta + \phi))$, $(\cos \phi, - \sin \phi)$,
$(\cos \theta, \sin \theta)$. It is evident geometrically that the
distances $|P_1 P_2|$ and $|P_3 P_4|$ are equal, whence
\begin{align*}
(1- \cos (\theta + \phi))^2 + (\sin (\theta + \phi))^2 & = (\cos \phi
  - \cos \theta)^2 + \\
& \qquad + (-\sin \phi - \sin \theta)^2
\end{align*}
or
$$
\cos (\theta + \phi) = \cos \theta \cos \phi - \sin \theta \sin \phi.
$$
The basic formulas for ``solving the triangle'' can be obtained by
similar applications of analytic geometry. To obtain the law of
cosines, for a triangle with sides $a, b, c$ and opposite angles
$\alpha, \beta, \gamma$ respectively, we introduce co-ordinates in the
plane so that  the vertices have the respective co-ordinates $(0,0)$
$(\alpha, 0)$, $(b \cos \gamma, b \sin \gamma)$; by computing the
distance between the last two we obtain
$$
c^2 = (a - b \cos \gamma)^2 +(-b \sin \gamma)^2 = a^2 - 2 ab \cos
\gamma + b^2.
$$
To obtain the law of sines, we use the same co-ordinates for the plane
and verify that the circle through the three vertices has the equation
$$
x^2 + y^2 - ax - \frac{b - a \cos \gamma}{\sin \gamma} y = 0, \quad
\sin \gamma > 0,
$$
and the diameter $d$, where
$$
d^2 = \frac{a^2 - 2 ab \cos \gamma + b^2}{\sin^2 \gamma} =
\frac{c^2}{\sin^2 \gamma}.
$$
Thus by symmetry we see that
$$
\frac{\sin \alpha}{a} = \frac{\sin \beta}{b} = \frac{\sin \gamma}{c} = \frac{1}{d}.
$$

Looking ahead, we may remark that the differentiation of the
trigonometric functions follows easily by a consideration of the
elementary geometry of the circle. Letting $\bw$ be the vector
determined by the ordered point-pair $(P_4, P_2)$ where $P_4$ and
$P_2$ are as above, we easily see that the vector $(1/\phi)$ $\bw$ has
a limit vector as $\phi$ tends to zero---namely the vector with
magnitude $c = \lim\limits_{\phi \to 0} |P_4 P_2| / | \phi |$\pageoriginale and
direction corresponding to the angle $\theta + \alpha$, where $\alpha$
is a right angle, and coinciding with the direction of a ray tangent
to the circle at its initial point $P_4$. The corresponding formulae
for differentiation are
\begin{align*}
D_\theta \cos \theta & = c \cos (\theta + \alpha) = - c \sin \theta,\\
D_\theta \sin \theta & = c \sin (\theta + \alpha) = c \cos \theta,
\end{align*}
obtained by considering the components of the variable vector
$(1/\phi) \bw$ and its limit vector. The meaning of these equations
for uniform motion in a circular orbit is obvious. Naturally, this
discussion rests upon the usual assumptions about the measurement of
the circle---namely, that the arc length is proportional to the
measure of the angle subtended at the center, and that the ratio of
arc length to chord length tends to unity as the common extremities of
arc and chord tend to coincidence. Accordingly the above derivation
still has a provisional character at this stage.

After various experiments, we are at present inclined to do nothing
more radical in treating the conic sections than to start from the
Boscovich definition and to limit the discussion to their most
elementary properties. A standard treatment of the general equation of
the second degree is to be included.

The calculus must be founded on a theory of limits, and we propose to
face this fact. In our opinion a stronger appeal is made to the
intuition by starting with the case of functions of a real variable
rather than with that of functions of a positive integral
variable---i.e. of sequences. Hence we start with an informal verbal
definition of the statement ``$\lim\limits_{x \to 0} f (x) = 0$'' and
its familiar formal counterpart. The informal meaning given to this
statement is that $f(x)$ is small whenever $|x|$ is small. At this
informal level the proposition that $\lim\limits_{x \to 0} f(x) = 0$
and $\lim\limits_{x \to 0} g(x) = 0$ imply $\lim\limits_{x \to 0}
(f(x) + g(x)) = 0$ and that $\lim\limits_{x \to 0} f(x) = 0$ and
$|g(x)| \leqslant C$ for $x$ small imply $\lim\limits_{x \to 0} f(x)
g(x) = 0$ are intuitively self-evident and the formal proofs can be
discussed as a refinement of common sense. The definitions, both
formal and\pageoriginale informal of the statements ``$\lim\limits_{x \to a} f (x) =
b$'', ``$\lim\limits_{x \to + \infty} f(x) = b$'', ``$\lim\limits_{x
  \to - \infty} f (x) = b$'' are then introduced, as are their
extensions to the cases where the domain of $f$ merely has one of $0,
a, + \infty, -\infty$ as a limit-point instead of containing some
deleted neighbourhood of that point. Thus, for example, we many say
that $\lim\limits_{x \to a} f(x) = b$ if and  only if the function $g$
defined by putting $g(y) = f(x) - b$ when $y = x - a$ has the property
$\lim\limits_{y \to 0} g(y) =0$. The corresponding formal definition
is then seen to be the familiar standard one, containing the original
special case where $a =b = 0$ in an obvious way and appearing as a
natural generalization of it. It is often convenient to describe the
relations ``$\lim\limits_{x \to a} f(x) = 0$'', ``$\lim\limits_{x \to
  + \infty} f(x) = 0$'', ``$\lim\limits_{x \to - \infty} f(x) = 0$''
by saying that $f$ vanishes at $a$, $+ \infty$, or $-\infty$
respectively. For examples, a function is said to be continuous at the
argument $a$ in its domain if and only if the function $f - f(a)$
vanishes at $a$. Again, the function $f$ has a derivative number $b$
if and only if $f$ is defined in a neighbourhood of $a$ and the
function $g$ given by
$$
g(x) = \frac{f(x) - f(a)}{x - a} - b, \quad x \neq a,
$$
vanishes at $a$---hence equivalently, if and only if there exists a
function $g$ defined near $a$ and vanishing there such that $f(x) =
f(a) + b(x -a) + (x -a) g(x)$. If we put $h(x) = g(x)$ when $ x\neq a$
and $g(a) = 0$, we see that $f(x) = f(a) + b(x - a) + (x-a) h(x)$
where $h(x)$ is continuous at $a$ and vanishes there, when $f$ has the
derivative number $b$ at $a$. Evidently $f$ is continuous at $a$ under
these circumstances.

Of course, the systematic development of the theory of limits and its
application to the study of derivatives follows more or less standard
lines once the fundamental concepts have been properly introduced. We
note that the basic criteria for the existence of limits are obtained
by applying the principle of the least upper bound, which we have
assumed to hold for the real number system. Thus we show that a
bounded increasing sequence of real numbers has a limit, and shall
probably give also a demonstration of the theorem  that every Cauchy
sequence has a limit.

We\pageoriginale find that many parts of the calculus, both
differential and integral, can conveniently be based on the mean-value
theorem. Since this theorem rests ultimately upon the proposition that
a continuous function defined on a bounded closed interval assumes
maximum and minimum values, we wish to give a simple demonstration of
this fact. It seems to me that we can do so by following a recent
suggestion of Professor L. R. Ford in the \textit{American
  Mathematical Monthly.} To take advantage of it, we first point out
that the desired result follows easily from the boundedness of a
continuous function defined on a bounded closed interval. Indeed,
suppose that $f$ is continuous and has the least upper bound $M$ on
the interval $a \leqslant x \leqslant b$. If we suppose further that
$f(x) \neq M$ for $a \leqslant x \leqslant b$, the function $g$ given
by $g(x) = \dfrac{1}{M - f(x)}$ is continuous. By assumption, then,
$g$ is bounded, $0 < g(x) \leqslant C$ for $a \leqslant x \leqslant
b$. Thus $f(x) = M - \dfrac{1}{g(x)} \leqslant M - \dfrac{1}{C}$ for
$a \leqslant x \leqslant b$; and by definition, therefore, $M
\leqslant M - \dfrac{1}{C}$---a contradiction. We now turn to the
proof that if $f$ is continuous on $[a,b]$ it is bounded there. We
consider the intervals $[a,c]$, $a \leqslant c \leqslant b$ on which
$f$ is bounded, i.e. for which there exists a constant $M_c$ such that
$|f(x)| \leqslant M_c$ for $a \leqslant x \leqslant c$. Let $d$ be the
least upper bound of the various numbers $c$ in question. Since $f$ is
bounded on $[a,a]$ and since $c \leqslant b$, it is clear that $a
\leqslant d \leqslant b$. We wish to prove that $d = b$ and that $f$
is bounded on $[a,d] = [a,b]$. Consider the part of the interval
$[d-h, d+h]$, $h>0$, which lies on $[a,b]$. It is an interval $[d-k,
  d+l]$ where $0 \leqslant k \leqslant h$, and $0 \leqslant l
\leqslant h$. If $h$ is small enough we know that $f$ is bounded on
this interval : in fact, since $f$ is continuous at $d$ we see that
$|f(x) - f(d)| \leqslant \epsilon$ for $|x -d|$ sufficiently small,
and hence that $|f(x)| \leqslant |f(d)| + |f(x) - f(d)| \leqslant
|f(d)| + \epsilon$ under the same condition on $x$. On the other hand,
the definition of $d$ entails the existence of a $c$ such that $d - k
\leqslant c \leqslant d$ and $|f(x)| \leqslant M_c$ on $[a,c]$. Denoting
by $M_{d+l}$ the greater of $M_c$ and $|f(d)| + \epsilon$, we now see
that $|f(x)| \leqslant M_{d+l}$ on $[a, d + l]$. Consequently we must
have $d+ l \leqslant d$, $l = 0$, $d = d + l =b$ and $|f(x)|
\leqslant M_{d+l}$ on $[a,d] = [a,d + l] = [a,b]$. This completes the
proof.

Using\pageoriginale the results of the preceding paragraph we can
easily prove Rolle's theorem in the following form : ``If $f$ is
defined and differentiable in the open interval $(a,b)$ and vanishes
at $a$ and at $b$, then there is a number $x_1$ in this interval such
that $f'(x_1) = 0$''. Here the interval may be unbounded with $a = -
\infty$ or $b = + \infty$ or both. The usual form of the mean-value
theorem can be established at once along familiar lines, and so can
the generalization stating that if $F$ and $G$ are defined and
differentiable in $(a,b)$ and have limits at $a$ and at $b$ denoted by
$F(a)$, $G(a)$, $F(b)$, $G(b)$ respectively, then
$$
\frac{F(b) - F(a)}{G(b)- G(a)} = \frac{F'(x_1)}{G'(x_1)} \quad a < x_1< b,
$$
so long as $G(b) \neq G(a)$ and there is no $x$ in $(a,b)$ for which
$F'(x) = G'(x) =0$. This generalized version of the mean-value theorem
is obtained by applying Rolle's theorem to the function $f$ given by 
$$
f(x) = (G(b) - G(a))~ (F(x) - F(a)) - (G(x) - G(a))~ (F(b) - F(a)).
$$
From it we can immediately derive de l'H$\hat{o}$pital's rule : under
suitable precautions, if $F$ and $G$ vanish at $b$ then the equation
$\lim\limits_{a \to b} \dfrac{F(a)}{G(a)} = \lim\limits_{x_1 \to b}
\dfrac{F'(x_1)}{G'(x_1)}$ holds in the sense that when the second
limit exists the first does also and is equal to it. A natural and
obvious generalization to the case where $F$ and $G$ are $n$-fold
differentiable and $F^{(k)}$ and $G^{(k)}$ vanish at $b$ for $k = 0,
\ldots, n - 1$ is now possible : an easy recursive application of the
generalized mean-value theorem shows that under suitable precautions
there are numbers $x_0 = a < x_1 < \ldots < x_{n-1} < x_n < b$   such
that 
$$
\frac{F(x_0)}{G(x_0)} = \frac{F'(x_1)}{G'(x_1)} = \ldots =
\frac{F^{(k)}(x_k)}{G^{(k)} (x_k)} = \ldots = \frac{F^{(n-1)}
  (x_{n-1})}{G^{(n-1)} (x_{n-1})} = \frac{F^{(n)} (x_n)}{G^{(n)}(x_n)}
$$
and in consequence the equation $\lim\limits_{x_0 \to b}
\dfrac{F(x_0)}{G(x_0)} = \lim\limits_{x_n \to b}
\dfrac{F^{(n)}(x_n)}{G^{(n)} (x_n)}$ holds in the sense that whenever
the second limit exists the first does also and is equal to it. It is
easy to see that the requirement $a<b$ is not essential here. We
observe next that Taylor's theorem with remainder can be obtained at a
single stroke from the equation $\dfrac{F(x_0)}{G(x_0)} =
\dfrac{F^{(n)} (x_n)}{G^{(n)} (x_n)}$ given above ; it suffices to
define $F, G,$ and $n$ by the relations
\begin{align*}
F(x) & = f(x) -\sum\limits^m_{k=0} \frac{f^{(k)}(b)}{k} (x-b)^k,\\
G(x) & = (x - b)^{m+1}, \quad n = m + 1,
\end{align*}\pageoriginale
since $F$ is then the remainder term in Taylor's formula and satisfies
the equation $\dfrac{F(x_0)}{(x_0 - b)^n} = \dfrac{f^{(n)} (x_n)}{n!}$
and thus the equation
$$
F(x) = \frac{f^{(m+1)} (x_{m+1})}{(m+1)!} (x-b)^{m+1}
$$
where $x_{m+1}$ is between $b$ and $x$. Other forms of the remainder
can be derived in a similar manner. The usual applications of Taylor's
theorem can then be developed.

In treating the integral calculus we propose to reduce the amount of
attention given to applications not because we underestimate their
importance but because we think that many of them can be taken up more
effectively in connection with the subjects where they occur
naturally. We intend, however, to show how integration gives us the
key to the concepts of area and arc length. On the other hand, we
shall pay more attention to the fundamentals than is usual in older
texts. Our present inclination is to adopt an analytical approach and
to carry out its details in the case of continuous functions. This
will require us to discuss the uniform continuity of continuous
functions. We believe that the exposition of this material can be made
simple enough and clear enough for its inclusion in an elementary text.

Accordingly, we begin the study of the integral calculus with the
consideration of the primitives $F$ of a function $f$ defined on an
open interval---that is to say, of the functions $F$ such that
$F'=f$. The mean-value theorem shows at once that any two primitives
of $f$ have a constant difference. Indeed, more generally it shows
that the increment $F(b) - F(a)$ across an interval $[a,b]$ is
expressible as $f(x') (b-a)$ where $x'$ is a suitably chosen number
between $a$ and $b$. Now let $\mathscr{D}$ be any subdivision of [$a,b$]
with dividing points $x_0, \ldots, x_n$ where $a = x_0 < x_1 < \ldots
< x_{n-1} < x_n = b$ ;  and let $|\mathscr|$ be the greatest of
the\pageoriginale lengths $x_{k+1} - x_k$, $k=0, \ldots, n -1$. Since
the increment of $F$ across $[a,b]$ can be expressed as the sum,
$\sum\limits^{n-1}_{k=0} (F(x_{k+1}) - F(x_k))$, of the partial
increments across the intervals $[x_k, x_{k+1}]$, we see that $F(b) -
F(a) = R(f,\mathscr{D})$ where 
$$
R(f, \mathscr{D}) = \sum\limits^{n-1}_{k=0} f(x'_{k+1}) ~ (x_{k+1} - x_k)
$$
and where $x'_{k+1}$ is a suitably chosen number between $x_k$ and
$x_{k+1}$. The expression $R(f, \mathscr{D})$ is called a Riemann sum
for $f$ and $[a,b]$. If $x'_{k+1}$ is allowed to vary in $[x_k,
  x_{k+1}]$ the equality for the increment will not remain true in
general, but $R(f, \mathscr{D})$ may still serve  as some sort of
approximation to $F(b) - F(a)$. Under suitable circumstances, it is
natural to hope, this approximation may be improved by altering
$\mathscr{D}$---in particular, by decreasing $|\mathscr{D}|$. We are
thus led to study the Riemann sums for a given function $f$ as a
possible tool for constructing its primitives. More precisely, when
looking for the primitive $F$ such that $F(a) = C$, we may seek to
define $F(b)$ for arbitrary $b, b \geqslant a$, as a limit of
expressions $R(f, \mathscr{D}) + C$. We shall be able to show that
this program is workable when $f$ is continuous.

In order to see what is involved, let us examine some simple
properties of the Riemann sums for a continuous function $f$ or, more
generally, for a bounded one. Putting $L(f,\mathscr{D}) =
\sum\limits^{n-1}_{k=0} m_{k+1} (x_{k+1} - x_k)$ and $U(f,\mathscr{D})
= \sum\limits^{n-1}_{k=0} M_{k+1} (x_{k+1} - x_k)$, where $m_{k+1}$
and $M_{k+1}$ are respectively the greatest lower bound and the least
upper bound of $f$ on $[x_k, x_{k+1}]$, we evidently have
$$
L (f,\mathscr{D}) \leqslant R (f, \mathscr{D}) \leqslant U (f, \mathscr{D}).
$$
Moreover by choosing $x'_{k+1}$ suitably in $[x_k, x_{k+1}]$ we can
make $R(f, \mathscr{D})$ approximate either $L(f, \mathscr{D})$ or
$U(f, \mathscr{D})$ as closely as we please---and we can even make the
Riemann sum equal to either of these extreme values when $f$ is
continuous. In case $\mathscr{D}'$ is a refinement of $\mathscr{D}$ we
see that
$$
L(f, \mathscr{D}) \leqslant L (f, \mathscr{D}') \leqslant U (f,
\mathscr{D}') \leqslant U (f, \mathscr{D}).
$$\pageoriginale
In consequence, if we let $L(f)$ and $U(f)$ denote respectively the
least upper bound of $L(f, \mathscr{D})$ and the greatest lower bound
of $U(f, \mathscr{D})$, we can easily show that
$$
L(f) \leqslant U (f).
$$
In fact, by definition there are for any $\eta > 0$ subdivisions
$\mathscr{D}_1$ and $\mathscr{D}_2$ such that $L(f) - \frac{1}{2} \eta
\leqslant L (f, \mathscr{D}_1)$, $U(f, \mathscr{D}_2) \leqslant U (f) +
\frac{1}{2} \eta$, and if $\mathscr{D}$ is any common refinement of
$\mathscr{D}_1$ and $\mathscr{D}_2$, we therefore have
$$
L(f) - \frac{1}{2} \eta \leqslant L (f, \mathscr{D}) \leqslant U (f,
\mathscr{D}) \leqslant U(f) + \frac{1}{2} \eta \text{~~ or ~~} L (f)
\leqslant U (f) + \eta,
$$
whence the desired result follows. Furthermore it is clear that we can
approximate both $L(f)$ and $U(f)$ as closely as we please by Riemann
sums $R(f,\mathscr{D})$. Thus our program cannot be carried at all
easily unless $f$ is such that $L(f) = U (f)$. On the other hand, when
$L(f)$ and $U(f)$ are equal and have the common value $R(f)$, there is
for each $\eta > 0$ a subdivision $\mathscr{D}_n$ such that for any
refinement $\mathscr{D}$ of $\mathscr{D}_n$ we have 
$$
|R(f,\mathscr{D}) - R(f)|  \leqslant U (f, \mathscr{D}_\eta) - L
(f,\mathscr{D}_\eta) \leqslant \eta.
$$
Thus $R(f)$ is, in a certain sense, the limit of the Riemann sum
$R(f,\mathscr{D})$. When $R(f)$ exists, it is called the Riemann
integral of the function $f$ over the interval $[a,b]$ and is denoted
by the classical symbol $\int\limits^b_a f(x) dx$.

We see, therefore, that the crux of the matter is to determine
conditions under which $L(f) = U (f)$. If $\mathscr{D}$ is a
subdivision such that $M_{k+1} - m_{k+1} \leqslant \epsilon$ for $k=0,
\ldots, n - 1$---we shall then call it an $\epsilon$-subdivision---it
is clear that $U(f)-L(f) \leqslant U (f,\mathscr{D}) -
L(f,\mathscr{D}) \leqslant \epsilon (b-a)$. Hence a sufficient
condition for the desired equality is that there be an
$\epsilon$-subdivision of $[a,b]$ for the given $f$ and for every
$\epsilon > 0$. We shall show that this criterion is verified whenever
$f$ is continuous. Our proof is almost word-for-word the same as that
given previously for the boundedness of $f$. We consider the intervals
$[a,c], a \leqslant c \leqslant b$, for which there are
$\epsilon$-subdivisions. Let $d$ be the least upper bound of the
various numbers $c$ in question. Since $[a,a]$ has an
$\epsilon$-subdivision for $f$ and since $c \leqslant b$, it is clear
that $a \leqslant d \leqslant b$. Consider\pageoriginale the part of
the interval $[d- h, d + h]$, $h >0$ which lies on $[a,b]$. It is an
interval $[d-k, d+l]$, where $0\leqslant k \leqslant h$ and $0
\leqslant l \leqslant h$. If $h$ is small enough, there is an
$\epsilon$-subdivision of this interval, with $n=1$ : in fact, since
$f$ is continuous at $d$ we see that $|f(x) - f(d)| \leqslant
\frac{1}{2} \epsilon$  for $|x-d|$ sufficiently small, and hence that
$f(d) - \frac{1}{2} \epsilon \leqslant f (x) \leqslant f(d) +
\frac{1}{2} \epsilon$ under the same condition on $x$. On the other
hand, the definition of $d$ requires the existence of a $c$ such that
$d - k \leqslant c \leqslant d$ and that $[a,c]$ has an
$\epsilon$-subdivision. It is now evident that the interval $[a, d
  +l]$ obtained by combining $[a,c]$ and $[d-k, d+l]$ has an
$\epsilon$-subdivision given by effecting simultaneously the
$\epsilon$-subdivisions for the latter intervals. Consequently we must
have $d+l \leqslant d$, $l=0$, $d = d+ l = b$, and the proof is completed.


Indeed, we can even sharpen the result just obtained, for we can show
that $f$ is uniformly continuous and hence that every subdivision
$\mathscr{D}$ with $|\mathscr{D}|$ sufficiently small is an
$\epsilon$-subdivision. If $\epsilon > 0$ is given, we determine a
$\frac{1}{2} \epsilon$-subdivision $\mathscr{D}$ for $f$ and put
$\delta (\epsilon) = \min\limits_{k=0,\ldots, n-1} (x_{k+1} - x_k)$,
noting that $|x-x_0| \leqslant \delta(\epsilon)$ requires $x$ and
$x_0$ to lie in the same or adjacent intervals of the subdivision
$\mathscr{D}$ and thus implies $|f(x) - f(x_0)| \leqslant
\epsilon$. It now follows that every subdivision $\mathscr{D}$ with
$|\mathscr{D}| \leqslant \delta (\epsilon)$ is an
$\epsilon$-subdivision. Accordingly $|\mathscr{D}| \leqslant \delta
(\epsilon)$ implies $|R(f,\mathscr{D}) - R(f)| \leqslant \epsilon (b
-a)$---that is, we have $R(f,\mathscr{D}) \to R (f)$ as
$|\mathscr{D}| \to 0$.

It is now an easy matter to show that the function $F$ given by $F(b)
= \int\limits^b_{a} f(x) dx + C$ is the primitive of $f$ such that
$F(a) = C$. First it must be shown, in a familiar manner, that
$$
\int\limits^c_a f(x) dx = \int\limits^b_a f(x) dx + \int\limits^c_b
f(x) dx;
$$
and then this identity is used to obtain the estimate
$$
\left|\frac{F(b+h) - F(b)}{h}  - f(b)\right| = \left|
\frac{1}{h} \int\limits^{b+h}_b [f(x) - f(b)] dx  \right|\leqslant \epsilon
$$
whenever $|h| \leqslant \delta (\epsilon)$. Here, of course, we must put
$$
\int\limits^{b+h}_b [f(x) - f(b)] dx = - \int\limits^b_{b+h} [f(x) -
  f(b)] dx \text{~~ when ~~} h < 0.
$$\pageoriginale
Thus we complete our proof that every continuous function $f$ has a
primitive, given in terms of the Riemann integral of $f$. We may
emphasize that this version of the fundamental theorem of the integral
calculus avoids the confusion which arises out of using the term
``integral'' with a double meaning, as is done in too many well-known texts.

In the preceding remarks we have touched upon a number of the more
outstanding features of the text which Dr. Strehler  and I are trying
to write. It is hoped that we have given an adequate idea of the
standards which we deem attainable at this level of instruction and of
the care which we have considered necessary in selecting our
approaches to the main themes of our book. It remains for me to
comment on one other matter which we think to be of great
importance. There is an obvious danger in giving too highly
specialized or over-ingenious proofs in an elementary text--not so
much because these will be difficult for the student, as because they
may not be easily extended to the more general situations which he
must eventually be expected to master. For example, the discussion of
the distance formula and the properties of perpendicular lines which
we have given above would be unsatisfactory if it could not be
extended to the corresponding situation in three-dimensional
space. Actually it is easy to see that the correlation between planes
and linear equations can first be established with the aid of a little
linear algebra and that the further discussion can then be based on
the identity
\begin{align*} 
& (A^2 + B^2 + C^2) ((x-x_0)^2 + (y+y_0)^2 + (z - z_0)^2)\\
& = (A (x-x_0) + B(y - y_0) + C (z-z_0))^2 + (-B (x-x_0) +
  A(y-y_0))^2\\
& \qquad\quad + (-C(y-y_0) + B(z-z_0))^2 + (-A (z-z_0) + C(x+x_0))^2.
\end{align*}
Thus the shortest distance from a point in the plane $Ax + By + Cz +
D= 0$ to the point $(x_0, y_0, z_0)$ is given by
$$
|Ax_0 + By_0 + Cz_0 + D |/\surd (A^2 + B^2 + C^2)
$$\pageoriginale
and is attained by the point where this plane is pierced by the
straight line
$$
\frac{x-x_0}{A} = \frac{y-y_0}{B} = \frac{z-z_0}{C},
$$
which from geometrical considerations must be the normal through\break
$(x_0, y_0,z_0)$ to the given plane. The proofs we have given for the
boundedness and the uniform continuity of a continuous function have
to be examined in a similar spirit. Since they obviously have such a
special character that they cannot be extended directly to functions
of two or more variables, we have to ask whether they lead readily to
modifications suitable for generalization. It seems to us that the two
theorems given here can be compared  and analysed in such a way as to
bring out their intimate connections with the Heine-Borel theorem for
the closed interval. It is then easily seen that the arguments used to
prove them apply equally well to prove the letter theorem. The
extension to $n$-dimensions can then be made by induction or by some
other standard method. Thus the ground-work is solidly laid for the
eventual discussion of the Heine-Borel theorem in general topology. We
feel, therefore, that there need be no misgivings on this score about
our admittedly special treatment of the properties of continuous
functions of a single real variable.

\bigskip
\bigskip
{\fontsize{9pt}{11pt}\selectfont
University of Chicago}\relax



