
\chapter{Introduction to Determinants}

\begin{center}
{\em By~} YASUO AKIZUKI
\end{center}

\setcounter{pageoriginal}{40}
The\pageoriginale history of the theory of determinants is very old
and that of matrices is rather new. The importance of matrices today
should be emphasized highly as the concepts of groups and several
fundamental algebras have occupied recently more and more importance
in all branches of mathematics. Not only in mathematics itself but
also in theoretical physics, matrices are indispensable. Also in
electric engineering, even in mathematical economics. So it is
desirable in the beginning of the college course (the common course of
mathematicians, natural scientists, engineers) to introduce the
concept of matrices and to teach their manipulation, consequently the
introduction of the theory of determinants should be revised in some
way. Moreover it is logically quite natural to introduce matrices
before the teaching of determinants. Artin had introduced in his book
on Galois theory the determinant of a matrix as an alternative mapping
of the matrix in the field (of elements of the matrix). After this
there have appeared many textbooks dealing with determinants along
this line. Taking one more step ahead I should like here to propose to
take the method of defining determinants by using Grassmann algebra,
as the Grassmann algebra has also shown itself very useful in geometry
and in he theory of multiple integrals. My plan is as follows :

At first introduce vector space over the real number field beginning
with intuitive examples in two and three dimensional euclidean spaces
and then generalizing it to the $n$-dimensional case as a module
(additive group). In this part there is only one point which needs to
be noticed : namely the dimension of the vector space generated by $n$
vectors
$$
e_1 = (1,0, \ldots, 0), ~~e_2 = (0, 1, \ldots, 0, \ldots, ~~e_n = (0,
\ldots , 0 ,1)
$$
is equal to $n$. This may be proved by mathematical
induction. Therefore if $n$ vectors
\begin{equation*}
e'_i = \sum\limits^n_{j=1} a_{ij} e_j (i= 1, 2, \ldots,n) \tag{1}
\end{equation*}\pageoriginale
are linearly independent, then we have
\begin{equation*}
e_j = \sum\limits^n_{k=1} b_{jk} e'_k. \tag{2}
\end{equation*}

Next introduce the notation of matrices, their multiplication and
their applications, so (1) and (2) may be expressed as follows
respectively
\begin{align*}
\begin{bmatrix}
e'_1\\
e'_2\\
\cdot\\
\cdot\\
\cdot\\
e'_n
\end{bmatrix} = A 
\begin{bmatrix}
e_1 \\
e_2 \\
\cdot\\
\cdot\\
\cdot\\
e_n
\end{bmatrix} , \quad A  = 
\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\
\cdot & \cdot & \ldots & \cdot\\
\cdot & \cdot & \ldots & \cdot\\
\cdot & \cdot & \ldots & \cdot\\
a_{n1} & a_{n2} & \ldots & a_{nn}
\end{bmatrix}~,\\
\begin{bmatrix}
e_1\\
e_2\\
\cdot\\
\cdot\\
\cdot\\
e_n
\end{bmatrix} = B 
\begin{bmatrix}
e'_1 \\
e'_2 \\
\cdot\\
\cdot\\
\cdot\\
e'_n
\end{bmatrix} , \quad B  = 
\begin{bmatrix}
b_{11} & b_{12} & \ldots & b_{1n} \\
b_{21} & b_{22} & \ldots & b_{2n} \\
\cdot & \cdot & \ldots & \cdot\\
\cdot & \cdot & \ldots & \cdot\\
\cdot & \cdot & \ldots & \cdot\\
b_{n1} & b_{n2} & \ldots & b_{nn}
\end{bmatrix}~,
\end{align*}
and here $AB = I = BA$, where $I$ is the unit matrix.

Coming back to the two dimensional (euclidean) case consider the
geometrical meaning of matrices as the representation of
transformations. Denoting the unit vector on each of the rectangular
coordinate axes by $e_1$ and $e_2$ take two vectors
$$
e'_1 = a_{11} e_1 + a_{12} e_2, \quad e'_2 = a_{21} e_1 + a_{22} e_2
$$
and consider the relations of the new and old coordinates of a
point. And we remark here that the value $a_{11} a_{22} - a_{12}
a_{21}$ gives the area of the parallelogram generated by the two
vectors $e'_1$ and $e'_2$.

Now taking the rotation around the origin as a special case, we may
give the orthogonal matrix. Then we show that the totality
of\pageoriginale rotations composed with whole dilatations (having the
origin as the centre) forms a multiplicative group ; namely the
totality of matrices
$$
\begin{pmatrix}
& a & b\\
- & b & a
\end{pmatrix}, \text{ ~ where~ } a, b \text{~ real except ~} a = b = 0,
$$
forms a multiplicative group. Introducing here the addition and the
zero-matrix, we remark that this system is isomorphic with the complex
number field ; namely
$$
\begin{pmatrix}
& a & b\\
-&b&a
\end{pmatrix} = a 
\begin{pmatrix}
1& 0\\
0&1
\end{pmatrix} +b 
\begin{pmatrix}
&0&1\\
-&1&0
\end{pmatrix} \simeq a + b i.
$$

Next introduce formally free monomials of $n$ variables using only the
associative law, and linear functions with real (or complex)
coefficients\footnote{Here draw attention to the non-commutativity of
  multiplication, taking some simple examples of matrices,}. Thus we
introduce free polynomial rings and define addition and multiplication
of two polynomials using the distributive law. (Though the tensor
product is essentially introduced here, I think, it is no use alluding
to the tensor product explicitly.)

Introducing then the commutative law (with respect to multiplication),
some classical fundamental theorems of polynomials should be studied.

This is the course to be covered before the introduction of the theory
of determinants and higher linear algebras.

Now introduce Grassmann algebra over an $n$-dimensional vector space
(over a number field $R$)
$$
Re_1 + Re_2 + \ldots + Re_n .
$$

Namely we introduce mechanically the relations
$$
e_i e_j = e_j e_i, \quad e_i e_i = 0
$$
in the free polynomial ring generated by $e_1, \ldots, e_n$ over
$R$. It is advisable to adopt the notation $\wedge$ for the
multiplication which satisfies the above relations to avoid confusion
with the ordinary commutative multiplication. Thus
$$
e_i \wedge e_j = - e_j \wedge e_i, ~~ e_i \wedge e_i = 0.
$$\pageoriginale
At first we notice it is always
$$
x \wedge y = - y \wedge x, \quad x \wedge x = 0,
$$
whenever $x =\sum a_i e_i$, $y = \sum b_i e_i$ and hence if $y_i =
\sum\limits^n_{j=1} a_{ij} x_j(i = 1,2)$, then $y_1 \wedge y_2 = \sum
(a_{1j_1} a_{2j_2} - a_{2j_1} a_{1j_2}) x_{j_1} \wedge x_{j_2}$ where
$1 \leqslant j_1 \leqslant j_2 \leqslant n$. And we remark $G$ is a
graded ring of finite degree $n$.

I think  that the existence proof of the algebra lies beyond the
ability of understanding of freshmen. From the standpoint of
instruction thus we do not touch the existence but only introduce the
algebra mechanically.

We take up the case $n=2$ specially. Namely take two vectors $\alpha_1
= a_{11} e_1 + a_{12} e_2$, $\alpha_2 = a_{21} e_1 + a_{22} e_2$ where
$e_1$, $e_2$ denote the unit vectors of the rectangular coordinate
axes. Then $\alpha_1 \wedge \alpha_2 = (a_{11} a_{22} -a_{12} a_{21})
e_1 \wedge e_2$. We remind the students of the geometrical meaning of
the coefficient of the right-hand-side. And we try to let students
imagine the meaning of the coefficient of $e_1 \wedge e_2 \wedge e_3$
in the exterior product $\alpha_1 \wedge \alpha_2 \wedge \alpha_3$,
where $\alpha_1$, $\alpha_2$, $\alpha_3$ are vectors of 3-dimensional
euclidean space.

After this define the determinant of a matrix $A = (a_{ij})$ of degree
$n$ generally. Namely associate with $A$ $n$ vectors
$$
\alpha_i = \alpha_{i1} e_1  + a_{i2} e_2 + \ldots + a_{in} e_n (i = 1,
2, \ldots, n),
$$
and define the determinant $|A|$ as the coefficient of $e_1 \wedge
\ldots \wedge e_n$ in the product
$$
\alpha_1 \wedge \alpha_2 \wedge \ldots \wedge \alpha_n = (a_{11} e_1 +
\ldots + a_{1n} e_n) \wedge \ldots \wedge (a_{n1} e_1 + \ldots +
a_{nn} e_n).
$$

This definition is nothing but the classical definition of $|A|$ as
the polynomials of the elements of the matrix $A$ :
$$
|A| = \sum ~\epsilon ~a_{1i_1} a_{2i_2} \ldots a_{ni_n},
$$
where $\epsilon$ is the sign of the permutation $\begin{bmatrix}
1&2&\ldots & n\\
i_1 &i_2 &\ldots &i_n \end{bmatrix}$.

This\pageoriginale is a reason why I do not touch the existence proof
of Grassmann algebra, as the determinant $|A|$ is expressed thus
explicitly. We may say that the above definition is thus the same as
the old one, nevertheless Grassmann algebra in the new definition
gives a very nice pattern in which all properties of determinants may
be drawn up very clearly and intuitively without almost any
calculations.

All fundamental properties of the determinant except the one : $|A| =
|^t/A|$ may be considered as self-evident. For the proof of the fact
$|A| = |^tA|$, we must use the fact, as in the classical proof, that
the sign of a permutation does not change by inversion.

The product formula $|AB| = |A| \cdot |B|$ follows at once from the
definition. Namely consider $e'' = Ae'$, $e'= Be$, where each $e$,
$e'$, $e''$ denotes a column vector of elements of the algebra, then
$$
e''_1 \wedge \ldots \wedge e''_n  = |A| e'_1 \wedge \ldots \wedge e'_n
= |A| \cdot |B| e_1 \wedge \ldots \wedge e_n,
$$
on the other hand $e''= ABe$, hence
$$
e''_1 \wedge \ldots \wedge e''_n = |AB| e_1 \wedge \ldots \wedge e_n,
$$
consequently $|AB|  = |A| \cdot |B|$.

The Laplace expansion may be written down also at once multiplying
together partially : namely, if $e' =Ae$, consider
$$
e'_1 \wedge \ldots \wedge e'_n = (e'_1 \wedge \ldots \wedge e'_r)
\wedge (e'_{r+1} \wedge \ldots \wedge e'_n).
$$
From the definition,
$$
e'_1 \wedge \ldots \wedge e'_r =\sum 
\begin{vmatrix}
a_{1,i_1} &\ldots &a_{1, i_r}\\
\cdot & \ldots & \cdot \\
a_{r,i_1} & \ldots & a_{r, i_r}
\end{vmatrix} e_{i_1} \wedge \ldots \wedge e_{i_r} ,
$$
and
$$
e'_{r+1} \wedge \ldots \wedge e'_n = \sum 
\begin{vmatrix}
a_{r+1, j_1} &\ldots & a_{r+1,j_s}\\
\cdot & \ldots & \cdot \\
a_{n,j_1} & \cdots & a_{n,j_s}
\end{vmatrix}
e_{j_1} \wedge \ldots \wedge e_{j_s},
$$
where $r + s = n$. Consequently
$$
\begin{vmatrix}
a_{11} &a_{12}& \ldots& a_{1n}\\
a_{21} &a_{22}& \ldots &a_{2n}\\
\cdot & \cdot & \ldots & \cdot \\
a_{n1} &a_{n2} &\ldots &a_{nn}\\
 \end{vmatrix} = \sum  \epsilon
\begin{vmatrix}
a_{1,i_1} &\ldots& a_{1, i_r}\\
\cdot & \cdots &\cdot \\
a_{r,i_1}& \ldots &a_{r, i_r}
\end{vmatrix}
\begin{vmatrix}
a_{r+1,j_1} & \ldots  &a_{r+1, j_s} \\
\cdot & \ldots & \cdot \\
a_{n,j_1} & \ldots & a_{n,j_s}
\end{vmatrix}
$$\pageoriginale
where $\epsilon$ is the sign of the permutation $\begin{pmatrix}
1 2 \ldots . \ldots . \ldots n\\
i_1 \ldots i_r ~ j_1 \ldots j_s
\end{pmatrix}$. As a special case of the expansion we have, as usual,
the fundamental properties of cofactors of a determinant. From this,
also as usual, we can derive the inverse matrix of a matrix $A$ with
$|A| \neq 0$.

Reducing the linear simultaneous equations in the form $Ax =b$, where
$x$, $b$ are column vectors, we obtain Cramer's solution in the form
explicitly, 
$$
x = A^{-1} b \text{ ~ when ~} |A| \neq 0.
$$
It might be very instructive comparing this with the linear equation
of one variable : $ax = b$.

It might be interesting to show that Cramer's solution may be obtained
also in the following way : denoting the column vector of coefficients
of $x_j$ and constants by $\alpha_j$, $\beta$ respectively, then the
linear equations can be expressed as $\sum \alpha_j x_j =
\beta$. Forming the exterior product of $\alpha_1 \wedge \ldots \wedge
\alpha_{j+1} \wedge \ldots \wedge \alpha_n$ with both sides, we can
eliminate all other terms but that of $x_j$ and, if a solution exists,
$$
|A| x_j = 
\begin{vmatrix}
a_{11}& \ldots &b_1 &\ldots& a_{1n}\\
a_{21}& \ldots &b_2 &\ldots& a_{2n}\\
\cdot& \ldots &\cdot & \ldots & \cdot \\
a_{n1}& \ldots &b_n &\ldots& a_{nn}\\
\end{vmatrix}
$$

Similarly without any special technique we can get many results. For
example :
$$
\begin{vmatrix}
A_{11}& \ldots& A_{1r}\\
\cdot &\ldots &\cdot \\
A_{r1} & \ldots & A_{rr}
\end{vmatrix} = |A|^{r-1} 
\begin{vmatrix}
a_{r+1, r+1}& \ldots &a_{r+1,n}\\
\cdot & \cdots & \cdot \\
a_{n,r+1} & \cdots & a_{n,n}
\end{vmatrix} ,
$$
where $A_{ij}$ is the cofactor of $a_{ij}$ of the determinant
$|A|$. Namely associate to the determinant $|A|$ a linear transformation
$$
e'_i = \sum\limits^n_{j=1} a_{ij, e_j}, \text{ consequently } e_j =
\sum\limits^n_{k=1} \frac{A_{kj}}{|A|} e'_k.  
$$\pageoriginale
Now take the exterior-product $(e_1 \wedge \ldots \wedge e_r) \wedge
(e'_{r+1} \wedge \ldots \wedge e'_n)$, then 
{\fontsize{10pt}{12pt}\selectfont
\begin{align*}
(e_1 \wedge \ldots \wedge e_r) \wedge (e'_{r+1} \wedge \ldots \wedge
  e'_n)  & =  \frac{1}{|A|^r} \begin{vmatrix}
A_{11} &\ldots &A_{1r}\\
\cdot &\cdots& \cdot\\
A_{r1} & \ldots & A_{rr}
  \end{vmatrix} e'_1 \wedge e'_2 \wedge \ldots \wedge e'_n\\
& = \frac{1}{|A|^{r-1}} \begin{vmatrix}
A_{11} & \ldots & A_{1r}\\
\cdot & \ldots & \cdot\\
A_{r1} & \ldots & A_{rr}
  \end{vmatrix} e_1 \wedge e_2 \wedge \ldots \wedge e_n ,
\end{align*}}\relax
on the other hand
$$
(e_1 \wedge \ldots \wedge e_r) \wedge (e'_{r+1} \wedge \ldots \wedge
e'_n) = \begin{vmatrix}
a_{r+1,r+1} &\ldots &a_{a+1,n}\\
\cdot& \ldots & \cdot \\
a_{n,r+1} & \ldots & a_{n,n}
\end{vmatrix} e_1 \wedge e_2 \wedge \ldots \wedge e_n.
$$

It would be important and interesting to introduce the Kronecker
product, as a background for the general theory of tensor products. As
an application, one can consider the following example : if $A,B$ are
matrices of real numbers, then 
$$
\begin{vmatrix}
& A&B\\
-&B&A
\end{vmatrix} \geqslant 0.
$$
We may write this $|A \otimes I + B \otimes J|$, where $I
= \begin{pmatrix}
1&0\\
0&1
\end{pmatrix}$, $J = \begin{pmatrix}
&0&1\\
-&1&0
\end{pmatrix}$. 
Transforming $J$ into $\begin{pmatrix}
i&0\\
0&-i
\end{pmatrix}$, we see that
$$
|A \otimes I + B \otimes J| = \begin{vmatrix}
A+ Bi & 0 \\
0 & A - Bi
\end{vmatrix} =  |A + Bi|~ |\overline{A+ Bi}| \geqslant 0.
$$
The formula $|A\otimes B| = |A|^{m_2} |B|^{m_1}$, where $m_1$, $m_2$
is the degree of the matrices $A,B$ respectively, we may prove by
using exterior products. Namely associate to this a linear
transformation of the vector space of dimension $m_1 m_2$,
$$
x'_i y'_j = \sum a_{i\alpha} b_{j\beta} x_{\alpha} y_{\beta} .
$$
Decompose this transformation in two steps $xy \to x'y$, $x'y \to
x'y'$ and consider the exterior products
$$
\Pi \wedge (x'_i ~y'_j), ~~\Pi \wedge (x'_i ~y_\beta), ~ \Pi \wedge
(x_{\alpha} ~y_{\beta}).
$$\pageoriginale
Then we have
$$
\Pi \wedge (x'_i ~ y'_j) = |B|^{m_1} \cdot \Pi \wedge (x'_i y_{\beta})
= |B|^{m_1} |A|^{m_2} \pi \wedge (x_{\alpha} ~ y_\beta),
$$
therefore
$$
|A \otimes B| = |A|^{m_2} |B|^{m_1}.
$$
Similarly we can deduce the other properties of determinants. The
above stated is not completely well prepared, but I believe that it
might be enough to show how the method is effective. 

\bigskip
\bigskip

\noindent
{\fontsize{9pt}{11pt}\selectfont
Kyoto University}\relax

